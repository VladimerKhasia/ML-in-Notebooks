{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "_J2RIvM0Lqsa",
      "metadata": {
        "id": "_J2RIvM0Lqsa"
      },
      "source": [
        "Dataset:\n",
        "\n",
        "you can take any .txt file format, create it yourself or download it from any source, e.g. from project gutenberg https://www.gutenberg.org/\n",
        "\n",
        "I created Rustavely.txt file, which you can download here and use: https://1drv.ms/t/s!AhnVhbVlzYkKgQmnRS7FrC4QrQNM?e=800gvv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yLW7sYTdIzaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLW7sYTdIzaJ",
        "outputId": "32cf94ae-cdb8-4d86-abc1-1954797239ba"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RVSwzQ5Bwt0m",
      "metadata": {
        "id": "RVSwzQ5Bwt0m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pickle\n",
        "import itertools\n",
        "import numpy\n",
        "import random as rnd\n",
        "\n",
        "import trax\n",
        "from trax import fastmath\n",
        "import trax.fastmath.numpy as tnp\n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kfamwXWF-SfG",
      "metadata": {
        "id": "kfamwXWF-SfG",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "dirname = './'\n",
        "filename = 'Rustaveli.txt'\n",
        "lines = [] # storing all the lines in a variable. \n",
        "\n",
        "counter = 0\n",
        "\n",
        "with open(os.path.join(dirname, filename)) as files:\n",
        "    for line in files:        \n",
        "        # remove leading and trailing whitespace\n",
        "        pure_line = line.strip()\n",
        "\n",
        "        # if pure_line is not the empty string,\n",
        "        if pure_line:\n",
        "            # append it to the list\n",
        "            lines.append(pure_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-zMCe7aJkGwA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zMCe7aJkGwA",
        "outputId": "8eded9f0-0612-4cd8-a16f-6b660c83a390"
      },
      "outputs": [],
      "source": [
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UBO9jI8EkGwE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBO9jI8EkGwE",
        "outputId": "218ad331-4d4b-45a7-e53b-93c246dfbf1a"
      },
      "outputs": [],
      "source": [
        "for i, line in enumerate(lines):\n",
        "    # convert to all lowercase\n",
        "    lines[i] = line.lower()\n",
        "\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o1g8CBWh-SfI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1g8CBWh-SfI",
        "outputId": "c15a1657-4728-48df-fe64-9fe848130fc7"
      },
      "outputs": [],
      "source": [
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "lines = lines[:-1000] # Leave the rest for training\n",
        "\n",
        "print(f\"Number of lines for training: {len(lines)}\")\n",
        "print(f\"Number of lines for validation: {len(eval_lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cc_B8ae3kGwI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc_B8ae3kGwI",
        "outputId": "fef8e2e2-865c-4a31-de23-84c3b5c05474"
      },
      "outputs": [],
      "source": [
        "# View the unique unicode integer associated with each character\n",
        "print(f\"ord('a'): {ord('a')}\")\n",
        "print(f\"ord('b'): {ord('b')}\")\n",
        "print(f\"ord('c'): {ord('c')}\")\n",
        "print(f\"ord(' '): {ord(' ')}\")\n",
        "print(f\"ord('x'): {ord('x')}\")\n",
        "print(f\"ord('y'): {ord('y')}\")\n",
        "print(f\"ord('z'): {ord('z')}\")\n",
        "print(f\"ord('1'): {ord('1')}\")\n",
        "print(f\"ord('2'): {ord('2')}\")\n",
        "print(f\"ord('3'): {ord('3')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IO4NSPkOITNK",
      "metadata": {
        "id": "IO4NSPkOITNK"
      },
      "outputs": [],
      "source": [
        "def line_to_tensor(line, EOS_int=1):\n",
        "    \"\"\"Turns a line of text into a tensor\n",
        "\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        list: a list of integers (unicode values) for the characters in the `line`.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the tensor as an empty list\n",
        "    tensor = []\n",
        "    \n",
        "    # for each character:\n",
        "    for c in line:\n",
        "        \n",
        "        # convert to unicode int\n",
        "        c_int = ord(c)\n",
        "        \n",
        "        # append the unicode integer to the tensor list\n",
        "        tensor.append(c_int)\n",
        "    \n",
        "    # include the end-of-sentence integer\n",
        "    tensor.append(EOS_int)\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D9Z_vtI7tTcw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Z_vtI7tTcw",
        "outputId": "582a6f07-4630-405c-9f21-4ab79fc78ee2"
      },
      "outputs": [],
      "source": [
        "line_to_tensor('abc xyz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OMingz5xzrGD",
      "metadata": {
        "id": "OMingz5xzrGD"
      },
      "outputs": [],
      "source": [
        "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        max_length (int): maximum length of the output tensor.\n",
        "        NOTE: max_length includes the end-of-sentence character that will be added\n",
        "                to the tensor.  \n",
        "                Keep in mind that the length of the tensor is always 1 + the length\n",
        "                of the original line of characters.\n",
        "        data_lines (list): list of the sentences to group into batches.\n",
        "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
        "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
        "\n",
        "    Yields:\n",
        "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
        "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
        "    \"\"\"\n",
        "    # initialize the index that points to the current position in the lines index array\n",
        "    index = 0\n",
        "    \n",
        "    # initialize the list that will contain the current batch\n",
        "    cur_batch = []\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(data_lines)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle line indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        # if the index is greater or equal than to the number of lines in data_lines\n",
        "        if index >= num_lines:\n",
        "            # then reset the index to 0\n",
        "            index = 0\n",
        "            # shuffle line indexes if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(lines_index)\n",
        "            \n",
        "        # get a line at the `lines_index[index]` position in data_lines\n",
        "        line = data_lines[lines_index[index]]\n",
        "        \n",
        "        # if the length of the line is less than max_length\n",
        "        if len(line) < max_length:\n",
        "            # append the line to the current batch\n",
        "            cur_batch.append(line)\n",
        "            \n",
        "        # increment the index by one\n",
        "        index += 1\n",
        "        \n",
        "        # if the current batch is now equal to the desired batch size\n",
        "        if len(cur_batch) == batch_size:\n",
        "            \n",
        "            batch = []\n",
        "            mask = []\n",
        "            \n",
        "            # go through each line (li) in cur_batch\n",
        "            for li in cur_batch:\n",
        "                # convert the line (li) to a tensor of integers\n",
        "                tensor = line_to_tensor(li)\n",
        "                \n",
        "                # Create a list of zeros to represent the padding\n",
        "                # so that the tensor plus padding will have length `max_length`\n",
        "                pad = [0] * (max_length - len(tensor))\n",
        "                \n",
        "                # combine the tensor plus pad\n",
        "                tensor_pad = tensor + pad\n",
        "                \n",
        "                # append the padded tensor to the batch\n",
        "                batch.append(tensor_pad)\n",
        "\n",
        "                # A mask for  tensor_pad is 1 wherever tensor_pad is not\n",
        "                # 0 and 0 wherever tensor_pad is 0, i.e. if tensor_pad is\n",
        "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
        "                # [1, 1, 1, 0, 0, 0]\n",
        "                # Hint: Use a list comprehension for this\n",
        "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
        "                mask.append(example_mask)\n",
        "               \n",
        "            # convert the batch (data type list) to a trax's numpy array\n",
        "            batch_np_arr = tnp.array(batch)\n",
        "            mask_np_arr = tnp.array(mask)\n",
        "            \n",
        "            # Yield two copies of the batch and mask.\n",
        "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
        "            \n",
        "            # reset the current batch to an empty list\n",
        "            cur_batch = []\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x3gOj3tC-SfO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3gOj3tC-SfO",
        "outputId": "6c828984-af8a-4f52-a22b-fefd25858eae"
      },
      "outputs": [],
      "source": [
        "tmp_lines = ['12345678901', #length 11\n",
        "             '123456789', # length 9\n",
        "             '234567890', # length 9\n",
        "             '345678901'] # length 9\n",
        "\n",
        "# Get a batch size of 2, max length 10\n",
        "tmp_data_gen = data_generator(batch_size=2, \n",
        "                              max_length=10, \n",
        "                              data_lines=tmp_lines,\n",
        "                              shuffle=False)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_data_gen)\n",
        "\n",
        "# view the batch\n",
        "tmp_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v589leeZETy7",
      "metadata": {
        "id": "v589leeZETy7"
      },
      "outputs": [],
      "source": [
        "infinite_data_generator = itertools.cycle(\n",
        "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0lJhBPgJFAxb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lJhBPgJFAxb",
        "outputId": "e50ef3e1-777a-4466-8c62-e0767e0ec5d6"
      },
      "outputs": [],
      "source": [
        "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
        "print(len(ten_lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hww76f8_wt0x",
      "metadata": {
        "id": "hww76f8_wt0x"
      },
      "outputs": [],
      "source": [
        "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
        "    \"\"\"Returns a GRU language model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
        "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
        "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "    \"\"\"\n",
        "    model = tl.Serial(\n",
        "     tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
        "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
        "      tl.Dense(n_units=vocab_size), # Dense layer\n",
        "      tl.LogSoftmax()  # Log Softmax\n",
        "    )\n",
        "    \n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kvQ_jf52-JAn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvQ_jf52-JAn",
        "outputId": "b470cf62-5693-4eac-fb71-bb5477e7a51c"
      },
      "outputs": [],
      "source": [
        "model = GRULM()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5Birerv82mLv",
      "metadata": {
        "id": "5Birerv82mLv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "max_length = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "T3NxHd-VtTcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3NxHd-VtTcb",
        "outputId": "746c356d-96bd-4c06-ba18-54bdbc5f4e79"
      },
      "outputs": [],
      "source": [
        "def n_used_lines(lines, max_length):\n",
        "    '''\n",
        "    Args: \n",
        "    lines: all lines of text an array of lines\n",
        "    max_length - max_length of a line in order to be considered an int\n",
        "    output_dir - folder to save your file an int\n",
        "    Return:\n",
        "    number of efective examples\n",
        "    '''\n",
        "\n",
        "    n_lines = 0\n",
        "    for l in lines:\n",
        "        if len(l) <= max_length:\n",
        "            n_lines += 1\n",
        "    return n_lines\n",
        "\n",
        "num_used_lines = n_used_lines(lines, 32)\n",
        "print('Number of used lines from the dataset:', num_used_lines)\n",
        "print('Batch size (a power of 2):', int(batch_size))\n",
        "steps_per_epoch = int(num_used_lines/batch_size)\n",
        "print('Number of steps to cover one epoch:', steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_kbtfz4T_m7x",
      "metadata": {
        "id": "_kbtfz4T_m7x"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_generator, lines, eval_lines, batch_size=32, max_length=64, n_steps=1, output_dir='model/'): \n",
        "    \"\"\"Function that trains the model\n",
        "\n",
        "    Args:\n",
        "        model (trax.layers.combinators.Serial): GRU model.\n",
        "        data_generator (function): Data generator function.\n",
        "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
        "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
        "        lines (list): List of lines to use for training. Defaults to lines.\n",
        "        eval_lines (list): List of lines to use for evaluation. Defaults to eval_lines.\n",
        "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
        "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
        "\n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop for the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
        "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
        "    \n",
        "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
        "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
        "   \n",
        "    train_task = training.TrainTask(\n",
        "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
        "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
        "        optimizer=trax.optimizers.Adam(0.0005)     # Don't forget to add the learning rate parameter\n",
        "    )\n",
        "\n",
        "    eval_task = training.EvalTask(\n",
        "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # Don't forget to instantiate these objects\n",
        "        n_eval_batches=3      # For better evaluation accuracy in reasonable time\n",
        "    )\n",
        "    \n",
        "    training_loop = training.Loop(model,\n",
        "                                  train_task,\n",
        "                                  eval_tasks=eval_task,\n",
        "                                  output_dir=output_dir)\n",
        "\n",
        "    training_loop.run(n_steps=n_steps)\n",
        "    \n",
        "    # We return this because it contains a handle to the model, which has the weights etc.\n",
        "    return training_loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SwP646GpK3pD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwP646GpK3pD",
        "outputId": "a016e850-2958-468e-893d-654c59be589b"
      },
      "outputs": [],
      "source": [
        "output_dir = './model/'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(output_dir)\n",
        "except OSError as e:\n",
        "    pass\n",
        "\n",
        "training_loop = train_model(GRULM(), data_generator, lines=lines, eval_lines=eval_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3OtmlEuOwt1D",
      "metadata": {
        "id": "3OtmlEuOwt1D"
      },
      "outputs": [],
      "source": [
        "def test_model(preds, target):\n",
        "    \"\"\"Function to test the model.\n",
        "    Args:\n",
        "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
        "    Returns:\n",
        "        float: log_perplexity of the model.\n",
        "    \"\"\"\n",
        "    total_log_ppx = tnp.sum(preds * tl.one_hot(target, preds.shape[-1]), axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
        "\n",
        "    non_pad = 1.0 - tnp.equal(target, 0)          # You should check if the target equals 0\n",
        "    ppx = total_log_ppx * non_pad                             # Get rid of the padding\n",
        "\n",
        "    log_ppx = tnp.sum(ppx) / tnp.sum(non_pad)\n",
        "    \n",
        "    return -log_ppx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xrOJHbXewt1J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrOJHbXewt1J",
        "outputId": "922711b9-21a7-40d4-bc9b-8a7194fd7a8d"
      },
      "outputs": [],
      "source": [
        "def gumbel_sample(log_probs, temperature=1.0):\n",
        "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
        "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
        "    g = -tnp.log(-tnp.log(u))\n",
        "    return tnp.argmax(log_probs + g * temperature, axis=-1)\n",
        "\n",
        "def predict(num_chars, prefix):\n",
        "    inp = [ord(c) for c in prefix]\n",
        "    result = [c for c in prefix]\n",
        "    max_len = len(prefix) + num_chars\n",
        "    for _ in range(num_chars):\n",
        "        cur_inp = tnp.array(inp + [0] * (max_len - len(inp)))\n",
        "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
        "        next_char = gumbel_sample(outp[0, len(inp)])\n",
        "        inp += [int(next_char)]\n",
        "       \n",
        "        if inp[-1] == 1:\n",
        "            break  # EOS\n",
        "        result.append(chr(int(next_char)))\n",
        "    \n",
        "    return \"\".join(result)\n",
        "\n",
        "print(predict(56, \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h2AWjRlc-SfZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2AWjRlc-SfZ",
        "outputId": "76dcb337-0eed-41bb-c305-d78de46657f0"
      },
      "outputs": [],
      "source": [
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
