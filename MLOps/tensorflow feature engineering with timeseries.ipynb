{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"3IrnqNnWO04A"},"source":["# Feature Engineering with Weather Data\n","\n","[Weather Dataset](https://www.bgc-jena.mpg.de/wetter/)\n","\n","[Max Planck Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qONoNgHNWKt3"},"outputs":[],"source":["!pip install tensorflow_transform==1.4.0\n","!pip install apache-beam==2.39.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1677490990142,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"tAlK5cEA1Idh","outputId":"29da4f4f-95bf-427d-a8bd-98995ece8f11"},"outputs":[],"source":["### https://www.tensorflow.org/tfx/tutorials/transform/simple\n","# This cell is only necessary because packages were installed while python was\n","# running. It avoids the need to restart the runtime when running in Colab.\n","import pkg_resources\n","import importlib\n","\n","importlib.reload(pkg_resources)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5isfzOcWbCl"},"outputs":[],"source":["import apache_beam as beam\n","print('Apache Beam version: {}'.format(beam.__version__))\n","\n","import tensorflow as tf\n","print('Tensorflow version: {}'.format(tf.__version__))\n","\n","import tensorflow_transform as tft\n","from tensorflow_transform import beam as tft_beam\n","from tensorflow_transform.tf_metadata import dataset_metadata\n","from tensorflow_transform.tf_metadata import schema_utils\n","print('TensorFlow Transform version: {}'.format(tft.__version__))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lu4uJ4RgEfE8"},"outputs":[],"source":["import os\n","\n","# Directory of the raw data files\n","DATA_DIR = './weather_data/'\n","\n","# Download the dataset\n","!wget -nc https://raw.githubusercontent.com/https-deeplearning-ai/MLEP-public/main/course2/week4-ungraded-lab/data/jena_climate_2009_2016.csv -P {DATA_DIR}\n","\n","# Assign data path to a variable for easy reference\n","INPUT_FILE = os.path.join(DATA_DIR, 'jena_climate_2009_2016.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ohkBeclndhL"},"outputs":[],"source":["import pandas as pd\n","\n","# Put dataset in a dataframe\n","df = pd.read_csv(INPUT_FILE, header=0, index_col=0)\n","\n","# Preview the last few rows\n","df.tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EvF4w5rGNUnf"},"outputs":[],"source":["df.describe().transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0QAfG_lJ__JK"},"outputs":[],"source":["# Define feature keys\n","\n","TIMESTAMP_FEATURES = [\"Date Time\"]\n","NUMERIC_FEATURES = [\n","    \"p (mbar)\",\n","    \"T (degC)\",\n","    \"Tpot (K)\",\n","    \"Tdew (degC)\", \n","    \"rh (%)\", \n","    \"VPmax (mbar)\", \n","    \"VPact (mbar)\", \n","    \"VPdef (mbar)\", \n","    \"sh (g/kg)\",\n","    \"H2OC (mmol/mol)\",\n","    \"rho (g/m**3)\",\n","    \"wv (m/s)\",\n","    \"max. wv (m/s)\",\n","    \"wd (deg)\",\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egnSszZ6Ia6k"},"outputs":[],"source":["#@ title Visualization Utilities\n","import matplotlib.pyplot as plt\n","\n","# Color Palette\n","colors = [\n","    \"blue\",\n","    \"orange\",\n","    \"green\",\n","    \"red\",\n","    \"purple\",\n","    \"brown\",\n","    \"pink\",\n","    \"gray\",\n","    \"olive\",\n","    \"cyan\",\n","]\n","\n","# Plots each column as a time series\n","def visualize_plots(dataset, columns):\n","    features = dataset[columns]\n","    fig, axes = plt.subplots(\n","        nrows=len(columns)//2 + len(columns)%2, ncols=2, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n","    )\n","    for i, col in enumerate(columns):\n","        c = colors[i % (len(colors))]\n","        t_data = dataset[col]\n","        t_data.index = dataset.index\n","        t_data.head()\n","        ax = t_data.plot(\n","            ax=axes[i // 2, i % 2],\n","            color=c,\n","            title=\"{}\".format(col),\n","            rot=25,\n","        )\n","    ax.legend([col])\n","    plt.tight_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLa8UAe3JDZ3"},"outputs":[],"source":["# Visualize the dataset\n","visualize_plots(df, NUMERIC_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0zP5RvoFU7KB"},"outputs":[],"source":["# Set the wind velocity outliers to 0\n","wv = df['wv (m/s)']\n","bad_wv = wv == -9999.0\n","wv[bad_wv] = 0.0\n","\n","# Set the max wind velocity outliers to 0\n","max_wv = df['max. wv (m/s)']\n","bad_max_wv = max_wv == -9999.0\n","max_wv[bad_max_wv] = 0.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wXkjakrEVEYn"},"outputs":[],"source":["visualize_plots(df, NUMERIC_FEATURES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sejFrBtwhjV_"},"outputs":[],"source":["import seaborn as sns\n","\n","def show_correlation_heatmap(dataframe):\n","    plt.figure(figsize=(20,20))\n","    cor = dataframe.corr()\n","    sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7uReJtJhqyM"},"outputs":[],"source":["show_correlation_heatmap(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63D0d5nhjbQ5"},"outputs":[],"source":["# Features to filter out\n","FEATURES_TO_REMOVE = [\"Tpot (K)\", \"Tdew (degC)\",\"VPact (mbar)\" , \"H2OC (mmol/mol)\", \"max. wv (m/s)\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Lv_foXhGpFD"},"outputs":[],"source":["from datetime import datetime\n","\n","# combine features into one list\n","ordered_columns = TIMESTAMP_FEATURES + NUMERIC_FEATURES\n","\n","# index of the date time string\n","date_time_idx = ordered_columns.index(TIMESTAMP_FEATURES[0])\n","\n","# index of the 'wv (m/s)' feature\n","wv_idx = ordered_columns.index('wv (m/s)')\n","\n","def clean_fn(line):\n","  '''\n","  Converts datetime strings in the CSV to Unix timestamps and removes outliers\n","  the wind velocity column. Used as part of\n","  the transform pipeline.\n","\n","  Args:\n","    line (string) - one row of a CSV file\n","  \n","  Returns:\n","\n","  '''\n","\n","  # Split the CSV string to a list\n","  line_split = line.split(b',')\n","\n","  # Decodes the timestamp string to utf-8\n","  date_time_string = line_split[date_time_idx].decode(\"utf-8\")\n","\n","  # Creates a datetime object from the timestamp string\n","  date_time = datetime.strptime(date_time_string, '%d.%m.%Y %H:%M:%S')\n","\n","  # Generates a timestamp from the object\n","  timestamp = datetime.timestamp(date_time)\n","\n","  # Overwrites the string timestamp in the row with the timestamp in seconds\n","  line_split[date_time_idx] = bytes(str(timestamp), 'utf-8')\n","\n","  # Check if wind velocity is an outlier\n","  if line_split[wv_idx] == b'-9999.0':\n","\n","    # Overwrite with default value of 0\n","    line_split[wv_idx] = b'0.0'\n","\n","  # rejoin the list item into one string\n","  mod_line = b','.join(line_split)\n","\n","  return mod_line"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"neDQbbY30Sdu"},"outputs":[],"source":["import numpy as np\n","import math as m\n","\n","def preprocessing_fn(inputs):\n","  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n","  \n","  outputs = inputs.copy()\n","\n","  # Filter redundant features\n","  for key in FEATURES_TO_REMOVE:\n","    del outputs[key]\n","\n","  # Convert degrees to radians\n","  pi = tf.constant(m.pi)\n","  wd_rad = inputs['wd (deg)'] * pi / 180.0\n","\n","  # Calculate the wind x and y components.\n","  outputs['Wx'] = inputs['wv (m/s)'] * tf.math.cos(wd_rad)\n","  outputs['Wy'] = inputs['wv (m/s)'] * tf.math.sin(wd_rad)\n","\n","  # Delete `wv (m/s)` and `wd (deg)` after getting the wind vector\n","  del outputs['wv (m/s)']\n","  del outputs['wd (deg)']\n","\n","  # Get day and year in seconds\n","  day = tf.cast(24*60*60, tf.float32)\n","  year = tf.cast((365.2425)*day, tf.float32)\n","\n","  # Get timestamp feature\n","  timestamp_s = outputs['Date Time']\n","\n","  # Convert timestamps into periodic signals\n","  outputs['Day sin'] = tf.math.sin(timestamp_s * (2 * pi / day))\n","  outputs['Day cos'] = tf.math.cos(timestamp_s * (2 * pi / day))\n","  outputs['Year sin'] = tf.math.sin(timestamp_s * (2 * pi / year))\n","  outputs['Year cos'] = tf.math.cos(timestamp_s * (2 * pi / year))\n","\n","  # Delete timestamp feature\n","  del outputs['Date Time']\n","\n","  # Declare final list of features\n","  FINAL_FEATURE_LIST =  [\"p (mbar)\",\n","    \"T (degC)\",\n","    \"rh (%)\", \n","    \"VPmax (mbar)\", \n","    \"VPdef (mbar)\", \n","    \"sh (g/kg)\",\n","    \"rho (g/m**3)\",\n","    \"Wx\",\n","    \"Wy\",\n","    \"Day sin\",\n","    'Day cos',\n","    'Year sin',\n","    'Year cos'\n","    ]\n","\n","  # Scale all features\n","  for key in FINAL_FEATURE_LIST:\n","    outputs[key] = tft.scale_to_0_1(outputs[key])\n","\n","  return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PJzIMFgBl-b"},"outputs":[],"source":["# Number of records to include in the train split\n","TRAIN_SPLIT = 300000\n","\n","# Get date time of the last element in the train split\n","date_time_train_boundary = df.iloc[TRAIN_SPLIT - 1].name\n","\n","# Creates a datetime object from the timestamp string\n","date_time_train_boundary = datetime.strptime(date_time_train_boundary, '%d.%m.%Y %H:%M:%S')\n","\n","# Convert date time string to Unix timestamp in seconds\n","date_time_train_boundary = bytes(str(datetime.timestamp(date_time_train_boundary)), 'utf-8')\n","\n","\n","def partition_fn(line, num_partitions):\n","  '''\n","  Partition function to work with Beam.partition\n","\n","  Args:\n","    line (string) - One record in the CSV file.\n","    num_partition (integer) - Number of partitions. Required argument by Beam. Unused in this function.\n","\n","  Returns:\n","    0 or 1 (integer) - 0 if line timestamp is below the date time boundary, 1 otherwise. \n","  '''\n","\n","  # Split the CSV string to a list\n","  line_split = line.split(b',')\n","\n","  # Get the timestamp of the current line\n","  line_dt = line_split[date_time_idx]\n","\n","  # Check if it is above or below the date time boundary\n","  partition_num = int(line_dt > date_time_train_boundary)\n","\n","  return partition_num"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-NpXb7N_vWkB"},"outputs":[],"source":["# Declare feature spec\n","RAW_DATA_FEATURE_SPEC = dict(\n","    [(name, tf.io.FixedLenFeature([], tf.float32))\n","     for name in TIMESTAMP_FEATURES] +\n","    [(name, tf.io.FixedLenFeature([], tf.float32))\n","     for name in NUMERIC_FEATURES]\n",")\n","\n","# Create schema from feature spec\n","RAW_DATA_SCHEMA = tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJ_eE8nntSSs"},"outputs":[],"source":["import shutil\n","from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder\n","from tfx_bsl.public import tfxio\n","\n","# Directory names for the TF Transform outputs\n","WORKING_DIR = './transform_dir'\n","TRANSFORM_TRAIN_FILENAME = 'transform_train'\n","TRANSFORM_TEST_FILENAME = 'transform_test'\n","TRANSFORM_TEMP_DIR = 'tft_temp'\n","\n","\n","# The \"with\" block will create a pipeline, and run that pipeline at the exit\n","#   of the block.\n","def read_and_transform_data(working_dir):\n","  '''\n","  Reads a CSV File and preprocesses the data using TF Transform\n","\n","  Args:\n","    working_dir (string) - directory to place TF Transform outputs\n","  \n","  Returns:\n","    transform_fn - transformation graph\n","    transformed_train_data - transformed training examples\n","    transformed_test_data - transformed test examples\n","    transformed_metadata - transform output metadata\n","  '''\n","\n","  # Delete TF Transform if it already exists\n","  if os.path.exists(working_dir):\n","    shutil.rmtree(working_dir)\n","\n","  with beam.Pipeline() as pipeline:\n","      with tft_beam.Context(temp_dir=os.path.join(working_dir, TRANSFORM_TEMP_DIR)):\n","        \n","        # Read the input CSV and clean the data\n","        raw_data = (\n","              pipeline\n","              | 'ReadTrainData' >> beam.io.ReadFromText(INPUT_FILE, coder=beam.coders.BytesCoder(), skip_header_lines=1)\n","              | 'CleanLines' >> beam.Map(clean_fn))\n","\n","        # Partition the dataset into train and test sets using the partition_fn defined earlier.    \n","        raw_train_data, raw_test_data = (raw_data\n","                                 | 'TrainTestSplit' >> beam.Partition(partition_fn, 2))\n","        \n","        # Create a TFXIO to read the data with the schema. You need\n","        # to list all columns in order since the schema doesn't specify the\n","        # order of columns in the csv.\n","        csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n","              physical_format='text',\n","              column_names=ordered_columns,\n","              schema=RAW_DATA_SCHEMA)\n","\n","        # Parse the raw train data into inputs for TF Transform\n","        raw_train_data = (raw_train_data \n","                          | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n","        \n","        # Get the raw data metadata\n","        RAW_DATA_METADATA = csv_tfxio.TensorAdapterConfig()\n","        \n","        # Pair the train data with the metadata into a tuple\n","        raw_train_dataset = (raw_train_data, RAW_DATA_METADATA)\n","\n","        # Training data transformation. The TFXIO (RecordBatch) output format\n","        # is chosen for improved performance.\n","        (transformed_train_data,transformed_metadata) , transform_fn = (\n","        raw_train_dataset | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn, output_record_batches=True))\n","\n","\n","        # Parse the raw data into inputs for TF Transform\n","        raw_test_data = (raw_test_data\n","                         | 'DecodeTestData' >> csv_tfxio.BeamSource())\n","        \n","        # Pair the test data with the metadata into a tuple\n","        raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n","        \n","        # Now apply the same transform function to the test data.\n","        # You don't need the transformed data schema. It's the same as before.\n","        transformed_test_data, _ = (\n","          (raw_test_dataset, transform_fn) | tft_beam.TransformDataset(output_record_batches=True))\n","        \n","        # Declare an encoder to convert output record batches to TF Examples \n","        transformed_data_coder = RecordBatchToExamplesEncoder(transformed_metadata.schema)\n","        \n","        # Encode transformed train data and write to disk\n","        _ = (\n","            transformed_train_data\n","            | 'EncodeTrainData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n","            | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n","                os.path.join(working_dir, TRANSFORM_TRAIN_FILENAME)))\n","\n","        # Encode transformed test data and write to disk\n","        _ = (\n","            transformed_test_data\n","            | 'EncodeTestData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n","            | 'WriteTestData' >> beam.io.WriteToTFRecord(\n","                os.path.join(working_dir, TRANSFORM_TEST_FILENAME)))\n","        \n","        # Write transform function to disk\n","        _ = (\n","          transform_fn\n","          | 'WriteTransformFn' >>\n","          tft_beam.WriteTransformFn(os.path.join(working_dir)))\n","\n","         \n","  return transform_fn, transformed_train_data, transformed_test_data, transformed_metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GPbh3qKzyTb"},"outputs":[],"source":["def main():\n","  return read_and_transform_data(WORKING_DIR)\n","\n","if __name__ == '__main__':\n","  transform_fn, transformed_train_data, trainsformed_test_data, transformed_metadata = main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho06omK_OAVH"},"outputs":[],"source":["# Constants to prepare the transformed data for modeling\n","\n","LABEL_KEY = 'T (degC)'\n","OBSERVATIONS_PER_HOUR = 6\n","HISTORY_SIZE = 120\n","FUTURE_TARGET = 12\n","BATCH_SIZE = 72\n","SHIFT = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1677491108585,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"9J8ljQwSQh8b","outputId":"57c41065-4736-4027-f42d-e87997397680"},"outputs":[{"name":"stdout","output_type":"stream","text":["2\n"]}],"source":["# Get the output of the Transform component\n","tf_transform_output = tft.TFTransformOutput(os.path.join(WORKING_DIR))\n","\n","# Get the index of the label key\n","index_of_label = list(tf_transform_output.transformed_feature_spec().keys()).index(LABEL_KEY)\n","print(index_of_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQxYFAXkNUwP"},"outputs":[],"source":["def parse_function(example_proto):\n","    \n","    feature_spec = tf_transform_output.transformed_feature_spec()\n","    \n","    # Define features with the example_proto (transformed data) and the feature_spec using tf.io.parse_single_example \n","    features = tf.io.parse_single_example(example_proto, feature_spec)\n","    values = list(features.values())\n","    values[index_of_label], values[len(features) - 1] = values[len(features) - 1], values[index_of_label]\n","    \n","    # Stack the values along the first axis\n","    stacked_features = tf.stack(values, axis=0)\n","\n","    return stacked_features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3xcDvgIOrvl"},"outputs":[],"source":["def map_features_target(elements):\n","    features = elements[:HISTORY_SIZE]\n","    target = elements[-1:,-1]\n","    return (features, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAUbyWGfPUha"},"outputs":[],"source":["def get_windowed_dataset(path):\n","        \n","    # Instantiate a tf.data.TFRecordDataset passing in the appropiate path\n","    dataset = tf.data.TFRecordDataset(path)\n","    \n","    # Use the dataset's map method to map the parse_function\n","    dataset = dataset.map(parse_function)\n","    \n","    # Use the window method with expected total size. Define stride and set drop_remainder to True\n","    dataset = dataset.window(HISTORY_SIZE + FUTURE_TARGET, shift=SHIFT, stride=OBSERVATIONS_PER_HOUR, drop_remainder=True)\n","    \n","    # Use the flat_map method passing in an anonymous function that given a window returns window.batch(HISTORY_SIZE + FUTURE_TARGET)\n","    dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE + FUTURE_TARGET))\n","    \n","    # Use the map method passing in the previously defined map_features_target function\n","    dataset = dataset.map(map_features_target) \n","    \n","    # Use the batch method and pass in the appropiate batch size\n","    dataset = dataset.batch(BATCH_SIZE)\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i0grFNuaTsk3"},"outputs":[],"source":["# Get list of train and test data tfrecord filenames from the transform outputs\n","train_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, f'{TRANSFORM_TRAIN_FILENAME}*'))\n","test_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, f'{TRANSFORM_TEST_FILENAME}*'))\n","\n","# Generate dataset windows\n","windowed_train_dataset = get_windowed_dataset(train_tfrecord_files[0])\n","windowed_test_dataset = get_windowed_dataset(test_tfrecord_files[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgTJDfGpt8af"},"outputs":[],"source":["ordered_feature_spec_names = tf_transform_output.transformed_feature_spec().keys()\n","\n","# Preview an example in the train dataset\n","for features, target  in windowed_train_dataset.take(1):\n","    print(f'Shape of input features for a batch: {features.shape}')\n","    print(f'Shape of targets for a batch: {target.shape}\\n')\n","\n","    print(f'INPUT FEATURES:')\n","    for value, name in zip(features[0][0].numpy(), ordered_feature_spec_names):\n","      print(f'{name} : {value}') \n","  \n","    print(f'\\nTARGET TEMPERATURE: {target[0][0]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60TEQ9tcMORG"},"outputs":[],"source":["# Preview an example in the test dataset\n","for features, target in windowed_test_dataset.take(1):\n","    print(f'Shape of input features for a batch: {features.shape}')\n","    print(f'Shape of targets for a batch: {target.shape}\\n')\n","\n","    print(f'INPUT FEATURES:')\n","    for value, name in zip(features[0][0].numpy(), ordered_feature_spec_names):\n","      print(f'{name} : {value}') \n","  \n","    print(f'\\nTARGET TEMPERATURE: {target[0][0]}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6dy5OkSQSnqf"},"source":["# Feature Engineering with Accelerometer Data\n","\n","[WISDM Human Activity Recognition Dataset](http://www.cis.fordham.edu/wisdm/dataset.php)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1XXQ0M-Snqj"},"outputs":[],"source":["# !pip install tensorflow_transform==1.4.0\n","# !pip install apache-beam==2.39.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fU0_GTa1S51n"},"outputs":[],"source":["# ### https://www.tensorflow.org/tfx/tutorials/transform/simple\n","# # This cell is only necessary because packages were installed while python was\n","# # running. It avoids the need to restart the runtime when running in Colab.\n","# import pkg_resources\n","# import importlib\n","\n","# importlib.reload(pkg_resources)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7jHo8yhSnql"},"outputs":[],"source":["import apache_beam as beam\n","print('Apache Beam version: {}'.format(beam.__version__))\n","\n","import tensorflow as tf\n","print('Tensorflow version: {}'.format(tf.__version__))\n","\n","import tensorflow_transform as tft\n","from tensorflow_transform import beam as tft_beam\n","from tensorflow_transform.tf_metadata import dataset_metadata\n","from tensorflow_transform.tf_metadata import schema_utils\n","print('TensorFlow Transform version: {}'.format(tft.__version__))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAlgKI-0Snqn"},"outputs":[],"source":["import os\n","\n","# Directory of the raw data files\n","DATA_DIR = './accelerator_data/'\n","\n","# Download the dataset\n","!wget -nc https://github.com/https-deeplearning-ai/machine-learning-engineering-for-production-public/raw/main/course2/week4-ungraded-lab/data/WISDM_ar_latest.tar.gz -P {DATA_DIR}\n","\n","# Extract the dataset\n","!tar -xvf {DATA_DIR}/WISDM_ar_latest.tar.gz -C {DATA_DIR}\n","\n","# Assign data path to a variable for easy reference\n","INPUT_FILE = os.path.join(DATA_DIR, 'WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MipCmynFWD47"},"outputs":[],"source":["import pandas as pd\n","\n","# Put dataset in a dataframe\n","df = pd.read_csv(INPUT_FILE, header=None, names=['user_id', 'activity', 'timestamp', 'x-acc','y-acc', 'z-acc'])\n","\n","# Preview the first few rows\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YK9tAyLVCgTe"},"outputs":[],"source":["# Visulaization Utilities\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def visualize_value_plots_for_categorical_feature(feature, colors=['b']):\n","    '''Plots a bar graph for categorical features'''\n","    counts = feature.value_counts()\n","    plt.bar(counts.index, counts.values, color=colors)\n","    plt.show()\n","\n","\n","def visualize_plots(dataset, activity, columns):\n","    '''Visualizes the accelerometer data against time'''\n","    features = dataset[dataset['activity'] == activity][columns][:200]\n","    if 'z-acc' in columns:\n","        # remove semicolons in the z-acc column\n","        features['z-acc'] = features['z-acc'].replace(regex=True, to_replace=r';', value=r'')\n","        features['z-acc'] = features['z-acc'].astype(np.float64)\n","    axis = features.plot(subplots=True, figsize=(16, 12), \n","                     title=activity)\n","\n","    for ax in axis:\n","        ax.legend(loc='lower left', bbox_to_anchor=(1.0, 0.5))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dva7JfmGDGQT"},"outputs":[],"source":["# Plot the histogram of activities\n","visualize_value_plots_for_categorical_feature(df['activity'], colors=['r', 'g', 'b', 'y', 'm', 'c'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jJhG5DMFj-d"},"outputs":[],"source":["# Plot the histogram for users\n","visualize_value_plots_for_categorical_feature(df['user_id'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n800M3nz3DFh"},"outputs":[],"source":["def partition_fn(line, num_partitions):\n","  '''\n","  Partition function to work with Beam.partition\n","\n","  Args:\n","    line (string) - One record in the CSV file.\n","    num_partition (integer) - Number of partitions. Required argument by Beam. Unused in this function.\n","\n","  Returns:\n","    0 or 1 (integer) - 0 if user id is less than 30, 1 otherwise. \n","  '''\n","  \n","  # Get the 1st substring delimited by a comma. Cast to an int.\n","  user_id = int(line[:line.index(b',')])\n","\n","  # Check if it is above or below 30\n","  partition_num = int(user_id <= 30)\n","\n","  return partition_num"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOj_0wkZSnqv"},"outputs":[],"source":["# Plot the measurements for `Jogging`\n","visualize_plots(df, 'Jogging', columns=['x-acc', 'y-acc', 'z-acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1CvWGpEQwKO"},"outputs":[],"source":["visualize_plots(df, 'Sitting', columns=['x-acc', 'y-acc', 'z-acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QeUDSzCr4y5F"},"outputs":[],"source":["STRING_FEATURES = ['activity']\n","INT_FEATURES = ['user_id', 'timestamp']\n","FLOAT_FEATURES = ['x-acc', 'y-acc', 'z-acc']\n","\n","# Declare feature spec\n","RAW_DATA_FEATURE_SPEC = dict(\n","    [(name, tf.io.FixedLenFeature([], tf.string))\n","     for name in STRING_FEATURES] +\n","    [(name, tf.io.FixedLenFeature([], tf.int64))\n","     for name in INT_FEATURES] +\n","    [(name, tf.io.FixedLenFeature([], tf.float32))\n","     for name in FLOAT_FEATURES]\n",")\n","\n","# Create schema from feature spec\n","RAW_DATA_SCHEMA = tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJ9y1lQoSnqz"},"outputs":[],"source":["LABEL_KEY = 'activity'\n","\n","def preprocessing_fn(inputs):\n","  \"\"\"Preprocess input columns into transformed columns.\"\"\"\n","\n","  # Copy inputs\n","  outputs = inputs.copy()\n","\n","  # Delete features not to be included as inputs to the model\n","  del outputs[\"user_id\"]\n","  del outputs[\"timestamp\"]\n","  \n","  # Create a vocabulary for the string labels\n","  outputs[LABEL_KEY] = tft.compute_and_apply_vocabulary(inputs[LABEL_KEY],vocab_filename=LABEL_KEY)\n","\n","  # Scale features by their min-max\n","  for key in FLOAT_FEATURES:\n","     outputs[key] = tft.scale_by_min_max(outputs[key])\n","\n","  return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TNtDrC9BSnq1"},"outputs":[],"source":["import shutil\n","from tfx_bsl.coders.example_coder import RecordBatchToExamplesEncoder\n","from tfx_bsl.public import tfxio\n","\n","# Directory names for the TF Transform outputs\n","WORKING_DIR = 'transform_dir'\n","TRANSFORM_TRAIN_FILENAME = 'transform_train'\n","TRANSFORM_TEST_FILENAME = 'transform_test'\n","TRANSFORM_TEMP_DIR = 'tft_temp'\n","\n","ordered_columns = ['user_id', 'activity', 'timestamp', 'x-acc','y-acc', 'z-acc']\n","\n","def transform_data(working_dir):\n","    '''\n","    Reads a CSV File and preprocesses the data using TF Transform\n","\n","    Args:\n","      working_dir (string) - directory to place TF Transform outputs\n","    \n","    Returns:\n","      transform_fn - transformation graph\n","      transformed_train_data - transformed training examples\n","      transformed_test_data - transformed test examples\n","      transformed_metadata - transform output metadata\n","    '''\n","\n","    # Delete TF Transform if it already exists\n","    if os.path.exists(working_dir):\n","      shutil.rmtree(working_dir)\n","\n","    with beam.Pipeline() as pipeline:\n","        with tft_beam.Context(temp_dir=os.path.join(working_dir, TRANSFORM_TEMP_DIR)):\n","  \n","          # Read the input CSV, clean and filter the data (replace semicolon and incomplete rows)\n","          raw_data = (\n","              pipeline\n","              | 'ReadTrainData' >> beam.io.ReadFromText(INPUT_FILE, coder=beam.coders.BytesCoder())\n","              | 'CleanLines' >> beam.Map(lambda line: line.replace(b',;', b'').replace(b';', b''))\n","              | 'FilterLines' >> beam.Filter(lambda line: line.count(b',') == len(ordered_columns) - 1 and line[-1:] != b','))\n","\n","          # Partition the data into training and test data using beam.Partition\n","          raw_train_data, raw_test_data = (raw_data\n","                                  | 'TrainTestSplit' >> beam.Partition(partition_fn, 2))\n","                    \n","          # Create a TFXIO to read the data with the schema. \n","          csv_tfxio = tfxio.BeamRecordCsvTFXIO(\n","              physical_format='text',\n","              column_names=ordered_columns,\n","              schema=RAW_DATA_SCHEMA)\n","\n","          # Parse the raw train data into inputs for TF Transform\n","          raw_train_data = (raw_train_data \n","                            | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n","\n","          # Get the raw data metadata\n","          RAW_DATA_METADATA = csv_tfxio.TensorAdapterConfig()\n","          \n","          # Pair the test data with the metadata into a tuple\n","          raw_train_dataset = (raw_train_data, RAW_DATA_METADATA)\n","\n","          # Training data transformation. The TFXIO (RecordBatch) output format\n","          # is chosen for improved performance.\n","          (transformed_train_data,transformed_metadata) , transform_fn = (\n","              raw_train_dataset \n","                | 'AnalyzeAndTransformTrainData' >> tft_beam.AnalyzeAndTransformDataset(preprocessing_fn, output_record_batches=True))\n","          \n","          # Parse the raw test data into inputs for TF Transform\n","          raw_test_data = (raw_test_data \n","                            |'DecodeTestData' >> csv_tfxio.BeamSource())\n","\n","          # Pair the test data with the metadata into a tuple\n","          raw_test_dataset = (raw_test_data, RAW_DATA_METADATA)\n","\n","          # Now apply the same transform function to the test data.\n","          # You don't need the transformed data schema. It's the same as before.\n","          transformed_test_data, _ = ((raw_test_dataset, transform_fn) \n","                                        | 'AnalyzeAndTransformTestData' >> tft_beam.TransformDataset(output_record_batches=True))\n","          \n","          # Declare an encoder to convert output record batches to TF Examples \n","          transformed_data_coder = RecordBatchToExamplesEncoder(transformed_metadata.schema)\n","\n","          \n","          # Encode transformed train data and write to disk\n","          _ = (\n","              transformed_train_data\n","              | 'EncodeTrainData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n","              | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n","                  os.path.join(working_dir, TRANSFORM_TRAIN_FILENAME)))\n","\n","          # Encode transformed test data and write to disk\n","          _ = (\n","              transformed_test_data\n","              | 'EncodeTestData' >> beam.FlatMapTuple(lambda batch, _: transformed_data_coder.encode(batch))\n","              | 'WriteTestData' >> beam.io.WriteToTFRecord(\n","                  os.path.join(working_dir, TRANSFORM_TEST_FILENAME)))\n","          \n","          # Write transform function to disk\n","          _ = (\n","            transform_fn\n","            | 'WriteTransformFn' >>\n","            tft_beam.WriteTransformFn(os.path.join(working_dir)))\n","\n","    return transform_fn, transformed_train_data, transformed_test_data, transformed_metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99474,"status":"ok","timestamp":1677491809488,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"EJzo_LZHSnq2","outputId":"bb31bded-b20d-4bbe-aa5f-8d408784d43a"},"outputs":[],"source":["def main():\n","  return transform_data(WORKING_DIR)\n","\n","if __name__ == '__main__':\n","  transform_fn, transformed_train_data,transformed_test_data, transformed_metadata = main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSPjZE-0Snq4"},"outputs":[],"source":["# Get the output of the Transform component\n","tf_transform_output = tft.TFTransformOutput(os.path.join(WORKING_DIR))\n","\n","# Parameters\n","HISTORY_SIZE = 80\n","BATCH_SIZE = 100\n","STEP_SIZE = 40\n","\n","def parse_function(example_proto):\n","    '''Parse the values from tf examples'''\n","    feature_spec = tf_transform_output.transformed_feature_spec()\n","    features = tf.io.parse_single_example(example_proto, feature_spec)\n","    values = list(features.values())\n","    values = [float(value) for value in values]\n","    features = tf.stack(values, axis=0)\n","    return features\n","\n","def add_mode(features):\n","    '''Calculate mode of activity for the current history size of elements'''\n","    unique, _, count = tf.unique_with_counts(features[:,0])\n","    max_occurrences = tf.reduce_max(count)\n","    max_cond = tf.equal(count, max_occurrences)\n","    max_numbers = tf.squeeze(tf.gather(unique, tf.where(max_cond)))\n","\n","    #Features (X) are all features except activity (x-acc, y-acc, z-acc)\n","    #Target(Y) is the mode of activity values of all rows in this window\n","    return (features[:,1:], max_numbers)\n","\n","def get_windowed_dataset(path):\n","  '''Get the dataset and group them into windows'''\n","  dataset = tf.data.TFRecordDataset(path)\n","  dataset = dataset.map(parse_function)\n","  dataset = dataset.window(HISTORY_SIZE, shift=STEP_SIZE, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(HISTORY_SIZE))\n","  dataset = dataset.map(add_mode)\n","  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","  dataset = dataset.repeat()\n","\n","  return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ntL1ed94Snq5"},"outputs":[],"source":["# Get list of train and test data tfrecord filenames from the transform outputs\n","train_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, TRANSFORM_TRAIN_FILENAME + '*'))\n","test_tfrecord_files = tf.io.gfile.glob(os.path.join(WORKING_DIR, TRANSFORM_TEST_FILENAME + '*'))\n","\n","# Generate dataset windows\n","windowed_train_dataset = get_windowed_dataset(train_tfrecord_files[0])\n","windowed_test_dataset = get_windowed_dataset(test_tfrecord_files[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89KG2jANSWt4"},"outputs":[],"source":["# Preview an example in the train dataset\n","for x, y in windowed_train_dataset.take(1):\n","  print(\"\\nFeatures (x-acc, y-acc, z-acc):\\n\")\n","  print(x)\n","  print(\"\\nTarget (activity):\\n\")\n","  print(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"klu7t6QGlhSc"},"outputs":[],"source":["# Preview an example in the train dataset\n","for x, y in windowed_test_dataset.take(1):\n","  print(\"\\nFeatures (x-acc, y-acc, z-acc):\\n\")\n","  print(x)\n","  print(\"\\nTarget (activity):\\n\")\n","  print(y)"]}],"metadata":{"colab":{"collapsed_sections":["3IrnqNnWO04A","6dy5OkSQSnqf"],"provenance":[{"file_id":"15Dip60LKwQwGZ56z_DpI0inWgTc57T1X","timestamp":1601093693126},{"file_id":"1zwpIqHj_QW6dszaqdgviqL6yrxtk9WOJ","timestamp":1600118027706},{"file_id":"1hG1J8RRAPtsJZFCsYHWatuJx57dJ66Jn","timestamp":1589189499813},{"file_id":"https://github.com/tensorflow/tfx/blob/master/docs/tutorials/tfx/components_keras.ipynb","timestamp":1588766915739}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
