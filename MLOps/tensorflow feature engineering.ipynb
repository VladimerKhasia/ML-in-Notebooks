{"cells":[{"cell_type":"markdown","metadata":{"id":"MMARm4UfgqrP"},"source":["get data files:\n","\n","metro traffic data: https://drive.google.com/file/d/1v1AmjCOxeoAZXrUR0nCFEfNnJBcTgukx/view?usp=sharing \n","\n","census data (you only need file 'adult.data' there): https://drive.google.com/file/d/1n8x0UhaadEfyixUCBoyXrgkiwniIm3ZM/view?usp=share_link"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dQs1X_dOfhM_"},"outputs":[],"source":["## put your data files in different directories. In those directories no other, not even hidden files should exist\n","## ./data directory is for metro traffic data and ./census_data is for Income dataset\n","!mkdir ./data   ./census_data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"854Y6E8AMctc"},"outputs":[],"source":["## !pip uninstall tensorflow -y\n","## !pip install -q tensorflow==2.6.0\n","## !pip uninstall keras -y\n","## !pip install keras==2.6.0\n","\n","!pip install -q tensorflow_transform\n","!pip install -q tfx    ## tfx==1.3.0\n","\n","### do not forget to !!!!!     RESTART THE RUNTIME     !!!!! after the execution of this cell !!! OR following cell\n","### https://www.tensorflow.org/tfx/tutorials/transform/simple\n","# # This cell is only necessary because packages were installed while python was\n","# # running. It avoids the need to restart the runtime when running in Colab.\n","# import pkg_resources\n","# import importlib\n","\n","# importlib.reload(pkg_resources)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qlSwoCHy023q"},"outputs":[],"source":["# This cell is only necessary because packages were installed while python was\n","# running. It avoids the need to restart the runtime when running in Colab.\n","import pkg_resources\n","import importlib\n","\n","importlib.reload(pkg_resources)"]},{"cell_type":"markdown","metadata":{"id":"23R0Z9RojXYW"},"source":["# Feature Engineering - Metro Traffic Data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sfSQ-kX-MLEr"},"source":["[Metro Interstate Traffic Volume dataset](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dx56i_bFGdzB"},"outputs":[],"source":["import keras, tfx, tensorflow_transform\n","import tensorflow as tf  \n","\n","tf.__version__, keras.__version__, tfx.__version__, tensorflow_transform.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"YIqpWK9efviJ","tags":["graded"]},"outputs":[],"source":["import os\n","import tensorflow as tf\n","\n","from tfx.components import CsvExampleGen\n","from tfx.components import ExampleValidator\n","from tfx.components import SchemaGen\n","from tfx.components import StatisticsGen\n","from tfx.components import Transform\n","\n","# from tfx import v1 as tfx\n","# from tfx.v1.components import CsvExampleGen\n","# from tfx.v1.components import StatisticsGen\n","# from tfx.v1.components import SchemaGen\n","# from tfx.v1.components import ExampleValidator\n","# from tfx.v1.components import Transform\n","\n","import tensorflow_transform.beam as tft_beam\n","from google.protobuf.json_format import MessageToDict\n","from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","\n","import tempfile\n","import pprint\n","import warnings\n","\n","pp = pprint.PrettyPrinter()\n","\n","# ignore tf warning messages\n","tf.get_logger().setLevel('ERROR')\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bNp9WZBkNRDO"},"outputs":[],"source":["#@title testing\n","\n","import tensorflow as tf\n","\n","feature_description = {\n","    \"holiday\": tf.io.FixedLenFeature([], tf.string),\n","    \"temp\": tf.io.FixedLenFeature([], tf.float32),\n","    \"rain_1h\": tf.io.FixedLenFeature([], tf.float32),\n","    \"snow_1h\": tf.io.FixedLenFeature([], tf.float32),\n","    \"clouds_all\": tf.io.FixedLenFeature([], tf.int64),\n","    \"weather_main\": tf.io.FixedLenFeature([], tf.string),\n","    \"weather_description\": tf.io.FixedLenFeature([], tf.string),\n","    \"date_time\": tf.io.FixedLenFeature([], tf.string),\n","    \"traffic_volume\": tf.io.FixedLenFeature([], tf.int64),\n","    \"month\": tf.io.FixedLenFeature([], tf.int64),\n","    \"day\": tf.io.FixedLenFeature([], tf.int64),\n","    \"day_of_week\": tf.io.FixedLenFeature([], tf.int64),\n","    \"hour\": tf.io.FixedLenFeature([], tf.int64),\n","}\n","\n","raw_data = [\n","    {\n","        \"holiday\": \"None\",\n","        \"temp\": 273.67,\n","        \"rain_1h\": 0.0,\n","        \"snow_1h\": 0.13,\n","        \"clouds_all\": 90,\n","        \"weather_main\": \"Snow\",\n","        \"weather_description\": \"light snow\",\n","        \"date_time\": \"2016-01-08 15:00:00\",\n","        \"traffic_volume\": 5548,\n","        \"month\": 1,\n","        \"day\": 8,\n","        \"day_of_week\": 4,\n","        \"hour\": 15,\n","    }\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"MJJXZvOSLpiJ","tags":["graded"]},"outputs":[],"source":["# location of the pipeline metadata store\n","_pipeline_root = './pipeline'\n","\n","# directory of the raw data files\n","_data_root = './data'\n","\n","# path to the raw training data\n","_data_filepath = os.path.join(_data_root, 'metro_traffic_volume.csv')"]},{"cell_type":"markdown","metadata":{"id":"blZC1sIQOWfH"},"source":["Take a quick look at the first few rows of the CSV file."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"c5YPeLPFOXaD","tags":["graded"]},"outputs":[],"source":["# Preview the dataset\n","!head {_data_filepath}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":796,"status":"ok","timestamp":1677442995219,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"0Rh6K5sUf9dd","outputId":"229f15aa-24e5-4972-a526-ffcea9b902a8","tags":["graded"]},"outputs":[],"source":["# Declare the InteractiveContext and use a local sqlite file as the metadata store.\n","# You can ignore the warning about the missing metadata config file\n","context = InteractiveContext(pipeline_root=_pipeline_root)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"PyXjuMt8f-9u","scrolled":true,"tags":["graded"]},"outputs":[],"source":["# Instantiate ExampleGen with the input CSV dataset\n","example_gen = CsvExampleGen(input_base=_data_root)\n","\n","# Run the component using the InteractiveContext instance\n","context.run(example_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":34,"status":"ok","timestamp":1677443021930,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"880KkTAkPeUg","outputId":"beff46bb-80ea-43ee-fe97-81f754fa0c82","tags":["graded"]},"outputs":[],"source":["try:\n","    # get the artifact object\n","    artifact = example_gen.outputs['examples'].get()[0]\n","    \n","    # print split names and uri\n","    print(f'split names: {artifact.split_names}')\n","    print(f'artifact uri: {artifact.uri}')\n","\n","# for grading since context.run() does not work outside the notebook\n","except IndexError:\n","    print(\"context.run() was no-op\")\n","    examples_path = './pipeline/CsvExampleGen/examples'\n","    dir_id = os.listdir(examples_path)[0]\n","    artifact_uri = f'{examples_path}/{dir_id}'\n","\n","else:\n","    artifact_uri = artifact.uri"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"H4XIXjiCPwzQ","tags":["graded"]},"outputs":[],"source":["# Get the URI of the output artifact representing the training examples, which is a directory\n","train_uri = os.path.join(artifact_uri, 'Split-train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"Jc_gqm6JLpiO","tags":["graded"]},"outputs":[],"source":["def get_records(dataset, num_records):\n","    '''Extracts records from the given dataset.\n","    Args:\n","        dataset (TFRecordDataset): dataset saved by ExampleGen\n","        num_records (int): number of records to preview\n","    '''\n","    \n","    # initialize an empty list\n","    records = []\n","\n","    # Use the `take()` method to specify how many records to get\n","    for tfrecord in dataset.take(num_records):\n","        \n","        # Get the numpy property of the tensor\n","        serialized_example = tfrecord.numpy()\n","        \n","        # Initialize a `tf.train.Example()` to read the serialized data\n","        example = tf.train.Example()\n","        \n","        # Read the example data (output is a protocol buffer message)\n","        example.ParseFromString(serialized_example)\n","        \n","        # convert the protocol buffer message to a Python dictionary\n","        example_dict = MessageToDict(example)\n","        \n","        # append to the records list\n","        records.append(example_dict)\n","        \n","    return records"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"tD9fwC2FLpiO","tags":["graded"]},"outputs":[],"source":["# Get 3 records from the dataset\n","sample_records = get_records(dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"MAscCCYWgA-9","tags":["graded"]},"outputs":[],"source":["# Instantiate StatisticsGen with the ExampleGen ingested dataset\n","statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n","    \n","# Run the component\n","context.run(statistics_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"NVYrZCKMLpiQ","tags":["graded"]},"outputs":[],"source":["# Plot the statistics generated\n","context.show(statistics_gen.outputs['statistics'])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"ygQvZ6hsiQ_J","tags":["graded"]},"outputs":[],"source":["# Instantiate SchemaGen with the output statistics from the StatisticsGen\n","schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n","    \n","# Run the component\n","context.run(schema_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Ec9vqDXpXeMb","tags":["graded"]},"outputs":[],"source":["# Visualize the output\n","context.show(schema_gen.outputs['schema'])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"XRlRUuGgiXks","tags":["graded"]},"outputs":[],"source":["# Instantiate ExampleValidator with the statistics and schema from the previous steps\n","example_validator = ExampleValidator(statistics=statistics_gen.outputs['statistics'],schema=schema_gen.outputs['schema'])\n","\n","# Run the component\n","context.run(example_validator)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"TDyAAozQcrk3","tags":["graded"]},"outputs":[],"source":["# Visualize the output\n","context.show(example_validator.outputs['anomalies'])"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"PuNSiUKb4YJf","tags":["graded"]},"outputs":[],"source":["# Set the constants module filename\n","_traffic_constants_module_file = 'traffic_constants.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1677443033957,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"HPjhXuIF4YJh","outputId":"d7cb1381-7107-49d9-f7f5-ff8c8ab7f3dc","tags":["graded"]},"outputs":[],"source":["%%writefile {_traffic_constants_module_file}\n","\n","# Features to be scaled to the z-score\n","DENSE_FLOAT_FEATURE_KEYS = ['temp', 'snow_1h']\n","\n","# Features to bucketize\n","BUCKET_FEATURE_KEYS = ['rain_1h']\n","\n","# Number of buckets used by tf.transform for encoding each feature.\n","FEATURE_BUCKET_COUNT = {'rain_1h': 3}\n","\n","# Feature to scale from 0 to 1\n","RANGE_FEATURE_KEYS = ['clouds_all']\n","\n","# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n","VOCAB_SIZE = 1000\n","\n","# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n","OOV_SIZE = 10\n","\n","# Features with string data types that will be converted to indices\n","VOCAB_FEATURE_KEYS = [\n","    'holiday',\n","    'weather_main',\n","    'weather_description'\n","]\n","\n","# Features with int data type that will be kept as is\n","CATEGORICAL_FEATURE_KEYS = [\n","    'hour', 'day', 'day_of_week', 'month'\n","]\n","\n","# Feature to predict\n","VOLUME_KEY = 'traffic_volume'\n","\n","def transformed_name(key):\n","    return key + '_xf'"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"4AJ9hBs94YJm","tags":["graded"]},"outputs":[],"source":["# Set the transform module filename\n","_traffic_transform_module_file = 'traffic_transform.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1677443034780,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"graded":true,"id":"MYmxxx9A4YJn","name":"preprocessing_fn_code","outputId":"b7f89dc2-f535-49b4-d84a-1bff9dd4ce80","tags":["graded"]},"outputs":[],"source":["%%writefile {_traffic_transform_module_file}\n","\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","\n","import traffic_constants\n","\n","# Unpack the contents of the constants module\n","_DENSE_FLOAT_FEATURE_KEYS = traffic_constants.DENSE_FLOAT_FEATURE_KEYS\n","_RANGE_FEATURE_KEYS = traffic_constants.RANGE_FEATURE_KEYS\n","_VOCAB_FEATURE_KEYS = traffic_constants.VOCAB_FEATURE_KEYS\n","_VOCAB_SIZE = traffic_constants.VOCAB_SIZE\n","_OOV_SIZE = traffic_constants.OOV_SIZE\n","_CATEGORICAL_FEATURE_KEYS = traffic_constants.CATEGORICAL_FEATURE_KEYS\n","_BUCKET_FEATURE_KEYS = traffic_constants.BUCKET_FEATURE_KEYS\n","_FEATURE_BUCKET_COUNT = traffic_constants.FEATURE_BUCKET_COUNT\n","_VOLUME_KEY = traffic_constants.VOLUME_KEY\n","_transformed_name = traffic_constants.transformed_name\n","\n","\n","def preprocessing_fn(inputs):\n","    \"\"\"tf.transform's callback function for preprocessing inputs.\n","    Args:\n","    inputs: map from feature keys to raw not-yet-transformed features.\n","    Returns:\n","    Map from string feature key to transformed feature operations.\n","    \"\"\"\n","    outputs = {}\n","\n","     # Scale these features to the z-score.\n","    for key in _DENSE_FLOAT_FEATURE_KEYS:\n","        # Scale these features to the z-score.\n","        outputs[_transformed_name(key)] = tft.scale_to_z_score(inputs[key])\n","            \n","\n","    # Scale these feature/s from 0 to 1\n","    for key in _RANGE_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.scale_to_0_1(inputs[key])\n","            \n","\n","    # Transform the strings into indices \n","    # hint: use the VOCAB_SIZE and OOV_SIZE to define the top_k and num_oov parameters\n","    for key in _VOCAB_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n","            inputs[key], \n","            top_k=_VOCAB_SIZE, \n","            num_oov_buckets=_OOV_SIZE)\n","            \n","            \n","            \n","\n","    # Bucketize the feature\n","    for key in _BUCKET_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.bucketize(\n","            inputs[key], \n","            _FEATURE_BUCKET_COUNT[key])\n","            \n","\n","    # Keep as is. No tft function needed.\n","    for key in _CATEGORICAL_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = inputs[key]\n","\n","        \n","    # Use `tf.cast` to cast the label key to float32 and fill in the missing values.\n","    traffic_volume = tf.cast(inputs[_VOLUME_KEY], tf.float32)\n","  \n","    \n","    # Create a feature that shows if the traffic volume is greater than the mean and cast to an int\n","    outputs[_transformed_name(_VOLUME_KEY)] = tf.cast(  \n","        \n","        # Use `tf.greater` to check if the traffic volume in a row is greater than the mean of the entire traffic volumn column\n","        tf.greater(traffic_volume, \n","                   tft.mean(tf.cast(traffic_volume, \n","                                   tf.float32))), tf.int64)\n","\n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2e-uIIFLpiV"},"outputs":[],"source":["# Test your preprocessing_fn\n","\n","import traffic_transform\n","#from testing_values import feature_description, raw_data\n","\n","# NOTE: These next two lines are for reloading your traffic_transform module in case you need to \n","# update your initial solution and re-run this cell. Please do not remove them especially if you\n","# have revised your solution. Else, your changes will not be detected.\n","import importlib\n","importlib.reload(traffic_transform)\n","\n","raw_data_metadata = dataset_metadata.DatasetMetadata(schema_utils.schema_from_feature_spec(feature_description))\n","\n","with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n","    transformed_dataset, _ = (\n","        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(traffic_transform.preprocessing_fn))\n","\n","transformed_data, transformed_metadata = transformed_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAblQEeGLpiV"},"outputs":[],"source":["# Test that the transformed data matches the expected output\n","transformed_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vlNNiYRRLpiW"},"outputs":[],"source":["# Test that the transformed metadata's schema matches the expected output\n","MessageToDict(transformed_metadata.schema)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"jHfhth_GiZI9","tags":["graded"]},"outputs":[],"source":["# Instantiate the Transform component\n","transform = Transform(\n","    examples=example_gen.outputs['examples'],\n","    schema=schema_gen.outputs['schema'],\n","    module_file=os.path.abspath(_traffic_transform_module_file))\n","\n","# Run the component.\n","# The `enable_cache` flag is disabled in case you need to update your transform module file.\n","context.run(transform, enable_cache=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"5tRw4DneR3i7","tags":["graded"]},"outputs":[],"source":["try:\n","    # Get the uri of the transform graph\n","    transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n","\n","except IndexError:\n","    print(\"context.run() was no-op\")\n","    transform_path = './pipeline/Transform/transformed_examples'\n","    dir_id = os.listdir(transform_path)[0]\n","    transform_graph_uri = f'{transform_path}/{dir_id}'\n","    \n","else:\n","    # List the subdirectories under the uri\n","    os.listdir(transform_graph_uri)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"XwcIxAC1LpiY","tags":["graded"]},"outputs":[],"source":["try:\n","    # Get the URI of the output artifact representing the transformed examples\n","    train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n","    \n","except IndexError:\n","    print(\"context.run() was no-op\")\n","    train_uri = os.path.join(transform_graph_uri, 'Split-train')"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"pwbW2zPKR_S4","tags":["graded"]},"outputs":[],"source":["# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"mSDZ2rJC7NQW","tags":["graded"]},"outputs":[],"source":["# Get 3 records from the dataset\n","sample_records_xf = get_records(transformed_dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records_xf)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"t0tVqwkhd8SD"},"source":["# Feature Engineering Pipeline - Income Data\n","\n"," [Census Income dataset](https://archive.ics.uci.edu/ml/datasets/Adult) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Cul_K6Ad8SU"},"outputs":[],"source":["import tensorflow as tf\n","\n","# from tfx import v1 as tfx\n","\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","from google.protobuf.json_format import MessageToDict\n","\n","import os\n","import pprint\n","pp = pprint.PrettyPrinter()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8HHwF02cAKQ"},"outputs":[],"source":["# location of the pipeline metadata store\n","_pipeline_root = './pipeline/'\n","\n","# directory of the raw data files\n","_data_root = './census_data'\n","\n","# path to the raw training data\n","_data_filepath = os.path.join(_data_root, 'adult.data')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FSB-dpid8SW"},"outputs":[],"source":["# preview the first few rows of the CSV file\n","!head {_data_filepath}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1677444279792,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"hrw84cXkd8SW","outputId":"3943dad9-0823-4869-f824-48cfa83f9899"},"outputs":[],"source":["# Initialize the InteractiveContext with a local sqlite file.\n","# If you leave `_pipeline_root` blank, then the db will be created in a temporary directory.\n","# You can safely ignore the warning about the missing config file.\n","context = InteractiveContext(pipeline_root=_pipeline_root)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J6cb0JH9d8SX","scrolled":true},"outputs":[],"source":["# Instantiate ExampleGen with the input CSV dataset\n","example_gen = tfx.components.CsvExampleGen(input_base=_data_root)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_RXrN-tcAKU"},"outputs":[],"source":["# Execute the component\n","context.run(example_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1677444292453,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"gewMkaw1d8SY","outputId":"8014840f-147d-46da-f907-96d2b3820f49"},"outputs":[],"source":["# get the artifact object\n","artifact = example_gen.outputs['examples'].get()[0]\n","\n","# print split names and uri\n","print(f'split names: {artifact.split_names}')\n","print(f'artifact uri: {artifact.uri}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1677444292454,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Y_DNg25RcAKV","outputId":"93eb96f7-61c0-4954-849f-d9180fbe6a0e"},"outputs":[],"source":["# Get the URI of the output artifact representing the training examples\n","train_uri = os.path.join(artifact.uri, 'Split-train')\n","\n","# See the contents of the `train` folder\n","!ls {train_uri}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bO_s1sosd8SZ"},"outputs":[],"source":["# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6W0cyvlWcAKW"},"outputs":[],"source":["# Define a helper function to get individual examples\n","def get_records(dataset, num_records):\n","    '''Extracts records from the given dataset.\n","    Args:\n","        dataset (TFRecordDataset): dataset saved by ExampleGen\n","        num_records (int): number of records to preview\n","    '''\n","    \n","    # initialize an empty list\n","    records = []\n","    \n","    # Use the `take()` method to specify how many records to get\n","    for tfrecord in dataset.take(num_records):\n","        \n","        # Get the numpy property of the tensor\n","        serialized_example = tfrecord.numpy()\n","        \n","        # Initialize a `tf.train.Example()` to read the serialized data\n","        example = tf.train.Example()\n","        \n","        # Read the example data (output is a protocol buffer message)\n","        example.ParseFromString(serialized_example)\n","        \n","        # convert the protocol bufffer message to a Python dictionary\n","        example_dict = (MessageToDict(example))\n","        \n","        # append to the records list\n","        records.append(example_dict)\n","        \n","    return records"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6UQyCTskcAKW"},"outputs":[],"source":["# Get 3 records from the dataset\n","sample_records = get_records(dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GhcTU9Q6d8Sa"},"outputs":[],"source":["# Instantiate StatisticsGen with the ExampleGen ingested dataset\n","statistics_gen = tfx.components.StatisticsGen(\n","    examples=example_gen.outputs['examples'])\n","\n","# Execute the component\n","context.run(statistics_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eXDGrXp3cAKY"},"outputs":[],"source":["# Show the output statistics\n","context.show(statistics_gen.outputs['statistics'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IPBMo1vvd8Sa"},"outputs":[],"source":["# Instantiate SchemaGen with the StatisticsGen ingested dataset\n","schema_gen = tfx.components.SchemaGen(\n","    statistics=statistics_gen.outputs['statistics'],\n","    )\n","\n","# Run the component\n","context.run(schema_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fytBA1J-d8Sb"},"outputs":[],"source":["# Visualize the schema\n","context.show(schema_gen.outputs['schema'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dvEA3uYjd8Sb"},"outputs":[],"source":["# Instantiate ExampleValidator with the StatisticsGen and SchemaGen ingested data\n","example_validator = tfx.components.ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=schema_gen.outputs['schema'])\n","\n","# Run the component.\n","context.run(example_validator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OPpV78rd8Sc"},"outputs":[],"source":["# Visualize the results\n","context.show(example_validator.outputs['anomalies'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XkMqIofqd8Sc"},"outputs":[],"source":["# Set the constants module filename\n","_census_constants_module_file = 'census_constants.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1677444300564,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"4egjHMzPd8Sd","outputId":"30c54af2-3ca1-491a-cddd-69f7af7ff048"},"outputs":[],"source":["%%writefile {_census_constants_module_file}\n","\n","# Features with string data types that will be converted to indices\n","CATEGORICAL_FEATURE_KEYS = [\n","    'education', 'marital-status', 'occupation', 'race', 'relationship', 'workclass', 'sex', 'native-country'\n","]\n","\n","# Numerical features that are marked as continuous\n","NUMERIC_FEATURE_KEYS = ['fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n","\n","# Feature that can be grouped into buckets\n","BUCKET_FEATURE_KEYS = ['age']\n","\n","# Number of buckets used by tf.transform for encoding each bucket feature.\n","FEATURE_BUCKET_COUNT = {'age': 4}\n","\n","# Feature that the model will predict\n","LABEL_KEY = 'label'\n","\n","# Utility function for renaming the feature\n","def transformed_name(key):\n","    return key + '_xf'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWwGlJnXd8Sd"},"outputs":[],"source":["# Set the transform module filename\n","_census_transform_module_file = 'census_transform.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1677444300565,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"GBVFoupOd8Sd","outputId":"379205ea-437b-4e5f-ba3a-f7ef3793ac5c"},"outputs":[],"source":["%%writefile {_census_transform_module_file}\n","\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","\n","import census_constants\n","\n","# Unpack the contents of the constants module\n","_NUMERIC_FEATURE_KEYS = census_constants.NUMERIC_FEATURE_KEYS\n","_CATEGORICAL_FEATURE_KEYS = census_constants.CATEGORICAL_FEATURE_KEYS\n","_BUCKET_FEATURE_KEYS = census_constants.BUCKET_FEATURE_KEYS\n","_FEATURE_BUCKET_COUNT = census_constants.FEATURE_BUCKET_COUNT\n","_LABEL_KEY = census_constants.LABEL_KEY\n","_transformed_name = census_constants.transformed_name\n","\n","\n","# Define the transformations\n","def preprocessing_fn(inputs):\n","    \"\"\"tf.transform's callback function for preprocessing inputs.\n","    Args:\n","        inputs: map from feature keys to raw not-yet-transformed features.\n","    Returns:\n","        Map from string feature key to transformed feature operations.\n","    \"\"\"\n","    outputs = {}\n","\n","    # Scale these features to the range [0,1]\n","    for key in _NUMERIC_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.scale_to_0_1(\n","            inputs[key])\n","    \n","    # Bucketize these features\n","    for key in _BUCKET_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.bucketize(\n","            inputs[key], _FEATURE_BUCKET_COUNT[key])\n","\n","    # Convert strings to indices in a vocabulary\n","    for key in _CATEGORICAL_FEATURE_KEYS:\n","        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(inputs[key])\n","\n","    # Convert the label strings to an index\n","    outputs[_transformed_name(_LABEL_KEY)] = tft.compute_and_apply_vocabulary(inputs[_LABEL_KEY])\n","\n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_GMa7VNFd8Se","scrolled":false},"outputs":[],"source":["# Ignore TF warning messages\n","tf.get_logger().setLevel('ERROR')\n","\n","# Instantiate the Transform component\n","transform = tfx.components.Transform(\n","    examples=example_gen.outputs['examples'],\n","    schema=schema_gen.outputs['schema'],\n","    module_file=os.path.abspath(_census_transform_module_file))\n","\n","# Run the component\n","context.run(transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1677444358515,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"A1dI1J5Id8Se","outputId":"5ebb3f3a-b009-4476-cc79-f1f195b74189"},"outputs":[],"source":["# Get the uri of the transform graph\n","transform_graph_uri = transform.outputs['transform_graph'].get()[0].uri\n","\n","# List the subdirectories under the uri\n","os.listdir(transform_graph_uri)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7Zr5RVid8Sf"},"outputs":[],"source":["# Get the URI of the output artifact representing the transformed examples\n","train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qKRdcrud8Sf"},"outputs":[],"source":["# Get 3 records from the dataset\n","sample_records_xf = get_records(transformed_dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records_xf)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NFBFVrdfvfZC"},"source":["# Feature Engineering with Images\n","\n","[CIFAR-10](https://www.tensorflow.org/datasets/catalog/cifar10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7pNW8zRxrKz"},"outputs":[],"source":["import os\n","import pprint\n","import tempfile\n","import urllib\n","\n","import absl\n","import tensorflow as tf\n","tf.get_logger().propagate = False\n","pp = pprint.PrettyPrinter()\n","\n","## from tfx import v1 as tfx\n","import tfx\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","from tfx.types import Channel\n","\n","from google.protobuf.json_format import MessageToDict\n","\n","print('TensorFlow version: {}'.format(tf.__version__))\n","print('TFX version: {}'.format(tfx.__version__))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se9m0HbDHBvv"},"outputs":[],"source":["# Location of the pipeline metadata store\n","_pipeline_root = './pipeline/'\n","\n","# Data files directory\n","_data_root = './cifar10'\n","\n","# Path to the training data\n","_data_filepath = os.path.join(_data_root, 'train.tfrecord')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BywX6OUEhAqn"},"outputs":[],"source":["# Create data folder for the images\n","!mkdir -p {_data_root}\n","\n","# URL of the hosted dataset\n","DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/v0.21.4/tfx/examples/cifar10/data/train.tfrecord'\n","\n","# Download the dataset and save locally\n","urllib.request.urlretrieve(DATA_PATH, _data_filepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_YCA5Y5xrK2"},"outputs":[],"source":["# Initialize the InteractiveContext\n","context = InteractiveContext(pipeline_root=_pipeline_root)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkh3RUCIxrK3"},"outputs":[],"source":["# Ingest the data through ExampleGen\n","example_gen = tfx.components.ImportExampleGen(input_base=_data_root)\n","\n","# Run the component\n","context.run(example_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1677449139695,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"hBu924x8xrK3","outputId":"a7dcf8fa-faf0-4f53-b0cf-564ed4b13596"},"outputs":[],"source":["# Print split names and URI\n","artifact = example_gen.outputs['examples'].get()[0]\n","print(artifact.split_names, artifact.uri)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r2G00sfaxrK4"},"outputs":[],"source":["import IPython.display as display\n","\n","# Get the URI of the output artifact representing the training examples, which is a directory\n","train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n","\n","# Description per example\n","image_feature_description = {\n","    'label': tf.io.FixedLenFeature([], tf.int64),\n","    'image_raw': tf.io.FixedLenFeature([], tf.string),\n","}\n","\n","# Image parser function\n","def _parse_image_function(example_proto):\n","  # Parse the input tf.Example proto using the dictionary above.\n","  return tf.io.parse_single_example(example_proto, image_feature_description)\n","\n","# Map the parser to the dataset\n","parsed_image_dataset = dataset.map(_parse_image_function)\n","\n","# Display the first three images\n","for features in parsed_image_dataset.take(3):\n","    image_raw = features['image_raw'].numpy()\n","    display.display(display.Image(data=image_raw))\n","    pprint.pprint('Class ID: {}'.format(features['label'].numpy()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VygmaSWKxrK5"},"outputs":[],"source":["# Run StatisticsGen\n","statistics_gen = tfx.components.StatisticsGen(\n","    examples=example_gen.outputs['examples'])\n","\n","context.run(statistics_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qHU-6-6JvfZN"},"outputs":[],"source":["# Visualize the results\n","context.show(statistics_gen.outputs['statistics'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9D1L5txxrK6"},"outputs":[],"source":["# Run SchemaGen\n","schema_gen = tfx.components.SchemaGen(\n","      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n","context.run(schema_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejgrCIeExrK6"},"outputs":[],"source":["# Visualize the results\n","context.show(schema_gen.outputs['schema'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwIatWd4xrK7"},"outputs":[],"source":["# Run ExampleValidator\n","example_validator = tfx.components.ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=schema_gen.outputs['schema'])\n","context.run(example_validator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9zsbQe4xrK7"},"outputs":[],"source":["# Visualize the results. There should be no anomalies.\n","context.show(example_validator.outputs['anomalies'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NoOZbCE1xrK8"},"outputs":[],"source":["_transform_module_file = 'cifar10_transform.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MkUKq3RExrK8"},"outputs":[],"source":["%%writefile {_transform_module_file}\n","\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","\n","# Keys\n","_LABEL_KEY = 'label'\n","_IMAGE_KEY = 'image_raw'\n","\n","\n","def _transformed_name(key):\n","    return key + '_xf'\n","\n","def _image_parser(image_str):\n","    '''converts the images to a float tensor'''\n","    image = tf.image.decode_image(image_str, channels=3)\n","    image = tf.reshape(image, (32, 32, 3))\n","    image = tf.cast(image, tf.float32)\n","    return image\n","\n","\n","def _label_parser(label_id):\n","    '''one hot encodes the labels'''\n","    label = tf.one_hot(label_id, 10)\n","    return label\n","\n","\n","def preprocessing_fn(inputs):\n","    \"\"\"tf.transform's callback function for preprocessing inputs.\n","    Args:\n","        inputs: map from feature keys to raw not-yet-transformed features.\n","    Returns:\n","        Map from string feature key to transformed feature operations.\n","    \"\"\"\n","    \n","    # Convert the raw image and labels to a float array and\n","    # one-hot encoded labels, respectively.\n","    with tf.device(\"/cpu:0\"):\n","        outputs = {\n","            _transformed_name(_IMAGE_KEY):\n","                tf.map_fn(\n","                    _image_parser,\n","                    tf.squeeze(inputs[_IMAGE_KEY], axis=1),\n","                    dtype=tf.float32),\n","            _transformed_name(_LABEL_KEY):\n","                tf.map_fn(\n","                    _label_parser,\n","                    tf.squeeze(inputs[_LABEL_KEY], axis=1),\n","                    dtype=tf.float32)\n","        }\n","    \n","    # scale the pixels from 0 to 1\n","    outputs[_transformed_name(_IMAGE_KEY)] = tft.scale_to_0_1(outputs[_transformed_name(_IMAGE_KEY)])\n","    \n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4S8_8SBuxrK9"},"outputs":[],"source":["# Ignore TF warning messages\n","tf.get_logger().setLevel('ERROR')\n","\n","# Setup the Transform component\n","transform = tfx.components.Transform(\n","    examples=example_gen.outputs['examples'],\n","    schema=schema_gen.outputs['schema'],\n","    module_file=os.path.abspath(_transform_module_file))\n","\n","# Run the component\n","context.run(transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wsl0iPzxxrK-"},"outputs":[],"source":["# Get the URI of the output artifact representing the transformed examples, which is a directory\n","train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wRVSuXc0vfZR"},"outputs":[],"source":["# Define a helper function to get individual examples\n","def get_records(dataset, num_records):\n","    '''Extracts records from the given dataset.\n","    Args:\n","        dataset (TFRecordDataset): dataset saved by ExampleGen\n","        num_records (int): number of records to preview\n","    '''\n","    \n","    # initialize an empty list\n","    records = []\n","    \n","    # Use the `take()` method to specify how many records to get\n","    for tfrecord in dataset.take(num_records):\n","        \n","        # Get the numpy property of the tensor\n","        serialized_example = tfrecord.numpy()\n","        \n","        # Initialize a `tf.train.Example()` to read the serialized data\n","        example = tf.train.Example()\n","        \n","        # Read the example data (output is a protocol buffer message)\n","        example.ParseFromString(serialized_example)\n","        \n","        # convert the protocol bufffer message to a Python dictionary\n","        example_dict = (MessageToDict(example))\n","        \n","        # append to the records list\n","        records.append(example_dict)\n","        \n","    return records"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwaFDD0fxrK_"},"outputs":[],"source":["# Get 1 record from the dataset\n","sample_records = get_records(dataset, 1)\n","\n","# Print the output\n","pp.pprint(sample_records)"]}],"metadata":{"colab":{"collapsed_sections":["23R0Z9RojXYW","t0tVqwkhd8SD","NFBFVrdfvfZC"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
