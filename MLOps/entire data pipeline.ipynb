{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"ppvHP8YzmJRu"},"source":["# Data Pipeline Components for Production ML -  Data ingestion, Data Validation, and Data Transformation\n","\n","*   Performing feature selection\n","*   Ingesting the dataset\n","*   Generating the statistics of the dataset\n","*   Creating a schema as per the domain knowledge\n","*   Creating schema environments\n","*   Visualizing the dataset anomalies\n","*   Preprocessing, transforming and engineering your features\n","*   Tracking the provenance of your data pipeline using ML Metadata"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDAgY1QtbRtY"},"outputs":[],"source":["!pip install -q tensorflow_transform\n","!pip install -q tfx    ## tfx==1.3.0\n","\n","### do not forget to !!!!!     RESTART THE RUNTIME     !!!!! after the execution of this cell !!! OR following cell\n","### https://www.tensorflow.org/tfx/tutorials/transform/simple\n","# # This cell is only necessary because packages were installed while python was\n","# # running. It avoids the need to restart the runtime when running in Colab.\n","# import pkg_resources\n","# import importlib\n","\n","# importlib.reload(pkg_resources)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMN5S2YSbfIx"},"outputs":[],"source":["# This cell is only necessary because packages were installed while python was\n","# running. It avoids the need to restart the runtime when running in Colab.\n","import pkg_resources\n","import importlib\n","\n","importlib.reload(pkg_resources)"]},{"cell_type":"markdown","metadata":{"id":"5ZXjh6D9nOUX"},"source":["<a name='1'></a>\n","## 1 - Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"TK6SyLhQP_s5","tags":["graded"]},"outputs":[],"source":["import tensorflow as tf\n","from tfx import v1 as tfx         ## do I need this?\n","\n","# TFX libraries\n","import tensorflow_data_validation as tfdv\n","import tensorflow_transform as tft\n","from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n","\n","# TFX components\n","from tfx.components import CsvExampleGen\n","from tfx.components import ExampleValidator\n","from tfx.components import SchemaGen\n","from tfx.components import StatisticsGen\n","from tfx.components import Transform\n","\n","# For performing feature selection\n","from sklearn.feature_selection import SelectKBest, f_classif\n","\n","# For feature visualization\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","\n","# Utilities\n","from tensorflow.python.lib.io import file_io\n","from tensorflow_metadata.proto.v0 import schema_pb2\n","from google.protobuf.json_format import MessageToDict\n","from  tfx.proto import example_gen_pb2\n","from tfx.types import standard_artifacts\n","from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils\n","import tensorflow_transform.beam as tft_beam\n","import os\n","import pprint\n","import tempfile\n","import pandas as pd\n","\n","# To ignore warnings from TF\n","tf.get_logger().setLevel('ERROR')\n","\n","# For formatting print statements\n","pp = pprint.PrettyPrinter()\n","\n","# Display versions of TF and TFX related packages\n","print('TensorFlow version: {}'.format(tf.__version__))\n","print('TFX version: {}'.format(tfx.__version__))\n","print('TensorFlow Data Validation version: {}'.format(tfdv.__version__))\n","print('TensorFlow Transform version: {}'.format(tft.__version__))"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","executionInfo":{"elapsed":12,"status":"ok","timestamp":1677494537151,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"q-eR9E0DdQat"},"outputs":[],"source":["#@title tests\n","\n","raw_data = [\n","    {\n","        \"Elevation\": 2596,\n","        \"Slope\": 3,\n","        \"Horizontal_Distance_To_Hydrology\": 258,\n","        \"Vertical_Distance_To_Hydrology\": 0,\n","        \"Horizontal_Distance_To_Roadways\": 510,\n","        \"Hillshade_9am\": 221,\n","        \"Hillshade_Noon\": 232,\n","        \"Horizontal_Distance_To_Fire_Points\": 6279,\n","        \"Wilderness_Area\": \"Rawah\",\n","        \"Soil_Type\": \"C7745\",\n","        \"Cover_Type\": 4,\n","    }\n","]\n","\n","feature_description = {\n","    \"Elevation\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Slope\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Horizontal_Distance_To_Hydrology\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Vertical_Distance_To_Hydrology\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Horizontal_Distance_To_Roadways\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Hillshade_9am\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Hillshade_Noon\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Horizontal_Distance_To_Fire_Points\": tf.io.FixedLenFeature([], tf.int64),\n","    \"Wilderness_Area\": tf.io.FixedLenFeature([], tf.string),\n","    \"Soil_Type\": tf.io.FixedLenFeature([], tf.string),\n","    \"Cover_Type\": tf.io.FixedLenFeature([], tf.int64),\n","}"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","executionInfo":{"elapsed":11,"status":"ok","timestamp":1677494537152,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"iL0xxHydc7q9"},"outputs":[],"source":["#@title helpers\n","\n","\n","def get_records(dataset, num_records):\n","    '''Extracts records from the given dataset.\n","    Args:\n","        dataset (TFRecordDataset): dataset saved by ExampleGen\n","        num_records (int): number of records to preview\n","    '''\n","    \n","    # initialize an empty list\n","    records = []\n","    \n","    # Use the `take()` method to specify how many records to get\n","    for tfrecord in dataset.take(num_records):\n","        \n","        # Get the numpy property of the tensor\n","        serialized_example = tfrecord.numpy()\n","        \n","        # Initialize a `tf.train.Example()` to read the serialized data\n","        example = tf.train.Example()\n","        \n","        # Read the example data (output is a protocol buffer message)\n","        example.ParseFromString(serialized_example)\n","        \n","        # convert the protocol bufffer message to a Python dictionary\n","        example_dict = (MessageToDict(example))\n","        \n","        # append to the records list\n","        records.append(example_dict)\n","        \n","    return records\n","\n","def display_types(types):\n","    # Helper function to render dataframes for the artifact and execution types\n","    table = {'id': [], 'name': []}\n","    for a_type in types:\n","        table['id'].append(a_type.id)\n","        table['name'].append(a_type.name)\n","    return pd.DataFrame(data=table)\n","\n","def display_artifacts(store, artifacts, base_dir):\n","    # Helper function to render dataframes for the input artifacts\n","    table = {'artifact id': [], 'type': [], 'uri': []}\n","    for a in artifacts:\n","        table['artifact id'].append(a.id)\n","        artifact_type = store.get_artifact_types_by_id([a.type_id])[0]\n","        table['type'].append(artifact_type.name)\n","        table['uri'].append(a.uri.replace(base_dir, './'))\n","    return pd.DataFrame(data=table)\n","\n","def display_properties(store, node):\n","    # Helper function to render dataframes for artifact and execution properties\n","    table = {'property': [], 'value': []}\n","    \n","    for k, v in node.properties.items():\n","        table['property'].append(k)\n","        table['value'].append(\n","            v.string_value if v.HasField('string_value') else v.int_value)\n","    \n","    for k, v in node.custom_properties.items():\n","        table['property'].append(k)\n","        table['value'].append(\n","            v.string_value if v.HasField('string_value') else v.int_value)\n","    \n","    return pd.DataFrame(data=table)"]},{"cell_type":"markdown","metadata":{"id":"bsHcencfobKL"},"source":["<a name='2'></a>\n","## 2 - Load the dataset\n","\n","You are going to use a variant of the [Cover Type](https://archive.ics.uci.edu/ml/datasets/covertype) dataset. This can be used to train a model that predicts the forest cover type based on cartographic variables. You can read more about the *original* dataset [here](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.info) and we've outlined the data columns below:\n","\n","| Column Name | Variable Type | Units / Range | Description |\n","| --------- | ------------ | ----- | ------------------- |\n","| Elevation | quantitative |meters | Elevation in meters |\n","| Aspect | quantitative | azimuth | Aspect in degrees azimuth |\n","| Slope | quantitative | degrees | Slope in degrees |\n","| Horizontal_Distance_To_Hydrology | quantitative | meters | Horz Dist to nearest surface water features |\n","| Vertical_Distance_To_Hydrology | quantitative | meters | Vert Dist to nearest surface water features |\n","| Horizontal_Distance_To_Roadways | quantitative | meters | Horz Dist to nearest roadway |\n","| Hillshade_9am | quantitative | 0 to 255 index | Hillshade index at 9am, summer solstice |\n","| Hillshade_Noon | quantitative | 0 to 255 index | Hillshade index at noon, summer soltice |\n","| Hillshade_3pm | quantitative | 0 to 255 index | Hillshade index at 3pm, summer solstice |\n","| Horizontal_Distance_To_Fire_Points | quantitative | meters | Horz Dist to nearest wildfire ignition points |\n","| Wilderness_Area (4 binary columns) | qualitative | 0 (absence) or 1 (presence) | Wilderness area designation |\n","| Soil_Type (40 binary columns) | qualitative | 0 (absence) or 1 (presence) | Soil Type designation |\n","| Cover_Type (7 types) | integer | 1 to 7 | Forest Cover Type designation |\n","\n","As you may notice, the qualitative data has already been one-hot encoded (e.g. `Soil_Type` has 40 binary columns where a `1` indicates presence of a feature). For learning, we will use a modified version of this dataset that shows a more raw format. This will let you practice your skills in handling different data types. You can see the code for preparing the dataset [here](https://github.com/GoogleCloudPlatform/mlops-on-gcp/blob/master/datasets/covertype/wrangle/prepare.ipynb) if you want but it is **not required for this assignment**. The main changes include:\n","\n","* Converting `Wilderness_Area` and `Soil_Type` to strings.\n","* Converting the `Cover_Type` range to [0, 6]\n","\n","Run the next cells to load the **modified** dataset to your workspace. "]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677494537152,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"hnSodqpbXORu"},"outputs":[],"source":["# # OPTIONAL: Just in case you want to restart the lab workspace *from scratch*, you\n","# # can uncomment and run this block to delete previously created files and\n","# # directories. \n","\n","# !rm -rf pipeline\n","# !rm -rf data"]},{"cell_type":"code","execution_count":7,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677494537153,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"TBJqC6-HfymZ","tags":["graded"]},"outputs":[],"source":["# Declare paths to the data\n","DATA_DIR = './data'\n","TRAINING_DIR = f'{DATA_DIR}/training'\n","TRAINING_DATA = f'{TRAINING_DIR}/dataset.csv'\n","\n","# Create the directory\n","!mkdir -p {TRAINING_DIR}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-o0Pk8nf7FZ"},"outputs":[],"source":["# download the dataset\n","!wget -nc https://storage.googleapis.com/mlep-public/course_2/week3/dataset.csv -P {TRAINING_DIR}"]},{"cell_type":"markdown","metadata":{"id":"N0gqgbTF2Cgl"},"source":["<a name='3'></a>\n","## 3 - Feature Selection\n","\n","For your first task, you will reduce the number of features to feed to the model. As mentioned in Week 2, this will help reduce the complexity of your model and save resources while training. Let's assume that you already have a baseline model that is trained on all features and you want to see if reducing the number of features will generate a better model. You will want to select a subset that has great predictive value to the label (in this case the `Cover_Type`). Let's do that in the following cells.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"I6MverMGJwg8","tags":["graded"]},"outputs":[],"source":["# Load the dataset to a dataframe\n","df = pd.read_csv(TRAINING_DATA)\n","\n","# Preview the dataset\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vkldr-PkXORw"},"outputs":[],"source":["# Show the data type of each column\n","df.dtypes"]},{"cell_type":"markdown","metadata":{"id":"8t--JcBpXORw"},"source":["Looking at the data types of each column and the dataset description at the start of this notebook, you can see that most of the features are numeric and only two are not. This needs to be taken into account when selecting the subset of features because numeric and categorical features are scored differently. Let's create a temporary dataframe that only contains the numeric features so we can use it in the next sections."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"uvmtVXtJeUOx","tags":["graded"]},"outputs":[],"source":["# Copy original dataset\n","df_num = df.copy()\n","\n","# Categorical columns\n","cat_columns = ['Wilderness_Area', 'Soil_Type']\n","\n","# Label column\n","label_column = ['Cover_Type']\n","\n","# Drop the categorical and label columns\n","df_num.drop(cat_columns, axis=1, inplace=True)\n","df_num.drop(label_column, axis=1, inplace=True)\n","\n","# Preview the resuls\n","df_num.head()"]},{"cell_type":"markdown","metadata":{"id":"kA1p8nQ-XORx"},"source":["You will use scikit-learn's built-in modules to perform [univariate feature selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection) on our dataset's numeric attributes. First, you need to prepare the input and target features:"]},{"cell_type":"code","execution_count":12,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677494538891,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"0ShO1olUeiUb","tags":["graded"]},"outputs":[],"source":["# Set the target values\n","y = df[label_column].values\n","\n","# Set the input values\n","X = df_num.values"]},{"cell_type":"markdown","metadata":{"id":"Kzz7WllQXORx"},"source":["Afterwards, you will use [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) to score each input feature against the target variable. Be mindful of the scoring function to pass in and make sure it is appropriate for the input (numeric) and target (categorical) values."]},{"cell_type":"markdown","metadata":{"id":"8QlfkDyfGKDv"},"source":["<a name='ex-1'></a>\n","### Exercise 1: Feature Selection\n","\n","Complete the code below to select the top 8 features of the numeric columns."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"bQgcpJqSenLq","tags":["graded"]},"outputs":[],"source":["# Create SelectKBest object using f_classif (ANOVA statistics) for 8 classes\n","select_k_best = SelectKBest(score_func=f_classif, k=8)\n","\n","# Fit and transform the input data using select_k_best\n","X_new = select_k_best.fit_transform(X,y)\n","\n","# Extract the features which are selected using get_support API\n","features_mask = select_k_best.get_support()\n","\n","# Print the results\n","reqd_cols = pd.DataFrame({'Columns': df_num.columns, 'Retain': features_mask})\n","print(reqd_cols)"]},{"cell_type":"markdown","metadata":{"id":"MOEfRpV2XORy"},"source":["If you got the expected results, you can now select this subset of features from the original dataframe and save it to a new directory in your workspace."]},{"cell_type":"code","execution_count":14,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677494539341,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"9A2jsRvhd-dR","tags":["graded"]},"outputs":[],"source":["# Set the paths to the reduced dataset\n","TRAINING_DIR_FSELECT = f'{TRAINING_DIR}/fselect'\n","TRAINING_DATA_FSELECT = f'{TRAINING_DIR_FSELECT}/dataset.csv'\n","\n","# Create the directory\n","!mkdir -p {TRAINING_DIR_FSELECT}"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"P3f_iNgne7Tk","tags":["graded"]},"outputs":[],"source":["# Get the feature names from SelectKBest\n","feature_names = list(df_num.columns[features_mask])\n","\n","# Append the categorical and label columns\n","feature_names = feature_names + cat_columns + label_column\n","\n","# Select the selected subset of columns\n","df_select = df[feature_names]\n","\n","# Write CSV to the created directory\n","df_select.to_csv(TRAINING_DATA_FSELECT, index=False)\n","\n","# Preview the results\n","df_select.head()"]},{"cell_type":"markdown","metadata":{"id":"8HeAEQLAhg8I"},"source":["<a name='4'></a>\n","## 4 - Data Pipeline\n","\n","With the selected subset of features prepared, you can now start building the data pipeline. This involves ingesting, validating, and transforming your data. You will be using the TFX components you've already encountered in the ungraded labs and you can look them up here in the [official documentation](https://www.tensorflow.org/tfx/api_docs/python/tfx/components)."]},{"cell_type":"markdown","metadata":{"id":"ZES9v8ggpDv8"},"source":["<a name='4-1'></a>\n","### 4.1 - Setup the Interactive Context\n","\n","As usual, you will first setup the Interactive Context so you can manually execute the pipeline components from the notebook. You will save the sqlite database in a pre-defined directory in your workspace. Please do not modify this path because you will need this in a later exercise involving ML Metadata."]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677494541517,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"8G-G-xLO3lkt","outputId":"dab81dae-32d7-4008-d0a6-eadb5fe9bb5b","tags":["graded"]},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at ./pipeline/metadata.sqlite.\n"]}],"source":["# Location of the pipeline metadata store\n","PIPELINE_DIR = './pipeline'\n","\n","# Declare the InteractiveContext and use a local sqlite file as the metadata store.\n","context = InteractiveContext(pipeline_root=PIPELINE_DIR)"]},{"cell_type":"markdown","metadata":{"id":"Rzm9d4G0hynL"},"source":["<a name='4-2'></a>\n","### 4.2 - Generating Examples\n","\n","The first step in the pipeline is to ingest the data. Using [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen), you can convert raw data to TFRecords for faster computation in the later stages of the pipeline."]},{"cell_type":"markdown","metadata":{"id":"C_jT4_QvOJOb"},"source":["<a name='ex-2'></a>\n","#### Exercise 2: ExampleGen\n","\n","Use `ExampleGen` to ingest the dataset we loaded earlier. Some things to note:\n","\n","* The input is in CSV format so you will need to use the appropriate type of `ExampleGen` to handle it. \n","* This function accepts a *directory* path to the training data and not the CSV file path itself. \n","\n","This will take a couple of minutes to run."]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677494541518,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"mq8P5Fx3hSOB"},"outputs":[],"source":["# # NOTE: Uncomment and run this if you get an error saying there are different \n","# # headers in the dataset. This is usually because of the notebook checkpoints saved in \n","# # that folder.\n","# !rm -rf {TRAINING_DIR}/.ipynb_checkpoints\n","# !rm -rf {TRAINING_DIR_FSELECT}/.ipynb_checkpoints\n","# !rm -rf {SERVING_DIR}/.ipynb_checkpoints"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"EL3CZQcg3lku","tags":["graded"]},"outputs":[],"source":["# Instantiate ExampleGen with the input CSV dataset\n","example_gen = CsvExampleGen(input_base=TRAINING_DIR_FSELECT)\n","\n","# Run the component using the InteractiveContext instance\n","context.run(example_gen)"]},{"cell_type":"markdown","metadata":{"id":"zd3k6iIni-FE"},"source":["<a name='4-3'></a>\n","### 4.3 - Computing Statistics\n","\n","Next, you will compute the statistics of your data. This will allow you to observe and analyze characteristics of your data through visualizations provided by the integrated [FACETS](https://pair-code.github.io/facets/) library."]},{"cell_type":"markdown","metadata":{"id":"vz5G_wYnVsHP"},"source":["<a name='ex-3'></a>\n","#### Exercise 3: StatisticsGen\n","\n","Use [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) to compute the statistics of the output examples of `ExampleGen`. "]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"5u2EOMOm3lkw","tags":["graded"]},"outputs":[],"source":["# Instantiate StatisticsGen with the ExampleGen ingested dataset\n","statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n","\n","# Run the component\n","context.run(statistics_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exyb-VKD3lkw","tags":[]},"outputs":[],"source":["# Display the results\n","context.show(statistics_gen.outputs['statistics'])"]},{"cell_type":"markdown","metadata":{"id":"5ZD97lK9UGct"},"source":["Once you've loaded the display, you may notice that the `zeros` column for `Cover_type` is highlighted in red. The visualization is letting us know that this might be a potential issue. In our case though, we know that the `Cover_Type` has a range of [0, 6] so having zeros in this column is something we expect."]},{"cell_type":"markdown","metadata":{"id":"K6K2Wd9-tfdx"},"source":["<a name='4-4'></a>\n","### 4.4 - Inferring the Schema\n","\n","You will need to create a schema to validate incoming datasets during training and serving. Fortunately, TFX allows you to infer a first draft of this schema with the [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component."]},{"cell_type":"markdown","metadata":{"id":"t6V9o7XnXKkV"},"source":["<a name='ex-4'></a>\n","#### Exercise 4: SchemaGen\n","\n","Use `SchemaGen` to infer a schema based on the computed statistics of `StatisticsGen`."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"_D9GZT1v3lkx","tags":["graded"]},"outputs":[],"source":["# Instantiate SchemaGen with the output statistics from the StatisticsGen\n","schema_gen = SchemaGen(\n","    statistics=statistics_gen.outputs['statistics'],\n","    )\n","\n","# Run the component\n","context.run(schema_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lW6Vhxy-3lkx"},"outputs":[],"source":["# Visualize the output\n","context.show(schema_gen.outputs['schema'])"]},{"cell_type":"markdown","metadata":{"id":"bpeVcYBbvret"},"source":["<a name='4-5'></a>\n","### 4.5 - Curating the schema\n","\n","You can see that the inferred schema is able to capture the data types correctly and also able to show the expected values for the qualitative (i.e. string) data. You can still fine-tune this however. For instance, we have features where we expect a certain range:\n","\n","* `Hillshade_9am`: 0 to 255\n","* `Hillshade_Noon`: 0 to 255\n","* `Slope`: 0 to 90\n","* `Cover_Type`:  0 to 6\n","\n","You want to update your schema to take note of these so the pipeline can detect if invalid values are being fed to the model."]},{"cell_type":"markdown","metadata":{"id":"5pUvj-IgYQIL"},"source":["<a name='ex-5'></a>\n","#### Exercise 5: Curating the Schema\n","\n","Use [TFDV](https://www.tensorflow.org/tfx/data_validation/get_started) to update the inferred schema to restrict a range of values to the features mentioned above.\n","\n","Things to note:\n","* You can use [tfdv.set_domain()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/set_domain) to define acceptable values for a particular feature.\n","* These should still be INT types after making your changes.\n","* Declare `Cover_Type` as a *categorical* variable. Unlike the other four features, the integers 0 to 6 here correspond to a designated label and not a quantitative measure. You can look at the available flags for `set_domain()` in the official doc to know how to set this."]},{"cell_type":"code","execution_count":26,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":22,"status":"ok","timestamp":1677494991055,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"dXrzLsgwXOR4","tags":["graded"]},"outputs":[],"source":["try:\n","    # Get the schema uri\n","    schema_uri = schema_gen.outputs['schema']._artifacts[0].uri\n","    \n","# for grading since context.run() does not work outside the notebook\n","except IndexError:\n","    print(\"context.run() was no-op\")\n","    schema_path = './pipeline/SchemaGen/schema'\n","    dir_id = os.listdir(schema_path)[0]\n","    schema_uri = f'{schema_path}/{dir_id}'"]},{"cell_type":"code","execution_count":27,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":22,"status":"ok","timestamp":1677494991055,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"27woxdsF3lky","tags":["graded"]},"outputs":[],"source":["# Get the schema pbtxt file from the SchemaGen output\n","schema = tfdv.load_schema_text(os.path.join(schema_uri, 'schema.pbtxt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"40wynuTzstP3","tags":["graded"]},"outputs":[],"source":["# Set the two `Hillshade` features to have a range of 0 to 255\n","tfdv.set_domain(schema, 'Hillshade_9am', schema_pb2.IntDomain(name='Hillshade_9am', min=0, max=255))\n","tfdv.set_domain(schema, 'Hillshade_Noon', schema_pb2.IntDomain(name='Hillshade_Noon', min=0, max=255))\n","\n","# Set the `Slope` feature to have a range of 0 to 90\n","tfdv.set_domain(schema, 'Slope', schema_pb2.IntDomain(name='Slope', min=0, max=90))\n","\n","# Set `Cover_Type` to categorical having minimum value of 0 and maximum value of 6\n","tfdv.set_domain(schema, 'Cover_Type', schema_pb2.IntDomain(name='Cover_Type', min=0, max=6, is_categorical=True))\n","\n","tfdv.display_schema(schema=schema)"]},{"cell_type":"markdown","metadata":{"id":"nkkrTVG3jrKl"},"source":["You should now see the ranges you declared in the `Domain` column of the schema."]},{"cell_type":"markdown","metadata":{"id":"fHN_j-d5mCCZ"},"source":["<a name='4-6'></a>\n","### 4.6 - Schema Environments\n","\n","In supervised learning, we train the model to make predictions by feeding a set of features with its corresponding label. Thus, our training dataset will have both the input features and label, and the schema is configured to detect these. \n","\n","However, after training and you serve the model for inference, the incoming data will no longer have the label. This will present problems when validating the data using the current version of the schema. Let's demonstrate that in the following cells. You will simulate a serving dataset by getting subset of the training set and dropping the label column (i.e. `Cover_Type`). Afterwards, you will validate this serving dataset using the schema you curated earlier."]},{"cell_type":"code","execution_count":29,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":458,"status":"ok","timestamp":1677494991496,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"MxyJHhoSl4-3","tags":["graded"]},"outputs":[],"source":["# Declare paths to the serving data\n","SERVING_DIR = f'{DATA_DIR}/serving'\n","SERVING_DATA = f'{SERVING_DIR}/serving_dataset.csv'\n","\n","# Create the directory\n","!mkdir -p {SERVING_DIR}"]},{"cell_type":"code","execution_count":30,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":4,"status":"ok","timestamp":1677494991497,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"cpSqjjXeafHr","tags":["graded"]},"outputs":[],"source":["# Read a subset of the training dataset\n","serving_data = pd.read_csv(TRAINING_DATA, nrows=100)\n","\n","# Drop the `Cover_Type` column\n","serving_data.drop(columns='Cover_Type', inplace=True)\n","\n","# Save the modified dataset\n","serving_data.to_csv(SERVING_DATA, index=False)\n","\n","# Delete unneeded variable from memory\n","del serving_data"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"dzKVqBogmG__","tags":["graded"]},"outputs":[],"source":["# Declare StatsOptions to use the curated schema\n","stats_options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\n","\n","# Compute the statistics of the serving dataset\n","serving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=stats_options)\n","\n","# Detect anomalies in the serving dataset\n","anomalies = tfdv.validate_statistics(serving_stats, schema=schema)\n","\n","# Display the anomalies detected\n","tfdv.display_anomalies(anomalies)"]},{"cell_type":"markdown","metadata":{"id":"wwIOMZYjt_Kb"},"source":["As expected, the missing column is flagged. To fix this, you need to configure the schema to detect when it's being used for training or for inference / serving. You can do this by setting [schema environments](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic#schema_environments)."]},{"cell_type":"markdown","metadata":{"id":"YBVVj8Hum0Ue"},"source":["<a name='ex-6'></a>\n","#### Exercise 6: Define the serving environment\n","\n","Complete the code below to ignore the `Cover_Type` feature when validating in the *SERVING* environment."]},{"cell_type":"code","execution_count":32,"metadata":{"deletable":false,"executionInfo":{"elapsed":11,"status":"ok","timestamp":1677494993425,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"zpgqThxImAso","tags":["graded"]},"outputs":[],"source":["schema.default_environment.append('TRAINING')\n","\n","# Hint: Create another default schema environment with name SERVING (pass in a string)\n","schema.default_environment.append('SERVING')\n","\n","# Remove Cover_Type feature from SERVING using TFDV\n","# Hint: Pass in the strings with the name of the feature and environment \n","tfdv.get_feature(schema, 'Cover_Type').not_in_environment.append('SERVING')"]},{"cell_type":"markdown","metadata":{"id":"Ge78zQcpxI-T"},"source":["If done correctly, running the cell below should show *No Anomalies*."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"rsDf37to4SoV","tags":["graded"]},"outputs":[],"source":["# Validate the serving dataset statistics in the `SERVING` environment\n","anomalies = tfdv.validate_statistics(serving_stats, schema=schema, environment='SERVING')\n","\n","# Display the anomalies detected\n","tfdv.display_anomalies(anomalies)"]},{"cell_type":"markdown","metadata":{"id":"8gpZl3CVyKyv"},"source":["We can now save this curated schema in a local directory so we can import it to our TFX pipeline."]},{"cell_type":"code","execution_count":34,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":370,"status":"ok","timestamp":1677494993786,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"fXnDWoGoxsmc","tags":["graded"]},"outputs":[],"source":["# Declare the path to the updated schema directory\n","UPDATED_SCHEMA_DIR = f'{PIPELINE_DIR}/updated_schema'\n","\n","# Create the said directory\n","!mkdir -p {UPDATED_SCHEMA_DIR}\n","\n","# Declare the path to the schema file\n","schema_file = os.path.join(UPDATED_SCHEMA_DIR, 'schema.pbtxt')\n","\n","# Save the curated schema to the said file\n","tfdv.write_schema_text(schema, schema_file)"]},{"cell_type":"markdown","metadata":{"id":"Ljco7hZ9x0ip"},"source":["As a sanity check, let's display the schema we just saved and verify that it contains the changes we introduced. It should still show the ranges in the `Domain` column and there should be two environments available."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"QNvH6V6yxtUm","tags":["graded"]},"outputs":[],"source":["# Load the schema from the directory we just created\n","new_schema = tfdv.load_schema_text(schema_file)\n","\n","# Display the schema. Check that the Domain column still contains the ranges.\n","tfdv.display_schema(schema=new_schema)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1677494993787,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"dxK7GqSgybrb","outputId":"0061d719-b11d-491e-ba47-e6ffd15dbbce"},"outputs":[{"data":{"text/plain":["['TRAINING', 'SERVING']"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# The environment list should show `TRAINING` and `SERVING`.\n","new_schema.default_environment"]},{"cell_type":"markdown","metadata":{"id":"npuw7JwMyQ6I"},"source":["<a name='4-7'></a>\n","### 4.7 - Generate new statistics using the updated schema\n","\n","You will now compute the statistics using the schema you just curated. Remember though that TFX components interact with each other by getting artifact information from the metadata store. So you first have to import the curated schema file into ML Metadata. You will do that by using an [ImportSchemaGen](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ImportSchemaGen) to create an artifact representing the curated schema."]},{"cell_type":"markdown","metadata":{"id":"78EzJ7Wfm5oa"},"source":["<a name='ex-7'></a>\n","#### Exercise 7: ImportSchemaGen\n","\n","Complete the code below to create a `Schema` artifact that points to the path of the curated schema file."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"bhaE8ha13lk0","tags":["graded"]},"outputs":[],"source":["user_schema_importer = tfx.components.ImportSchemaGen(schema_file=schema_file)\n","# Run the component\n","context.run(user_schema_importer, enable_cache=False)\n","\n","context.show(user_schema_importer.outputs['schema'])"]},{"cell_type":"markdown","metadata":{"id":"DR1SG-KYnBTR"},"source":["With the artifact successfully created, you can now use `StatisticsGen` and pass in a `schema` parameter to use the curated schema.\n","\n","<a name='ex-8'></a>\n","#### Exercise 8: Statistics with the new schema\n","\n","Use `StatisticsGen` to compute the statistics with the schema you updated in the previous section."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"_5HmdXKKjXF-","tags":["graded"]},"outputs":[],"source":["# Use StatisticsGen to compute the statistics using the curated schema\n","statistics_gen_updated = StatisticsGen(examples=example_gen.outputs['examples'], \n","                                       stats_options=tfdv.StatsOptions(infer_type_from_schema=True),\n","                                      schema=user_schema_importer.outputs['schema'])\n","\n","# Run the component\n","context.run(statistics_gen_updated)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9e1f6cH2jiPD"},"outputs":[],"source":["context.show(statistics_gen_updated.outputs['statistics'])"]},{"cell_type":"markdown","metadata":{"id":"8Ya6FRcLy11U"},"source":["The chart will look mostly the same from the previous runs but you can see that the `Cover Type` is now under the categorical features. That shows that `StatisticsGen` is indeed using the updated schema."]},{"cell_type":"markdown","metadata":{"id":"z7IurTkkwUNP"},"source":["<a name='4-8'></a>\n","### 4.8 - Check anomalies\n","\n","You will now check if the dataset has any anomalies with respect to the schema. You can do that easily with the [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component."]},{"cell_type":"markdown","metadata":{"id":"Rx3kqz0CnEqr"},"source":["<a name='ex-9'></a>\n","#### Exercise 9: ExampleValidator\n","\n","Check if there are any anomalies using `ExampleValidator`. You will need to pass in the updated statistics and schema from the previous sections."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"graded":true,"id":"kvBzLuyPqboL","name":"training_anomalies","tags":["graded"]},"outputs":[],"source":["example_validator = ExampleValidator(\n","    statistics=statistics_gen.outputs['statistics'],\n","    schema=user_schema_importer.outputs['schema'])\n","\n","# Run the component.\n","context.run(example_validator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FdBbjraG2PZf"},"outputs":[],"source":["# Visualize the results\n","context.show(example_validator.outputs['anomalies'])"]},{"cell_type":"markdown","metadata":{"id":"duwFgpTYIaYD"},"source":["<a name='4-10'></a>\n","### 4.10 - Feature engineering\n","\n","You will now proceed to transforming your features to a form suitable for training a model. This can include several methods such as scaling and converting strings to vocabulary indices. It is important for these transformations to be consistent across your training data, and also for the serving data when the model is deployed for inference. TFX ensures this by generating a graph that will process incoming data both during training and inference.\n","\n","Let's first declare the constants and utility function you will use for the exercise."]},{"cell_type":"code","execution_count":42,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":39,"status":"ok","timestamp":1677495021406,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"vz5VLpMF0R_s","tags":["graded"]},"outputs":[],"source":["# Set the constants module filename\n","_cover_constants_module_file = 'cover_constants.py'"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":39,"status":"ok","timestamp":1677495021406,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"FOkTSEia0UE3","outputId":"b99ff7aa-0ab9-4550-a7cb-fde3a4f0fbfd","tags":["graded"]},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing cover_constants.py\n"]}],"source":["%%writefile {_cover_constants_module_file}\n","\n","SCALE_MINMAX_FEATURE_KEYS = [\n","        \"Horizontal_Distance_To_Hydrology\",\n","        \"Vertical_Distance_To_Hydrology\",\n","    ]\n","\n","SCALE_01_FEATURE_KEYS = [\n","        \"Hillshade_9am\",\n","        \"Hillshade_Noon\",\n","        \"Horizontal_Distance_To_Fire_Points\",\n","    ]\n","\n","SCALE_Z_FEATURE_KEYS = [\n","        \"Elevation\",\n","        \"Slope\",\n","        \"Horizontal_Distance_To_Roadways\",\n","    ]\n","\n","VOCAB_FEATURE_KEYS = [\"Wilderness_Area\"]\n","\n","HASH_STRING_FEATURE_KEYS = [\"Soil_Type\"]\n","\n","LABEL_KEY = \"Cover_Type\"\n","\n","# Utility function for renaming the feature\n","def transformed_name(key):\n","    return key + '_xf'"]},{"cell_type":"markdown","metadata":{"id":"YYZX-xB03lk8"},"source":["Next you will define the `preprocessing_fn` to apply transformations to the features. "]},{"cell_type":"markdown","metadata":{"id":"tdXFOnB4nKRn"},"source":["<a name='ex-10'></a>\n","#### Exercise 10: Preprocessing function\n","\n","Complete the module to transform your features. Refer to the code comments to get hints on what operations to perform.\n","\n","Here are some links to the docs of the functions you will need to complete this function:\n","\n","- [`tft.scale_by_min_max`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_by_min_max)\n","- [`tft.scale_to_0_1`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_0_1)\n","- [`tft.scale_to_z_score`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score)\n","- [`tft.compute_and_apply_vocabulary`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/compute_and_apply_vocabulary)\n","- [`tft.hash_strings`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/hash_strings)"]},{"cell_type":"code","execution_count":44,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":34,"status":"ok","timestamp":1677495021407,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Elp0jej91iiT","tags":["graded"]},"outputs":[],"source":["# Set the transform module filename\n","_cover_transform_module_file = 'cover_transform.py'"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"graded":true,"id":"PJpPxzh6kdNM","name":"exercise_10","tags":["graded"]},"outputs":[],"source":["%%writefile {_cover_transform_module_file}\n","\n","import tensorflow as tf\n","import tensorflow_transform as tft\n","\n","import cover_constants\n","\n","_SCALE_MINMAX_FEATURE_KEYS = cover_constants.SCALE_MINMAX_FEATURE_KEYS\n","_SCALE_01_FEATURE_KEYS = cover_constants.SCALE_01_FEATURE_KEYS\n","_SCALE_Z_FEATURE_KEYS = cover_constants.SCALE_Z_FEATURE_KEYS\n","_VOCAB_FEATURE_KEYS = cover_constants.VOCAB_FEATURE_KEYS\n","_HASH_STRING_FEATURE_KEYS = cover_constants.HASH_STRING_FEATURE_KEYS\n","_LABEL_KEY = cover_constants.LABEL_KEY\n","_transformed_name = cover_constants.transformed_name\n","\n","def preprocessing_fn(inputs):\n","\n","    features_dict = {}\n","\n","    for feature in _SCALE_MINMAX_FEATURE_KEYS:\n","        data_col = inputs[feature] \n","        # Transform using scaling of min_max function\n","        # Hint: Use tft.scale_by_min_max by passing in the respective column\n","        features_dict[_transformed_name(feature)] = tft.scale_by_min_max(data_col)\n","\n","    for feature in _SCALE_01_FEATURE_KEYS:\n","        data_col = inputs[feature] \n","        # Transform using scaling of 0 to 1 function\n","        # Hint: tft.scale_to_0_1\n","        features_dict[_transformed_name(feature)] = tft.scale_to_0_1(data_col)\n","\n","    for feature in _SCALE_Z_FEATURE_KEYS:\n","        data_col = inputs[feature] \n","        # Transform using scaling to z score\n","        # Hint: tft.scale_to_z_score\n","        features_dict[_transformed_name(feature)] = tft.scale_to_z_score(data_col)\n","\n","    for feature in _VOCAB_FEATURE_KEYS:\n","        data_col = inputs[feature] \n","        # Transform using vocabulary available in column\n","        # Hint: Use tft.compute_and_apply_vocabulary\n","        features_dict[_transformed_name(feature)] = tft.compute_and_apply_vocabulary(data_col)\n","\n","    for feature in _HASH_STRING_FEATURE_KEYS:\n","        data_col = inputs[feature] \n","        # Transform by hashing strings into buckets\n","        # Hint: Use tft.hash_strings with the param hash_buckets set to 10\n","        features_dict[_transformed_name(feature)] = tft.hash_strings(data_col, hash_buckets=10)  \n","\n","    # No change in the label\n","    features_dict[_LABEL_KEY] = inputs[_LABEL_KEY]\n","\n","    return features_dict\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgC1LD7RXOSB"},"outputs":[],"source":["# Test your preprocessing_fn\n","\n","import cover_transform\n","## from testing_values import feature_description, raw_data\n","\n","# NOTE: These next two lines are for reloading your cover_transform module in case you need to \n","# update your initial solution and re-run this cell. Please do not remove them especially if you\n","# have revised your solution. Else, your changes will not be detected.\n","import importlib\n","importlib.reload(cover_transform)\n","\n","raw_data_metadata = dataset_metadata.DatasetMetadata(schema_utils.schema_from_feature_spec(feature_description))\n","\n","with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n","    transformed_dataset, _ = (\n","        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(cover_transform.preprocessing_fn))\n","\n","transformed_data, transformed_metadata = transformed_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wj9KDx8uXOSC"},"outputs":[],"source":["# Test that the transformed data matches the expected output\n","transformed_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1B4PKv_XXOSC"},"outputs":[],"source":["# Test that the transformed metadata's schema matches the expected output\n","MessageToDict(transformed_metadata.schema)"]},{"cell_type":"markdown","metadata":{"id":"eNhFUd68nTYQ"},"source":["<a name='ex-11'></a>\n","#### Exercise 11: Transform\n","\n","Use the [TFX Transform component](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Transform) to perform the transformations and generate the transformation graph. You will need to pass in the dataset examples, *curated* schema, and the module that contains the preprocessing function."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"id":"dq88l0XjkdQI","tags":["graded"]},"outputs":[],"source":["# Instantiate the Transform component\n","transform = Transform(\n","    examples=example_gen.outputs['examples'],\n","    schema=schema_gen.outputs['schema'],\n","    module_file=os.path.abspath(_cover_transform_module_file))\n","\n","# Run the component\n","context.run(transform, enable_cache=False)\n"]},{"cell_type":"markdown","metadata":{"id":"11IQ3mYOXOSD"},"source":["Let's inspect a few examples of the transformed dataset to see if the transformations are done correctly."]},{"cell_type":"code","execution_count":51,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":23,"status":"ok","timestamp":1677495169216,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"UVgNSTfBXOSD","tags":["graded"]},"outputs":[],"source":["try:\n","    transform_uri = transform.outputs['transformed_examples'].get()[0].uri\n","\n","# for grading since context.run() does not work outside the notebook\n","except IndexError:\n","    print(\"context.run() was no-op\")\n","    examples_path = './pipeline/Transform/transformed_examples'\n","    dir_id = os.listdir(examples_path)[0]\n","    transform_uri = f'{examples_path}/{dir_id}'"]},{"cell_type":"code","execution_count":52,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":23,"status":"ok","timestamp":1677495169217,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"9fU8Xa_hkdUS","tags":["graded"]},"outputs":[],"source":["# Get the URI of the output artifact representing the transformed examples\n","train_uri = os.path.join(transform_uri, 'Split-train')\n","\n","# Get the list of files in this directory (all compressed TFRecord files)\n","tfrecord_filenames = [os.path.join(train_uri, name)\n","                      for name in os.listdir(train_uri)]\n","\n","# Create a `TFRecordDataset` to read these files\n","transformed_dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"CYOiQrUS5Kvz","tags":["graded"]},"outputs":[],"source":["# Get 3 records from the dataset\n","sample_records_xf = get_records(transformed_dataset, 3)\n","\n","# Print the output\n","pp.pprint(sample_records_xf)"]},{"cell_type":"markdown","metadata":{"id":"EMFcfxSQZag-"},"source":["<a name='5'></a>\n","## 5 - ML Metadata\n","\n","TFX uses [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd) under the hood to keep records of artifacts that each component uses. This makes it easier to track how the pipeline is run so you can troubleshoot if needed or want to reproduce results.\n","\n","In this final section of the assignment, you will demonstrate going through this metadata store to retrieve related artifacts. This skill is useful for when you want to recall which inputs are fed to a particular stage of the pipeline. For example, you can know where to locate the schema used to perform feature transformation, or you can determine which set of examples were used to train a model."]},{"cell_type":"markdown","metadata":{"id":"3-dp2vLBXOSE"},"source":["You will start by importing the relevant modules and setting up the connection to the metadata store. We have also provided some helper functions for displaying artifact information and you can review its code in the external `util.py` module in your lab workspace."]},{"cell_type":"code","execution_count":55,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":445,"status":"ok","timestamp":1677495195433,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"txI2RjkVJcU5","tags":["graded"]},"outputs":[],"source":["# Import mlmd and utilities\n","import ml_metadata as mlmd\n","from ml_metadata.proto import metadata_store_pb2\n","\n","# Get the connection config to connect to the metadata store\n","connection_config = context.metadata_connection_config\n","\n","# Instantiate a MetadataStore instance with the connection config\n","store = mlmd.MetadataStore(connection_config)\n","\n","# Declare the base directory where All TFX artifacts are stored\n","base_dir = connection_config.sqlite.filename_uri.split('metadata.sqlite')[0]"]},{"cell_type":"markdown","metadata":{"id":"OSVNzeWwFGkq"},"source":["<a name='5-1'></a>\n","#### 5.1 -  Accessing stored artifacts\n","\n","With the connection setup, you can now interact with the metadata store. For instance, you can retrieve all artifact types stored with the `get_artifact_types()` function. For reference, the API is documented [here](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore)."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"pkZHeeKoJnf7","tags":["graded"]},"outputs":[],"source":["# Get the artifact types\n","types = store.get_artifact_types()\n","\n","# Display the results\n","display_types(types)"]},{"cell_type":"markdown","metadata":{"id":"uPom8wqlXOSF"},"source":["You can also get a list of artifacts for a particular type to see if there are variations used in the pipeline. For example, you curated a schema in an earlier part of the assignment so this should appear in the records. Running the cell below should show at least two rows: one for the inferred schema, and another for the updated schema. If you ran this notebook before, then you might see more rows because of the different schema artifacts saved under the `./SchemaGen/schema` directory."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"fU7gS-XnJqWx","tags":["graded"]},"outputs":[],"source":["# Retrieve the transform graph list\n","schema_list = store.get_artifacts_by_type('Schema')\n","\n","# Display artifact properties from the results\n","display_artifacts(store, schema_list, base_dir)\n"]},{"cell_type":"markdown","metadata":{"id":"--Kp1G0lXOSG"},"source":["Moreover, you can also get the properties of a particular artifact. TFX declares some properties automatically for each of its components. You will most likely see `name`, `state` and `producer_component` for each artifact type. Additional properties are added where appropriate. For example, a `split_names` property is added in `ExampleStatistics` artifacts to indicate which splits the statistics are generated for."]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"UdAs1SRNJstv","tags":["graded"]},"outputs":[],"source":["# Get the latest TransformGraph artifact\n","statistics_artifact = store.get_artifacts_by_type('ExampleStatistics')[-1]\n","\n","# Display the properties of the retrieved artifact\n","display_properties(store, statistics_artifact)"]},{"cell_type":"markdown","metadata":{"id":"eUPy2F4pFWyT"},"source":["<a name='5-2'></a>\n","#### 5.2 - Tracking artifacts\n","\n","For this final exercise, you will build a function to return the parent artifacts of a given one. For example, this should be able to list the artifacts that were used to generate a particular `TransformGraph` instance. "]},{"cell_type":"markdown","metadata":{"id":"AskXcyG2EGcO"},"source":["<a name='ex-12'></a>\n","##### Exercise 12: Get parent artifacts\n","\n","Complete the code below to track the inputs of a particular artifact.\n","\n","Tips:\n","\n","* You may find [get_events_by_artifact_ids()](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore#get_events_by_artifact_ids) and [get_events_by_execution_ids()](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore#get_executions_by_id) useful here. \n","\n","* Some of the methods of the MetadataStore class (such as the two given above) only accepts iterables so remember to convert to a list (or set) if you only have an int (e.g. pass `[x]` instead of `x`).\n","\n"]},{"cell_type":"code","execution_count":59,"metadata":{"deletable":false,"executionInfo":{"elapsed":335,"status":"ok","timestamp":1677495213420,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"T_g-QAGQJpBO","tags":["graded"]},"outputs":[],"source":["def get_parent_artifacts(store, artifact):\n","\n","    # Get the artifact id of the input artifact\n","    artifact_id = artifact.id\n","    \n","    # Get events associated with the artifact id\n","    artifact_id_events = store.get_events_by_artifact_ids([artifact_id])\n","    \n","    # From the `artifact_id_events`, get the execution ids of OUTPUT events.\n","    # Cast to a set to remove duplicates if any.\n","    execution_id = set( \n","        event.execution_id\n","        for event in artifact_id_events \n","        if event.type == metadata_store_pb2.Event.OUTPUT \n","    )\n","    \n","    # Get the events associated with the execution_id\n","    execution_id_events = store.get_events_by_execution_ids(execution_id)\n","\n","    # From execution_id_events, get the artifact ids of INPUT events.\n","    # Cast to a set to remove duplicates if any.\n","    parent_artifact_ids = set( \n","        event.artifact_id\n","        for event in execution_id_events\n","        if event.type == metadata_store_pb2.Event.INPUT\n","    )\n","    \n","    # Get the list of artifacts associated with the parent_artifact_ids\n","    parent_artifact_list = [artifact for artifact in store.get_artifacts_by_id(parent_artifact_ids)]\n","    \n","    return parent_artifact_list"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"2n4gfM64Jujt","tags":["graded"]},"outputs":[],"source":["# Get an artifact instance from the metadata store\n","artifact_instance = store.get_artifacts_by_type('TransformGraph')[0]\n","\n","# Retrieve the parent artifacts of the instance\n","parent_artifacts = get_parent_artifacts(store, artifact_instance)\n","\n","# Display the results\n","display_artifacts(store, parent_artifacts, base_dir)"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
