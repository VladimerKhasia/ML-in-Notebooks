{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IvRt6wC2n8Y"
      },
      "outputs": [],
      "source": [
        "# Install the KFP SDK\n",
        "!pip install --upgrade kfp==1.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSt2DEJA2ttR"
      },
      "outputs": [],
      "source": [
        "# Import the modules you will use\n",
        "import kfp\n",
        "\n",
        "# For creating the pipeline\n",
        "from kfp.v2 import dsl\n",
        "\n",
        "# For building components\n",
        "from kfp.v2.dsl import component\n",
        "\n",
        "# Type annotations for the component artifacts\n",
        "from kfp.v2.dsl import (\n",
        "    Input,\n",
        "    Output,\n",
        "    Artifact,\n",
        "    Dataset,\n",
        "    Model,\n",
        "    Metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gT4SZtZM22Gc"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    packages_to_install=[\"pandas\", \"openpyxl\"],\n",
        "    output_component_file=\"download_data_component.yaml\"\n",
        ")\n",
        "def download_data(url:str, output_csv:Output[Dataset]):\n",
        "    import pandas as pd\n",
        "\n",
        "    # Use pandas excel reader\n",
        "    df = pd.read_excel(url)\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    df.to_csv(output_csv.path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpItc-Ob6pnO"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    packages_to_install=[\"pandas\", \"sklearn\"],\n",
        "    output_component_file=\"split_data_component.yaml\"\n",
        ")\n",
        "def split_data(input_csv: Input[Dataset], train_csv: Output[Dataset], test_csv: Output[Dataset]):\n",
        "    import pandas as pd\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    df = pd.read_csv(input_csv.path)\n",
        "    train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "    train.to_csv(train_csv.path, index=False)\n",
        "    test.to_csv(test_csv.path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ-U_xsbLOIH"
      },
      "outputs": [],
      "source": [
        "@dsl.pipeline(\n",
        "    name=\"my-pipeline\",\n",
        ")\n",
        "def my_pipeline(url: str):\n",
        "    download_data_task = download_data(url=url)\n",
        "    split_data_task = split_data(input_csv=download_data_task.outputs['output_csv'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKFD7AGgLvHV"
      },
      "outputs": [],
      "source": [
        "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
        "    pipeline_func=my_pipeline,\n",
        "    package_path='pipeline.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF6gLo0w6nA4"
      },
      "outputs": [],
      "source": [
        "@component(\n",
        "    packages_to_install=[\"pandas\", \"numpy\"],\n",
        "    output_component_file=\"preprocess_data_component.yaml\"\n",
        ")\n",
        "def preprocess_data(input_train_csv: Input[Dataset], input_test_csv: Input[Dataset], \n",
        "                    output_train_x: Output[Dataset], output_test_x: Output[Dataset],\n",
        "                    output_train_y: Output[Artifact], output_test_y: Output[Artifact]):\n",
        "    \n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    \n",
        "    def format_output(data):\n",
        "        y1 = data.pop('Y1')\n",
        "        y1 = np.array(y1)\n",
        "        y2 = data.pop('Y2')\n",
        "        y2 = np.array(y2)\n",
        "        return y1, y2\n",
        "\n",
        "    def norm(x, train_stats):\n",
        "        return (x - train_stats['mean']) / train_stats['std']\n",
        "\n",
        "    train = pd.read_csv(input_train_csv.path)\n",
        "    test = pd.read_csv(input_test_csv.path)\n",
        "\n",
        "    train_stats = train.describe()\n",
        "\n",
        "    # Get Y1 and Y2 as the 2 outputs and format them as np arrays\n",
        "    train_stats.pop('Y1')\n",
        "    train_stats.pop('Y2')\n",
        "    train_stats = train_stats.transpose()\n",
        "    \n",
        "    train_Y = format_output(train)\n",
        "    with open(output_train_y.path, \"wb\") as file:\n",
        "      pickle.dump(train_Y, file)\n",
        "    \n",
        "    test_Y = format_output(test)\n",
        "    with open(output_test_y.path, \"wb\") as file:\n",
        "      pickle.dump(test_Y, file)\n",
        "\n",
        "    # Normalize the training and test data\n",
        "    norm_train_X = norm(train, train_stats)\n",
        "    norm_test_X = norm(test, train_stats)\n",
        "\n",
        "    norm_train_X.to_csv(output_train_x.path, index=False)\n",
        "    norm_test_X.to_csv(output_test_x.path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"tensorflow\", \"pandas\"],\n",
        "    output_component_file=\"train_model_component.yaml\"\n",
        ")\n",
        "def train_model(input_train_x: Input[Dataset], input_train_y: Input[Artifact], \n",
        "                output_model: Output[Model], output_history: Output[Artifact]):\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import pickle\n",
        "    \n",
        "    from tensorflow.keras.models import Model\n",
        "    from tensorflow.keras.layers import Dense, Input\n",
        "    \n",
        "    norm_train_X = pd.read_csv(input_train_x.path)\n",
        "\n",
        "    with open(input_train_y.path, \"rb\") as file:\n",
        "        train_Y = pickle.load(file)\n",
        "\n",
        "    def model_builder(train_X):\n",
        "\n",
        "      # Define model layers.\n",
        "      input_layer = Input(shape=(len(train_X.columns),))\n",
        "      first_dense = Dense(units='128', activation='relu')(input_layer)\n",
        "      second_dense = Dense(units='128', activation='relu')(first_dense)\n",
        "\n",
        "      # Y1 output will be fed directly from the second dense\n",
        "      y1_output = Dense(units='1', name='y1_output')(second_dense)\n",
        "      third_dense = Dense(units='64', activation='relu')(second_dense)\n",
        "\n",
        "      # Y2 output will come via the third dense\n",
        "      y2_output = Dense(units='1', name='y2_output')(third_dense)\n",
        "\n",
        "      # Define the model with the input layer and a list of output layers\n",
        "      model = Model(inputs=input_layer, outputs=[y1_output, y2_output])\n",
        "\n",
        "      print(model.summary())\n",
        "\n",
        "      return model\n",
        "\n",
        "    model = model_builder(norm_train_X)\n",
        "\n",
        "    # Specify the optimizer, and compile the model with loss functions for both outputs\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss={'y1_output': 'mse', 'y2_output': 'mse'},\n",
        "                  metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n",
        "                          'y2_output': tf.keras.metrics.RootMeanSquaredError()})\n",
        "    # Train the model for 500 epochs\n",
        "    history = model.fit(norm_train_X, train_Y, epochs=100, batch_size=10)\n",
        "    model.save(output_model.path)\n",
        "\n",
        "    with open(output_history.path, \"wb\") as file:\n",
        "        train_Y = pickle.dump(history.history, file)\n",
        "\n",
        "\n",
        "\n",
        "@component(\n",
        "    packages_to_install=[\"tensorflow\", \"pandas\"],\n",
        "    output_component_file=\"eval_model_component.yaml\"\n",
        ")\n",
        "def eval_model(input_model: Input[Model], input_history: Input[Artifact], \n",
        "               input_test_x: Input[Dataset], input_test_y: Input[Artifact], \n",
        "               MLPipeline_Metrics: Output[Metrics]):\n",
        "    import pandas as pd\n",
        "    import tensorflow as tf\n",
        "    import pickle\n",
        "\n",
        "    model = tf.keras.models.load_model(input_model.path)\n",
        "    \n",
        "    norm_test_X = pd.read_csv(input_test_x.path)\n",
        "\n",
        "    with open(input_test_y.path, \"rb\") as file:\n",
        "        test_Y = pickle.load(file)\n",
        "\n",
        "    # Test the model and print loss and mse for both outputs\n",
        "    loss, Y1_loss, Y2_loss, Y1_rmse, Y2_rmse = model.evaluate(x=norm_test_X, y=test_Y)\n",
        "    print(\"Loss = {}, Y1_loss = {}, Y1_mse = {}, Y2_loss = {}, Y2_mse = {}\".format(loss, Y1_loss, Y1_rmse, Y2_loss, Y2_rmse))\n",
        "    \n",
        "    MLPipeline_Metrics.log_metric(\"loss\", loss)\n",
        "    MLPipeline_Metrics.log_metric(\"Y1_loss\", Y1_loss)\n",
        "    MLPipeline_Metrics.log_metric(\"Y2_loss\", Y2_loss)\n",
        "    MLPipeline_Metrics.log_metric(\"Y1_rmse\", Y1_rmse)\n",
        "    MLPipeline_Metrics.log_metric(\"Y2_rmse\", Y2_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqD895So2-h2"
      },
      "outputs": [],
      "source": [
        "# Define a pipeline and create a task from a component:\n",
        "@dsl.pipeline(\n",
        "    name=\"my-pipeline\",\n",
        ")\n",
        "def my_pipeline(url: str):\n",
        "    \n",
        "    download_data_task = download_data(url=url)\n",
        "    \n",
        "    split_data_task = split_data(input_csv=download_data_task.outputs['output_csv'])\n",
        "    \n",
        "    preprocess_data_task = preprocess_data(input_train_csv=split_data_task.outputs['train_csv'],\n",
        "                                           input_test_csv=split_data_task.outputs['test_csv'])\n",
        "    \n",
        "    train_model_task = train_model(input_train_x=preprocess_data_task.outputs[\"output_train_x\"],\n",
        "                                   input_train_y=preprocess_data_task.outputs[\"output_train_y\"])\n",
        "    \n",
        "    eval_model_task = eval_model(input_model=train_model_task.outputs[\"output_model\"],\n",
        "                                 input_history=train_model_task.outputs[\"output_history\"],\n",
        "                                   input_test_x=preprocess_data_task.outputs[\"output_test_x\"],\n",
        "                                   input_test_y=preprocess_data_task.outputs[\"output_test_y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNPq9D263A3d"
      },
      "outputs": [],
      "source": [
        "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(\n",
        "    pipeline_func=my_pipeline,\n",
        "    package_path='pipeline.yaml')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
