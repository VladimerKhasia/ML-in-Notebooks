{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xh2G62DR0GEQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3-cg2_rYfe6"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sL5kmRZbZxX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import tempfile\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEuiXyPZMKQm"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "# String constants for model filenames\n",
        "FILE_WEIGHTS = 'baseline_weights.h5'\n",
        "FILE_NON_QUANTIZED_H5 = 'non_quantized.h5'\n",
        "FILE_NON_QUANTIZED_TFLITE = 'non_quantized.tflite'\n",
        "FILE_PT_QUANTIZED = 'post_training_quantized.tflite'\n",
        "FILE_QAT_QUANTIZED = 'quant_aware_quantized.tflite'\n",
        "FILE_PRUNED_MODEL_H5 = 'pruned_model.h5'\n",
        "FILE_PRUNED_QUANTIZED_TFLITE = 'pruned_quantized.tflite'\n",
        "FILE_PRUNED_NON_QUANTIZED_TFLITE = 'pruned_non_quantized.tflite'\n",
        "\n",
        "# Dictionaries to hold measurements\n",
        "MODEL_SIZE = {}\n",
        "ACCURACY = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqdSGWccdk8G"
      },
      "source": [
        "# UTILITY FUNCTIONS\n",
        "\n",
        "def print_metric(metric_dict, metric_name):\n",
        "  '''Prints key and values stored in a dictionary'''\n",
        "  for metric, value in metric_dict.items():\n",
        "    print(f'{metric_name} for {metric}: {value}')\n",
        "\n",
        "\n",
        "def model_builder():\n",
        "  '''Returns a shallow CNN for training on the MNIST dataset'''\n",
        "\n",
        "  keras = tf.keras\n",
        "\n",
        "  # Define the model architecture.\n",
        "  model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "    keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "    keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def evaluate_tflite_model(filename, x_test, y_test):\n",
        "  '''\n",
        "  Measures the accuracy of a given TF Lite model and test set\n",
        "  \n",
        "  Args:\n",
        "    filename (string) - filename of the model to load\n",
        "    x_test (numpy array) - test images\n",
        "    y_test (numpy array) - test labels\n",
        "\n",
        "  Returns\n",
        "    float showing the accuracy against the test set\n",
        "  '''\n",
        "\n",
        "  # Initialize the TF Lite Interpreter and allocate tensors\n",
        "  interpreter = tf.lite.Interpreter(model_path=filename)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Get input and output index\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Initialize empty predictions list\n",
        "  prediction_digits = []\n",
        "  \n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  for i, test_image in enumerate(x_test):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == y_test).mean()\n",
        "  \n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def get_gzipped_model_size(file):\n",
        "  '''Returns size of gzipped model, in bytes.'''\n",
        "  _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5f5Y08r0sob"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ild5juYXu4j"
      },
      "source": [
        "# Create the baseline model\n",
        "baseline_model = model_builder()\n",
        "\n",
        "# Save the initial weights for use later\n",
        "baseline_model.save_weights(FILE_WEIGHTS)\n",
        "\n",
        "# Print the model summary\n",
        "baseline_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xViB61FuY0Pf"
      },
      "source": [
        "# Setup the model for training\n",
        "baseline_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "baseline_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQSVh1_t4Z2h"
      },
      "source": [
        "# Get the baseline accuracy\n",
        "_, ACCURACY['baseline Keras model'] = baseline_model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A8WPjzqLbH3"
      },
      "source": [
        "# Save the Keras model\n",
        "baseline_model.save(FILE_NON_QUANTIZED_H5, include_optimizer=False)\n",
        "\n",
        "# Save and get the model size\n",
        "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
        "\n",
        "# Print records so far\n",
        "print_metric(ACCURACY, \"test accuracy\")\n",
        "print_metric(MODEL_SIZE, \"model size in bytes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQYM0A0SgCNS"
      },
      "source": [
        "def convert_tflite(model, filename, quantize=False):\n",
        "  '''\n",
        "  Converts the model to TF Lite format and writes to a file\n",
        "\n",
        "  Args:\n",
        "    model (Keras model) - model to convert to TF Lite\n",
        "    filename (string) - string to use when saving the file\n",
        "    quantize (bool) - flag to indicate quantization\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  '''\n",
        "  \n",
        "  # Initialize the converter\n",
        "  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "  # Set for quantization if flag is set to True\n",
        "  if quantize:\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "  # Convert the model\n",
        "  tflite_model = converter.convert()\n",
        "\n",
        "  # Save the model.\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H61feiOZkcI"
      },
      "source": [
        "# Convert baseline model\n",
        "convert_tflite(baseline_model, FILE_NON_QUANTIZED_TFLITE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmlNGwbCBo8v"
      },
      "source": [
        "MODEL_SIZE['non quantized tflite'] = os.path.getsize(FILE_NON_QUANTIZED_TFLITE)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model size in bytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQFkh5ukiiZE"
      },
      "source": [
        "ACCURACY['non quantized tflite'] = evaluate_tflite_model(FILE_NON_QUANTIZED_TFLITE, test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CplCOws3jaB0"
      },
      "source": [
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdWNTJ2J1OpL"
      },
      "source": [
        "# Convert and quantize the baseline model\n",
        "convert_tflite(baseline_model, FILE_PT_QUANTIZED, quantize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTFHf4Rw1bCJ"
      },
      "source": [
        "# Get the model size\n",
        "MODEL_SIZE['post training quantized tflite'] = os.path.getsize(FILE_PT_QUANTIZED)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model size')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhEYoQ83-pT_"
      },
      "source": [
        "ACCURACY['post training quantized tflite'] = evaluate_tflite_model(FILE_PT_QUANTIZED, test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D0Srsjb_inn"
      },
      "source": [
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WSt6OQGoNAt"
      },
      "source": [
        "# Install the toolkit\n",
        "!pip install tensorflow_model_optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dGSpz0on2C4"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# method to quantize a Keras model\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# Define the model architecture.\n",
        "model_to_quantize = model_builder()\n",
        "\n",
        "# Reinitialize weights with saved file\n",
        "model_to_quantize.load_weights(FILE_WEIGHTS)\n",
        "\n",
        "# Quantize the model\n",
        "q_aware_model = quantize_model(model_to_quantize)\n",
        "\n",
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl4jbjllomDw"
      },
      "source": [
        "# Train the model\n",
        "q_aware_model.fit(train_images, train_labels, epochs=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7rOuwM_ozI_"
      },
      "source": [
        "# Reinitialize the dictionary\n",
        "ACCURACY = {}\n",
        "\n",
        "# Get the accuracy of the quantization aware trained model (not yet quantized)\n",
        "_, ACCURACY['quantization aware non-quantized'] = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6liE_Cp3rzAy"
      },
      "source": [
        "# Convert and quantize the model.\n",
        "convert_tflite(q_aware_model, FILE_QAT_QUANTIZED, quantize=True)\n",
        "\n",
        "# Get the accuracy of the quantized model\n",
        "ACCURACY['quantization aware quantized'] = evaluate_tflite_model(FILE_QAT_QUANTIZED, test_images, test_labels)\n",
        "print_metric(ACCURACY, 'test accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwvaMflTYNgo"
      },
      "source": [
        "# Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpqizJsKYPBA"
      },
      "source": [
        "# Get the pruning method\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = train_images.shape[0] * (1 - validation_split)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define pruning schedule.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "# Pass in the trained baseline model\n",
        "model_for_pruning = prune_low_magnitude(baseline_model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5ekdEBigB5l"
      },
      "source": [
        "# Preview model weights\n",
        "model_for_pruning.weights[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCz6PL371Bx"
      },
      "source": [
        "# Callback to update pruning wrappers at each step\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "]\n",
        "\n",
        "# Train and prune the model\n",
        "model_for_pruning.fit(train_images, train_labels,\n",
        "                  epochs=epochs, validation_split=validation_split,\n",
        "                  callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOK4TidJhXpT"
      },
      "source": [
        "# Preview model weights\n",
        "model_for_pruning.weights[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbfLhZv68vwc"
      },
      "source": [
        "# Remove pruning wrappers\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "model_for_export.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG6-aF9yiraG"
      },
      "source": [
        "# Preview model weights (index 1 earlier is now 0 because pruning wrappers were removed)\n",
        "model_for_export.weights[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjjDMqJCTjqz"
      },
      "source": [
        "# Save Keras model\n",
        "model_for_export.save(FILE_PRUNED_MODEL_H5, include_optimizer=False)\n",
        "\n",
        "# Get uncompressed model size of baseline and pruned models\n",
        "MODEL_SIZE = {}\n",
        "MODEL_SIZE['baseline h5'] = os.path.getsize(FILE_NON_QUANTIZED_H5)\n",
        "MODEL_SIZE['pruned non quantized h5'] = os.path.getsize(FILE_PRUNED_MODEL_H5)\n",
        "\n",
        "print_metric(MODEL_SIZE, 'model_size in bytes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWQ_AgiX_yiP"
      },
      "source": [
        "# Get compressed size of baseline and pruned models\n",
        "MODEL_SIZE = {}\n",
        "MODEL_SIZE['baseline h5'] = get_gzipped_model_size(FILE_NON_QUANTIZED_H5)\n",
        "MODEL_SIZE['pruned non quantized h5'] = get_gzipped_model_size(FILE_PRUNED_MODEL_H5)\n",
        "\n",
        "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIY6n9XWCvt5"
      },
      "source": [
        "# Convert and quantize the pruned model.\n",
        "pruned_quantized_tflite = convert_tflite(model_for_export, FILE_PRUNED_QUANTIZED_TFLITE, quantize=True)\n",
        "\n",
        "# Compress and get the model size\n",
        "MODEL_SIZE['pruned quantized tflite'] = get_gzipped_model_size(FILE_PRUNED_QUANTIZED_TFLITE)\n",
        "print_metric(MODEL_SIZE, \"gzipped model size in bytes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZBAdJmuWN0A"
      },
      "source": [
        "# Get accuracy of pruned Keras and TF Lite models\n",
        "ACCURACY = {}\n",
        "\n",
        "_, ACCURACY['pruned model h5'] = model_for_pruning.evaluate(test_images, test_labels)\n",
        "ACCURACY['pruned and quantized tflite'] = evaluate_tflite_model(FILE_PRUNED_QUANTIZED_TFLITE, test_images, test_labels)\n",
        "\n",
        "print_metric(ACCURACY, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh2G62DR0GEQ"
      },
      "source": [
        "# Dimensionality Reduction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7DtysD30vLM"
      },
      "source": [
        "# General use imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tF29GljKdR6"
      },
      "source": [
        "# Download zip file\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip\n",
        "\n",
        "# Unzip it\n",
        "!unzip HTRU2.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRje5zNoJFnN"
      },
      "source": [
        "# Load data into a pandas dataframe\n",
        "data = pd.read_csv(\"HTRU_2.csv\", names=['mean_ip', 'sd_ip', 'ec_ip', \n",
        "                                        'sw_ip', 'mean_dm', 'sd_dm', \n",
        "                                        'ec_dm', 'sw_dm', 'pulsar'])\n",
        "\n",
        "# Take a look at the data\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLYmitEkJlAj"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Split features from labels\n",
        "features = data[[col for col in data.columns if col != \"pulsar\"]]\n",
        "labels = data[\"pulsar\"]\n",
        "\n",
        "# Scale data\n",
        "robust_data = RobustScaler().fit_transform(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHBa-7I5J8h-"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Instantiate PCA without specifying number of components\n",
        "pca_all = PCA()\n",
        "\n",
        "# Fit to scaled data\n",
        "pca_all.fit(robust_data)\n",
        "\n",
        "# Save cumulative explained variance\n",
        "cum_var = (np.cumsum(pca_all.explained_variance_ratio_))\n",
        "n_comp = [i for i in range(1, pca_all.n_components_ + 1)]\n",
        "\n",
        "# Plot cumulative variance\n",
        "ax = sns.pointplot(x=n_comp, y=cum_var)\n",
        "ax.set(xlabel='number of  principal components', ylabel='cumulative explained variance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCj4yZkeK9-c"
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Instantiate PCA with 3 components\n",
        "pca_3 = PCA(3)\n",
        "\n",
        "# Fit to scaled data\n",
        "pca_3.fit(robust_data)\n",
        "\n",
        "# Transform scaled data\n",
        "data_3pc = pca_3.transform(robust_data)\n",
        "\n",
        "# Render the 3D plot\n",
        "fig = plt.figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "\n",
        "ax.scatter(data_3pc[:, 0], data_3pc[:, 1], data_3pc[:, 2], c=labels,\n",
        "           cmap=plt.cm.Set1, edgecolor='k', s=25, label=data['pulsar'])\n",
        "\n",
        "ax.legend([\"non-pulsars\"], fontsize=\"large\")\n",
        "\n",
        "ax.set_title(\"First three PCA directions\")\n",
        "ax.set_xlabel(\"1st principal component\")\n",
        "ax.w_xaxis.set_ticklabels([])\n",
        "ax.set_ylabel(\"2nd principal component\")\n",
        "ax.w_yaxis.set_ticklabels([])\n",
        "ax.set_zlabel(\"3rd principal component\")\n",
        "ax.w_zaxis.set_ticklabels([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKsZa_rjKEFU"
      },
      "source": [
        "# Instantiate PCA with 2 components\n",
        "pca_2 = PCA(2)\n",
        "\n",
        "# Fit and transform scaled data\n",
        "pca_2.fit(robust_data)\n",
        "data_2pc = pca_2.transform(robust_data)\n",
        "\n",
        "# Render the 2D plot\n",
        "ax = sns.scatterplot(x=data_2pc[:,0], \n",
        "                     y=data_2pc[:,1], \n",
        "                     hue=labels,\n",
        "                     palette=sns.color_palette(\"muted\", n_colors=2))\n",
        "\n",
        "ax.set(xlabel='1st principal component', ylabel='2nd principal component', title='First two PCA directions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvtQsotB3taQ"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load the digits dataset\n",
        "digits = load_digits()\n",
        "\n",
        "# Plot first digit\n",
        "image = digits.data[0].reshape((8, 8))\n",
        "plt.matshow(image, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn9KmwoXiYTl"
      },
      "source": [
        "# Save data into X variable\n",
        "X = digits.data\n",
        "\n",
        "# Normalize pixel values\n",
        "X = X/255\n",
        "\n",
        "# Print shapes of dataset and data points\n",
        "print(f\"Digits data has shape {X.shape}\\n\")\n",
        "print(f\"Each data point has shape {X[0].shape}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yavX0PsNppOp"
      },
      "source": [
        "image = X[0].reshape((8, 8))\n",
        "plt.matshow(image, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdyxjZtJo5aJ"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# Instantiate Truncated SVD with (original dimension - 1) components\n",
        "org_dim = X.shape[1]\n",
        "tsvd = TruncatedSVD(org_dim - 1)\n",
        "tsvd.fit(X)\n",
        "\n",
        "# Save cumulative explained variance\n",
        "cum_var = (np.cumsum(tsvd.explained_variance_ratio_))\n",
        "n_comp = [i for i in range(1, org_dim)]\n",
        "\n",
        "# Plot cumulative variance\n",
        "ax = sns.scatterplot(x=n_comp, y=cum_var)\n",
        "ax.set(xlabel='number of  components', ylabel='cumulative explained variance')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A46cvALvWha3"
      },
      "source": [
        "print(f\"Explained variance with 5 components: {float(cum_var[4:5])*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m5v4LRO2ELC"
      },
      "source": [
        "# Instantiate a Truncated SVD with 5 components\n",
        "tsvd = TruncatedSVD(n_components=5)\n",
        "\n",
        "# Get the transformed data\n",
        "X_tsvd = tsvd.fit_transform(X)\n",
        "\n",
        "# Print shapes of dataset and data points\n",
        "print(f\"Original data points have shape {X[0].shape}\\n\")\n",
        "print(f\"Transformed data points have shape {X_tsvd[0].shape}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uikZXMc3Ot0"
      },
      "source": [
        "image_reduced_5 = tsvd.inverse_transform(X_tsvd[0].reshape(1, -1))\n",
        "image_reduced_5 = image_reduced_5.reshape((8, 8))\n",
        "plt.matshow(image_reduced_5, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIkuX6vwqvMA"
      },
      "source": [
        "def image_given_components(n_components, verbose=True):\n",
        "  tsvd = TruncatedSVD(n_components=n_components)\n",
        "  X_tsvd = tsvd.fit_transform(X)\n",
        "  if verbose:\n",
        "    print(f\"Explained variance with {n_components} components: {float(tsvd.explained_variance_ratio_.sum())*100:.2f}%\\n\")\n",
        "  image = tsvd.inverse_transform(X_tsvd[0].reshape(1, -1))\n",
        "  image = image.reshape((8, 8))\n",
        "  return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC8nnFAEYN5D"
      },
      "source": [
        "image_reduced_32 = image_given_components(32)\n",
        "plt.matshow(image_reduced_32, cmap = 'gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgZhbQeRY2vV"
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "# Original image\n",
        "ax1 = fig.add_subplot(1,4,1)\n",
        "ax1.matshow(image, cmap = 'gray')\n",
        "ax1.title.set_text('Original')\n",
        "ax1.axis('off') \n",
        "\n",
        "# Using 32 components\n",
        "ax2 = fig.add_subplot(1,4,2)\n",
        "ax2.matshow(image_reduced_32, cmap = 'gray')\n",
        "ax2.title.set_text('32 components')\n",
        "ax2.axis('off') \n",
        "\n",
        "# Using 5 components\n",
        "ax3 = fig.add_subplot(1,4,3)\n",
        "ax3.matshow(image_reduced_5, cmap = 'gray')\n",
        "ax3.title.set_text('5 components')\n",
        "ax3.axis('off') \n",
        "\n",
        "# Using 1 components\n",
        "ax4 = fig.add_subplot(1,4,4)\n",
        "ax4.matshow(image_given_components(1), cmap = 'gray') # Change this parameter to see other representations\n",
        "ax4.title.set_text('1 component')\n",
        "ax4.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9btFVismWmz"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Download data\n",
        "data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Get the actual text data from the sklearn Bunch\n",
        "data = data.get(\"data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6toK2WNq8lX"
      },
      "source": [
        "print(f\"Data has {len(data)} elements.\\n\")\n",
        "print(f\"First 2 elements: \\n\")\n",
        "for n, d in enumerate(data[:2], start=1):\n",
        "  print(\"======\"*10)\n",
        "  print(f\"Element number {n}:\\n\\n{d}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k2cCmaftiPc"
      },
      "source": [
        "# Instantiate vectorizer setting dimensionality of data\n",
        "# The stop_words param refer to words (in english) that don't add much value to the content of the document and must be ommited\n",
        "vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
        "\n",
        "# Vectorize original data\n",
        "vect_data = vectorizer.fit_transform(data)\n",
        "\n",
        "\n",
        "# Print dimensionality\n",
        "print(f\"Data has shape {vect_data.shape} after vectorization.\")\n",
        "print(f\"Each data point has shape {vect_data[0].shape} after vectorization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc10PMmzmcp3"
      },
      "source": [
        "# Desired number of components\n",
        "n_comp = 5\n",
        "\n",
        "# Instantiate NMF with the desired number of components\n",
        "nmf = NMF(n_components=n_comp, random_state=42)\n",
        "\n",
        "# Apply NMF to the vectorized data\n",
        "nmf.fit(vect_data)\n",
        "\n",
        "reduced_vect_data = nmf.transform(vect_data)\n",
        "\n",
        "# Print dimensionality\n",
        "print(f\"Data has shape {reduced_vect_data.shape} after NMF.\")\n",
        "print(f\"Each data point has shape {reduced_vect_data[0].shape} after NMF.\")\n",
        "\n",
        "# Save feature names for plotting\n",
        "feature_names = vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6win8Aii1mJm"
      },
      "source": [
        "print(f\"Original text:\\n{data[0]}\\n\")\n",
        "\n",
        "print(f\"Representation based on topics:\\n{reduced_vect_data[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ehHXYtlrVBl"
      },
      "source": [
        "# Define function for plotting top 20 words for each topic\n",
        "def plot_words_for_topics(n_comp, nmf, feature_names):\n",
        "  fig, axes = plt.subplots(((n_comp-1)//5)+1, 5, figsize=(25, 15))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  for num_topic, topic in enumerate(nmf.components_, start=1):\n",
        "\n",
        "    # Plot only the top 20 words\n",
        "\n",
        "    # Get the top 20 indexes\n",
        "    top_indexes = np.flip(topic.argsort()[-20:])\n",
        "\n",
        "    # Get the corresponding feature name\n",
        "    top_features = [feature_names[i] for i in top_indexes]\n",
        "\n",
        "    # Get the importance of each word\n",
        "    importance = topic[top_indexes]\n",
        "\n",
        "    # Plot a barplot\n",
        "    ax = axes[num_topic-1]\n",
        "    ax.barh(top_features, importance, color=\"green\")\n",
        "    ax.set_title(f\"Topic {num_topic}\", {\"fontsize\": 20})\n",
        "    ax.invert_yaxis()\n",
        "    ax.tick_params(labelsize=15)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "# Run the function\n",
        "plot_words_for_topics(n_comp, nmf, feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qeWwPAIsVEn"
      },
      "source": [
        "def try_NMF(n_comp):\n",
        "  nmf = NMF(n_components=n_comp, random_state=42)\n",
        "  nmf.fit(vect_data)\n",
        "  feature_names = vectorizer.get_feature_names()\n",
        "  plot_words_for_topics(n_comp, nmf, feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9tYNerssi0d"
      },
      "source": [
        "# Try different values!\n",
        "try_NMF(20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}