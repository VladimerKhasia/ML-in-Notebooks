{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TZ-PuwoyX4qQ"
      },
      "source": [
        "# Data Validation - Diabetic Data\n",
        "\n",
        "[Tensorflow Data Validation (TFDV)](https://cloud.google.com/solutions/machine-learning/analyzing-and-validating-data-at-scale-for-ml-using-tfx) \n",
        "\n",
        "Download files here: https://drive.google.com/file/d/1n8x0UhaadEfyixUCBoyXrgkiwniIm3ZM/view?usp=sharing\n",
        "\n",
        "[Diabetes 130-US hospitals for years 1999-2008 Data Set](https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv-O75eG8W0r"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow_data_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "zrLPRsQgImel",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tempfile, urllib, zipfile\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from tensorflow_data_validation.utils import slicing_util\n",
        "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics\n",
        "\n",
        "# Set TF's logger to only display errors to avoid internal warnings being shown\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "deletable": false,
        "editable": false,
        "id": "YyO3RSuLF0Nf",
        "outputId": "57bcdedc-f931-4a08-eab7-02a5884411d4",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Read CSV data into a dataframe and recognize the missing data that is encoded with '?' string as NaN\n",
        "df = pd.read_csv('./diabetic_data.csv', header=0, na_values = '?')\n",
        "\n",
        "# Preview the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Tv1I6Dd2IS5J",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def prepare_data_splits_from_dataframe(df):\n",
        "    '''\n",
        "    Splits a Pandas Dataframe into training, evaluation and serving sets.\n",
        "\n",
        "    Parameters:\n",
        "            df : pandas dataframe to split\n",
        "\n",
        "    Returns:\n",
        "            train_df: Training dataframe(70% of the entire dataset)\n",
        "            eval_df: Evaluation dataframe (15% of the entire dataset) \n",
        "            serving_df: Serving dataframe (15% of the entire dataset, label column dropped)\n",
        "    '''\n",
        "    \n",
        "    # 70% of records for generating the training set\n",
        "    train_len = int(len(df) * 0.7)\n",
        "    \n",
        "    # Remaining 30% of records for generating the evaluation and serving sets\n",
        "    eval_serv_len = len(df) - train_len\n",
        "    \n",
        "    # Half of the 30%, which makes up 15% of total records, for generating the evaluation set\n",
        "    eval_len = eval_serv_len // 2\n",
        "    \n",
        "    # Remaining 15% of total records for generating the serving set\n",
        "    serv_len = eval_serv_len - eval_len \n",
        " \n",
        "    # Split the dataframe into the three subsets\n",
        "    train_df = df.iloc[:train_len].reset_index(drop=True)\n",
        "    eval_df = df.iloc[train_len: train_len + eval_len].reset_index(drop=True)\n",
        "    serving_df = df.iloc[train_len + eval_len: train_len + eval_len + serv_len].reset_index(drop=True)\n",
        " \n",
        "    # Serving data emulates the data that would be submitted for predictions, so it should not have the label column.\n",
        "    serving_df = serving_df.drop(['readmitted'], axis=1)\n",
        "\n",
        "    return train_df, eval_df, serving_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rJV6__uVhz0B",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Split the datasets\n",
        "train_df, eval_df, serving_df = prepare_data_splits_from_dataframe(df)\n",
        "print('Training dataset has {} records\\nValidation dataset has {} records\\nServing dataset has {} records'.format(len(train_df),len(eval_df),len(serving_df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "z4jKM0gyj8Qc",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Define features to remove\n",
        "features_to_remove = {'encounter_id', 'patient_nbr'}\n",
        "\n",
        "# Collect features to include while computing the statistics\n",
        "approved_cols = [col for col in df.columns if (col not in features_to_remove)]\n",
        "\n",
        "# Instantiate a StatsOptions class and define the feature_allowlist property\n",
        "stats_options = tfdv.StatsOptions(feature_allowlist=approved_cols)\n",
        "\n",
        "# Review the features to generate the statistics\n",
        "for feature in stats_options.feature_allowlist:\n",
        "    print(feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "EE481oMbT-H0",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "train_stats = tfdv.generate_statistics_from_dataframe(train_df, stats_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Nxuwq8_44bqx",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# TEST CODE\n",
        "\n",
        "# get the number of features used to compute statistics\n",
        "print(f\"Number of features used: {len(train_stats.datasets[0].features)}\")\n",
        "\n",
        "# check the number of examples used\n",
        "print(f\"Number of examples used: {train_stats.datasets[0].num_examples}\")\n",
        "\n",
        "# check the column names of the first and last feature\n",
        "print(f\"First feature: {train_stats.datasets[0].features[0].path.step[0]}\")\n",
        "print(f\"Last feature: {train_stats.datasets[0].features[-1].path.step[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "graded": true,
        "id": "U3tUKgh7Up3x",
        "name": "train_stats_visualize_statistics",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "tfdv.visualize_statistics(train_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "6LLkRJThVr9m",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Infer the data schema by using the training statistics that you generated\n",
        "schema = tfdv.infer_schema(train_stats)\n",
        "\n",
        "# Display the data schema\n",
        "tfdv.display_schema(schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3RwTy8uC4bqz",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# TEST CODE\n",
        "\n",
        "# Check number of features\n",
        "print(f\"Number of features in schema: {len(schema.feature)}\")\n",
        "\n",
        "# Check domain name of 2nd feature\n",
        "print(f\"Second feature in schema: {list(schema.feature)[1].domain}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "graded": true,
        "id": "j_P0RLYlV6XG",
        "name": "eval_stats_visualize_statistics",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Generate evaluation dataset statistics\n",
        "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df, stats_options=stats_options)\n",
        "\n",
        "# Compare evaluation data with training data \n",
        "tfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n",
        "                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "RBXElxxX4bq1",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# TEST CODE\n",
        "\n",
        "# get the number of features used to compute statistics\n",
        "print(f\"Number of features: {len(eval_stats.datasets[0].features)}\")\n",
        "\n",
        "# check the number of examples used\n",
        "print(f\"Number of examples: {eval_stats.datasets[0].num_examples}\")\n",
        "\n",
        "# check the column names of the first and last feature\n",
        "print(f\"First feature: {eval_stats.datasets[0].features[0].path.step[0]}\")\n",
        "print(f\"Last feature: {eval_stats.datasets[0].features[-1].path.step[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "frtyWGid4bq2",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "train_df[\"glimepiride-pioglitazone\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "uCZKEq5F4bq2",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "eval_df[\"glimepiride-pioglitazone\"].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfaf6Tnp4bq3"
      },
      "source": [
        "It is possible but highly inefficient to visually inspect and determine all the anomalies. So, let's instead use TFDV functions to detect and display these.\n",
        "\n",
        "You can use the function [`tfdv.validate_statistics()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/validate_statistics) for detecting anomalies and [`tfdv.display_anomalies()`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_anomalies) for displaying them.\n",
        "\n",
        "The `validate_statistics()` method has two required arguments:\n",
        "- an instance of `DatasetFeatureStatisticsList`\n",
        "- an instance of `Schema`\n",
        "\n",
        "Fill in the following graded function which, given the statistics and schema, displays the anomalies found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "QBUX-ocHs5NK",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def calculate_and_display_anomalies(statistics, schema):\n",
        "    '''\n",
        "    Calculate and display anomalies.\n",
        "\n",
        "            Parameters:\n",
        "                    statistics : Data statistics in statistics_pb2.DatasetFeatureStatisticsList format\n",
        "                    schema : Data schema in schema_pb2.Schema format\n",
        "\n",
        "            Returns:\n",
        "                    display of calculated anomalies\n",
        "    '''\n",
        "    anomalies = tfdv.validate_statistics(statistics, schema)\n",
        "    tfdv.display_anomalies(anomalies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vyjwFU54bq3"
      },
      "source": [
        "You should see detected anomalies in the `medical_specialty` and `glimepiride-pioglitazone` features by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "graded": true,
        "id": "T7uGVeL2WOam",
        "name": "calculate_and_display_anomalies",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Check evaluation data for errors by validating the evaluation data staticss using the previously inferred schema\n",
        "calculate_and_display_anomalies(eval_stats, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "legN2nXLWZAc",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Get the domain associated with the input feature, glimepiride-pioglitazone, from the schema\n",
        "glimepiride_pioglitazone_domain = tfdv.get_domain(schema, 'glimepiride-pioglitazone') \n",
        "\n",
        "# HINT: Append the missing value 'Steady' to the domain\n",
        "glimepiride_pioglitazone_domain.value.append('Steady')\n",
        "\n",
        "# Get the domain associated with the input feature, medical_specialty, from the schema\n",
        "medical_specialty_domain = tfdv.get_domain(schema, 'medical_specialty') \n",
        "\n",
        "# HINT: Append the missing value 'Neurophysiology' to the domain\n",
        "medical_specialty_domain.value.append('Neurophysiology')\n",
        "\n",
        "# HINT: Re-calculate and re-display anomalies with the new schema\n",
        "calculate_and_display_anomalies(eval_stats, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "hU-hw2Rn4bq5",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Define a new statistics options by the tfdv.StatsOptions class for the serving data by passing the previously inferred schema\n",
        "options = tfdv.StatsOptions(schema=schema, \n",
        "                            infer_type_from_schema=True, \n",
        "                            feature_allowlist=approved_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "OhtYF8aAczpd",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "serving_stats = tfdv.generate_statistics_from_dataframe(serving_df, stats_options=options)\n",
        "calculate_and_display_anomalies(serving_stats, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "IYDz09xi4bq6",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# This relaxes the minimum fraction of values that must come from the domain for the feature.\n",
        "\n",
        "# Get the feature and relax to match 90% of the domain\n",
        "payer_code = tfdv.get_feature(schema, 'payer_code')\n",
        "payer_code.distribution_constraints.min_domain_mass = 0.9 \n",
        "\n",
        "# Get the feature and relax to match 90% of the domain\n",
        "medical_specialty = tfdv.get_feature(schema, 'medical_specialty')\n",
        "medical_specialty.distribution_constraints.min_domain_mass = 0.9 \n",
        "\n",
        "# Detect anomalies with the updated constraints\n",
        "calculate_and_display_anomalies(serving_stats, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SVTwkWEv4bq7",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "tfdv.display_schema(schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "1ACLvPPTkZfp",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def modify_domain_of_features(features_list, schema, to_domain_name):\n",
        "    '''\n",
        "    Modify a list of features' domains.\n",
        "\n",
        "            Parameters:\n",
        "                    features_list : Features that need to be modified\n",
        "                    schema: Inferred schema\n",
        "                    to_domain_name : Target domain to be transferred to the features list\n",
        "\n",
        "            Returns:\n",
        "                    schema: new schema\n",
        "    '''\n",
        "    for feature in features_list:\n",
        "        tfdv.set_domain(schema, feature, to_domain_name)\n",
        "\n",
        "    return schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4_jNanzjfeS-",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "domain_change_features = ['repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
        "                          'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
        "                          'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', \n",
        "                          'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', \n",
        "                          'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone']\n",
        "\n",
        "\n",
        "# Infer new schema by using your modify_domain_of_features function \n",
        "# and the defined domain_change_features feature list\n",
        "schema = modify_domain_of_features(domain_change_features, schema, 'metformin')\n",
        "\n",
        "# Display new schema\n",
        "tfdv.display_schema(schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9UVzpDAm4bq8",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# TEST CODE\n",
        "\n",
        "# check that the domain of some features are now switched to `metformin`\n",
        "print(f\"Domain name of 'chlorpropamide': {tfdv.get_feature(schema, 'chlorpropamide').domain}\")\n",
        "print(f\"Domain values of 'chlorpropamide': {tfdv.get_domain(schema, 'chlorpropamide').value}\")\n",
        "print(f\"Domain name of 'repaglinide': {tfdv.get_feature(schema, 'repaglinide').domain}\")\n",
        "print(f\"Domain values of 'repaglinide': {tfdv.get_domain(schema, 'repaglinide').value}\")\n",
        "print(f\"Domain name of 'nateglinide': {tfdv.get_feature(schema, 'nateglinide').domain}\")\n",
        "print(f\"Domain values of 'nateglinide': {tfdv.get_domain(schema, 'nateglinide').value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "X0bOiySo4bq9",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "calculate_and_display_anomalies(serving_stats, schema=schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "BRR2enfj4bq-",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# All features are by default in both TRAINING and SERVING environments.\n",
        "schema.default_environment.append('TRAINING')\n",
        "schema.default_environment.append('SERVING')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "bnbnw8H6Lp2M",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Specify that 'readmitted' feature is not in SERVING environment.\n",
        "tfdv.get_feature(schema, 'readmitted').not_in_environment.append('SERVING')\n",
        "\n",
        "# inferred schema and the SERVING environment parameter.\n",
        "serving_anomalies_with_env = tfdv.validate_statistics(serving_stats, schema, environment='SERVING')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "epUYxaTk4bq_",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Display anomalies\n",
        "tfdv.display_anomalies(serving_anomalies_with_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wEUsZm_rOd1Q",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Calculate skew for the diabetesMed feature\n",
        "diabetes_med = tfdv.get_feature(schema, 'diabetesMed')\n",
        "diabetes_med.skew_comparator.infinity_norm.threshold = 0.03 # domain knowledge helps to determine this threshold\n",
        "\n",
        "# Calculate drift for the payer_code feature\n",
        "payer_code = tfdv.get_feature(schema, 'payer_code')\n",
        "payer_code.drift_comparator.infinity_norm.threshold = 0.03 # domain knowledge helps to determine this threshold\n",
        "\n",
        "# Calculate anomalies\n",
        "skew_drift_anomalies = tfdv.validate_statistics(train_stats, schema,\n",
        "                                          previous_statistics=eval_stats,\n",
        "                                          serving_statistics=serving_stats)\n",
        "\n",
        "# Display anomalies\n",
        "tfdv.display_anomalies(skew_drift_anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9ia3Cdgn4brB",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def split_datasets(dataset_list):\n",
        "    '''\n",
        "    split datasets.\n",
        "\n",
        "            Parameters:\n",
        "                    dataset_list: List of datasets to split\n",
        "\n",
        "            Returns:\n",
        "                    datasets: sliced data\n",
        "    '''\n",
        "    datasets = []\n",
        "    for dataset in dataset_list.datasets:\n",
        "        proto_list = DatasetFeatureStatisticsList()\n",
        "        proto_list.datasets.extend([dataset])\n",
        "        datasets.append(proto_list)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def display_stats_at_index(index, datasets):\n",
        "    '''\n",
        "    display statistics at the specified data index\n",
        "\n",
        "            Parameters:\n",
        "                    index : index to show the anomalies\n",
        "                    datasets: split data\n",
        "\n",
        "            Returns:\n",
        "                    display of generated sliced data statistics at the specified index\n",
        "    '''\n",
        "    if index < len(datasets):\n",
        "        print(datasets[index].datasets[0].name)\n",
        "        tfdv.visualize_statistics(datasets[index])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5S9EOdt1UbMF",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "def sliced_stats_for_slice_fn(slice_fn, approved_cols, dataframe, schema):\n",
        "    '''\n",
        "    generate statistics for the sliced data.\n",
        "\n",
        "            Parameters:\n",
        "                    slice_fn : slicing definition\n",
        "                    approved_cols: list of features to pass to the statistics options\n",
        "                    dataframe: pandas dataframe to slice\n",
        "                    schema: the schema\n",
        "\n",
        "            Returns:\n",
        "                    slice_info_datasets: statistics for the sliced dataset\n",
        "    '''\n",
        "    # Set the StatsOptions\n",
        "    slice_stats_options = tfdv.StatsOptions(schema=schema,\n",
        "                                            slice_functions=[slice_fn],\n",
        "                                            infer_type_from_schema=True,\n",
        "                                            feature_allowlist=approved_cols)\n",
        "    \n",
        "    # Convert Dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv`\n",
        "    CSV_PATH = 'slice_sample.csv'\n",
        "    dataframe.to_csv(CSV_PATH)\n",
        "    \n",
        "    # Calculate statistics for the sliced dataset\n",
        "    sliced_stats = tfdv.generate_statistics_from_csv(CSV_PATH, stats_options=slice_stats_options)\n",
        "    \n",
        "    # Split the dataset using the previously defined split_datasets function\n",
        "    slice_info_datasets = split_datasets(sliced_stats)\n",
        "    \n",
        "    return slice_info_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "F1_tkHX_UxwJ",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Generate slice function for the `medical_speciality` feature\n",
        "slice_fn = slicing_util.get_feature_value_slicer(features={'medical_specialty': None})\n",
        "\n",
        "# Generate stats for the sliced dataset\n",
        "slice_datasets = sliced_stats_for_slice_fn(slice_fn, approved_cols, dataframe=train_df, schema=schema)\n",
        "\n",
        "# Print name of slices for reference\n",
        "print(f'Statistics generated for:\\n')\n",
        "print('\\n'.join([sliced.datasets[0].name for sliced in slice_datasets]))\n",
        "\n",
        "# Display at index 10, which corresponds to the slice named `medical_specialty_Gastroenterology`\n",
        "display_stats_at_index(10, slice_datasets) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ydkL4DkIWn18",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "OUTPUT_DIR = \"output\"\n",
        "file_io.recursive_create_dir(OUTPUT_DIR)\n",
        "\n",
        "# Use TensorFlow text output format pbtxt to store the schema\n",
        "schema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\n",
        "\n",
        "# write_schema_text function expect the defined schema and output path as parameters\n",
        "tfdv.write_schema_text(schema, schema_file) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2V_OLlgtQ0DW"
      },
      "source": [
        "# Tensorflow Data Validation (TFDV) - Income Data\n",
        "\n",
        "[Census Income Dataset](http://archive.ics.uci.edu/ml/datasets/Census+Income)\n",
        "\n",
        "[in this data description file.](https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHi-tkOeBOis",
        "outputId": "65be1429-8f42-401e-af36-d951a678ee02"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "print('TFDV Version: {}'.format(tfdv.__version__))\n",
        "print('Tensorflow Version: {}'.format(tf.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKTfuT2rga-_"
      },
      "outputs": [],
      "source": [
        "# Read in the training and evaluation datasets\n",
        "df = pd.read_csv('./adult.data', skipinitialspace=True)\n",
        "\n",
        "# Split the dataset. Do not shuffle for this demo notebook.\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDFD6dR0PYH"
      },
      "outputs": [],
      "source": [
        "# Preview the train set\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyrmQLCm0a5V"
      },
      "outputs": [],
      "source": [
        "# Preview the eval set\n",
        "eval_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "AgGlCxVlE5HN"
      },
      "outputs": [],
      "source": [
        "#@title helper function\n",
        "\n",
        "def add_extra_rows(df):\n",
        "    rows = [\n",
        "        {\n",
        "            'age': 46, \n",
        "            'fnlwgt': 257473, \n",
        "            'education': 'Bachelors', \n",
        "            'education-num': 8,\n",
        "            'marital-status': 'Married-civ-spouse', \n",
        "            'occupation': 'Plumber', \n",
        "            'relationship': 'Husband', \n",
        "            'race': 'Other', \n",
        "            'sex': 'Male',\n",
        "            'capital-gain': 1000, \n",
        "            'capital-loss': 0, \n",
        "            'hours-per-week': 41, \n",
        "            'native-country': 'Australia',\n",
        "            'label': '>50K'\n",
        "        },\n",
        "        {\n",
        "            'age': 0, \n",
        "            'workclass': 'Private', \n",
        "            'fnlwgt': 257473, \n",
        "            'education': 'Masters', \n",
        "            'education-num': 8,\n",
        "            'marital-status': 'Married-civ-spouse', \n",
        "            'occupation': 'Adm-clerical', \n",
        "            'relationship': 'Wife', \n",
        "            'race': 'Asian', \n",
        "            'sex': 'Female',\n",
        "            'capital-gain': 0, \n",
        "            'capital-loss': 0, \n",
        "            'hours-per-week': 40, \n",
        "            'native-country': 'Pakistan',\n",
        "            'label': '>50K'\n",
        "        },\n",
        "        {\n",
        "            'age': 1000, \n",
        "            'workclass': 'Private', \n",
        "            'fnlwgt': 257473, \n",
        "            'education': 'Masters', \n",
        "            'education-num': 8,\n",
        "            'marital-status': 'Married-civ-spouse', \n",
        "            'occupation': 'Prof-specialty', \n",
        "            'relationship': 'Husband', \n",
        "            'race': 'Black', \n",
        "            'sex': 'Male',\n",
        "            'capital-gain': 0, \n",
        "            'capital-loss': 0, \n",
        "            'hours-per-week': 20, \n",
        "            'native-country': 'Cameroon',\n",
        "            'label': '<=50K'\n",
        "        },\n",
        "        {\n",
        "            'age': 25, \n",
        "            'workclass': '?', \n",
        "            'fnlwgt': 257473, \n",
        "            'education': 'Masters', \n",
        "            'education-num': 8,\n",
        "            'marital-status': 'Married-civ-spouse', \n",
        "            'occupation': 'gamer', \n",
        "            'relationship': 'Husband', \n",
        "            'race': 'Asian', \n",
        "            'sex': 'Female',\n",
        "            'capital-gain': 0, \n",
        "            'capital-loss': 0, \n",
        "            'hours-per-week': 50, \n",
        "            'native-country': 'Mongolia',\n",
        "            'label': '<=50K'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    df = df.append(rows, ignore_index=True)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjDCBfv4Dkvp"
      },
      "outputs": [],
      "source": [
        "# add extra rows\n",
        "eval_df = add_extra_rows(eval_df)\n",
        "\n",
        "# preview the added rows\n",
        "eval_df.tail(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vTx9Qkk4yGc"
      },
      "outputs": [],
      "source": [
        "# Generate training dataset statistics\n",
        "train_stats = tfdv.generate_statistics_from_dataframe(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1D1wP3mm5ebW"
      },
      "outputs": [],
      "source": [
        "# Visualize training dataset statistics\n",
        "tfdv.visualize_statistics(train_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9skjM-M44Jz"
      },
      "outputs": [],
      "source": [
        "# Infer schema from the computed statistics.\n",
        "schema = tfdv.infer_schema(statistics=train_stats)\n",
        "\n",
        "# Display the inferred schema\n",
        "tfdv.display_schema(schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzZy1x3c6Mi0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Generate evaluation dataset statistics\n",
        "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df)\n",
        "\n",
        "# Compare training with evaluation\n",
        "tfdv.visualize_statistics(\n",
        "    lhs_statistics=eval_stats, \n",
        "    rhs_statistics=train_stats, \n",
        "    lhs_name='EVAL_DATASET', \n",
        "    rhs_name='TRAIN_DATASET'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmkvVddLDkvu"
      },
      "outputs": [],
      "source": [
        "# filter the age range\n",
        "eval_df = eval_df[eval_df['age'] > 16]\n",
        "eval_df = eval_df[eval_df['age'] < 91]\n",
        "\n",
        "# drop missing values\n",
        "eval_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcnxvOCuDkvu"
      },
      "outputs": [],
      "source": [
        "# Generate evaluation dataset statistics\n",
        "eval_stats = tfdv.generate_statistics_from_dataframe(eval_df)\n",
        "\n",
        "# Compare training with evaluation\n",
        "tfdv.visualize_statistics(\n",
        "    lhs_statistics=eval_stats, \n",
        "    rhs_statistics=train_stats, \n",
        "    lhs_name='EVAL_DATASET', \n",
        "    rhs_name='TRAIN_DATASET'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR5dBqpW6ky2"
      },
      "outputs": [],
      "source": [
        "# Check evaluation data for errors by validating the evaluation dataset statistics using the reference schema\n",
        "anomalies =  tfdv.validate_statistics(statistics=eval_stats, schema=schema)\n",
        "\n",
        "# Visualize anomalies\n",
        "tfdv.display_anomalies(anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae9eX1W6Dkvw"
      },
      "outputs": [],
      "source": [
        "# Relax the minimum fraction of values that must come from the domain for the feature `native-country`\n",
        "country_feature = tfdv.get_feature(schema, 'native-country')\n",
        "country_feature.distribution_constraints.min_domain_mass = 0.9\n",
        "\n",
        "# Relax the minimum fraction of values that must come from the domain for the feature `occupation`\n",
        "occupation_feature = tfdv.get_feature(schema, 'occupation')\n",
        "occupation_feature.distribution_constraints.min_domain_mass = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTCWS04p6lDh"
      },
      "outputs": [],
      "source": [
        "# Add new value to the domain of the feature `race`\n",
        "race_domain = tfdv.get_domain(schema, 'race')\n",
        "race_domain.value.append('Asian')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFjAlaiqDkvx"
      },
      "outputs": [],
      "source": [
        "# Restrict the range of the `age` feature\n",
        "tfdv.set_domain(schema, 'age', schema_pb2.IntDomain(name='age', min=17, max=90))\n",
        "\n",
        "# Display the modified schema. Notice the `Domain` column of `age`.\n",
        "tfdv.display_schema(schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 57
        },
        "id": "d-vA3brqDkvx",
        "outputId": "da440158-5f03-4678-ad7c-aaa1144a05c9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<h4 style=\"color:green;\">No anomalies found.</h4>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Validate eval stats after updating the schema \n",
        "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
        "tfdv.display_anomalies(updated_anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DD07MsSDkvy"
      },
      "outputs": [],
      "source": [
        "from tensorflow_data_validation.utils import slicing_util\n",
        "\n",
        "slice_fn = slicing_util.get_feature_value_slicer(features={'sex': None})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1_EriOcDkvz"
      },
      "outputs": [],
      "source": [
        "# Declare stats options\n",
        "slice_stats_options = tfdv.StatsOptions(schema=schema,\n",
        "                                        slice_functions=[slice_fn],\n",
        "                                        infer_type_from_schema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBn1DS8WDkvz"
      },
      "outputs": [],
      "source": [
        "# Convert dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv`\n",
        "CSV_PATH = 'slice_sample.csv'\n",
        "train_df.to_csv(CSV_PATH)\n",
        "\n",
        "# Calculate statistics for the sliced dataset\n",
        "sliced_stats = tfdv.generate_statistics_from_csv(CSV_PATH, stats_options=slice_stats_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHUVrOxxDkv0",
        "outputId": "571dfff7-ce81-45d9-ba7b-b926cedafbb4"
      },
      "outputs": [],
      "source": [
        "print(f'Datasets generated: {[sliced.name for sliced in sliced_stats.datasets]}')\n",
        "\n",
        "print(f'Type of sliced_stats elements: {type(sliced_stats.datasets[0])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "RznOGKnwDkv0",
        "outputId": "bd2d07ba-b5a3-4bfc-d72d-883b484d5afc"
      },
      "outputs": [],
      "source": [
        "from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList\n",
        "\n",
        "# Convert `Male` statistics (index=1) to the correct type and get the dataset name\n",
        "male_stats_list = DatasetFeatureStatisticsList()\n",
        "male_stats_list.datasets.extend([sliced_stats.datasets[1]])\n",
        "male_stats_name = sliced_stats.datasets[1].name\n",
        "\n",
        "# Convert `Female` statistics (index=2) to the correct type and get the dataset name\n",
        "female_stats_list = DatasetFeatureStatisticsList()\n",
        "female_stats_list.datasets.extend([sliced_stats.datasets[2]])\n",
        "female_stats_name = sliced_stats.datasets[2].name\n",
        "\n",
        "# Visualize the two slices side by side\n",
        "tfdv.visualize_statistics(\n",
        "    lhs_statistics=male_stats_list,\n",
        "    rhs_statistics=female_stats_list,\n",
        "    lhs_name=male_stats_name,\n",
        "    rhs_name=female_stats_name\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "TZ-PuwoyX4qQ",
        "2V_OLlgtQ0DW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
