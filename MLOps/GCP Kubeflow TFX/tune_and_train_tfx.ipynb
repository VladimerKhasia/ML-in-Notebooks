{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [],
      "source": [
        "!pip install --use-deprecated=legacy-resolver tfx==1.3.0\n",
        "!pip install apache-beam==2.32.0\n",
        "\n",
        "# These are downgraded to work with the packages used by TFX 1.3\n",
        "!pip install tensorflow==2.6.0\n",
        "!pip install tensorflow-serving-api==2.6.0\n",
        "!pip install --upgrade tensorflow-estimator==2.6.0\n",
        "!pip install --upgrade keras==2.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_leAIdFKAxAD"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "\n",
        "from tfx.components import ImportExampleGen\n",
        "from tfx.components import ExampleValidator\n",
        "from tfx.components import SchemaGen\n",
        "from tfx.components import StatisticsGen\n",
        "from tfx.components import Transform\n",
        "from tfx.components import Tuner\n",
        "from tfx.components import Trainer\n",
        "\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNQlwf5_t8Fc"
      },
      "outputs": [],
      "source": [
        "# Location of the pipeline metadata store\n",
        "_pipeline_root = './pipeline/'\n",
        "\n",
        "# Directory of the raw data files\n",
        "_data_root = './data/fmnist'\n",
        "\n",
        "# Temporary directory\n",
        "tempdir = './tempdir'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqwtVwAsslgN"
      },
      "outputs": [],
      "source": [
        "# Create the dataset directory\n",
        "!mkdir -p {_data_root}\n",
        "\n",
        "# Create the TFX pipeline files directory\n",
        "!mkdir {_pipeline_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUzvq3WFvKyl"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "ds, ds_info = tfds.load('fashion_mnist', data_dir=tempdir, with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74BnhUcG1A-x"
      },
      "outputs": [],
      "source": [
        "# Display info about the dataset\n",
        "print(ds_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A501bxQd1Qxo"
      },
      "outputs": [],
      "source": [
        "# Define the location of the train tfrecord downloaded via TFDS\n",
        "tfds_data_path = f'{tempdir}/{ds_info.name}/{ds_info.version}'\n",
        "\n",
        "# Display contents of the TFDS data directory\n",
        "os.listdir(tfds_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49ZklvN8d64e"
      },
      "outputs": [],
      "source": [
        "# Define the train tfrecord filename\n",
        "train_filename = 'fashion_mnist-train.tfrecord-00000-of-00001'\n",
        "\n",
        "# Copy the train tfrecord into the data root folder\n",
        "!cp {tfds_data_path}/{train_filename} {_data_root}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeCZ5mAvvlD4"
      },
      "outputs": [],
      "source": [
        "# Initialize the InteractiveContext\n",
        "context = InteractiveContext(pipeline_root=_pipeline_root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xolw1d8lvqNW"
      },
      "outputs": [],
      "source": [
        "# Specify 80/20 split for the train and eval set\n",
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(splits=[\n",
        "        example_gen_pb2.SplitConfig.Split(name='train', hash_buckets=8),\n",
        "        example_gen_pb2.SplitConfig.Split(name='eval', hash_buckets=2),\n",
        "    ]))\n",
        "\n",
        "# Ingest the data through ExampleGen\n",
        "example_gen = ImportExampleGen(input_base=_data_root, output_config=output)\n",
        "\n",
        "# Run the component\n",
        "context.run(example_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIdWfRWGxvHp"
      },
      "outputs": [],
      "source": [
        "# Print split names and URI\n",
        "artifact = example_gen.outputs['examples'].get()[0]\n",
        "print(artifact.split_names, artifact.uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVDS4oEIzZ83"
      },
      "outputs": [],
      "source": [
        "# Run StatisticsGen\n",
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs['examples'])\n",
        "\n",
        "context.run(statistics_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UhV3Jr7zp7p"
      },
      "outputs": [],
      "source": [
        "# Run SchemaGen\n",
        "schema_gen = SchemaGen(\n",
        "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
        "context.run(schema_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtS2ZEgCzvAf"
      },
      "outputs": [],
      "source": [
        "# Visualize the results\n",
        "context.show(schema_gen.outputs['schema'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaTJiYPpzzZM"
      },
      "outputs": [],
      "source": [
        "# Run ExampleValidator\n",
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema'])\n",
        "context.run(example_validator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6YzedBSz5KE"
      },
      "outputs": [],
      "source": [
        "# Visualize the results. There should be no anomalies.\n",
        "context.show(example_validator.outputs['anomalies'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL4zrcJ7z9K9"
      },
      "outputs": [],
      "source": [
        "_transform_module_file = 'fmnist_transform.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43xmp2UD0Cc5"
      },
      "outputs": [],
      "source": [
        "%%writefile {_transform_module_file}\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "# Keys\n",
        "_LABEL_KEY = 'label'\n",
        "_IMAGE_KEY = 'image'\n",
        "\n",
        "\n",
        "def _transformed_name(key):\n",
        "    return key + '_xf'\n",
        "\n",
        "def _image_parser(image_str):\n",
        "    '''converts the images to a float tensor'''\n",
        "    image = tf.image.decode_image(image_str, channels=1)\n",
        "    image = tf.reshape(image, (28, 28, 1))\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    return image\n",
        "\n",
        "\n",
        "def _label_parser(label_id):\n",
        "    '''converts the labels to a float tensor'''\n",
        "    label = tf.cast(label_id, tf.float32)\n",
        "    return label\n",
        "\n",
        "\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
        "    Args:\n",
        "        inputs: map from feature keys to raw not-yet-transformed features.\n",
        "    Returns:\n",
        "        Map from string feature key to transformed feature operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert the raw image and labels to a float array\n",
        "    with tf.device(\"/cpu:0\"):\n",
        "        outputs = {\n",
        "            _transformed_name(_IMAGE_KEY):\n",
        "                tf.map_fn(\n",
        "                    _image_parser,\n",
        "                    tf.squeeze(inputs[_IMAGE_KEY], axis=1),\n",
        "                    dtype=tf.float32),\n",
        "            _transformed_name(_LABEL_KEY):\n",
        "                tf.map_fn(\n",
        "                    _label_parser,\n",
        "                    inputs[_LABEL_KEY],\n",
        "                    dtype=tf.float32)\n",
        "        }\n",
        "    \n",
        "    # scale the pixels from 0 to 1\n",
        "    outputs[_transformed_name(_IMAGE_KEY)] = tft.scale_to_0_1(outputs[_transformed_name(_IMAGE_KEY)])\n",
        "    \n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qthHA2hO1JST"
      },
      "outputs": [],
      "source": [
        "# Ignore TF warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# Setup the Transform component\n",
        "transform = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(_transform_module_file))\n",
        "\n",
        "# Run the component\n",
        "context.run(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aE1PLAs_6CVt"
      },
      "outputs": [],
      "source": [
        "# Declare name of module file\n",
        "_tuner_module_file = 'tuner.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0F-XhqVlUDB"
      },
      "outputs": [],
      "source": [
        "%%writefile {_tuner_module_file}\n",
        "\n",
        "# Define imports\n",
        "from kerastuner.engine import base_tuner\n",
        "import kerastuner as kt\n",
        "from tensorflow import keras\n",
        "from typing import NamedTuple, Dict, Text, Any, List\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs, DataAccessor\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "# Declare namedtuple field names\n",
        "TunerFnResult = NamedTuple('TunerFnResult', [('tuner', base_tuner.BaseTuner),\n",
        "                                             ('fit_kwargs', Dict[Text, Any])])\n",
        "\n",
        "# Label key\n",
        "LABEL_KEY = 'label_xf'\n",
        "\n",
        "# Callback for the search strategy\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  '''Load compressed dataset\n",
        "  \n",
        "  Args:\n",
        "    filenames - filenames of TFRecords to load\n",
        "\n",
        "  Returns:\n",
        "    TFRecordDataset loaded from the filenames\n",
        "  '''\n",
        "\n",
        "  # Load the dataset. Specify the compression type since it is saved as `.gz`\n",
        "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "  \n",
        "\n",
        "def _input_fn(file_pattern,\n",
        "              tf_transform_output,\n",
        "              num_epochs=None,\n",
        "              batch_size=32) -> tf.data.Dataset:\n",
        "  '''Create batches of features and labels from TF Records\n",
        "\n",
        "  Args:\n",
        "    file_pattern - List of files or patterns of file paths containing Example records.\n",
        "    tf_transform_output - transform output graph\n",
        "    num_epochs - Integer specifying the number of times to read through the dataset. \n",
        "            If None, cycles through the dataset forever.\n",
        "    batch_size - An int representing the number of records to combine in a single batch.\n",
        "\n",
        "  Returns:\n",
        "    A dataset of dict elements, (or a tuple of dict elements and label). \n",
        "    Each dict maps feature keys to Tensor or SparseTensor objects.\n",
        "  '''\n",
        "\n",
        "  # Get feature specification based on transform output\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "  \n",
        "  # Create batches of features and labels\n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      file_pattern=file_pattern,\n",
        "      batch_size=batch_size,\n",
        "      features=transformed_feature_spec,\n",
        "      reader=_gzip_reader_fn,\n",
        "      num_epochs=num_epochs,\n",
        "      label_key=LABEL_KEY)\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "  '''\n",
        "  Builds the model and sets up the hyperparameters to tune.\n",
        "\n",
        "  Args:\n",
        "    hp - Keras tuner object\n",
        "\n",
        "  Returns:\n",
        "    model with hyperparameters to tune\n",
        "  '''\n",
        "\n",
        "  # Initialize the Sequential API and start stacking the layers\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28, 1)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu', name='dense_1'))\n",
        "\n",
        "  # Add next layers\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "def tuner_fn(fn_args: FnArgs) -> TunerFnResult:\n",
        "  \"\"\"Build the tuner using the KerasTuner API.\n",
        "  Args:\n",
        "    fn_args: Holds args as name/value pairs.\n",
        "\n",
        "      - working_dir: working dir for tuning.\n",
        "      - train_files: List of file paths containing training tf.Example data.\n",
        "      - eval_files: List of file paths containing eval tf.Example data.\n",
        "      - train_steps: number of train steps.\n",
        "      - eval_steps: number of eval steps.\n",
        "      - schema_path: optional schema of the input data.\n",
        "      - transform_graph_path: optional transform graph produced by TFT.\n",
        "  \n",
        "  Returns:\n",
        "    A namedtuple contains the following:\n",
        "      - tuner: A BaseTuner that will be used for tuning.\n",
        "      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n",
        "                    model , e.g., the training and validation dataset. Required\n",
        "                    args depend on the above tuner's implementation.\n",
        "  \"\"\"\n",
        "\n",
        "  # Define tuner search strategy\n",
        "  tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory=fn_args.working_dir,\n",
        "                     project_name='kt_hyperband')\n",
        "\n",
        "  # Load transform output\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "\n",
        "  # Use _input_fn() to extract input features and labels from the train and val set\n",
        "  train_set = _input_fn(fn_args.train_files[0], tf_transform_output)\n",
        "  val_set = _input_fn(fn_args.eval_files[0], tf_transform_output)\n",
        "\n",
        "\n",
        "  return TunerFnResult(\n",
        "      tuner=tuner,\n",
        "      fit_kwargs={ \n",
        "          \"callbacks\":[stop_early],\n",
        "          'x': train_set,\n",
        "          'validation_data': val_set,\n",
        "          'steps_per_epoch': fn_args.train_steps,\n",
        "          'validation_steps': fn_args.eval_steps\n",
        "      }\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqVSc6sS5A1m"
      },
      "outputs": [],
      "source": [
        "from tfx.proto import trainer_pb2\n",
        "\n",
        "# Setup the Tuner component\n",
        "tuner = Tuner(\n",
        "    module_file=_tuner_module_file,\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train'], num_steps=500),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval'], num_steps=100)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdycQnAR7AvG"
      },
      "outputs": [],
      "source": [
        "# Run the component. This will take around 10 minutes to run.\n",
        "# When done, it will summarize the results and show the 10 best trials.\n",
        "context.run(tuner, enable_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abSJjDM2ipKS"
      },
      "outputs": [],
      "source": [
        "# Declare trainer module file\n",
        "_trainer_module_file = 'trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdgbwOFFihSg"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from tensorflow import keras\n",
        "from typing import NamedTuple, Dict, Text, Any, List\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs, DataAccessor\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft\n",
        "\n",
        "# Define the label key\n",
        "LABEL_KEY = 'label_xf'\n",
        "\n",
        "def _gzip_reader_fn(filenames):\n",
        "  '''Load compressed dataset\n",
        "  \n",
        "  Args:\n",
        "    filenames - filenames of TFRecords to load\n",
        "\n",
        "  Returns:\n",
        "    TFRecordDataset loaded from the filenames\n",
        "  '''\n",
        "\n",
        "  # Load the dataset. Specify the compression type since it is saved as `.gz`\n",
        "  return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        "  \n",
        "\n",
        "def _input_fn(file_pattern,\n",
        "              tf_transform_output,\n",
        "              num_epochs=None,\n",
        "              batch_size=32) -> tf.data.Dataset:\n",
        "  '''Create batches of features and labels from TF Records\n",
        "\n",
        "  Args:\n",
        "    file_pattern - List of files or patterns of file paths containing Example records.\n",
        "    tf_transform_output - transform output graph\n",
        "    num_epochs - Integer specifying the number of times to read through the dataset. \n",
        "            If None, cycles through the dataset forever.\n",
        "    batch_size - An int representing the number of records to combine in a single batch.\n",
        "\n",
        "  Returns:\n",
        "    A dataset of dict elements, (or a tuple of dict elements and label). \n",
        "    Each dict maps feature keys to Tensor or SparseTensor objects.\n",
        "  '''\n",
        "  transformed_feature_spec = (\n",
        "      tf_transform_output.transformed_feature_spec().copy())\n",
        "  \n",
        "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "      file_pattern=file_pattern,\n",
        "      batch_size=batch_size,\n",
        "      features=transformed_feature_spec,\n",
        "      reader=_gzip_reader_fn,\n",
        "      num_epochs=num_epochs,\n",
        "      label_key=LABEL_KEY)\n",
        "  \n",
        "  return dataset\n",
        "\n",
        "\n",
        "def model_builder(hp):\n",
        "  '''\n",
        "  Builds the model and sets up the hyperparameters to tune.\n",
        "\n",
        "  Args:\n",
        "    hp - Keras tuner object\n",
        "\n",
        "  Returns:\n",
        "    model with hyperparameters to tune\n",
        "  '''\n",
        "\n",
        "  # Initialize the Sequential API and start stacking the layers\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28, 1)))\n",
        "\n",
        "  # Get the number of units from the Tuner results\n",
        "  hp_units = hp.get('units')\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "\n",
        "  # Add next layers\n",
        "  model.add(keras.layers.Dropout(0.2))\n",
        "  model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "  # Get the learning rate from the Tuner results\n",
        "  hp_learning_rate = hp.get('learning_rate')\n",
        "\n",
        "  # Setup model for training\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  # Print the model summary\n",
        "  model.summary()\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n",
        "def run_fn(fn_args: FnArgs) -> None:\n",
        "  \"\"\"Defines and trains the model.\n",
        "  Args:\n",
        "    fn_args: Holds args as name/value pairs. Refer here for the complete attributes: \n",
        "    https://www.tensorflow.org/tfx/api_docs/python/tfx/components/trainer/fn_args_utils/FnArgs#attributes\n",
        "  \"\"\"\n",
        "\n",
        "  # Callback for TensorBoard\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "  \n",
        "  # Load transform output\n",
        "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "  \n",
        "  # Create batches of data good for 10 epochs\n",
        "  train_set = _input_fn(fn_args.train_files[0], tf_transform_output, 10)\n",
        "  val_set = _input_fn(fn_args.eval_files[0], tf_transform_output, 10)\n",
        "\n",
        "  # Load best hyperparameters\n",
        "  hp = fn_args.hyperparameters.get('values')\n",
        "\n",
        "  # Build the model\n",
        "  model = model_builder(hp)\n",
        "\n",
        "  # Train the model\n",
        "  model.fit(\n",
        "      x=train_set,\n",
        "      validation_data=val_set,\n",
        "      callbacks=[tensorboard_callback]\n",
        "      )\n",
        "  \n",
        "  # Save the model\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0JOuqSKGsoQ"
      },
      "outputs": [],
      "source": [
        "# Setup the Trainer component\n",
        "trainer = Trainer(\n",
        "    module_file=_trainer_module_file,\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    hyperparameters=tuner.outputs['best_hyperparameters'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwM2743um1w3"
      },
      "outputs": [],
      "source": [
        "# Run the component\n",
        "context.run(trainer, enable_cache=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQPZBkw_yl2i"
      },
      "outputs": [],
      "source": [
        "# Get artifact uri of trainer model output\n",
        "model_artifact_dir = trainer.outputs['model'].get()[0].uri\n",
        "\n",
        "# List subdirectories artifact uri\n",
        "print(f'contents of model artifact directory:{os.listdir(model_artifact_dir)}')\n",
        "\n",
        "# Define the model directory\n",
        "model_dir = os.path.join(model_artifact_dir, 'Format-Serving')\n",
        "\n",
        "# List contents of model directory\n",
        "print(f'contents of model directory: {os.listdir(model_dir)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPqoMMXv5NoY"
      },
      "outputs": [],
      "source": [
        "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {model_run_artifact_dir}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
