{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XgILznpPt5Cp"
      },
      "source": [
        "Data: https://drive.google.com/file/d/1pSiTEnXe9rToK5nzmAwe44feg2-xk8yu/view?usp=sharing\n",
        "\n",
        "http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZersTw6TH1Zj"
      },
      "outputs": [],
      "source": [
        "# for data processing and manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# scikit-learn modules for feature selection and model evaluation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE, SelectKBest, SelectFromModel, chi2, f_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# libraries for visualization\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DspE2DYYPpRp"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('./breast_cancer_data.csv')\n",
        "\n",
        "# Print datatypes\n",
        "print(df.dtypes)\n",
        "\n",
        "# Describe columns\n",
        "df.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzRp5ORgoqpn"
      },
      "outputs": [],
      "source": [
        "# Preview the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73EVqL6_N2-T"
      },
      "outputs": [],
      "source": [
        "# Check if there are null values in any of the columns. You will see `Unnamed: 32` has a lot.\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4l4BzTbZfTO"
      },
      "outputs": [],
      "source": [
        "# Remove Unnamed: 32 and id columns\n",
        "columns_to_remove = ['Unnamed: 32', 'id']\n",
        "df.drop(columns_to_remove, axis=1, inplace=True)\n",
        "\n",
        "# Check that the columns are indeed dropped\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPDxg0AO4g-N"
      },
      "outputs": [],
      "source": [
        "# Integer encode the target variable, diagnosis\n",
        "df[\"diagnosis_int\"] = (df[\"diagnosis\"] == 'M').astype('int')\n",
        "\n",
        "# Drop the previous string column\n",
        "df.drop(['diagnosis'], axis=1, inplace=True)\n",
        "\n",
        "# Check the new column\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTuyLttI5h0w"
      },
      "outputs": [],
      "source": [
        "# Split feature and target vectors\n",
        "X = df.drop(\"diagnosis_int\", 1)\n",
        "Y = df[\"diagnosis_int\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JVl3UGpq7_I"
      },
      "outputs": [],
      "source": [
        "def fit_model(X, Y):\n",
        "    '''Use a RandomForestClassifier for this problem.'''\n",
        "    \n",
        "    # define the model to use\n",
        "    model = RandomForestClassifier(criterion='entropy', random_state=47)\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X, Y)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg-QoSiErLgv"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(model, X_test_scaled, Y_test):\n",
        "    '''Get model evaluation metrics on the test set.'''\n",
        "    \n",
        "    # Get model predictions\n",
        "    y_predict_r = model.predict(X_test_scaled)\n",
        "    \n",
        "    # Calculate evaluation metrics for assesing performance of the model.\n",
        "    acc = accuracy_score(Y_test, y_predict_r)\n",
        "    roc = roc_auc_score(Y_test, y_predict_r)\n",
        "    prec = precision_score(Y_test, y_predict_r)\n",
        "    rec = recall_score(Y_test, y_predict_r)\n",
        "    f1 = f1_score(Y_test, y_predict_r)\n",
        "    \n",
        "    return acc, roc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F06PrANXrLrL"
      },
      "outputs": [],
      "source": [
        "def train_and_get_metrics(X, Y):\n",
        "    '''Train a Random Forest Classifier and get evaluation metrics'''\n",
        "    \n",
        "    # Split train and test sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
        "\n",
        "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Call the fit model function to train the model on the normalized features and the diagnosis values\n",
        "    model = fit_model(X_train_scaled, Y_train)\n",
        "\n",
        "    # Make predictions on test dataset and calculate metrics.\n",
        "    acc, roc, prec, rec, f1 = calculate_metrics(model, X_test_scaled, Y_test)\n",
        "\n",
        "    return acc, roc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdOOXiqSmH6p"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_on_features(X, Y):\n",
        "    '''Train model and display evaluation metrics.'''\n",
        "    \n",
        "    # Train the model, predict values and get metrics\n",
        "    acc, roc, prec, rec, f1 = train_and_get_metrics(X, Y)\n",
        "\n",
        "    # Construct a dataframe to display metrics.\n",
        "    display_df = pd.DataFrame([[acc, roc, prec, rec, f1, X.shape[1]]], columns=[\"Accuracy\", \"ROC\", \"Precision\", \"Recall\", \"F1 Score\", 'Feature Count'])\n",
        "    \n",
        "    return display_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sXRVKV-nlwR"
      },
      "outputs": [],
      "source": [
        "# Calculate evaluation metrics\n",
        "all_features_eval_df = evaluate_model_on_features(X, Y)\n",
        "all_features_eval_df.index = ['All features']\n",
        "\n",
        "# Initialize results dataframe\n",
        "results = all_features_eval_df\n",
        "\n",
        "# Check the metrics\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8rBZqEfw45p"
      },
      "outputs": [],
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Calculate correlation matrix\n",
        "cor = df.corr() \n",
        "\n",
        "# Plot the correlation matrix\n",
        "sns.heatmap(cor, annot=True, cmap=plt.cm.PuBu)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m34lMYEy9MK"
      },
      "outputs": [],
      "source": [
        "# Get the absolute value of the correlation\n",
        "cor_target = abs(cor[\"diagnosis_int\"])\n",
        "\n",
        "# Select highly correlated features (thresold = 0.2)\n",
        "relevant_features = cor_target[cor_target>0.2]\n",
        "\n",
        "# Collect the names of the features\n",
        "names = [index for index, value in relevant_features.iteritems()]\n",
        "\n",
        "# Drop the target variable from the results\n",
        "names.remove('diagnosis_int')\n",
        "\n",
        "# Display the results\n",
        "print(names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiHBfYEc8Wqb"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model with new features\n",
        "strong_features_eval_df = evaluate_model_on_features(df[names], Y)\n",
        "strong_features_eval_df.index = ['Strong features']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(strong_features_eval_df)\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yIfyep00eQb"
      },
      "outputs": [],
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Calculate the correlation matrix for target relevant features that you previously determined\n",
        "new_corr = df[names].corr()\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrzX3gpwwfKB"
      },
      "outputs": [],
      "source": [
        "# Set figure size\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "# Select a subset of features\n",
        "new_corr = df[['perimeter_mean', 'radius_worst', 'perimeter_worst', 'area_worst', 'radius_mean']].corr()\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "sns.heatmap(new_corr, annot=True, cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlDWwAZi8LPi"
      },
      "outputs": [],
      "source": [
        "# Remove the features with high correlation to other features\n",
        "subset_feature_corr_names = [x for x in names if x not in ['radius_worst', 'perimeter_worst', 'area_worst']]\n",
        "\n",
        "# Calculate and check evaluation metrics\n",
        "subset_feature_eval_df = evaluate_model_on_features(df[subset_feature_corr_names], Y)\n",
        "subset_feature_eval_df.index = ['Subset features']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(subset_feature_eval_df)\n",
        "results.head(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcJLnTwEAg9U"
      },
      "outputs": [],
      "source": [
        "def univariate_selection():\n",
        "    \n",
        "    # Split train and test sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
        "    \n",
        "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # User SelectKBest to select top 20 features based on f-test\n",
        "    selector = SelectKBest(f_classif, k=20)\n",
        "    \n",
        "    # Fit to scaled data, then transform it\n",
        "    X_new = selector.fit_transform(X_train_scaled, Y_train)\n",
        "    \n",
        "    # Print the results\n",
        "    feature_idx = selector.get_support()\n",
        "    for name, included in zip(df.drop(\"diagnosis_int\",1 ).columns, feature_idx):\n",
        "        print(\"%s: %s\" % (name, included))\n",
        "    \n",
        "    # Drop the target variable\n",
        "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[feature_idx]\n",
        "    \n",
        "    return feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaeEb5f_HjQC"
      },
      "outputs": [],
      "source": [
        "univariate_feature_names = univariate_selection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTF7GQuvAzIk"
      },
      "outputs": [],
      "source": [
        "# Calculate and check model metrics\n",
        "univariate_eval_df = evaluate_model_on_features(df[univariate_feature_names], Y)\n",
        "univariate_eval_df.index = ['F-test']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(univariate_eval_df)\n",
        "results.head(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6PGlE5gM8ca",
        "outputId": "4f5a5d3b-bafc-4dc8-9743-3179a0d17751"
      },
      "outputs": [],
      "source": [
        "def run_rfe():\n",
        "    \n",
        "    # Split train and test sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
        "    \n",
        "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Define the model\n",
        "    model = RandomForestClassifier(criterion='entropy', random_state=47)\n",
        "    \n",
        "    # Wrap RFE around the model\n",
        "    rfe = RFE(model, n_features_to_select=20)\n",
        "    \n",
        "    # Fit RFE\n",
        "    rfe = rfe.fit(X_train_scaled, Y_train)\n",
        "    feature_names = df.drop(\"diagnosis_int\", 1 ).columns[rfe.get_support()]\n",
        "    \n",
        "    return feature_names\n",
        "\n",
        "rfe_feature_names = run_rfe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-p-YdDuOD1B"
      },
      "outputs": [],
      "source": [
        "# Calculate and check model metrics\n",
        "rfe_eval_df = evaluate_model_on_features(df[rfe_feature_names], Y)\n",
        "rfe_eval_df.index = ['RFE']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(rfe_eval_df)\n",
        "results.head(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey_Qr2a89yxL"
      },
      "outputs": [],
      "source": [
        "def feature_importances_from_tree_based_model_():\n",
        "    \n",
        "    # Split train and test set\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
        "    \n",
        "    # Define the model to use\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    model = RandomForestClassifier()\n",
        "    model = model.fit(X_train_scaled,Y_train)\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 12))\n",
        "    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "    feat_importances.sort_values(ascending=False).plot(kind='barh')\n",
        "    plt.show()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def select_features_from_model(model):\n",
        "    \n",
        "    model = SelectFromModel(model, prefit=True, threshold=0.013)\n",
        "    feature_idx = model.get_support()\n",
        "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[feature_idx]\n",
        "        \n",
        "    return feature_names\n",
        "\n",
        "model = feature_importances_from_tree_based_model_()\n",
        "feature_imp_feature_names = select_features_from_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqNy8BXD4mOk"
      },
      "outputs": [],
      "source": [
        "# Calculate and check model metrics\n",
        "feat_imp_eval_df = evaluate_model_on_features(df[feature_imp_feature_names], Y)\n",
        "feat_imp_eval_df.index = ['Feature Importance']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(feat_imp_eval_df)\n",
        "results.head(n=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9J_gU9kQSPF",
        "outputId": "f2e17bdd-33aa-4441-bb96-e252d455cf79"
      },
      "outputs": [],
      "source": [
        "def run_l1_regularization():\n",
        "    \n",
        "    # Split train and test set\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,stratify=Y, random_state = 123)\n",
        "    \n",
        "    # All features of dataset are float values. You normalize all features of the train and test dataset here.\n",
        "    scaler = StandardScaler().fit(X_train)\n",
        "    X_train_scaled = scaler.transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    \n",
        "    # Select L1 regulated features from LinearSVC output \n",
        "    selection = SelectFromModel(LinearSVC(C=1, penalty='l1', dual=False))\n",
        "    selection.fit(X_train_scaled, Y_train)\n",
        "\n",
        "    feature_names = df.drop(\"diagnosis_int\",1 ).columns[(selection.get_support())]\n",
        "    \n",
        "    return feature_names\n",
        "\n",
        "l1reg_feature_names = run_l1_regularization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8B3PtRAN2_L"
      },
      "outputs": [],
      "source": [
        "# Calculate and check model metrics\n",
        "l1reg_eval_df = evaluate_model_on_features(df[l1reg_feature_names], Y)\n",
        "l1reg_eval_df.index = ['L1 Reg']\n",
        "\n",
        "# Append to results and display\n",
        "results = results.append(l1reg_eval_df)\n",
        "results.head(n=10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
