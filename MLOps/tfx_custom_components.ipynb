{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as4OTe2ukSqm"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyQtljP-qPHY"
      },
      "outputs": [],
      "source": [
        "!pip install -U tfx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from absl import logging\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from typing import Any, Dict, List, Text, Optional, Iterable, Union\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# import the decorator\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "# import type annotations\n",
        "from tfx.dsl.component.experimental.annotations import InputArtifact, OutputArtifact, Parameter, OutputDict\n",
        "from tfx.dsl.components.base import base_beam_component\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "\n",
        "from tfx import types\n",
        "#import artifact type that you will use\n",
        "from tfx.types.standard_artifacts import String\n",
        "from tfx.types.component_spec import ChannelParameter\n",
        "from tfx.types.component_spec import ExecutionParameter\n",
        "from tfx.types.component_spec import ComponentSpec\n",
        "from tfx.types import standard_artifacts\n",
        "from tfx.types import standard_component_specs\n",
        "from tfx.types import artifact_utils\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.proto import range_config_pb2\n",
        "from tfx.components.example_gen import utils\n",
        "from tfx.components.example_gen.csv_example_gen.executor import Executor as CsvExampleGenExecutor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jh7vKSRqPHb"
      },
      "outputs": [],
      "source": [
        "\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcUseqJaE2XN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pipeline label\n",
        "PIPELINE_NAME = \"penguin-simple\"\n",
        "\n",
        "# Output directory to store artifacts generated from the pipeline.\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "# Set default logging level.\n",
        "logging.set_verbosity(logging.INFO)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fxMs6u86acP"
      },
      "outputs": [],
      "source": [
        "# Create directory\n",
        "DATA_ROOT = 'data'\n",
        "!mkdir {DATA_ROOT}\n",
        "\n",
        "# Copy dataset to directory\n",
        "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
        "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
        "urllib.request.urlretrieve(_data_url, _data_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyinPEAKrm9z"
      },
      "outputs": [],
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbQOc2VIqe-E"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx import v1 as tfx\n",
        "from tfx_bsl.public import tfxio\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "# Since we're not generating or creating a schema, we will instead create\n",
        "# a feature spec.  Since there are a fairly small number of features this is\n",
        "# manageable for this dataset.\n",
        "_FEATURE_SPEC = {\n",
        "    **{\n",
        "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
        "           for feature in _FEATURE_KEYS\n",
        "       },\n",
        "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str],\n",
        "              data_accessor: tfx.components.DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int = 200) -> tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      tfxio.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema).repeat()\n",
        "\n",
        "\n",
        "def _build_keras_model() -> tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3)(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
        "  # version provided by pipeline author. A schema can also derived from TFT\n",
        "  # graph if a Transform component is used. In the case when either is missing,\n",
        "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
        "  # feature_spec, but the schema returned would be very primitive.\n",
        "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  model = _build_keras_model()\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      steps_per_epoch=fn_args.train_steps,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps)\n",
        "\n",
        "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "  # directory.\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78NRGIGNsNPD"
      },
      "outputs": [],
      "source": [
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     module_file: str, serving_model_dir: str,\n",
        "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
        "  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
        "  # Brings data into the pipeline.\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = tfx.components.Trainer(\n",
        "      module_file=module_file,\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      train_args=tfx.proto.TrainArgs(num_steps=100),\n",
        "      eval_args=tfx.proto.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a filesystem destination.\n",
        "  pusher = tfx.components.Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=tfx.proto.PushDestination(\n",
        "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  # Following three components will be included in the pipeline.\n",
        "  components = [\n",
        "      example_gen,\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return tfx.dsl.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(metadata_path),\n",
        "      components=components)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-TCt1Res8rH"
      },
      "outputs": [],
      "source": [
        "tfx.orchestration.LocalDagRunner().run(\n",
        "  _create_pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      data_root=DATA_ROOT,\n",
        "      module_file=_trainer_module_file,\n",
        "      serving_model_dir=SERVING_MODEL_DIR,\n",
        "      metadata_path=METADATA_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2EAJyXD3za_"
      },
      "outputs": [],
      "source": [
        "# search directory for one or more CSVs\n",
        "files = glob.glob(f'{DATA_ROOT}/*.csv')\n",
        "\n",
        "# filter the dataset\n",
        "for file in files:\n",
        "  df = pd.read_csv(file, index_col=False)\n",
        "  filtered_df = df[df['culmen_length_mm'] > 0.3].reset_index(drop=True)\n",
        "\n",
        "# print latest modified file\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwLgLLY3m51s"
      },
      "outputs": [],
      "source": [
        "@component\n",
        "def CustomFilterComponent(input_base: Parameter[str], \n",
        "                    output_base: Parameter[str],\n",
        "                    ) -> OutputDict(output_path=str):\n",
        "  '''\n",
        "  Args:\n",
        "    input_base - location of the raw CSV\n",
        "    output_base - location where you want to save the filtered CSV\n",
        "  \n",
        "  Returns:\n",
        "    OutputDict:\n",
        "      output_path - String artifact that just holds the `output_base` value\n",
        "  '''\n",
        "\n",
        "  # create the output base if it does not exist yet\n",
        "  if not os.path.exists(output_base):\n",
        "      os.mkdir(output_base)\n",
        "  \n",
        "  # search for CSVs in the input base\n",
        "  files = glob.glob(f'{input_base}/*.csv')\n",
        "\n",
        "  # loop through CSVs\n",
        "  for file in files:\n",
        "\n",
        "    # read the CSV\n",
        "    df = pd.read_csv(file, index_col=False)\n",
        "\n",
        "    # filter the data\n",
        "    filtered_df = df[df['culmen_length_mm'] > 0.3].reset_index(drop=True)\n",
        "\n",
        "    # compose output filename\n",
        "    filename = os.path.basename(file).replace('.csv','')\n",
        "    filtered_filename = f'{filename}_filtered.csv'\n",
        "  \n",
        "    # save filtered CSV to output base\n",
        "    filtered_df.to_csv(f'{output_base}/{filtered_filename}', index=False)\n",
        "\n",
        "  # define the output artifact\n",
        "  return {'output_path': output_base}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGRzLRR-ukuN"
      },
      "outputs": [],
      "source": [
        "# define a filter task\n",
        "filter_task = CustomFilterComponent(input_base=DATA_ROOT, \n",
        "                                        output_base=f'{DATA_ROOT}_filtered')\n",
        "\n",
        "# include the task\n",
        "components = [filter_task]\n",
        "\n",
        "# define a pipeline with only the single component\n",
        "pipeline = tfx.dsl.Pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(METADATA_PATH),\n",
        "      components=components)\n",
        "\n",
        "# run the pipeline\n",
        "tfx.orchestration.LocalDagRunner().run(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cDdNBhku0j9"
      },
      "outputs": [],
      "source": [
        "# number of rows in original csv\n",
        "!cat data/data.csv | wc -l\n",
        "\n",
        "# number of rows in filtered csv\n",
        "!cat data_filtered/data_filtered.csv | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3WzxHyA56M8"
      },
      "outputs": [],
      "source": [
        "# Define filter task\n",
        "filter_task = CustomFilterComponent(input_base=DATA_ROOT, \n",
        "                                        output_base=f'{DATA_ROOT}_filtered')\n",
        "\n",
        "# Try using the custom component with CsvExampleGen. This code will expectedly throw an error.\n",
        "try:\n",
        "  example_gen = tfx.components.CsvExampleGen(input_base=filter_task.outputs['output_path'])\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"Error thrown as expected!\")\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZlH_YRp0mVY"
      },
      "outputs": [],
      "source": [
        "# Key for example_gen input that we want to use\n",
        "INPUT_BASE_KEY = 'input_base'\n",
        "\n",
        "# Other keys\n",
        "INPUT_CONFIG_KEY = 'input_config'\n",
        "OUTPUT_CONFIG_KEY = 'output_config'\n",
        "OUTPUT_DATA_FORMAT_KEY = 'output_data_format'\n",
        "RANGE_CONFIG_KEY = 'range_config'\n",
        "CUSTOM_CONFIG_KEY = 'custom_config'\n",
        "EXAMPLES_KEY = 'examples'\n",
        "\n",
        "class MyCustomExampleGenSpec(ComponentSpec):\n",
        "  \"\"\"File-based ExampleGen component spec.\"\"\"\n",
        "  \n",
        "  PARAMETERS = {\n",
        "      INPUT_CONFIG_KEY:\n",
        "          ExecutionParameter(type=example_gen_pb2.Input),\n",
        "      OUTPUT_CONFIG_KEY:\n",
        "          ExecutionParameter(type=example_gen_pb2.Output),\n",
        "      OUTPUT_DATA_FORMAT_KEY:\n",
        "          ExecutionParameter(type=int),\n",
        "      CUSTOM_CONFIG_KEY:\n",
        "          ExecutionParameter(type=example_gen_pb2.CustomConfig, optional=True),\n",
        "      RANGE_CONFIG_KEY:\n",
        "          ExecutionParameter(type=range_config_pb2.RangeConfig, optional=True),\n",
        "  }\n",
        "\n",
        "  # Now accepts a channel\n",
        "  INPUTS = {\n",
        "      INPUT_BASE_KEY:\n",
        "          ChannelParameter(type=standard_artifacts.String),\n",
        "  }\n",
        "  \n",
        "  OUTPUTS = {\n",
        "      EXAMPLES_KEY: ChannelParameter(type=standard_artifacts.Examples),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_agzAOu42v_"
      },
      "outputs": [],
      "source": [
        "class MyCustomExecutor(CsvExampleGenExecutor):\n",
        "  \"\"\"Generic TFX CSV example gen executor.\"\"\"\n",
        "\n",
        "  def Do(\n",
        "      self,\n",
        "      input_dict: Dict[Text, List[types.Artifact]],\n",
        "      output_dict: Dict[Text, List[types.Artifact]],\n",
        "      exec_properties: Dict[Text, Any],\n",
        "  ) -> None:\n",
        "    \"\"\"Take input data source and generates serialized data splits.\n",
        "    The output is intended to be serialized tf.train.Examples or\n",
        "    tf.train.SequenceExamples protocol buffer in gzipped TFRecord format,\n",
        "    but subclasses can choose to override to write to any serialized records\n",
        "    payload into gzipped TFRecord as specified, so long as downstream\n",
        "    component can consume it. The format of payload is added to\n",
        "    `payload_format` custom property of the output Example artifact.\n",
        "    Args:\n",
        "      input_dict: Input dict from input key to a list of Artifacts. Depends on\n",
        "        detailed example gen implementation.\n",
        "        - input_base: an external directory containing the data files.\n",
        "      output_dict: Output dict from output key to a list of Artifacts.\n",
        "        - examples: splits of serialized records.\n",
        "      exec_properties: A dict of execution properties. Depends on detailed\n",
        "        example gen implementation.\n",
        "        - input_config: JSON string of example_gen_pb2.Input instance,\n",
        "          providing input configuration.\n",
        "        - output_config: JSON string of example_gen_pb2.Output instance,\n",
        "          providing output configuration.\n",
        "        - output_data_format: Payload format of generated data in output\n",
        "          artifact, one of example_gen_pb2.PayloadFormat enum.\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    self._log_startup(input_dict, output_dict, exec_properties)\n",
        "\n",
        "    # Get the artifact from the Channel input\n",
        "    filter_component_artifact = artifact_utils.get_single_instance(\n",
        "        input_dict[standard_component_specs.INPUT_BASE_KEY])\n",
        "    \n",
        "    # Put the input string value into the exec_properties fictionary\n",
        "    exec_properties[standard_component_specs.INPUT_BASE_KEY] = filter_component_artifact.value\n",
        "    \n",
        "    # execute superclass\n",
        "    super(MyCustomExecutor, self).Do(input_dict=input_dict, output_dict=output_dict, exec_properties=exec_properties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOlYnQumyWfX"
      },
      "outputs": [],
      "source": [
        "class MyCustomExampleGen(base_beam_component.BaseBeamComponent):\n",
        "\n",
        "  # Define the Spec class and executor spec using the functions and\n",
        "  # classes you defined earlier.\n",
        "  SPEC_CLASS = MyCustomExampleGenSpec\n",
        "  EXECUTOR_SPEC = executor_spec.BeamExecutorSpec(MyCustomExecutor)\n",
        "\n",
        "  # Define init function. Notice that `input_base` now accepts a Channel.\n",
        "  def __init__(self,\n",
        "               input_base: types.Channel = None,\n",
        "               input_config: Optional[Union[example_gen_pb2.Input,\n",
        "                                            Dict[Text, Any]]] = None,\n",
        "               output_config: Optional[Union[example_gen_pb2.Output,\n",
        "                                             Dict[Text, Any]]] = None,\n",
        "               range_config: Optional[Union[range_config_pb2.RangeConfig,\n",
        "                                            Dict[Text, Any]]] = None,\n",
        "               output_data_format: Optional[int] = example_gen_pb2.FORMAT_TF_EXAMPLE):\n",
        "    \"\"\"Customized ExampleGen component.\n",
        "    Args:\n",
        "      input_base: an external directory containing the CSV files. Accepts a Channel\n",
        "        from a previous TFX component.\n",
        "      input_config: An example_gen_pb2.Input instance, providing input\n",
        "        configuration. If unset, the files under input_base will be treated as a\n",
        "        single split. If any field is provided as a RuntimeParameter,\n",
        "        input_config should be constructed as a dict with the same field names\n",
        "        as Input proto message.\n",
        "      output_config: An example_gen_pb2.Output instance, providing output\n",
        "        configuration. If unset, default splits will be 'train' and 'eval' with\n",
        "        size 2:1. If any field is provided as a RuntimeParameter, output_config\n",
        "        should be constructed as a dict with the same field names as Output\n",
        "        proto message.\n",
        "      range_config: An optional range_config_pb2.RangeConfig instance,\n",
        "        specifying the range of span values to consider. If unset, driver will\n",
        "        default to searching for latest span with no restrictions.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Configure inputs and outputs.\n",
        "    input_config = input_config or utils.make_default_input_config()\n",
        "    output_config = output_config or utils.make_default_output_config(\n",
        "        input_config)\n",
        "    \n",
        "    # Define output type.\n",
        "    example_artifacts = types.Channel(type=standard_artifacts.Examples)\n",
        "    \n",
        "    # Pass input arguments to your custom ExampleGen spec.\n",
        "    spec = MyCustomExampleGenSpec(\n",
        "        input_base=input_base,\n",
        "        input_config=input_config,\n",
        "        output_config=output_config,\n",
        "        range_config=range_config,\n",
        "        output_data_format=output_data_format,\n",
        "        examples=example_artifacts)\n",
        "    \n",
        "    # This will check if the values passed are the correct type else\n",
        "    # it will throw the error you saw earlier.\n",
        "    super(MyCustomExampleGen, self).__init__(\n",
        "        spec=spec)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXOlqQ_9y-rL"
      },
      "outputs": [],
      "source": [
        "# Filter the dataset\n",
        "filter_task = CustomFilterComponent(input_base=DATA_ROOT, \n",
        "                                        output_base=f'{DATA_ROOT}_filtered')\n",
        "\n",
        "# Use the output of filter_task to know the input_base for this custom ExampleGen\n",
        "custom_example_gen_task = MyCustomExampleGen(input_base=filter_task.outputs['output_path'])\n",
        "\n",
        "# Define components to include\n",
        "components = [filter_task,\n",
        "              custom_example_gen_task]\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = tfx.dsl.Pipeline(\n",
        "      pipeline_name=PIPELINE_NAME,\n",
        "      pipeline_root=PIPELINE_ROOT,\n",
        "      metadata_connection_config=tfx.orchestration.metadata\n",
        "      .sqlite_metadata_connection_config(METADATA_PATH),\n",
        "      components=components)\n",
        "\n",
        "# Run the pipeline\n",
        "tfx.orchestration.LocalDagRunner().run(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DxCauSIPl_X"
      },
      "outputs": [],
      "source": [
        "EXECUTION_ID = 16 # PLACE THE EXECUTION ID HERE\n",
        "\n",
        "# Create a `TFRecordDataset` to read these files\n",
        "train_dataset = tf.data.TFRecordDataset(f'{PIPELINE_ROOT}/MyCustomExampleGen/examples/{EXECUTION_ID}/Split-train/data_tfrecord-00000-of-00001.gz', compression_type=\"GZIP\")\n",
        "eval_dataset = tf.data.TFRecordDataset(f'{PIPELINE_ROOT}/MyCustomExampleGen/examples/{EXECUTION_ID}/Split-eval/data_tfrecord-00000-of-00001.gz', compression_type=\"GZIP\")\n",
        "\n",
        "# Get number of records for each dataset (only use for small datasets to avoid memory issues)\n",
        "num_train_data = len(list(train_dataset))\n",
        "num_eval_data = len(list(eval_dataset))\n",
        "\n",
        "# Get the total\n",
        "total_examples = num_train_data + num_eval_data\n",
        "\n",
        "print(f'total number of examples: {total_examples}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
