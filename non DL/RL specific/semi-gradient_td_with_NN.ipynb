{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"UeVgTitSWQMu","nbgrader":{"cell_type":"markdown","checksum":"9f4e1f41e5a3745101371efa1ef4e091","grade":false,"grade_id":"cell-4774adbee156b2dc","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Semi-gradient TD with a Neural Network"]},{"cell_type":"code","execution_count":14,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":297,"status":"ok","timestamp":1673201780833,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"iU1wbZk5WQM1","nbgrader":{"cell_type":"code","checksum":"82a5d2886220d5cf6288bc67624a8f9c","grade":false,"grade_id":"cell-38bff794ab578cbf","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import os, shutil\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":15,"metadata":{"cellView":"form","executionInfo":{"elapsed":39,"status":"ok","timestamp":1673201783267,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Gi5khZJLWdI-"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1673201783268,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"TT-6kdYaXh6c"},"outputs":[],"source":["#  An additional abstract class that specifies the optimizer API \n","\n","\n","from abc import ABCMeta, abstractmethod\n","\n","class BaseOptimizer:\n","\t__metaclass__ = ABCMeta\n","\n","\tdef __init__(self):\n","\t\tpass\n","\n","\t@abstractmethod\n","\tdef optimizer_init(self, optimizer_info):\n","\t\t\"\"\"Setup for the optimizer.\"\"\"\n","\n","\t@abstractmethod\n","\tdef update_weights(self, weights, g):\n","\t\t\"\"\"\n","        Given weights and update g, return updated weights\n","        \"\"\"\n"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1673201783268,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"8VeCnXmIW2GG"},"outputs":[],"source":["#@title RandomWalk environment \n","\n","\n","class RandomWalkEnvironment(BaseEnvironment):\n","    def env_init(self, env_info={}):\n","        \"\"\"\n","        Setup for the environment called when the experiment first starts.\n","        \n","        Set parameters needed to setup the 500-state random walk environment.\n","        \n","        Assume env_info dict contains:\n","        {\n","            num_states: 500,\n","            start_state: 250,\n","            left_terminal_state: 0,\n","            right_terminal_state: 501,\n","            seed: int\n","        }\n","        \"\"\"\n","        # set random seed for each run\n","        self.rand_generator = np.random.RandomState(env_info.get(\"seed\")) \n","        \n","        ### Set each attributes correctly \n","        self.num_states = env_info[\"num_states\"] \n","        self.start_state = env_info[\"start_state\"] \n","        self.left_terminal_state = env_info[\"left_terminal_state\"] \n","        self.right_terminal_state = env_info[\"right_terminal_state\"]\n","        \n","    def env_start(self):\n","        \"\"\"\n","        The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","        ### set self.reward_obs_term tuple accordingly \n","        reward = 0.0\n","        observation = self.start_state\n","        is_terminal = False\n","        \n","        self.reward_obs_term = (reward, observation, is_terminal)\n","        \n","        # return first state observation from the environment\n","        return self.reward_obs_term[1]\n","        \n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","        \n","        ### set reward, current_state, and is_terminal correctly\n","        # current state: next state after taking action from the last state [int]\n","        # action: represents how many states to move from the last state [int]\n","        last_state = self.reward_obs_term[1]\n","        \n","        if action == 0: # left\n","            current_state = max(self.left_terminal_state, last_state + self.rand_generator.choice(range(-100,0)))\n","        elif action == 1: # right\n","            current_state = min(self.right_terminal_state, last_state + self.rand_generator.choice(range(1,101)))\n","        else: \n","            raise ValueError(\"Wrong action value\")\n","        \n","        reward = 0.0\n","        is_terminal = False\n","        if current_state == self.left_terminal_state: \n","            reward = -1.0\n","            is_terminal = True\n","\n","        elif current_state == self.right_terminal_state:\n","            reward = 1.0\n","            is_terminal = True\n","\n","        self.reward_obs_term = (reward, current_state, is_terminal)\n","        \n","        return self.reward_obs_term"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1673201783269,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"0F3Bi62JXTG2"},"outputs":[],"source":["#@title helpers for visualization\n","\n","plt1_legend_dict = {\"td_agent\": \"approximate values learned by\\n TD with neural network\", \n","                    \"td_agent_5000_episodes\": \"approximate values learned by\\n TD with neural network\",\n","                    \"td_agent_tilecoding\": \"approximate values learned by\\n TD with tile-coding\"}\n","\n","\n","plt2_legend_dict = {\"td_agent\": \"TD with neural network\", \n","                    \"td_agent_5000_episodes\": \"TD with neural network\",\n","                    \"td_agent_tilecoding\": \"TD with tile-coding\"}\n","\n","\n","plt2_label_dict = {\"td_agent\": \"RMSVE\\n averaged\\n over\\n 20 runs\", \n","                   \"td_agent_5000_episodes\": \"RMSVE\\n averaged\\n over\\n 20 runs\",\n","                   \"td_agent_tilecoding\": \"RMSVE\\n averaged\\n over\\n 20 runs\"}\n","\n","\n","# Function to plot result\n","def plot_result(data_name_array):\n","    \n","    true_V = np.load('./true_V.npy')\n","\n","    plt1_agent_sweeps = []\n","    plt2_agent_sweeps = []\n","    \n","    # two plots: learned state-value and learning curve (RMSVE)\n","    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n","    \n","    for data_name in data_name_array:\n","\n","        # plot1\n","        filename = 'V_{}'.format(data_name).replace('.','')\n","        current_agent_V = np.load('./results/{}.npy'.format(filename))\n","        current_agent_V = current_agent_V[-1, :]\n","\n","\n","        plt1_x_legend = range(1,len(current_agent_V[:]) + 1)\n","        graph_current_agent_V, = ax[0].plot(plt1_x_legend, current_agent_V[:], label=plt1_legend_dict[data_name])\n","        plt1_agent_sweeps.append(graph_current_agent_V)\n","        \n","        # plot2\n","        filename = 'RMSVE_{}'.format(data_name).replace('.','')\n","        RMSVE_data = np.load('./results/{}.npz'.format(filename))\n","        current_agent_RMSVE = np.mean(RMSVE_data[\"rmsve\"], axis = 0)\n","\n","        plt2_x_legend = np.arange(0, RMSVE_data[\"num_episodes\"]+1, RMSVE_data[\"eval_freq\"])\n","        graph_current_agent_RMSVE, = ax[1].plot(plt2_x_legend, current_agent_RMSVE[:], label=plt2_legend_dict[data_name])\n","        plt2_agent_sweeps.append(graph_current_agent_RMSVE)\n","                \n","          \n","    # plot1: \n","    # add True V\n","    plt1_x_legend = range(1,len(true_V[:]) + 1)\n","    graph_true_V, = ax[0].plot(plt1_x_legend, true_V[:], label=\"$v_{\\pi}$\")\n","    \n","    ax[0].legend(handles=[*plt1_agent_sweeps, graph_true_V], fontsize = 13)\n","    \n","    ax[0].set_title(\"State Value\", fontsize = 15)\n","    ax[0].set_xlabel('State', fontsize = 14)\n","    ax[0].set_ylabel('Value\\n scale', rotation=0, labelpad=15, fontsize = 14)\n","\n","    plt1_xticks = [1, 100, 200, 300, 400, 500]\n","    plt1_yticks = [-1.0, 0.0, 1.0]\n","    ax[0].set_xticks(plt1_xticks)\n","    ax[0].set_xticklabels(plt1_xticks, fontsize=13)\n","    ax[0].set_yticks(plt1_yticks)\n","    ax[0].set_yticklabels(plt1_yticks, fontsize=13)\n","    \n","    \n","    # plot2:\n","    ax[1].legend(handles=plt2_agent_sweeps, fontsize = 13)\n","    \n","    ax[1].set_title(\"Learning Curve\", fontsize = 15)\n","    ax[1].set_xlabel('Episodes', fontsize = 14)\n","    ax[1].set_ylabel(plt2_label_dict[data_name_array[0]], rotation=0, labelpad=40, fontsize = 14)\n","\n","    plt2_yticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","    ax[1].tick_params(axis=\"x\", labelsize=13)\n","    ax[1].set_yticks(plt2_yticks)\n","    ax[1].set_yticklabels(plt2_yticks, fontsize = 13)\n","\n","    plt.tight_layout()\n","    plt.show()      "]},{"cell_type":"code","execution_count":19,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673201783270,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"G-8p7wgKWQM5","nbgrader":{"cell_type":"code","checksum":"4f8ec434322d3ad6495ac5487be48e81","grade":false,"grade_id":"cell-e6785c6caf24ec0b","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def my_matmul(x1, x2):\n","    \"\"\"\n","    Given matrices x1 and x2, return the multiplication of them\n","    \"\"\"\n","    \n","    result = np.zeros((x1.shape[0], x2.shape[1]))\n","    x1_non_zero_indices = x1.nonzero()\n","    if x1.shape[0] == 1 and len(x1_non_zero_indices[1]) == 1:\n","        result = x2[x1_non_zero_indices[1], :]\n","    elif x1.shape[1] == 1 and len(x1_non_zero_indices[0]) == 1:\n","        result[x1_non_zero_indices[0], :] = x2 * x1[x1_non_zero_indices[0], 0]\n","    else:\n","        result = np.matmul(x1, x2)\n","    return result"]},{"cell_type":"code","execution_count":20,"metadata":{"deletable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673201783271,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"aVW3OR-mWQM6","nbgrader":{"cell_type":"code","checksum":"9e930c44dfc2c8afa24e19fec9e99b09","grade":false,"grade_id":"cell-1cceb6da8f9e9a81","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def get_value(s, weights):\n","    \"\"\"\n","    Compute value of input s given the weights of a neural network\n","    \"\"\"\n","    ### Compute the ouput of the neural network, v, for input s\n","\n","    psi = my_matmul(s, weights[0][\"W\"]) + weights[0][\"b\"]\n","    x = np.maximum(psi, 0)\n","    v = my_matmul(x, weights[1][\"W\"]) + weights[1][\"b\"]    \n","    return v"]},{"cell_type":"code","execution_count":22,"metadata":{"deletable":false,"executionInfo":{"elapsed":19,"status":"ok","timestamp":1673201783273,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"2vKFzG-yWQM-","nbgrader":{"cell_type":"code","checksum":"ec551777bb2afb289bef59a6dcd99d01","grade":false,"grade_id":"cell-7c2e341b5e17073f","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def get_gradient(s, weights):\n","    \"\"\"\n","    Given inputs s and weights, return the gradient of v with respect to the weights\n","    \"\"\"\n","\n","    ### Compute the gradient of the value function with respect to W0, b0, W1, b1 for input s\n","    grads = [dict() for i in range(len(weights))]\n","    x = np.maximum(my_matmul(s, weights[0][\"W\"]) + weights[0][\"b\"], 0)\n","    grads[0][\"W\"] = my_matmul(s.T, (weights[1][\"W\"].T * (x > 0)))\n","    grads[0][\"b\"] = weights[1][\"W\"].T * (x > 0)\n","    grads[1][\"W\"] = x.T\n","    grads[1][\"b\"] = 1    \n","\n","    return grads\n"]},{"cell_type":"code","execution_count":24,"metadata":{"deletable":false,"executionInfo":{"elapsed":18,"status":"ok","timestamp":1673201783274,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"x5p81cmwWQNB","nbgrader":{"cell_type":"code","checksum":"1a60a3c6222a9723c128919b71f5f5fd","grade":false,"grade_id":"cell-6e90dc8c0b50f536","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class SGD(BaseOptimizer):\n","    def __init__(self):\n","        pass\n","    \n","    def optimizer_init(self, optimizer_info):\n","        \"\"\"Setup for the optimizer.\n","\n","        Set parameters needed to setup the stochastic gradient descent method.\n","\n","        Assume optimizer_info dict contains:\n","        {\n","            step_size: float\n","        }\n","        \"\"\"\n","        self.step_size = optimizer_info.get(\"step_size\")\n","    \n","    def update_weights(self, weights, g):\n","        \"\"\"\n","        Given weights and update g, return updated weights\n","        \"\"\"\n","        for i in range(len(weights)):\n","            for param in weights[i].keys():\n","                \n","                ### update weights\n","                weights[i][param] += self.step_size * g[i][param]\n","                \n","        return weights"]},{"cell_type":"code","execution_count":26,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":18,"status":"ok","timestamp":1673201783276,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"N6G5ipPnWQND","nbgrader":{"cell_type":"code","checksum":"693d46afb36061fa97532bd63656a034","grade":false,"grade_id":"cell-d5eb6f0601a69b03","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["class Adam(BaseOptimizer):\n","    def __init__(self):\n","        pass\n","    \n","    def optimizer_init(self, optimizer_info):\n","        \"\"\"Setup for the optimizer.\n","\n","        Set parameters needed to setup the Adam algorithm.\n","\n","        Assume optimizer_info dict contains:\n","        {\n","            num_states: integer,\n","            num_hidden_layer: integer,\n","            num_hidden_units: integer,\n","            step_size: float, \n","            self.beta_m: float\n","            self.beta_v: float\n","            self.epsilon: float\n","        }\n","        \"\"\"\n","        \n","        self.num_states = optimizer_info.get(\"num_states\")\n","        self.num_hidden_layer = optimizer_info.get(\"num_hidden_layer\")\n","        self.num_hidden_units = optimizer_info.get(\"num_hidden_units\")\n","\n","        # Specify Adam algorithm's hyper parameters\n","        self.step_size = optimizer_info.get(\"step_size\")\n","        self.beta_m = optimizer_info.get(\"beta_m\")\n","        self.beta_v = optimizer_info.get(\"beta_v\")\n","        self.epsilon = optimizer_info.get(\"epsilon\")\n","\n","        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n","\n","        # Initialize Adam algorithm's m and v\n","        self.m = [dict() for i in range(self.num_hidden_layer+1)]\n","        self.v = [dict() for i in range(self.num_hidden_layer+1)]\n","\n","        for i in range(self.num_hidden_layer+1):\n","\n","            # Initialize self.m[i][\"W\"], self.m[i][\"b\"], self.v[i][\"W\"], self.v[i][\"b\"] to zero\n","            self.m[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i+1]))\n","            self.m[i][\"b\"] = np.zeros((1, self.layer_size[i+1]))\n","            self.v[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i+1]))\n","            self.v[i][\"b\"] = np.zeros((1, self.layer_size[i+1]))\n","\n","        # Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat\n","        self.beta_m_product = self.beta_m\n","        self.beta_v_product = self.beta_v\n","\n","    def update_weights(self, weights, g):\n","        \"\"\"\n","        Given weights and update g, return updated weights\n","        \"\"\"\n","        \n","        for i in range(len(weights)):\n","            for param in weights[i].keys():\n","\n","                ### update self.m and self.v\n","                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * g[i][param]\n","                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (g[i][param] * g[i][param])\n","\n","                ### compute m_hat and v_hat\n","                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n","                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n","\n","                ### update weights\n","                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n","                \n","        ### update self.beta_m_product and self.beta_v_product\n","        self.beta_m_product *= self.beta_m\n","        self.beta_v_product *= self.beta_v\n","        \n","        return weights\n"]},{"cell_type":"code","execution_count":27,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":598,"status":"ok","timestamp":1673201783857,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"BSWNXEQqWQNE","nbgrader":{"cell_type":"code","checksum":"fd3b6cf36205312bb4d46b50064433c6","grade":false,"grade_id":"cell-513bae9622f7055c","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def one_hot(state, num_states):\n","    \"\"\"\n","    Given num_state and a state, return the one-hot encoding of the state\n","    \"\"\"\n","    # Create the one-hot encoding of state\n","    # one_hot_vector is a numpy array of shape (1, num_states)\n","    \n","    one_hot_vector = np.zeros((1, num_states))\n","    one_hot_vector[0, int((state - 1))] = 1\n","    \n","    return one_hot_vector"]},{"cell_type":"code","execution_count":28,"metadata":{"deletable":false,"executionInfo":{"elapsed":29,"status":"ok","timestamp":1673201783858,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ggHRIeHmWQNF","nbgrader":{"cell_type":"code","checksum":"36476121667837625334e9ec711b1840","grade":false,"grade_id":"cell-23b7497bda6c8936","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class TDAgent(BaseAgent):\n","    def __init__(self):\n","        self.name = \"td_agent\"\n","        pass\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","\n","        Set parameters needed to setup the semi-gradient TD with a Neural Network.\n","\n","        Assume agent_info dict contains:\n","        {\n","            num_states: integer,\n","            num_hidden_layer: integer,\n","            num_hidden_units: integer,\n","            step_size: float, \n","            discount_factor: float,\n","            self.beta_m: float\n","            self.beta_v: float\n","            self.epsilon: float\n","            seed: int\n","        }\n","        \"\"\"\n","    \n","        # Set random seed for weights initialization for each run\n","        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\")) \n","        \n","        # Set random seed for policy for each run\n","        self.policy_rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n","\n","        # Set attributes according to agent_info\n","        self.num_states = agent_info.get(\"num_states\")\n","        self.num_hidden_layer = agent_info.get(\"num_hidden_layer\")\n","        self.num_hidden_units = agent_info.get(\"num_hidden_units\")\n","        self.discount_factor = agent_info.get(\"discount_factor\")\n","\n","        ### Define the neural network's structure\n","        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n","        # Initialize the neural network's parameter\n","        self.weights = [dict() for i in range(self.num_hidden_layer+1)]\n","        for i in range(self.num_hidden_layer+1):\n","\n","            ### Initialize self.weights[i][\"W\"] and self.weights[i][\"b\"] using self.rand_generator.normal()\n","            # Note that The parameters of self.rand_generator.normal are mean of the distribution, \n","            # standard deviation of the distribution, and output shape in the form of tuple of integers.\n","            # To specify output shape, use self.layer_size.\n","            ins, outs = self.layer_size[i], self.layer_size[i+1]\n","            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2/ins), (ins, outs))\n","            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2/ins), (1, outs))            \n","        \n","        # Specify the optimizer\n","        self.optimizer = Adam()\n","        self.optimizer.optimizer_init({\n","            \"num_states\": agent_info[\"num_states\"],\n","            \"num_hidden_layer\": agent_info[\"num_hidden_layer\"],\n","            \"num_hidden_units\": agent_info[\"num_hidden_units\"],\n","            \"step_size\": agent_info[\"step_size\"],\n","            \"beta_m\": agent_info[\"beta_m\"],\n","            \"beta_v\": agent_info[\"beta_v\"],\n","            \"epsilon\": agent_info[\"epsilon\"],\n","        })\n","        \n","        self.last_state = None\n","        self.last_action = None\n","\n","    def agent_policy(self, state):\n","\n","        ### Set chosen_action as 0 or 1 with equal probability. \n","        chosen_action = self.policy_rand_generator.choice([0,1])    \n","        return chosen_action\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (Numpy array): the state from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","        ### select action given state (using self.agent_policy()), and save current state and action\n","        self.last_state = state\n","        self.last_action = self.agent_policy(state)        \n","\n","        return self.last_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (Numpy array): the state from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","        \n","        ### Compute TD error\n","        last_state_vec = one_hot(self.last_state, self.num_states)\n","        last_value = get_value(last_state_vec, self.weights)\n","        \n","        state_vec = one_hot(state, self.num_states)\n","        value = get_value(state_vec, self.weights)\n","        \n","        delta = reward + self.discount_factor * value - last_value        \n","\n","        ### Retrieve gradients\n","        grads = get_gradient(last_state_vec, self.weights)\n","\n","        ### Compute g (1 line)\n","        g = [dict() for i in range(self.num_hidden_layer+1)]\n","        for i in range(self.num_hidden_layer+1):\n","            for param in self.weights[i].keys():\n","\n","                # g[i][param] = None\n","                g[i][param] = delta * grads[i][param]\n","\n","        ### update the weights using self.optimizer\n","        self.weights = self.optimizer.update_weights(self.weights, g)\n","\n","        ### update self.last_state and self.last_action\n","        self.last_state = state\n","        self.last_action = self.agent_policy(state)        \n","\n","        return self.last_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        ### compute TD error\n","        last_state_vec = one_hot(self.last_state, self.num_states)\n","        last_value = get_value(last_state_vec, self.weights)\n","        delta = reward - last_value        \n","\n","        ### Retrieve gradients\n","        grads = get_gradient(last_state_vec, self.weights) \n","\n","        ### Compute g\n","        g = [dict() for i in range(self.num_hidden_layer+1)]\n","        for i in range(self.num_hidden_layer+1):\n","            for param in self.weights[i].keys():\n","\n","                g[i][param] = delta * grads[i][param]\n","\n","        ### update the weights using self.optimizer\n","        self.weights = self.optimizer.update_weights(self.weights, g)\n","\n","    def agent_message(self, message):\n","        if message == 'get state value':\n","            state_value = np.zeros(self.num_states)\n","            for state in range(1, self.num_states + 1):\n","                s = one_hot(state, self.num_states)\n","                state_value[state - 1] = get_value(s, self.weights)\n","            return state_value"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"IDnh9oIHil5B"},"outputs":[],"source":["#@title random walk policy evaluation - create true_V.npy file\n","\n","\n","def compute_value_function(num_states, action_range):\n","    \"\"\"\n","    Computes the value function for the 1000 state random walk as described in Sutton and Barto (2017).\n","    :return: The value function for states 1 to 1000. Index 0 is not used in this array (i.e. should remain 0).\n","    \"\"\"\n","    state_prob = 0.5 / float(action_range)\n","    gamma = 1\n","    theta = 0.000001\n","\n","    V = np.zeros(num_states+1)\n","\n","    delta = np.infty\n","    i = 0\n","    while delta > theta:\n","        i += 1\n","        delta = 0.0\n","        for s in range(1, num_states+1):\n","            v = V[s]\n","            value_sum = 0.0\n","            for transition in range(1, action_range+1):\n","                right = s + transition\n","                right_reward = 0\n","                if right > num_states:\n","                    right_reward = 1\n","                    right = 0\n","\n","                left = s - transition\n","                left_reward = 0\n","                if left < 1:\n","                    left_reward = -1\n","                    left = 0\n","\n","                value_sum += state_prob * ((right_reward + gamma * V[right]) + (left_reward + gamma * V[left]))\n","\n","            V[s] = value_sum\n","            delta = max(delta, np.abs(v-V[s]))\n","    \n","    np.save(\"./true_V\", V[1:])\n","    return V[1:]\n","\n","### if __name__ == '__main__':\n","num_states = 500\n","action_range = 100\n","V = compute_value_function(num_states, action_range)\n","\n","np.save(\"./true_V\", V)\n","plt.plot(range(1, num_states), V[1:])\n","plt.xlabel('State')\n","plt.ylabel('Value')\n","plt.show()\n","\n","### data1 = np.load('./test.npy') \n","### data2 = np.load('./true_V.npy')\n","### if (data1 == data2).all:\n","###   print('all right')\n","### np.save('./true_V.npy', np.array('here goes the array'))"]},{"cell_type":"code","execution_count":34,"metadata":{"cellView":"form","executionInfo":{"elapsed":24,"status":"ok","timestamp":1673201790489,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"mwYmdFGCj6rX"},"outputs":[],"source":["#@title we have written known actual state distribution - in state_distribution.npy file\n","\n","# data = np.load('./state_distribution.npy') \n","# data\n","\n","np.save('./state_distribution.npy', np.array([0.00050342, 0.00051537, 0.00051983, 0.00052783, 0.00054043,\n","       0.00054095, 0.00055063, 0.0005632 , 0.0005723 , 0.00057897,\n","       0.00058859, 0.00059669, 0.00060697, 0.00060958, 0.00062441,\n","       0.00062788, 0.00064452, 0.00065015, 0.00065469, 0.00066292,\n","       0.00067739, 0.00068159, 0.00069353, 0.00069643, 0.00071412,\n","       0.00071868, 0.00072884, 0.00075045, 0.00074827, 0.0007581 ,\n","       0.00077134, 0.00077444, 0.00079118, 0.00080066, 0.00080477,\n","       0.00081647, 0.00082221, 0.00083824, 0.00085436, 0.00085694,\n","       0.00085677, 0.00088564, 0.00088572, 0.00089417, 0.00090236,\n","       0.00092084, 0.00093196, 0.00093285, 0.0009458 , 0.00096468,\n","       0.00097549, 0.00098674, 0.00099336, 0.00100658, 0.0010176 ,\n","       0.00103931, 0.00103993, 0.00105593, 0.00106808, 0.00108118,\n","       0.00109397, 0.00110813, 0.00111901, 0.0011267 , 0.0011282 ,\n","       0.00114946, 0.00116087, 0.00118098, 0.00118296, 0.00120058,\n","       0.00121217, 0.00122346, 0.00123674, 0.00125523, 0.00125821,\n","       0.00128223, 0.00128554, 0.00130661, 0.00132195, 0.00132658,\n","       0.001337  , 0.00134872, 0.00136579, 0.00136663, 0.00139404,\n","       0.00139848, 0.001419  , 0.00142815, 0.00144499, 0.00146277,\n","       0.00147143, 0.00148438, 0.00149689, 0.00150875, 0.00151946,\n","       0.00152384, 0.00154264, 0.00156307, 0.00157091, 0.00158682,\n","       0.00161182, 0.001613  , 0.0016272 , 0.00163835, 0.00164645,\n","       0.00165212, 0.00165297, 0.00168245, 0.00168208, 0.00170092,\n","       0.0017003 , 0.00172867, 0.0017209 , 0.0017406 , 0.00175318,\n","       0.00177206, 0.00177385, 0.00179813, 0.00179566, 0.00181001,\n","       0.00180087, 0.00182957, 0.00183724, 0.00185188, 0.00185671,\n","       0.00187875, 0.00187678, 0.00189374, 0.00189855, 0.00192199,\n","       0.00193324, 0.0019392 , 0.00193704, 0.00194829, 0.00196719,\n","       0.00198096, 0.00197397, 0.00200054, 0.00200993, 0.00201649,\n","       0.00201641, 0.00202741, 0.00204063, 0.00205622, 0.00206629,\n","       0.00208476, 0.00208844, 0.00209567, 0.00210024, 0.00231221,\n","       0.00232935, 0.00233713, 0.00234784, 0.00236371, 0.00235567,\n","       0.00238131, 0.00238263, 0.00238604, 0.00239073, 0.00240666,\n","       0.00240008, 0.00241935, 0.00243903, 0.00243995, 0.0024497 ,\n","       0.00246551, 0.002471  , 0.00247267, 0.00248375, 0.00249487,\n","       0.00250789, 0.00250768, 0.00251874, 0.00251356, 0.00254146,\n","       0.00255166, 0.00254615, 0.00255837, 0.00256789, 0.0025661 ,\n","       0.00258586, 0.0025787 , 0.0025969 , 0.00259521, 0.00259499,\n","       0.00261569, 0.0026059 , 0.00263882, 0.00262394, 0.00263241,\n","       0.0026314 , 0.00264764, 0.00263598, 0.00266424, 0.00267746,\n","       0.00266757, 0.00268087, 0.00269125, 0.00268402, 0.00269576,\n","       0.00270081, 0.00270205, 0.00270602, 0.00272495, 0.00273038,\n","       0.00272314, 0.00273873, 0.00273984, 0.00273815, 0.00275035,\n","       0.0027457 , 0.00276034, 0.00276651, 0.00276281, 0.00278392,\n","       0.00277704, 0.00278361, 0.00279305, 0.00280185, 0.00278799,\n","       0.00279638, 0.00278878, 0.0028044 , 0.00280381, 0.00281524,\n","       0.00279908, 0.00281635, 0.00282225, 0.00281668, 0.00283035,\n","       0.00282996, 0.00282696, 0.00283561, 0.00283615, 0.00283524,\n","       0.00283788, 0.00285564, 0.00285225, 0.0028593 , 0.00284709,\n","       0.0028474 , 0.00285042, 0.00286586, 0.00285936, 0.00286968,\n","       0.00286822, 0.00285015, 0.00285607, 0.00285977, 0.04377746,\n","       0.00286391, 0.00286415, 0.00286389, 0.00285858, 0.00285776,\n","       0.00284733, 0.00284927, 0.00285587, 0.00284777, 0.00285377,\n","       0.00284738, 0.00286175, 0.00284187, 0.00284762, 0.00284123,\n","       0.00283183, 0.00283757, 0.00284797, 0.00282714, 0.00281623,\n","       0.00282297, 0.00280843, 0.00280987, 0.0028102 , 0.00280245,\n","       0.00280249, 0.00281425, 0.0027992 , 0.00279464, 0.0027992 ,\n","       0.00279439, 0.0027763 , 0.00277307, 0.00277019, 0.00278148,\n","       0.00277169, 0.00275489, 0.00276468, 0.00275734, 0.00274451,\n","       0.00274257, 0.00274218, 0.00275072, 0.00273704, 0.00271091,\n","       0.00270943, 0.00271375, 0.00271696, 0.00271064, 0.00270924,\n","       0.00269711, 0.00269526, 0.00267912, 0.00268229, 0.00267865,\n","       0.00266763, 0.00264579, 0.00264951, 0.00264949, 0.00263432,\n","       0.00261388, 0.00262694, 0.00263121, 0.00261701, 0.00261951,\n","       0.00260331, 0.00259392, 0.00259246, 0.00258876, 0.00258065,\n","       0.00256513, 0.00256525, 0.00255693, 0.00253721, 0.00253501,\n","       0.00253614, 0.00252541, 0.00251543, 0.00250009, 0.00248048,\n","       0.00248539, 0.00249251, 0.002474  , 0.002468  , 0.00243952,\n","       0.00243311, 0.00244324, 0.00243121, 0.00242437, 0.00242182,\n","       0.00241158, 0.00237987, 0.00238674, 0.00237942, 0.00236205,\n","       0.00235565, 0.00234366, 0.00234175, 0.00232925, 0.00232238,\n","       0.00209493, 0.00210441, 0.00208087, 0.00207885, 0.00207061,\n","       0.00206829, 0.00204614, 0.00204075, 0.00202141, 0.00202227,\n","       0.00201676, 0.00199749, 0.00198952, 0.00198503, 0.0019751 ,\n","       0.00196309, 0.00193906, 0.00193038, 0.00192386, 0.00192063,\n","       0.00191264, 0.00188757, 0.00187937, 0.00187628, 0.00185891,\n","       0.00184988, 0.00184497, 0.00183835, 0.00182716, 0.00181147,\n","       0.00181871, 0.00177915, 0.00178643, 0.00177317, 0.00176385,\n","       0.00175148, 0.00174691, 0.00173135, 0.00171385, 0.00170172,\n","       0.00170096, 0.00167898, 0.00167445, 0.00165476, 0.00165786,\n","       0.0016416 , 0.00163549, 0.00162126, 0.00160757, 0.00159714,\n","       0.00157578, 0.00156311, 0.00156005, 0.00154052, 0.00152658,\n","       0.00152329, 0.00150449, 0.00149567, 0.00147009, 0.00146162,\n","       0.00146175, 0.00142883, 0.00142605, 0.00140757, 0.00139762,\n","       0.00139032, 0.00137479, 0.00136601, 0.00134701, 0.00133754,\n","       0.00132832, 0.00131237, 0.00129362, 0.00127879, 0.0012761 ,\n","       0.00126337, 0.00125122, 0.00124542, 0.00122344, 0.00120191,\n","       0.00119003, 0.00118888, 0.00116669, 0.0011473 , 0.00114438,\n","       0.00113435, 0.00112177, 0.00110673, 0.00109323, 0.00109464,\n","       0.00107809, 0.00106341, 0.00104904, 0.00102916, 0.00102669,\n","       0.00101378, 0.00100317, 0.00098875, 0.00098067, 0.00096445,\n","       0.00095721, 0.00094804, 0.00094091, 0.00091977, 0.00092059,\n","       0.00090069, 0.00089598, 0.00088747, 0.00087217, 0.00086631,\n","       0.00085375, 0.00084779, 0.00083345, 0.00082541, 0.00081696,\n","       0.00079618, 0.00078721, 0.00077798, 0.00077422, 0.00076572,\n","       0.00075419, 0.00074821, 0.0007377 , 0.00072146, 0.0007267 ,\n","       0.00070789, 0.00069964, 0.00069625, 0.00067577, 0.00067271,\n","       0.00066238, 0.00065126, 0.00064614, 0.00063833, 0.00062539,\n","       0.00061828, 0.00060413, 0.00060095, 0.00059326, 0.00058701,\n","       0.00057868, 0.000573  , 0.00056377, 0.0005522 , 0.00054181,\n","       0.00053344, 0.00052228, 0.00051599, 0.00051122, 0.00050052]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":399},"deletable":false,"editable":false,"executionInfo":{"elapsed":147605,"status":"ok","timestamp":1673201938075,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"P1YHs1f0WQNK","nbgrader":{"cell_type":"code","checksum":"c3b9e5598db38d8c1f2178d0adaa7b86","grade":false,"grade_id":"cell-a8a2e1fd49dde097","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"907a1391-aff2-48fa-9452-2b3589e0bacf"},"outputs":[],"source":["true_state_val = np.load('./true_V.npy')    \n","state_distribution = np.load('./state_distribution.npy')\n","\n","def calc_RMSVE(learned_state_val):\n","    assert(len(true_state_val) == len(learned_state_val) == len(state_distribution))\n","    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))\n","    RMSVE = np.sqrt(MSVE)\n","    return RMSVE\n","\n","# Define function to run experiment\n","def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n","    \n","    rl_glue = RLGlue(environment, agent)\n","        \n","    # save rmsve at the end of each episode\n","    agent_rmsve = np.zeros((experiment_parameters[\"num_runs\"], \n","                            int(experiment_parameters[\"num_episodes\"]/experiment_parameters[\"episode_eval_frequency\"]) + 1))\n","    \n","    # save learned state value at the end of each run\n","    agent_state_val = np.zeros((experiment_parameters[\"num_runs\"], \n","                                environment_parameters[\"num_states\"]))\n","\n","    env_info = {\"num_states\": environment_parameters[\"num_states\"],\n","                \"start_state\": environment_parameters[\"start_state\"],\n","                \"left_terminal_state\": environment_parameters[\"left_terminal_state\"],\n","                \"right_terminal_state\": environment_parameters[\"right_terminal_state\"]}\n","\n","    agent_info = {\"num_states\": environment_parameters[\"num_states\"],\n","                  \"num_hidden_layer\": agent_parameters[\"num_hidden_layer\"],\n","                  \"num_hidden_units\": agent_parameters[\"num_hidden_units\"],\n","                  \"step_size\": agent_parameters[\"step_size\"],\n","                  \"discount_factor\": environment_parameters[\"discount_factor\"],\n","                  \"beta_m\": agent_parameters[\"beta_m\"],\n","                  \"beta_v\": agent_parameters[\"beta_v\"],\n","                  \"epsilon\": agent_parameters[\"epsilon\"]\n","                 }\n","    \n","    print('Setting - Neural Network with 100 hidden units')\n","    os.system('sleep 1')\n","\n","    # one agent setting\n","    for run in tqdm(range(1, experiment_parameters[\"num_runs\"]+1)):\n","        env_info[\"seed\"] = run\n","        agent_info[\"seed\"] = run\n","        rl_glue.rl_init(agent_info, env_info)\n","        \n","        # Compute initial RMSVE before training\n","        current_V = rl_glue.rl_agent_message(\"get state value\")\n","        agent_rmsve[run-1, 0] = calc_RMSVE(current_V)\n","        \n","        for episode in range(1, experiment_parameters[\"num_episodes\"]+1):\n","            # run episode\n","            rl_glue.rl_episode(0) # no step limit\n","\n","            if episode % experiment_parameters[\"episode_eval_frequency\"] == 0:\n","                current_V = rl_glue.rl_agent_message(\"get state value\")\n","                agent_rmsve[run-1, int(episode/experiment_parameters[\"episode_eval_frequency\"])] = calc_RMSVE(current_V)\n","            elif episode == experiment_parameters[\"num_episodes\"]: # if last episode\n","                current_V = rl_glue.rl_agent_message(\"get state value\")\n","\n","        agent_state_val[run-1, :] = current_V\n","\n","    save_name = \"{}\".format(rl_glue.agent.name).replace('.','')\n","    \n","    if not os.path.exists('./results'):\n","                os.makedirs('./results')\n","    \n","    # save avg. state value\n","    np.save(\"./results/V_{}\".format(save_name), agent_state_val)\n","\n","    # save avg. rmsve\n","    np.savez(\"./results/RMSVE_{}\".format(save_name), rmsve = agent_rmsve,\n","                                                   eval_freq = experiment_parameters[\"episode_eval_frequency\"],\n","                                                   num_episodes = experiment_parameters[\"num_episodes\"])\n","\n","\n","# Run Experiment\n","\n","# Experiment parameters\n","experiment_parameters = {\n","    \"num_runs\" : 20,\n","    \"num_episodes\" : 1000,\n","    \"episode_eval_frequency\" : 10 # evaluate every 10 episode\n","}\n","\n","# Environment parameters\n","environment_parameters = {\n","    \"num_states\" : 500,\n","    \"start_state\" : 250,\n","    \"left_terminal_state\" : 0,\n","    \"right_terminal_state\" : 501,\n","    \"discount_factor\" : 1.0\n","}\n","\n","# Agent parameters\n","agent_parameters = {\n","    \"num_hidden_layer\": 1,\n","    \"num_hidden_units\": 100,\n","    \"step_size\": 0.001,\n","    \"beta_m\": 0.9,\n","    \"beta_v\": 0.999,\n","    \"epsilon\": 0.0001,\n","}\n","\n","current_env = RandomWalkEnvironment\n","current_agent = TDAgent\n","\n","# run experiment\n","run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n","\n","# plot result\n","plot_result([\"td_agent\"])\n","\n","shutil.make_archive('results', 'zip', './results')"]}],"metadata":{"colab":{"provenance":[]},"coursera":{"course_slug":"prediction-control-function-approximation","graded_item_id":"ZJrJN","launcher_item_id":"jSYQa"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
