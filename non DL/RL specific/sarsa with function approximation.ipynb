{"cells":[{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"t70L4vDvq2Yd","nbgrader":{"cell_type":"markdown","checksum":"f91befe560efc058705938cd516c0cbf","grade":false,"grade_id":"cell-d7aa7f0ccfbe6764","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Function Approximation and Control"]},{"cell_type":"code","execution_count":1,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":33,"status":"ok","timestamp":1673253663156,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"UMLf0SNOq2Yk","nbgrader":{"cell_type":"code","checksum":"c3bf50501352096f22c673e3f781ca93","grade":false,"grade_id":"cell-68be8d91fe7fd3dd","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import time\n","import numpy as np\n","import itertools\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":32,"status":"ok","timestamp":1673253663158,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"riOmG-kLq30E"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL-Glue environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","        \n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward;\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","executionInfo":{"elapsed":32,"status":"ok","timestamp":1673253663160,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Uj5Zugnxt2K2"},"outputs":[],"source":["#@title Mountaincar Environment\n","\n","class Environment(BaseEnvironment):\n","    \"\"\"Implements the environment for an RLGlue environment\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    actions = [0, 1, 2]\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.current_state = None\n","        self.reward_obs_term = (reward, observation, termination)\n","        self.count = 0\n","\n","    def env_init(self, agent_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","        local_observation = 0  # An empty NumPy array\n","\n","        self.reward_obs_term = (0.0, local_observation, False)\n","\n","\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","        position = np.random.uniform(-0.6, -0.4)\n","        velocity = 0.0\n","        self.current_state = np.array([position, velocity]) # position, velocity\n","\n","        return self.current_state\n","\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","        Args:\n","            action: The action taken by the agent\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","        position, velocity = self.current_state\n","\n","        terminal = False\n","        reward = -1.0\n","        velocity = self.bound_velocity(velocity + 0.001 * (action - 1) - 0.0025 * np.cos(3 * position))\n","        position = self.bound_position(position + velocity)\n","\n","        if position == -1.2:\n","            velocity = 0.0\n","        elif position == 0.5:\n","            self.current_state = None\n","            terminal = True\n","            reward = 0.0\n","\n","        self.current_state = np.array([position, velocity])\n","\n","        self.reward_obs_term = (reward, self.current_state, terminal)\n","\n","        return self.reward_obs_term\n","\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","        pass\n","\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","        Args:\n","            message (string): the message passed to the environment\n","        Returns:\n","            string: the response (or answer) to the message\n","        \"\"\"\n","        if message == \"what is the current reward?\":\n","            return \"{}\".format(self.reward_obs_term[0])\n","\n","        # else\n","        return \"I don't know how to respond to your message\"\n","\n","    def bound_velocity(self, velocity):\n","        if velocity > 0.07:\n","            return 0.07\n","        if velocity < -0.07:\n","            return -0.07\n","        return velocity\n","\n","    def bound_position(self, position):\n","        if position > 0.5:\n","            return 0.5\n","        if position < -1.2:\n","            return -1.2\n","        return position"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","executionInfo":{"elapsed":31,"status":"ok","timestamp":1673253663161,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"eG8Tfr0nuKzd"},"outputs":[],"source":["#@title Random Agent\n","\n","\n","class Agent(BaseAgent):\n","    \"\"\"agent does *no* learning, selects action 0 always\"\"\"\n","    def __init__(self):\n","        self.last_action = None\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","        if \"actions\" in agent_info:\n","            self.actions = agent_info[\"actions\"]\n","\n","        if \"state_array\" in agent_info:\n","            self.q_values = agent_info[\"state_array\"]\n","\n","        self.last_action = 0\n","\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","        self.last_action = np.random.choice(3)  # set first action to 0\n","\n","        return self.last_action\n","\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","        # local_action = 0  # choose the action here\n","        self.last_action = np.random.choice(3)\n","\n","        return self.last_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","        pass\n","\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","        pass\n","\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","        pass"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673253663162,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"m54EeBa0tR5P"},"outputs":[],"source":["#@title tiles (specific case of coarse coding tile coding used here with 3 tiles)\n","\n","\"\"\"\n","Tile Coding Software version 3.0beta\n","by Rich Sutton\n","based on a program created by Steph Schaeffer and others\n","External documentation and recommendations on the use of this code is available in the \n","reinforcement learning textbook by Sutton and Barto, and on the web.\n","These need to be understood before this code is.\n","\n","This software is for Python 3 or more.\n","\n","This is an implementation of grid-style tile codings, based originally on\n","the UNH CMAC code (see http://www.ece.unh.edu/robots/cmac.htm), but by now highly changed. \n","Here we provide a function, \"tiles\", that maps floating and integer\n","variables to a list of tiles, and a second function \"tiles-wrap\" that does the same while\n","wrapping some floats to provided widths (the lower wrap value is always 0).\n","\n","The float variables will be gridded at unit intervals, so generalization\n","will be by approximately 1 in each direction, and any scaling will have \n","to be done externally before calling tiles.\n","\n","Num-tilings should be a power of 2, e.g., 16. To make the offsetting work properly, it should\n","also be greater than or equal to four times the number of floats.\n","\n","The first argument is either an index hash table of a given size (created by (make-iht size)), \n","an integer \"size\" (range of the indices from 0), or nil (for testing, indicating that the tile \n","coordinates are to be returned without being converted to indices).\n","\"\"\"\n","\n","basehash = hash\n","\n","class IHT:\n","    \"Structure to handle collisions\"\n","    def __init__(self, sizeval):\n","        self.size = sizeval                        \n","        self.overfullCount = 0\n","        self.dictionary = {}\n","\n","    def __str__(self):\n","        \"Prepares a string for printing whenever this object is printed\"\n","        return \"Collision table:\" + \\\n","               \" size:\" + str(self.size) + \\\n","               \" overfullCount:\" + str(self.overfullCount) + \\\n","               \" dictionary:\" + str(len(self.dictionary)) + \" items\"\n","\n","    def count (self):\n","        return len(self.dictionary)\n","    \n","    def fullp (self):\n","        return len(self.dictionary) >= self.size\n","    \n","    def getindex (self, obj, readonly=False):\n","        d = self.dictionary\n","        if obj in d: return d[obj]\n","        elif readonly: return None\n","        size = self.size\n","        count = self.count()\n","        if count >= size:\n","            if self.overfullCount==0: print('IHT full, starting to allow collisions')\n","            self.overfullCount += 1\n","            return basehash(obj) % self.size\n","        else:\n","            d[obj] = count\n","            return count\n","\n","def hashcoords(coordinates, m, readonly=False):\n","    if type(m)==IHT: return m.getindex(tuple(coordinates), readonly)\n","    if type(m)==int: return basehash(tuple(coordinates)) % m\n","    if m==None: return coordinates\n","\n","from math import floor, log\n","from itertools import zip_longest\n","\n","def main_tiles(ihtORsize, numtilings, floats, ints=[], readonly=False):\n","    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n","    qfloats = [floor(f*numtilings) for f in floats]\n","    Tiles = []\n","    for tiling in range(numtilings):\n","        tilingX2 = tiling*2\n","        coords = [tiling]\n","        b = tiling\n","        for q in qfloats:\n","            coords.append( (q + b) // numtilings )\n","            b += tilingX2\n","        coords.extend(ints)\n","        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n","    return Tiles\n","\n","def tileswrap (ihtORsize, numtilings, floats, wrapwidths, ints=[], readonly=False):\n","    \"\"\"returns num-tilings tile indices corresponding to the floats and ints, wrapping some floats\"\"\"\n","    qfloats = [floor(f*numtilings) for f in floats]\n","    Tiles = []\n","    for tiling in range(numtilings):\n","        tilingX2 = tiling*2\n","        coords = [tiling]\n","        b = tiling\n","        for q, width in zip_longest(qfloats, wrapwidths):\n","            c = (q + b%numtilings) // numtilings\n","            coords.append(c%width if width else c)\n","            b += tilingX2\n","        coords.extend(ints)\n","        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n","    return Tiles\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1673253663163,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"gUf9VLDJtubO"},"outputs":[],"source":["def argmax(q_values):\n","    top = float(\"-inf\")\n","    ties = []\n","\n","    for i in range(len(q_values)):\n","        if q_values[i] > top:\n","            top = q_values[i]\n","            ties = []\n","\n","        if q_values[i] == top:\n","            ties.append(i)\n","\n","    return np.random.choice(ties)"]},{"cell_type":"code","execution_count":7,"metadata":{"deletable":false,"executionInfo":{"elapsed":29,"status":"ok","timestamp":1673253663164,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"M2pDtPegq2Yq","nbgrader":{"cell_type":"code","checksum":"1f9f621377db1b80790b2203fdf266a5","grade":false,"grade_id":"cell-5d4b035fb7a71186","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class MountainCarTileCoder:\n","    def __init__(self, iht_size=4096, num_tilings=8, num_tiles=8):\n","        \"\"\"\n","        Initializes the MountainCar Tile Coder\n","        Initializers:\n","        iht_size -- int, the size of the index hash table, typically a power of 2\n","        num_tilings -- int, the number of tilings\n","        num_tiles -- int, the number of tiles. Here both the width and height of the\n","                     tile coder are the same\n","        Class Variables:\n","        self.iht -- IHT, the index hash table that the tile coder will use\n","        self.num_tilings -- int, the number of tilings the tile coder will use\n","        self.num_tiles -- int, the number of tiles the tile coder will use\n","        \"\"\"\n","        self.iht = IHT(iht_size)\n","        self.num_tilings = num_tilings\n","        self.num_tiles = num_tiles\n","    \n","    def get_tiles(self, position, velocity):\n","        \"\"\"\n","        Takes in a position and velocity from the mountaincar environment\n","        and returns a numpy array of active tiles.\n","        \n","        Arguments:\n","        position -- float, the position of the agent between -1.2 and 0.5\n","        velocity -- float, the velocity of the agent between -0.07 and 0.07\n","        returns:\n","        tiles - np.array, active tiles\n","        \"\"\"\n","        # Use the ranges above and self.num_tiles to scale position and velocity to the range [0, 1]\n","        # then multiply that range with self.num_tiles so it scales from [0, num_tiles]\n","        min_position = -1.2\n","        max_position = 0.5\n","        min_velocity = -0.07\n","        max_velocity = 0.07\n","        position_scale = (position - min_position) / (max_position - min_position)\n","        position_scaled = position_scale * self.num_tiles\n","        velocity_scale = (velocity - min_velocity) / (max_velocity - min_velocity)  \n","        velocity_scaled = velocity_scale * self.num_tiles       \n","        \n","        # get the tiles using tiles, with self.iht, self.num_tilings and [scaled position, scaled velocity]\n","        # nothing to implment here\n","        tiles = main_tiles\n","        tiles = tiles(self.iht, self.num_tilings, [position_scaled, velocity_scaled])\n","        \n","        return np.array(tiles)"]},{"cell_type":"code","execution_count":8,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":28,"status":"ok","timestamp":1673253663165,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"FdLywyoEq2Yr","nbgrader":{"cell_type":"code","checksum":"734698474359c7766846c7d26ffbaf67","grade":true,"grade_id":"cell-beac2fa8ff1ef94e","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","# create a range of positions and velocities to test\n","# then test every element in the cross-product between these lists\n","pos_tests = np.linspace(-1.2, 0.5, num=5)\n","vel_tests = np.linspace(-0.07, 0.07, num=5)\n","tests = list(itertools.product(pos_tests, vel_tests))\n","\n","mctc = MountainCarTileCoder(iht_size=1024, num_tilings=8, num_tiles=2)\n","\n","t = []\n","for test in tests:\n","    position, velocity = test\n","    tiles = mctc.get_tiles(position=position, velocity=velocity)\n","    t.append(tiles)\n","\n","expected = [\n","    [0, 1, 2, 3, 4, 5, 6, 7],\n","    [0, 1, 8, 3, 9, 10, 6, 11],\n","    [12, 13, 8, 14, 9, 10, 15, 11],\n","    [12, 13, 16, 14, 17, 18, 15, 19],\n","    [20, 21, 16, 22, 17, 18, 23, 19],\n","    [0, 1, 2, 3, 24, 25, 26, 27],\n","    [0, 1, 8, 3, 28, 29, 26, 30],\n","    [12, 13, 8, 14, 28, 29, 31, 30],\n","    [12, 13, 16, 14, 32, 33, 31, 34],\n","    [20, 21, 16, 22, 32, 33, 35, 34],\n","    [36, 37, 38, 39, 24, 25, 26, 27],\n","    [36, 37, 40, 39, 28, 29, 26, 30],\n","    [41, 42, 40, 43, 28, 29, 31, 30],\n","    [41, 42, 44, 43, 32, 33, 31, 34],\n","    [45, 46, 44, 47, 32, 33, 35, 34],\n","    [36, 37, 38, 39, 48, 49, 50, 51],\n","    [36, 37, 40, 39, 52, 53, 50, 54],\n","    [41, 42, 40, 43, 52, 53, 55, 54],\n","    [41, 42, 44, 43, 56, 57, 55, 58],\n","    [45, 46, 44, 47, 56, 57, 59, 58],\n","    [60, 61, 62, 63, 48, 49, 50, 51],\n","    [60, 61, 64, 63, 52, 53, 50, 54],\n","    [65, 66, 64, 67, 52, 53, 55, 54],\n","    [65, 66, 68, 67, 56, 57, 55, 58],\n","    [69, 70, 68, 71, 56, 57, 59, 58],\n","]\n","assert np.all(expected == np.array(t))"]},{"cell_type":"code","execution_count":9,"metadata":{"deletable":false,"executionInfo":{"elapsed":28,"status":"ok","timestamp":1673253663166,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"B3K7DfrQq2Yt","nbgrader":{"cell_type":"code","checksum":"c1dd6c5e729fc638934b67090e2c92a0","grade":false,"grade_id":"cell-50303440b2e9be74","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class SarsaAgent(BaseAgent):\n","    \"\"\"\n","    Initialization of Sarsa Agent. All values are set to None so they can\n","    be initialized in the agent_init method.\n","    \"\"\"\n","    def __init__(self):\n","        self.last_action = None\n","        self.last_state = None\n","        self.epsilon = None\n","        self.gamma = None\n","        self.iht_size = None\n","        self.w = None\n","        self.alpha = None\n","        self.num_tilings = None\n","        self.num_tiles = None\n","        self.mctc = None\n","        self.initial_weights = None\n","        self.num_actions = None\n","        self.previous_tiles = None\n","\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","        self.num_tilings = agent_info.get(\"num_tilings\", 8)\n","        self.num_tiles = agent_info.get(\"num_tiles\", 8)\n","        self.iht_size = agent_info.get(\"iht_size\", 4096)\n","        self.epsilon = agent_info.get(\"epsilon\", 0.0)\n","        self.gamma = agent_info.get(\"gamma\", 1.0)\n","        self.alpha = agent_info.get(\"alpha\", 0.5) / self.num_tilings\n","        self.initial_weights = agent_info.get(\"initial_weights\", 0.0)\n","        self.num_actions = agent_info.get(\"num_actions\", 3)\n","        \n","        # We initialize self.w to three times the iht_size. Recall this is because\n","        # we need to have one set of weights for each action.\n","        self.w = np.ones((self.num_actions, self.iht_size)) * self.initial_weights\n","        \n","        # We initialize self.mctc to the mountaincar verions of the \n","        # tile coder that we created\n","        self.tc = MountainCarTileCoder(iht_size=self.iht_size, \n","                                         num_tilings=self.num_tilings, \n","                                         num_tiles=self.num_tiles)\n","\n","    def select_action(self, tiles):\n","        \"\"\"\n","        Selects an action using epsilon greedy\n","        Args:\n","        tiles - np.array, an array of active tiles\n","        Returns:\n","        (chosen_action, action_value) - (int, float), tuple of the chosen action\n","                                        and it's value\n","        \"\"\"\n","        action_values = []\n","        chosen_action = None\n","        \n","        # First loop through the weights of each action and populate action_values\n","        # with the action value for each action and tiles instance\n","        \n","        # Use np.random.random to decide if an exploritory action should be taken\n","        # and set chosen_action to a random action if it is\n","        # Otherwise choose the greedy action using the given argmax \n","        # function and the action values (don't use numpy's armax)\n","        action_values = np.zeros(self.num_actions)\n","        for i in range(self.num_actions):\n","            action_values[i] = self.w[i][tiles].sum()\n","        \n","        if np.random.random() < self.epsilon:\n","            chosen_action = np.random.choice(self.num_actions)\n","        else:\n","            chosen_action = argmax(action_values)        \n","\n","        return chosen_action, action_values[chosen_action]\n","    \n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (Numpy array): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","        position, velocity = state\n","        \n","        # Use self.tc to set active_tiles using position and velocity\n","        # set current_action to the epsilon greedy chosen action using\n","        # the select_action function above with the active tiles\n","        active_tiles = self.tc.get_tiles(position, velocity)\n","        current_action, action_value = self.select_action(active_tiles)        \n","        \n","        self.last_action = current_action\n","        self.previous_tiles = np.copy(active_tiles)\n","        return self.last_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","        # choose the action here\n","        position, velocity = state\n","        \n","        # Use self.tc to set active_tiles using position and velocity\n","        # set current_action and action_value to the epsilon greedy chosen action using\n","        # the select_action function above with the active tiles\n","        \n","        # Update self.w at self.previous_tiles and self.previous action\n","        # using the reward, action_value, self.gamma, self.w,\n","        # self.alpha, and the Sarsa update from the textbook\n","        active_tiles = self.tc.get_tiles(position, velocity)\n","        current_action, action_value = self.select_action(active_tiles)\n","        last_action_value = self.w[self.last_action][self.previous_tiles].sum()\n","        delta = reward + self.gamma * action_value - last_action_value\n","        grad = np.zeros_like(self.w)\n","        grad[self.last_action][self.previous_tiles] = 1\n","        self.w += self.alpha * delta * grad        \n","        \n","        self.last_action = current_action\n","        self.previous_tiles = np.copy(active_tiles)\n","        return self.last_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","        # Update self.w at self.previous_tiles and self.previous action\n","        # using the reward, self.gamma, self.w,\n","        # self.alpha, and the Sarsa update from the textbook\n","        # Hint - there is no action_value used here because this is the end\n","        # of the episode.\n","        active_tiles = self.tc.get_tiles(position, velocity)\n","        current_action, action_value = self.select_action(active_tiles)\n","        last_action_value = self.w[self.last_action][self.previous_tiles].sum()\n","        delta = reward - last_action_value\n","        grad = np.zeros_like(self.w)\n","        grad[self.last_action][self.previous_tiles] = 1        \n","        \n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","        pass\n","\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","        pass\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":27,"status":"ok","timestamp":1673253663167,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"lmbE1vARq2Yu","nbgrader":{"cell_type":"code","checksum":"692ac428d5e59bae3f74450657877a50","grade":true,"grade_id":"cell-0cf3e9c19ac6be06","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"outputId":"77b0ea03-afc2-497e-8fb0-1937acec145b"},"outputs":[{"name":"stdout","output_type":"stream","text":["action distribution: [ 29.  35. 936.]\n"]}],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","agent = SarsaAgent()\n","agent.agent_init({\"epsilon\": 0.1})\n","agent.w = np.array([np.array([1, 2, 3]), np.array([4, 5, 6]), np.array([7, 8, 9])])\n","\n","action_distribution = np.zeros(3)\n","for i in range(1000):\n","    chosen_action, action_value = agent.select_action(np.array([0,1]))\n","    action_distribution[chosen_action] += 1\n","    \n","print(\"action distribution:\", action_distribution)\n","# notice that the two non-greedy actions are roughly uniformly distributed\n","assert np.all(action_distribution == [29, 35, 936])\n","\n","agent = SarsaAgent()\n","agent.agent_init({\"epsilon\": 0.0})\n","agent.w = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","chosen_action, action_value = agent.select_action([0, 1])\n","assert chosen_action == 2\n","assert action_value == 15\n","\n","# -----------\n","# test update\n","# -----------\n","agent = SarsaAgent()\n","agent.agent_init({\"epsilon\": 0.1})\n","\n","agent.agent_start((0.1, 0.3))\n","agent.agent_step(1, (0.02, 0.1))\n","\n","assert np.all(agent.w[0,0:8] == 0.0625)\n","assert np.all(agent.w[1:] == 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":334},"deletable":false,"editable":false,"executionInfo":{"elapsed":19100,"status":"ok","timestamp":1673253754115,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"oL3cZDabq2Yv","nbgrader":{"cell_type":"code","checksum":"31da193410fe9153637b4e5043c81176","grade":true,"grade_id":"cell-5e2a49e089992132","locked":true,"points":25,"schema_version":3,"solution":false,"task":false},"outputId":"7e87e579-95a7-498c-d962-839cc71b2348"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","num_runs = 10\n","num_episodes = 50\n","env_info = {\"num_tiles\": 8, \"num_tilings\": 8}\n","agent_info = {}\n","all_steps = []\n","\n","agent = SarsaAgent\n","env = Environment\n","start = time.time()\n","\n","for run in range(num_runs):\n","    if run % 5 == 0:\n","        print(\"RUN: {}\".format(run))\n","\n","    rl_glue = RLGlue(env, agent)\n","    rl_glue.rl_init(agent_info, env_info)\n","    steps_per_episode = []\n","\n","    for episode in range(num_episodes):\n","        rl_glue.rl_episode(15000)\n","        steps_per_episode.append(rl_glue.num_steps)\n","\n","    all_steps.append(np.array(steps_per_episode))\n","\n","print(\"Run time: {}\".format(time.time() - start))\n","\n","mean = np.mean(all_steps, axis=0)\n","plt.plot(mean)\n","\n","# because we set the random seed, these values should be *exactly* the same\n","#assert np.allclose(mean, [1432.5, 837.9, 694.4, 571.4, 515.2, 380.6, 379.4, 369.6, 357.2, 316.5, 291.1, 305.3, 250.1, 264.9, 235.4, 242.1, 244.4, 245., 221.2, 229., 238.3, 211.2, 201.1, 208.3, 185.3, 207.1, 191.6, 204., 214.5, 207.9, 195.9, 206.4, 194.9, 191.1, 195., 186.6, 171., 177.8, 171.1, 174., 177.1, 174.5, 156.9, 174.3, 164.1, 179.3, 167.4, 156.1, 158.4, 154.4])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"deletable":false,"editable":false,"executionInfo":{"elapsed":204720,"status":"ok","timestamp":1673253970578,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"2pXBg9m2q2Yw","nbgrader":{"cell_type":"code","checksum":"80a5fffe28f2e72b4745e265bbd276a7","grade":false,"grade_id":"cell-f608c2e9a0d94727","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"1a102290-997d-4dae-eda9-311d490758ed"},"outputs":[],"source":["np.random.seed(0)\n","\n","# Compare the three\n","num_runs = 20\n","num_episodes = 100\n","env_info = {}\n","\n","agent_runs = []\n","# alphas = [0.2, 0.4, 0.5, 1.0]\n","alphas = [0.5]\n","agent_info_options = [{\"num_tiles\": 16, \"num_tilings\": 2, \"alpha\": 0.5},\n","                      {\"num_tiles\": 4, \"num_tilings\": 32, \"alpha\": 0.5},\n","                      {\"num_tiles\": 8, \"num_tilings\": 8, \"alpha\": 0.5}]\n","agent_info_options = [{\"num_tiles\" : agent[\"num_tiles\"], \n","                       \"num_tilings\": agent[\"num_tilings\"],\n","                       \"alpha\" : alpha} for agent in agent_info_options for alpha in alphas]\n","\n","agent = SarsaAgent\n","env = Environment\n","for agent_info in agent_info_options:\n","    all_steps = []\n","    start = time.time()\n","    for run in range(num_runs):\n","        if run % 5 == 0:\n","            print(\"RUN: {}\".format(run))\n","        env = Environment\n","        \n","        rl_glue = RLGlue(env, agent)\n","        rl_glue.rl_init(agent_info, env_info)\n","        steps_per_episode = []\n","\n","        for episode in range(num_episodes):\n","            rl_glue.rl_episode(15000)\n","            steps_per_episode.append(rl_glue.num_steps)\n","        all_steps.append(np.array(steps_per_episode))\n","    \n","    agent_runs.append(np.mean(np.array(all_steps), axis=0))\n","    print(\"stepsize:\", rl_glue.agent.alpha)\n","    print(\"Run Time: {}\".format(time.time() - start))\n","\n","plt.figure(figsize=(15, 10), dpi= 80, facecolor='w', edgecolor='k')\n","plt.plot(np.array(agent_runs).T)\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Steps Per Episode\")\n","plt.yscale(\"linear\")\n","plt.ylim(0, 1000)\n","plt.legend([\"num_tiles: {}, num_tilings: {}, alpha: {}\".format(agent_info[\"num_tiles\"], \n","                                                               agent_info[\"num_tilings\"],\n","                                                               agent_info[\"alpha\"])\n","            for agent_info in agent_info_options])"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
