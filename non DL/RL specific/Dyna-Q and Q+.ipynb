{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"jqiQcMR1xnkG","nbgrader":{"cell_type":"markdown","checksum":"456971b7c32e2bf5364ff3e844755588","grade":false,"grade_id":"cell-2379d0e980554734","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Dyna-Q and Dyna-Q+"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3757,"status":"ok","timestamp":1673188948977,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ovAf4Tej1EGI"},"outputs":[],"source":["!pip install -q jdc"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"e63MPrGtxnkQ","nbgrader":{"cell_type":"code","checksum":"120eb20b7f1dddd120d76b2aa7919153","grade":false,"grade_id":"cell-bee88a7e78d66006","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import jdc\n","import os\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":6,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":39,"status":"ok","timestamp":1673188948978,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"gVjEnrrDxnkS","nbgrader":{"cell_type":"code","checksum":"ee4fd0b140763673eeaa4eb9568f651c","grade":false,"grade_id":"cell-028a2dd8d19ea3a7","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["plt.rcParams.update({'font.size': 15})\n","plt.rcParams.update({'figure.figsize': [8,5]})"]},{"cell_type":"code","execution_count":7,"metadata":{"cellView":"form","executionInfo":{"elapsed":39,"status":"ok","timestamp":1673188948979,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"080eU78Ux1fX"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","executionInfo":{"elapsed":38,"status":"ok","timestamp":1673188948979,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Xtit0d5IyeRz"},"outputs":[],"source":["#@title Maze Environment\n","\n","class MazeEnvironment(BaseEnvironment):\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    def __init__(self):\n","\n","        self.maze_dim = [6, 9]\n","        self.obstacles = [[1, 2], [2, 2], [3, 2], [4, 5], [0, 7], [1, 7], [2, 7]]\n","\n","        self.start_state = [2, 0]\n","        self.end_state = [0, 8]\n","        self.current_state = [None, None]\n","\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = [reward, observation, termination]\n","\n","    def env_init(self, agent_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","        self.reward_obs_term = [0.0, None, False]\n","\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","        self.current_state = self.start_state\n","        self.reward_obs_term[1] = self.get_observation(self.current_state)\n","\n","        return self.reward_obs_term[1]\n","\n","    # check if current state is within the gridworld and return bool\n","    def out_of_bounds(self, row, col):\n","        if row < 0 or row > self.maze_dim[0]-1 or col < 0 or col > self.maze_dim[1]-1:\n","            return True\n","        else:\n","            return False\n","\n","    # check if there is an obstacle at (row, col)\n","    def is_obstacle(self, row, col):\n","        if [row, col] in self.obstacles:\n","            return True\n","        else:\n","            return False\n","\n","    def get_observation(self, state):\n","        return state[0] * self.maze_dim[1] + state[1]\n","\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","        reward = 0.0\n","        is_terminal = False\n","\n","        row = self.current_state[0]\n","        col = self.current_state[1]\n","\n","        # update current_state with the action (also check validity of action)\n","        if action == 0: # up\n","            if not (self.out_of_bounds(row-1, col) or self.is_obstacle(row-1, col)):\n","                self.current_state = [row-1, col]\n","\n","        elif action == 1: # right\n","            if not (self.out_of_bounds(row, col+1) or self.is_obstacle(row, col+1)):\n","                self.current_state = [row, col+1]\n","\n","        elif action == 2: # down\n","            if not (self.out_of_bounds(row+1, col) or self.is_obstacle(row+1, col)):\n","                self.current_state = [row+1, col]\n","\n","        elif action == 3: # left\n","            if not (self.out_of_bounds(row, col-1) or self.is_obstacle(row, col-1)):\n","                self.current_state = [row, col-1]\n","\n","        if self.current_state == self.end_state: # terminate if goal is reached\n","            reward = 1.0\n","            is_terminal = True\n","\n","        self.reward_obs_term = [reward, self.get_observation(self.current_state), is_terminal]\n","\n","        return self.reward_obs_term\n","\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","        current_state = None\n","\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message (string): the message passed to the environment\n","\n","        Returns:\n","            string: the response (or answer) to the message\n","        \"\"\"\n","        if message == \"what is the current reward?\":\n","            return \"{}\".format(self.reward_obs_term[0])\n","\n","        # else\n","        return \"I don't know how to respond to your message\"\n","\n","\n","\n","\n","class ShortcutMazeEnvironment(BaseEnvironment):\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    def __init__(self):\n","\n","        self.maze_dim = [6,9]\n","        self.obstacles = [[3,1],[3,2],[3,3],[3,4],[3,5],[3,6],[3,7],[3,8]]\n","\n","        self.start_state = [5,3]\n","        self.end_state = [0,8]\n","        self.current_state = [None, None]\n","\n","        # a shortcut opens up after n timesteps\n","        self.change_at_n = 0\n","        self.timesteps = 0\n","\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = [reward, observation, termination]\n","\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","        self.change_at_n = env_info.get('change_at_n', 100000)\n","        self.timesteps = 0\n","        self.reward_obs_term = [0.0, None, False]\n","\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","        self.current_state = self.start_state\n","        self.reward_obs_term[1] = self.get_observation(self.current_state)\n","\n","        return self.reward_obs_term[1]\n","\n","    # check if current state is within the gridworld and return bool\n","    def out_of_bounds(self, row, col):\n","        if row < 0 or row > self.maze_dim[0]-1 or col < 0 or col > self.maze_dim[1]-1:\n","            return True\n","        else:\n","            return False\n","\n","    # check if there is an obstacle at (row, col)\n","    def is_obstacle(self, row, col):\n","        if [row, col] in self.obstacles:\n","            return True\n","        else:\n","            return False\n","\n","    def get_observation(self, state):\n","        return state[0] * self.maze_dim[1] + state[1]\n","\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","        self.timesteps += 1\n","        if self.timesteps == self.change_at_n:\n","            self.obstacles = self.obstacles[:-1]\n","\n","        reward = 0.0\n","        is_terminal = False\n","\n","        row = self.current_state[0]\n","        col = self.current_state[1]\n","\n","        # update current_state with the action (also check validity of action)\n","        if action == 0: # up\n","            if not (self.out_of_bounds(row-1, col) or self.is_obstacle(row-1, col)):\n","                self.current_state = [row-1, col]\n","\n","        elif action == 1: # right\n","            if not (self.out_of_bounds(row, col+1) or self.is_obstacle(row, col+1)):\n","                self.current_state = [row, col+1]\n","\n","        elif action == 2: # down\n","            if not (self.out_of_bounds(row+1, col) or self.is_obstacle(row+1, col)):\n","                self.current_state = [row+1, col]\n","\n","        elif action == 3: # left\n","            if not (self.out_of_bounds(row, col-1) or self.is_obstacle(row, col-1)):\n","                self.current_state = [row, col-1]\n","\n","        if self.current_state == self.end_state: # terminate if goal is reached\n","            reward = 1.0\n","            is_terminal = True\n","\n","        self.reward_obs_term = [reward, self.get_observation(self.current_state), is_terminal]\n","\n","        return self.reward_obs_term\n","\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","        current_state = None\n","\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message (string): the message passed to the environment\n","\n","        Returns:\n","            string: the response (or answer) to the message\n","        \"\"\"\n","        if message == \"what is the current reward?\":\n","            return \"{}\".format(self.reward_obs_term[0])\n","\n","        # else\n","        return \"I don't know how to respond to your message\"\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"8oEuCG12xnkT","nbgrader":{"cell_type":"markdown","checksum":"8af78c99916d2bef7b8950c06c91ca1b","grade":false,"grade_id":"cell-05b0c5c488d26a90","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Section 1: Dyna-Q"]},{"cell_type":"code","execution_count":9,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":37,"status":"ok","timestamp":1673188948980,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"_dQw5h7yxnkW","nbgrader":{"cell_type":"code","checksum":"fcc0e80f7f9aee52e7128caa88d2c7ba","grade":false,"grade_id":"cell-5d0e8c43378d5e30","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["class DynaQAgent(BaseAgent):\n","\n","    def agent_init(self, agent_info):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","\n","        Args:\n","            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n","            {\n","                num_states (int): The number of states,\n","                num_actions (int): The number of actions,\n","                epsilon (float): The parameter for epsilon-greedy exploration,\n","                step_size (float): The step-size,\n","                discount (float): The discount factor,\n","                planning_steps (int): The number of planning steps per environmental interaction\n","\n","                random_seed (int): the seed for the RNG used in epsilon-greedy\n","                planning_random_seed (int): the seed for the RNG used in the planner\n","            }\n","        \"\"\"\n","\n","        # First, we get the relevant information from agent_info \n","        # NOTE: we use np.random.RandomState(seed) to set the two different RNGs\n","        # for the planner and the rest of the code\n","        try:\n","            self.num_states = agent_info[\"num_states\"]\n","            self.num_actions = agent_info[\"num_actions\"]\n","        except:\n","            print(\"You need to pass both 'num_states' and 'num_actions' \\\n","                   in agent_info to initialize the action-value table\")\n","        self.gamma = agent_info.get(\"discount\", 0.95)\n","        self.step_size = agent_info.get(\"step_size\", 0.1)\n","        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n","        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n","\n","        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n","        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n","\n","        # Next, we initialize the attributes required by the agent, e.g., q_values, model, etc.\n","        # A simple way to implement the model is to have a dictionary of dictionaries, \n","        #        mapping each state to a dictionary which maps actions to (reward, next state) tuples.\n","        self.q_values = np.zeros((self.num_states, self.num_actions))\n","        self.actions = list(range(self.num_actions))\n","        self.past_action = -1\n","        self.past_state = -1\n","        self.model = {} # model is a dictionary of dictionaries, which maps states to actions to \n","                        # (reward, next_state) tuples"]},{"cell_type":"code","execution_count":10,"metadata":{"deletable":false,"executionInfo":{"elapsed":37,"status":"ok","timestamp":1673188948980,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"doS2C5dxxnkY","nbgrader":{"cell_type":"code","checksum":"d6dd59f9c730360c26df3035b85ea17a","grade":false,"grade_id":"cell-59c91c0887f0eaea","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to DynaQAgent\n","\n","def update_model(self, past_state, past_action, state, reward):\n","    \"\"\"updates the model \n","    \n","    Args:\n","        past_state       (int): s\n","        past_action      (int): a\n","        state            (int): s'\n","        reward           (int): r\n","    Returns:\n","        Nothing\n","    \"\"\"\n","    # Update the model with the (s,a,s',r) tuple (1~4 lines)\n","    \n","    self.model[past_state] = self.model.get(past_state, {}) \n","    self.model[past_state][past_action] = state, reward    "]},{"cell_type":"code","execution_count":11,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":37,"status":"ok","timestamp":1673188948981,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"X27DZk61xnkY","nbgrader":{"cell_type":"code","checksum":"ab016ddc9bcf9816b2a62407532dede7","grade":true,"grade_id":"cell-d4fa9f9e0a14ccfa","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["actions = []\n","agent_info = {\"num_actions\": 4, \n","              \"num_states\": 3, \n","              \"epsilon\": 0.1, \n","              \"step_size\": 0.1, \n","              \"discount\": 1.0, \n","              \"random_seed\": 0,\n","              \"planning_random_seed\": 0}\n","\n","agent = DynaQAgent()\n","agent.agent_init(agent_info)\n","\n","# (past_state, past_action, state, reward)\n","agent.update_model(0,2,0,1)\n","agent.update_model(2,0,1,1)\n","agent.update_model(0,3,1,2)\n","\n","expected_model = {\n","    # action 2 in state 0 leads back to state 0 with a reward of 1\n","    # or taking action 3 leads to state 1 with reward of 2\n","    0: {\n","        2: (0, 1),\n","        3: (1, 2),\n","    },\n","    # taking action 0 in state 2 leads to state 1 with a reward of 1\n","    2: {\n","        0: (1, 1),\n","    },\n","}\n","\n","assert agent.model == expected_model\n"]},{"cell_type":"code","execution_count":12,"metadata":{"deletable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673188948981,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"jqDlxyYlxnkZ","nbgrader":{"cell_type":"code","checksum":"2c48cb05d902ca761858cc4c81846350","grade":false,"grade_id":"cell-1a90876a079f6ea2","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to DynaQAgent\n","\n","def planning_step(self):\n","    \"\"\"performs planning, i.e. indirect RL.\n","\n","    Args:\n","        None\n","    Returns:\n","        Nothing\n","    \"\"\"\n","    \n","    # The indirect RL step:\n","    # - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)\n","    # - Query the model with this state-action pair for the predicted next state and reward.(~1 line)\n","    # - Update the action values with this simulated experience.                            (2~4 lines)\n","    # - Repeat for the required number of planning steps.\n","    #\n","    # Note that the update equation is different for terminal and non-terminal transitions. \n","    # To differentiate between a terminal and a non-terminal next state, assume that the model stores\n","    # the terminal state as a dummy state like -1\n","\n","\n","    for i in range(self.planning_steps):\n","        past_state = self.planning_rand_generator.choice(list(self.model.keys()))\n","        past_action = self.planning_rand_generator.choice(list(self.model[past_state].keys()))\n","        next_state, reward = self.model[past_state][past_action]\n","    \n","        if next_state == -1:\n","            q_max = 0\n","        else:\n","            q_max = np.max(self.q_values[next_state])\n","    \n","        self.q_values[past_state, past_action] += self.step_size * (reward + self.gamma * q_max - self.q_values[past_state, past_action])    "]},{"cell_type":"code","execution_count":13,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673188948982,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"KNA1CRBVxnka","nbgrader":{"cell_type":"code","checksum":"f8e02d9152bf919f6755239ef071f37c","grade":true,"grade_id":"cell-8ae4b7a941ad7767","locked":true,"points":20,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["np.random.seed(0)\n","\n","actions = []\n","agent_info = {\"num_actions\": 4, \n","              \"num_states\": 3, \n","              \"epsilon\": 0.1, \n","              \"step_size\": 0.1, \n","              \"discount\": 1.0, \n","              \"planning_steps\": 4,\n","              \"random_seed\": 0,\n","              \"planning_random_seed\": 5}\n","\n","agent = DynaQAgent()\n","agent.agent_init(agent_info)\n","\n","agent.update_model(0,2,1,1)\n","agent.update_model(2,0,1,1)\n","agent.update_model(0,3,0,1)\n","agent.update_model(0,1,-1,1)\n","\n","expected_model = {\n","    0: {\n","        2: (1, 1),\n","        3: (0, 1),\n","        1: (-1, 1),\n","    },\n","    2: {\n","        0: (1, 1),\n","    },\n","}\n","\n","assert agent.model == expected_model\n","\n","agent.planning_step()\n","\n","expected_values = np.array([\n","    [0, 0.1, 0, 0.2],\n","    [0, 0, 0, 0],\n","    [0.1, 0, 0, 0],\n","])\n","assert np.all(np.isclose(agent.q_values, expected_values))"]},{"cell_type":"code","execution_count":14,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673188948982,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"GV6kePJcxnkb","nbgrader":{"cell_type":"code","checksum":"7d55430e58877032febb23ecb4ba8efd","grade":false,"grade_id":"cell-cc975f6b2f1a6661","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to DynaQAgent\n","\n","def argmax(self, q_values):\n","    \"\"\"argmax with random tie-breaking\n","    Args:\n","        q_values (Numpy array): the array of action values\n","    Returns:\n","        action (int): an action with the highest value\n","    \"\"\"\n","    top = float(\"-inf\")\n","    ties = []\n","\n","    for i in range(len(q_values)):\n","        if q_values[i] > top:\n","            top = q_values[i]\n","            ties = []\n","\n","        if q_values[i] == top:\n","            ties.append(i)\n","\n","    return self.rand_generator.choice(ties)\n","\n","def choose_action_egreedy(self, state):\n","    \"\"\"returns an action using an epsilon-greedy policy w.r.t. the current action-value function.\n","\n","    Important: assume you have a random number generator 'rand_generator' as a part of the class\n","                which you can use as self.rand_generator.choice() or self.rand_generator.rand()\n","\n","    Args:\n","        state (List): coordinates of the agent (two elements)\n","    Returns:\n","        The action taken w.r.t. the aforementioned epsilon-greedy policy\n","    \"\"\"\n","\n","    if self.rand_generator.rand() < self.epsilon:\n","        action = self.rand_generator.choice(self.actions)\n","    else:\n","        values = self.q_values[state]\n","        action = self.argmax(values)\n","\n","    return action"]},{"cell_type":"code","execution_count":15,"metadata":{"deletable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673188948983,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"WqrjHQeBxnkc","nbgrader":{"cell_type":"code","checksum":"ae45bcd826ba619bf18f2513c80b4079","grade":false,"grade_id":"cell-34d9e8a161d6e5b4","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to DynaQAgent\n","\n","def agent_start(self, state):\n","    \"\"\"The first method called when the experiment starts, \n","    called after the environment starts.\n","    Args:\n","        state (Numpy array): the state from the\n","            environment's env_start function.\n","    Returns:\n","        (int) the first action the agent takes.\n","    \"\"\"\n","    \n","    # given the state, select the action using self.choose_action_egreedy()), \n","    # and save current state and action (~2 lines)\n","\n","    action = self.choose_action_egreedy(state)\n","    self.past_state = state\n","    self.past_action = action    \n","    \n","    return self.past_action\n","\n","def agent_step(self, reward, state):\n","    \"\"\"A step taken by the agent.\n","\n","    Args:\n","        reward (float): the reward received for taking the last action taken\n","        state (Numpy array): the state from the\n","            environment's step based on where the agent ended up after the\n","            last step\n","    Returns:\n","        (int) The action the agent takes given this state.\n","    \"\"\"\n","    \n","    # - Direct-RL step (~1-3 lines)\n","    # - Model Update step (~1 line)\n","    # - `planning_step` (~1 line)\n","    # - Action Selection step (~1 line)\n","    # Save the current state and action before returning the action to be performed. (~2 lines)\n","\n","    q_max = np.max(self.q_values[state])\n","    self.q_values[self.past_state, self.past_action] += self.step_size * (reward + self.gamma * q_max - self.q_values[self.past_state, self.past_action])\n","    self.update_model(self.past_state, self.past_action, state, reward)\n","    self.planning_step()\n","    action = self.choose_action_egreedy(state)\n","    self.past_state = state\n","    self.past_action = action   \n","    \n","    return self.past_action\n","\n","def agent_end(self, reward):\n","    \"\"\"Called when the agent terminates.\n","\n","    Args:\n","        reward (float): the reward the agent received for entering the\n","            terminal state.\n","    \"\"\"\n","    \n","    # - Direct RL update with this final transition (1~2 lines)\n","    # - Model Update step with this final transition (~1 line)\n","    # - One final `planning_step` (~1 line)\n","\n","    self.q_values[self.past_state, self.past_action] += self.step_size * (reward - self.q_values[self.past_state, self.past_action])\n","    self.update_model(self.past_state, self.past_action, -1, reward)\n","    self.planning_step()    "]},{"cell_type":"code","execution_count":16,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":766,"status":"ok","timestamp":1673188949715,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"JpXTJ2laxnke","nbgrader":{"cell_type":"code","checksum":"8ce595f374dc31897a6698cae3652bef","grade":true,"grade_id":"cell-02b41cfa4e281a4f","locked":true,"points":20,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","agent_info = {\"num_actions\": 4, \n","              \"num_states\": 3, \n","              \"epsilon\": 0.1, \n","              \"step_size\": 0.1, \n","              \"discount\": 1.0, \n","              \"random_seed\": 0,\n","              \"planning_steps\": 2,\n","              \"planning_random_seed\": 0}\n","\n","agent = DynaQAgent()\n","agent.agent_init(agent_info)\n","\n","# ----------------\n","# test agent start\n","# ----------------\n","\n","action = agent.agent_start(0)\n","\n","assert action == 1\n","assert agent.model == {}\n","assert np.all(agent.q_values == 0)\n","\n","# ---------------\n","# test agent step\n","# ---------------\n","\n","action = agent.agent_step(1, 2)\n","assert action == 3\n","\n","action = agent.agent_step(0, 1)\n","assert action == 1\n","\n","expected_model = {\n","    0: {\n","        1: (2, 1),\n","    },\n","    2: {\n","        3: (1, 0),\n","    },\n","}\n","assert agent.model == expected_model\n","\n","expected_values = np.array([\n","    [0, 0.3439, 0, 0],\n","    [0, 0, 0, 0],\n","    [0, 0, 0, 0],\n","])\n","assert np.allclose(agent.q_values, expected_values)\n","\n","# --------------\n","# test agent end\n","# --------------\n","\n","agent.agent_end(1)\n","\n","expected_model = {\n","    0: {\n","        1: (2, 1),\n","    },\n","    2: {\n","        3: (1, 0),\n","    },\n","    1: {\n","        1: (-1, 1),\n","    },\n","}\n","assert agent.model == expected_model\n","\n","expected_values = np.array([\n","    [0, 0.41051, 0, 0],\n","    [0, 0.1, 0, 0],\n","    [0, 0, 0, 0.01],\n","])\n","assert np.allclose(agent.q_values, expected_values)"]},{"cell_type":"code","execution_count":17,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":13,"status":"ok","timestamp":1673188949716,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"e2r1N642xnke","nbgrader":{"cell_type":"code","checksum":"6f1ce118374c859b81ca1a743bc1bd9b","grade":false,"grade_id":"cell-744f017993777ec8","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def run_experiment(env, agent, env_parameters, agent_parameters, exp_parameters):\n","\n","    # Experiment settings\n","    num_runs = exp_parameters['num_runs']\n","    num_episodes = exp_parameters['num_episodes']\n","    planning_steps_all = agent_parameters['planning_steps']\n","\n","    env_info = env_parameters                     \n","    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  # We pass the agent the information it needs. \n","                  \"num_actions\" : agent_parameters[\"num_actions\"],\n","                  \"epsilon\": agent_parameters[\"epsilon\"], \n","                  \"discount\": env_parameters[\"discount\"],\n","                  \"step_size\" : agent_parameters[\"step_size\"]}\n","\n","    all_averages = np.zeros((len(planning_steps_all), num_runs, num_episodes)) # for collecting metrics \n","    log_data = {'planning_steps_all' : planning_steps_all}                     # that shall be plotted later\n","\n","    for idx, planning_steps in enumerate(planning_steps_all):\n","\n","        print('Planning steps : ', planning_steps)\n","        os.system('sleep 0.5')                    # to prevent tqdm printing out-of-order before the above print()\n","        agent_info[\"planning_steps\"] = planning_steps  \n","\n","        for i in tqdm(range(num_runs)):\n","\n","            agent_info['random_seed'] = i\n","            agent_info['planning_random_seed'] = i\n","\n","            rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n","            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n","\n","            for j in range(num_episodes):\n","\n","                rl_glue.rl_start()                # We start an episode. Here we aren't using rl_glue.rl_episode()\n","                                                  # like the other assessments because we'll be requiring some \n","                is_terminal = False               # data from within the episodes in some of the experiments here \n","                num_steps = 0\n","                while not is_terminal:\n","                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step \n","                    num_steps += 1                                      # and return the reward and action taken.\n","\n","                all_averages[idx][i][j] = num_steps\n","\n","    log_data['all_averages'] = all_averages\n","    \n","    return log_data\n","    \n","\n","def plot_steps_per_episode(data):\n","    all_averages = data['all_averages']\n","    planning_steps_all = data['planning_steps_all']\n","\n","    for i, planning_steps in enumerate(planning_steps_all):\n","        plt.plot(np.mean(all_averages[i], axis=0), label='Planning steps = '+str(planning_steps))\n","\n","    plt.legend(loc='upper right')\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Steps\\nper\\nepisode', rotation=0, labelpad=40)\n","    plt.axhline(y=16, linestyle='--', color='grey', alpha=0.4)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":448},"deletable":false,"editable":false,"executionInfo":{"elapsed":143674,"status":"ok","timestamp":1673189093378,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"tkchRuSLxnkf","nbgrader":{"cell_type":"code","checksum":"f4b740a35fbe720e8ecc73ade69dd3cd","grade":false,"grade_id":"cell-b7c90063cc0888e0","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"30964bb6-d1bc-40e9-afc6-4fccc4755968"},"outputs":[],"source":["# Experiment parameters\n","experiment_parameters = {\n","    \"num_runs\" : 30,                     # The number of times we run the experiment\n","    \"num_episodes\" : 40,                 # The number of episodes per experiment\n","}\n","\n","# Environment parameters\n","environment_parameters = { \n","    \"discount\": 0.95,\n","}\n","\n","# Agent parameters\n","agent_parameters = {  \n","    \"num_states\" : 54,\n","    \"num_actions\" : 4, \n","    \"epsilon\": 0.1, \n","    \"step_size\" : 0.125,\n","    \"planning_steps\" : [0, 5, 50]       # The list of planning_steps we want to try\n","}\n","\n","current_env = ShortcutMazeEnvironment   # The environment\n","current_agent = DynaQAgent              # The agent\n","\n","dataq = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n","plot_steps_per_episode(dataq)   "]},{"cell_type":"code","execution_count":19,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":14,"status":"ok","timestamp":1673189093379,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"PicHIP91xnkg","nbgrader":{"cell_type":"code","checksum":"e89fe28e52a88aeed2388ac7afad4ab3","grade":false,"grade_id":"cell-422bb22d0465830f","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["## Dyna-Q in changing environment\n","\n","def run_experiment_with_state_visitations(env, agent, env_parameters, agent_parameters, exp_parameters, result_file_name):\n","\n","    # Experiment settings\n","    num_runs = exp_parameters['num_runs']\n","    num_max_steps = exp_parameters['num_max_steps']\n","    planning_steps_all = agent_parameters['planning_steps']\n","\n","    env_info = {\"change_at_n\" : env_parameters[\"change_at_n\"]}                     \n","    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  \n","                  \"num_actions\" : agent_parameters[\"num_actions\"],\n","                  \"epsilon\": agent_parameters[\"epsilon\"], \n","                  \"discount\": env_parameters[\"discount\"],\n","                  \"step_size\" : agent_parameters[\"step_size\"]}\n","\n","    state_visits_before_change = np.zeros((len(planning_steps_all), num_runs, 54))  # For saving the number of\n","    state_visits_after_change = np.zeros((len(planning_steps_all), num_runs, 54))   #     state-visitations \n","    cum_reward_all = np.zeros((len(planning_steps_all), num_runs, num_max_steps))   # For saving the cumulative reward\n","    log_data = {'planning_steps_all' : planning_steps_all}\n","\n","    for idx, planning_steps in enumerate(planning_steps_all):\n","\n","        print('Planning steps : ', planning_steps)\n","        os.system('sleep 1')          # to prevent tqdm printing out-of-order before the above print()\n","        agent_info[\"planning_steps\"] = planning_steps  # We pass the agent the information it needs. \n","\n","        for run in tqdm(range(num_runs)):\n","\n","            agent_info['random_seed'] = run\n","            agent_info['planning_random_seed'] = run\n","\n","            rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose above\n","            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n","\n","            num_steps = 0\n","            cum_reward = 0\n","\n","            while num_steps < num_max_steps-1 :\n","\n","                state, _ = rl_glue.rl_start()  # We start the experiment. We'll be collecting the \n","                is_terminal = False            # state-visitation counts to visiualize the learned policy\n","                if num_steps < env_parameters[\"change_at_n\"]: \n","                    state_visits_before_change[idx][run][state] += 1\n","                else:\n","                    state_visits_after_change[idx][run][state] += 1\n","\n","                while not is_terminal and num_steps < num_max_steps-1 :\n","                    reward, state, action, is_terminal = rl_glue.rl_step()  \n","                    num_steps += 1\n","                    cum_reward += reward\n","                    cum_reward_all[idx][run][num_steps] = cum_reward\n","                    if num_steps < env_parameters[\"change_at_n\"]:\n","                        state_visits_before_change[idx][run][state] += 1\n","                    else:\n","                        state_visits_after_change[idx][run][state] += 1\n","\n","    log_data['state_visits_before'] = state_visits_before_change\n","    log_data['state_visits_after'] = state_visits_after_change\n","    log_data['cum_reward_all'] = cum_reward_all\n","    \n","    return log_data\n","\n","def plot_cumulative_reward(data_all, item_key, y_key, y_axis_label, legend_prefix, title):\n","    data_y_all = data_all[y_key]\n","    items = data_all[item_key]\n","\n","    for i, item in enumerate(items):\n","        plt.plot(np.mean(data_y_all[i], axis=0), label=legend_prefix+str(item))\n","\n","    plt.axvline(x=3000, linestyle='--', color='grey', alpha=0.4)\n","    plt.xlabel('Timesteps')\n","    plt.ylabel(y_axis_label, rotation=0, labelpad=60)\n","    plt.legend(loc='upper left')\n","    plt.title(title)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468},"deletable":false,"editable":false,"executionInfo":{"elapsed":214078,"status":"ok","timestamp":1673189307444,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"5j9JcNmpxnkh","nbgrader":{"cell_type":"code","checksum":"20b0026f54442a7ba37d7096128e03ed","grade":false,"grade_id":"cell-9f7872900ce6b40f","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"e2d64b4d-10bc-4b9b-8d16-84d37a3f43a5"},"outputs":[],"source":["# Experiment parameters\n","experiment_parameters = {\n","    \"num_runs\" : 10,                     # The number of times we run the experiment\n","    \"num_max_steps\" : 6000,              # The number of steps per experiment\n","}\n","\n","# Environment parameters\n","environment_parameters = { \n","    \"discount\": 0.95,\n","    \"change_at_n\": 3000\n","}\n","\n","# Agent parameters\n","agent_parameters = {  \n","    \"num_states\" : 54,\n","    \"num_actions\" : 4, \n","    \"epsilon\": 0.1, \n","    \"step_size\" : 0.125,\n","    \"planning_steps\" : [5, 10, 50]      # The list of planning_steps we want to try\n","}\n","\n","current_env = ShortcutMazeEnvironment   # The environment\n","current_agent = DynaQAgent              # The agent\n","\n","dataq = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, \"Dyna-Q_shortcut_steps\")    \n","plot_cumulative_reward(dataq, 'planning_steps_all', 'cum_reward_all', 'Cumulative\\nreward', 'Planning steps = ', 'Dyna-Q : Varying planning_steps')"]},{"cell_type":"code","execution_count":21,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":10,"status":"ok","timestamp":1673189307445,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"tXINgwplxnki","nbgrader":{"cell_type":"code","checksum":"bfe46c5772be65c97fa8ba81d947f985","grade":false,"grade_id":"cell-c21d98bc4f7296d6","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def plot_state_visitations(data, plot_titles, idx):\n","    data_keys = [\"state_visits_before\", \"state_visits_after\"]\n","    positions = [211,212]\n","    titles = plot_titles\n","    wall_ends = [None,-1]\n","\n","    for i in range(2):\n","\n","        state_visits = data[data_keys[i]][idx]\n","        average_state_visits = np.mean(state_visits, axis=0)\n","        grid_state_visits = np.rot90(average_state_visits.reshape((6,9)).T)\n","        grid_state_visits[2,1:wall_ends[i]] = np.nan # walls\n","        #print(average_state_visits.reshape((6,9)))\n","        plt.subplot(positions[i])\n","        plt.pcolormesh(grid_state_visits, edgecolors='gray', linewidth=1, cmap='viridis')\n","        plt.text(3+0.5, 0+0.5, 'S', horizontalalignment='center', verticalalignment='center')\n","        plt.text(8+0.5, 5+0.5, 'G', horizontalalignment='center', verticalalignment='center')\n","        plt.title(titles[i])\n","        plt.axis('off')\n","        cm = plt.get_cmap()\n","        cm.set_bad('gray')\n","\n","    plt.subplots_adjust(bottom=0.0, right=0.7, top=1.0)\n","    cax = plt.axes([1., 0.0, 0.075, 1.])\n","    cbar = plt.colorbar(cax=cax)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"deletable":false,"editable":false,"executionInfo":{"elapsed":427,"status":"ok","timestamp":1673189307862,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"AMQyZ4hTxnkj","nbgrader":{"cell_type":"code","checksum":"ee68fcbd81419dd6d30abaaa38f5a48d","grade":false,"grade_id":"cell-aa17be852a4fa1e1","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"891daba3-7750-4c58-a87a-f0bfbd93687e"},"outputs":[],"source":["plot_state_visitations(dataq, ['Dyna-Q : State visitations before the env changes', 'Dyna-Q : State visitations after the env changes'], 1)"]},{"cell_type":"code","execution_count":23,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":11,"status":"ok","timestamp":1673189307863,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"nwPnjZY-xnkj","nbgrader":{"cell_type":"code","checksum":"c2dcbc40b05319c4b4efc75ae0128e4d","grade":false,"grade_id":"cell-27a96a3ebc8bd13a","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def run_experiment_only_cumulative_reward(env, agent, env_parameters, agent_parameters, exp_parameters):\n","\n","    # Experiment settings\n","    num_runs = exp_parameters['num_runs']\n","    num_max_steps = exp_parameters['num_max_steps']\n","    epsilons = agent_parameters['epsilons']\n","\n","    env_info = {\"change_at_n\" : env_parameters[\"change_at_n\"]}                     \n","    agent_info = {\"num_states\" : agent_parameters[\"num_states\"],  \n","                  \"num_actions\" : agent_parameters[\"num_actions\"],\n","                  \"planning_steps\": agent_parameters[\"planning_steps\"], \n","                  \"discount\": env_parameters[\"discount\"],\n","                  \"step_size\" : agent_parameters[\"step_size\"]}\n","\n","    log_data = {'epsilons' : epsilons} \n","    cum_reward_all = np.zeros((len(epsilons), num_runs, num_max_steps))\n","\n","    for eps_idx, epsilon in enumerate(epsilons):\n","\n","        print('Agent : Dyna-Q, epsilon : %f' % epsilon)\n","        os.system('sleep 1')          # to prevent tqdm printing out-of-order before the above print()\n","        agent_info[\"epsilon\"] = epsilon\n","\n","        for run in tqdm(range(num_runs)):\n","\n","            agent_info['random_seed'] = run\n","            agent_info['planning_random_seed'] = run\n","\n","            rl_glue = RLGlue(env, agent)  # Creates a new RLGlue experiment with the env and agent we chose above\n","            rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n","\n","            num_steps = 0\n","            cum_reward = 0\n","\n","            while num_steps < num_max_steps-1 :\n","\n","                rl_glue.rl_start()  # We start the experiment\n","                is_terminal = False\n","\n","                while not is_terminal and num_steps < num_max_steps-1 :\n","                    reward, _, action, is_terminal = rl_glue.rl_step()  # The environment and agent take a step and return\n","                    # the reward, and action taken.\n","                    num_steps += 1\n","                    cum_reward += reward\n","                    cum_reward_all[eps_idx][run][num_steps] = cum_reward\n","\n","    log_data['cum_reward_all'] = cum_reward_all\n","    return log_data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":536},"deletable":false,"editable":false,"executionInfo":{"elapsed":258031,"status":"error","timestamp":1673189565883,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"rVMmWcjJxnkk","nbgrader":{"cell_type":"code","checksum":"35b1244013e6641a28af6ee1c5e19020","grade":false,"grade_id":"cell-7e4c0e42c445b2dc","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"b5532897-472b-4f81-9cf4-1af664b55918"},"outputs":[],"source":["# Experiment parameters\n","experiment_parameters = {\n","    \"num_runs\" : 30,                     # The number of times we run the experiment\n","    \"num_max_steps\" : 6000,              # The number of steps per experiment\n","}\n","\n","# Environment parameters\n","environment_parameters = { \n","    \"discount\": 0.95,\n","    \"change_at_n\": 3000\n","}\n","\n","# Agent parameters\n","agent_parameters = {  \n","    \"num_states\" : 54,\n","    \"num_actions\" : 4, \n","    \"step_size\" : 0.125,\n","    \"planning_steps\" : 10,\n","    \"epsilons\": [0.1, 0.2, 0.4, 0.8]    # The list of epsilons we want to try\n","}\n","\n","current_env = ShortcutMazeEnvironment   # The environment\n","current_agent = DynaQAgent              # The agent\n","\n","data = run_experiment_only_cumulative_reward(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n","plot_cumulative_reward(data, 'epsilons', 'cum_reward_all', 'Cumulative\\nreward', r'$\\epsilon$ = ', r'Dyna-Q : Varying $\\epsilon$')"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"_EcHbETNxnkk","nbgrader":{"cell_type":"markdown","checksum":"88675c8ce603f560311089a74104f394","grade":false,"grade_id":"cell-62df4f966a370995","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Section 2: Dyna-Q+"]},{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"q_jvyLQexnkl","nbgrader":{"cell_type":"markdown","checksum":"15faa0c27e0b1427655f666914540c23","grade":false,"grade_id":"cell-7961458a916a28a8","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["The motivation behind Dyna-Q+ is to give a bonus reward for actions that haven't been tried for a long time, since there is a greater chance that the dynamics for that actions might have changed.\n","\n","In particular, if the modeled reward for a transition is $r$, and the transition has not been tried in $\\tau(s,a)$ time steps, then planning updates are done as if that transition produced a reward of $r + \\kappa \\sqrt{ \\tau(s,a)}$, for some small $\\kappa$. \n"]},{"cell_type":"code","execution_count":25,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":321,"status":"ok","timestamp":1673189571850,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"KiYDppx7xnkl","nbgrader":{"cell_type":"code","checksum":"f941a227e6e8174f497769e87d5968b5","grade":false,"grade_id":"cell-539ab8af016fc473","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["class DynaQPlusAgent(BaseAgent):\n","    \n","    def agent_init(self, agent_info):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","\n","        Args:\n","            agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n","            {\n","                num_states (int): The number of states,\n","                num_actions (int): The number of actions,\n","                epsilon (float): The parameter for epsilon-greedy exploration,\n","                step_size (float): The step-size,\n","                discount (float): The discount factor,\n","                planning_steps (int): The number of planning steps per environmental interaction\n","                kappa (float): The scaling factor for the reward bonus\n","\n","                random_seed (int): the seed for the RNG used in epsilon-greedy\n","                planning_random_seed (int): the seed for the RNG used in the planner\n","            }\n","        \"\"\"\n","\n","        # First, we get the relevant information from agent_info \n","        # Note: we use np.random.RandomState(seed) to set the two different RNGs\n","        # for the planner and the rest of the code\n","        try:\n","            self.num_states = agent_info[\"num_states\"]\n","            self.num_actions = agent_info[\"num_actions\"]\n","        except:\n","            print(\"You need to pass both 'num_states' and 'num_actions' \\\n","                   in agent_info to initialize the action-value table\")\n","        self.gamma = agent_info.get(\"discount\", 0.95)\n","        self.step_size = agent_info.get(\"step_size\", 0.1)\n","        self.epsilon = agent_info.get(\"epsilon\", 0.1)\n","        self.planning_steps = agent_info.get(\"planning_steps\", 10)\n","        self.kappa = agent_info.get(\"kappa\", 0.001)\n","\n","        self.rand_generator = np.random.RandomState(agent_info.get('random_seed', 42))\n","        self.planning_rand_generator = np.random.RandomState(agent_info.get('planning_random_seed', 42))\n","\n","        # Next, we initialize the attributes required by the agent, e.g., q_values, model, tau, etc.\n","        # The visitation-counts can be stored as a table as well, like the action values \n","        self.q_values = np.zeros((self.num_states, self.num_actions))\n","        self.tau = np.zeros((self.num_states, self.num_actions))\n","        self.actions = list(range(self.num_actions))\n","        self.past_action = -1\n","        self.past_state = -1\n","        self.model = {}"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"iKq1KbGmxnkm","nbgrader":{"cell_type":"markdown","checksum":"1a7b620740e82640f572213177bee2ef","grade":false,"grade_id":"cell-1cad0227d9ff16d5","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Now first up, implement the `update_model` method. Note that this is different from Dyna-Q in the aforementioned way.\n"]},{"cell_type":"code","execution_count":26,"metadata":{"deletable":false,"executionInfo":{"elapsed":308,"status":"ok","timestamp":1673189575106,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ovBGpLNsxnkm","nbgrader":{"cell_type":"code","checksum":"ff36e4ae144e4409bd1ea34b1918000f","grade":false,"grade_id":"cell-d4452e4cd395456a","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to DynaQPlusAgent\n","\n","\n","def update_model(self, past_state, past_action, state, reward):\n","    \"\"\"updates the model \n","\n","    Args:\n","        past_state  (int): s\n","        past_action (int): a\n","        state       (int): s'\n","        reward      (int): r\n","    Returns:\n","        Nothing\n","    \"\"\"\n","\n","    # Recall that when adding a state-action to the model, if the agent is visiting the state\n","    #    for the first time, then the remaining actions need to be added to the model as well\n","    #    with zero reward and a transition into itself.\n","\n","\n","    if past_state not in self.model:\n","        self.model[past_state] = {past_action : (state, reward)}\n","\n","        for action in self.actions:\n","            if action != past_action:\n","                self.model[past_state][action] = (past_state, 0)        \n","    else:\n","        self.model[past_state][past_action] = (state, reward)\n","    "]},{"cell_type":"code","execution_count":27,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":354,"status":"ok","timestamp":1673189578510,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"glmizuVHxnkn","nbgrader":{"cell_type":"code","checksum":"fc850bdd9ff71c46e5e9b7246c7625d4","grade":true,"grade_id":"cell-8cdef71644d2952f","locked":true,"points":5,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","actions = []\n","agent_info = {\"num_actions\": 4, \n","              \"num_states\": 3, \n","              \"epsilon\": 0.1, \n","              \"step_size\": 0.1, \n","              \"discount\": 1.0, \n","              \"random_seed\": 0,\n","              \"planning_random_seed\": 0}\n","\n","agent = DynaQPlusAgent()\n","agent.agent_init(agent_info)\n","\n","agent.update_model(0,2,0,1)\n","agent.update_model(2,0,1,1)\n","agent.update_model(0,3,1,2)\n","agent.tau[0][0] += 1\n","\n","expected_model = {\n","    0: {\n","        0: (0, 0),\n","        1: (0, 0),\n","        2: (0, 1),\n","        3: (1, 2),\n","    },\n","    2: {\n","        0: (1, 1),\n","        1: (2, 0),\n","        2: (2, 0),\n","        3: (2, 0),\n","    },\n","}\n","assert agent.model == expected_model"]},{"cell_type":"code","execution_count":28,"metadata":{"deletable":false,"executionInfo":{"elapsed":336,"status":"ok","timestamp":1673189583611,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"RUuoSVPGxnkn","nbgrader":{"cell_type":"code","checksum":"6ef80ec707602f554d0a56412d066855","grade":false,"grade_id":"cell-b3605364bf724124","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to DynaQPlusAgent\n","\n","def planning_step(self):\n","    \"\"\"performs planning, i.e. indirect RL.\n","\n","    Args:\n","        None\n","    Returns:\n","        Nothing\n","    \"\"\"\n","    \n","    # The indirect RL step:\n","    # - Choose a state and action from the set of experiences that are stored in the model. (~2 lines)\n","    # - Query the model with this state-action pair for the predicted next state and reward.(~1 line)\n","    # - **Add the bonus to the reward** (~1 line)\n","    # - Update the action values with this simulated experience.                            (2~4 lines)\n","    # - Repeat for the required number of planning steps.\n","    #\n","    # Note that the update equation is different for terminal and non-terminal transitions. \n","    # To differentiate between a terminal and a non-terminal next state, assume that the model stores\n","    # the terminal state as a dummy state like -1\n","\n","    for i in range(self.planning_steps):\n","        past_state = self.planning_rand_generator.choice(list(self.model.keys()))\n","        past_action = self.planning_rand_generator.choice(list(self.model[past_state].keys()))\n","        next_state, reward = self.model[past_state][past_action]\n","        reward += self.kappa * np.sqrt(self.tau[past_state, past_action])\n","    \n","        if next_state == -1:\n","            q_max = 0\n","        else:\n","            q_max = np.max(self.q_values[next_state])\n","    \n","        self.q_values[past_state, past_action] += self.step_size * (reward + self.gamma * q_max - self.q_values[past_state, past_action])    "]},{"cell_type":"code","execution_count":29,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673189586037,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"XSuU2YQhxnko","nbgrader":{"cell_type":"code","checksum":"506a78d3a89c1a04c8f59e6a69515623","grade":true,"grade_id":"cell-1bae4d3c34b953a2","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\n","## Test code for planning_step() ##\n","\n","actions = []\n","agent_info = {\"num_actions\": 4, \n","              \"num_states\": 3, \n","              \"epsilon\": 0.1, \n","              \"step_size\": 0.1, \n","              \"discount\": 1.0, \n","              \"kappa\": 0.001,\n","              \"planning_steps\": 4,\n","              \"random_seed\": 0,\n","              \"planning_random_seed\": 1}\n","\n","agent = DynaQPlusAgent()\n","agent.agent_init(agent_info)\n","\n","agent.update_model(0,1,-1,1)\n","agent.tau += 1\n","agent.tau[0][1] = 0\n","\n","agent.update_model(0,2,1,1)\n","agent.tau += 1\n","agent.tau[0][2] = 0\n","\n","agent.update_model(2,0,1,1)\n","agent.tau += 1\n","agent.tau[2][0] = 0\n","\n","agent.planning_step()\n","\n","expected_model = {\n","    0: {\n","        1: (-1, 1), \n","        0: (0, 0), \n","        2: (1, 1), \n","        3: (0, 0),\n","    }, \n","    2: {\n","        0: (1, 1), \n","        1: (2, 0), \n","        2: (2, 0), \n","        3: (2, 0),\n","    },\n","}\n","assert agent.model == expected_model\n","\n","expected_values = np.array([\n","    [0, 0.10014142, 0, 0],\n","    [0, 0, 0, 0],\n","    [0, 0.00036373, 0, 0.00017321],\n","])\n","assert np.allclose(agent.q_values, expected_values)"]},{"cell_type":"code","execution_count":30,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":311,"status":"ok","timestamp":1673189590003,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"lXgQHTFXxnkp","nbgrader":{"cell_type":"code","checksum":"81bcd74d211cf70c7259d7e035ed6393","grade":false,"grade_id":"cell-0550ca807b59d14c","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to DynaQPlusAgent\n","\n","\n","def argmax(self, q_values):\n","    \"\"\"argmax with random tie-breaking\n","    Args:\n","        q_values (Numpy array): the array of action values\n","    Returns:\n","        action (int): an action with the highest value\n","    \"\"\"\n","    top = float(\"-inf\")\n","    ties = []\n","\n","    for i in range(len(q_values)):\n","        if q_values[i] > top:\n","            top = q_values[i]\n","            ties = []\n","\n","        if q_values[i] == top:\n","            ties.append(i)\n","\n","    return self.rand_generator.choice(ties)\n","\n","def choose_action_egreedy(self, state):\n","    \"\"\"returns an action using an epsilon-greedy policy w.r.t. the current action-value function.\n","\n","    Important: assume you have a random number generator 'rand_generator' as a part of the class\n","                which you can use as self.rand_generator.choice() or self.rand_generator.rand()\n","\n","    Args:\n","        state (List): coordinates of the agent (two elements)\n","    Returns:\n","        The action taken w.r.t. the aforementioned epsilon-greedy policy\n","    \"\"\"\n","\n","    if self.rand_generator.rand() < self.epsilon:\n","        action = self.rand_generator.choice(self.actions)\n","    else:\n","        values = self.q_values[state]\n","        action = self.argmax(values)\n","\n","    return action"]},{"cell_type":"code","execution_count":31,"metadata":{"deletable":false,"executionInfo":{"elapsed":341,"status":"ok","timestamp":1673189595079,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"K1waeYPbxnkp","nbgrader":{"cell_type":"code","checksum":"9ea6edbc6526bfb8d57d8d6a03514ba1","grade":false,"grade_id":"cell-675ebe1d175f5730","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to DynaQPlusAgent\n","\n","    \n","def agent_start(self, state):\n","    \"\"\"The first method called when the experiment starts, called after\n","    the environment starts.\n","    Args:\n","        state (Numpy array): the state from the\n","            environment's env_start function.\n","    Returns:\n","        (int) The first action the agent takes.\n","    \"\"\"\n","    \n","    # given the state, select the action using self.choose_action_egreedy(), \n","    # and save current state and action (~2 lines)\n"," \n","    action = self.choose_action_egreedy(state)\n","    self.past_state = state\n","    self.past_action = action    \n","    \n","    return self.past_action\n","\n","def agent_step(self, reward, state):\n","    \"\"\"A step taken by the agent.\n","    Args:\n","        reward (float): the reward received for taking the last action taken\n","        state (Numpy array): the state from the\n","            environment's step based on where the agent ended up after the\n","            last step\n","    Returns:\n","        (int) The action the agent is taking.\n","    \"\"\"  \n","    \n","    # Update the last-visited counts (~2 lines)\n","    # - Direct-RL step (1~3 lines)\n","    # - Model Update step (~1 line)\n","    # - `planning_step` (~1 line)\n","    # - Action Selection step (~1 line)\n","    # Save the current state and action before returning the action to be performed. (~2 lines)\n","\n","    self.tau += 1\n","    self.tau[self.past_state, self.past_action] = 0\n","    q_max = np.max(self.q_values[state])\n","    self.q_values[self.past_state, self.past_action] += self.step_size * (reward + self.gamma * q_max - self.q_values[self.past_state, self.past_action])\n","\n","    self.update_model(self.past_state, self.past_action, state, reward)\n","    self.planning_step()\n","    action = self.choose_action_egreedy(state)\n","    self.past_state = state\n","    self.past_action = action    \n","    \n","    return self.past_action\n","\n","def agent_end(self, reward):\n","    \"\"\"Called when the agent terminates.\n","    Args:\n","        reward (float): the reward the agent received for entering the\n","            terminal state.\n","    \"\"\"\n","    # Again, add the same components you added in agent_step to augment Dyna-Q into Dyna-Q+\n","    \n","    self.tau += 1\n","    self.tau[self.past_state, self.past_action] = 0\n","    self.q_values[self.past_state, self.past_action] += self.step_size * (reward - self.q_values[self.past_state, self.past_action])\n","    self.update_model(self.past_state, self.past_action, -1, reward)\n","    self.planning_step()    "]},{"cell_type":"code","execution_count":32,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":353,"status":"ok","timestamp":1673189597986,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"b4C77Yjuxnkq","nbgrader":{"cell_type":"code","checksum":"44a3a0b6fcb2e7f37c933bd18ff378f8","grade":true,"grade_id":"cell-9cf838836ad39efb","locked":true,"points":15,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\"num_actions\": 4, \n","              \"num_states\": 3, \n","              \"epsilon\": 0.1, \n","              \"step_size\": 0.1, \n","              \"discount\": 1.0,\n","              \"kappa\": 0.001,\n","              \"random_seed\": 0,\n","              \"planning_steps\": 4,\n","              \"planning_random_seed\": 0}\n","\n","agent = DynaQPlusAgent()\n","agent.agent_init(agent_info)\n","\n","action = agent.agent_start(0) # state\n","assert action == 1\n","\n","assert np.allclose(agent.tau, 0)\n","assert np.allclose(agent.q_values, 0)\n","assert agent.model == {}\n","\n","# ---------------\n","# test agent step\n","# ---------------\n","\n","action = agent.agent_step(1, 2)\n","assert action == 3\n","\n","action = agent.agent_step(0, 1)\n","assert action == 1\n","\n","expected_tau = np.array([\n","    [2, 1, 2, 2],\n","    [2, 2, 2, 2],\n","    [2, 2, 2, 0],\n","])\n","assert np.all(agent.tau == expected_tau)\n","\n","expected_values = np.array([\n","    [0.0191, 0.271, 0.0, 0.0191],\n","    [0, 0, 0, 0],\n","    [0, 0.000183847763, 0.000424264069, 0],\n","])\n","assert np.allclose(agent.q_values, expected_values)\n","\n","expected_model = {\n","    0: {\n","        1: (2, 1), \n","        0: (0, 0), \n","        2: (0, 0), \n","        3: (0, 0),\n","    }, \n","    2: {\n","        3: (1, 0), \n","        0: (2, 0), \n","        1: (2, 0), \n","        2: (2, 0),\n","    },\n","}\n","assert agent.model == expected_model\n","\n","# --------------\n","# test agent end\n","# --------------\n","agent.agent_end(1)\n","\n","expected_tau = np.array([\n","    [3, 2, 3, 3],\n","    [3, 0, 3, 3],\n","    [3, 3, 3, 1],\n","])\n","assert np.all(agent.tau == expected_tau)\n","\n","expected_values = np.array([\n","    [0.0191, 0.344083848, 0, 0.0444632051],\n","    [0.0191732051, 0.19, 0, 0],\n","    [0, 0.000183847763, 0.000424264069, 0],\n","])\n","assert np.allclose(agent.q_values, expected_values)\n","\n","expected_model = {0: {1: (2, 1), 0: (0, 0), 2: (0, 0), 3: (0, 0)}, 2: {3: (1, 0), 0: (2, 0), 1: (2, 0), 2: (2, 0)}, 1: {1: (-1, 1), 0: (1, 0), 2: (1, 0), 3: (1, 0)}}\n","assert agent.model == expected_model"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"WyPaFbT1xnkr","nbgrader":{"cell_type":"code","checksum":"7b694d2c1d02154058ad127123594b44","grade":false,"grade_id":"cell-22a658123d08fafa","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["## Dyna-Q+ agent in the _changing_ environment\n","\n","# Experiment parameters\n","experiment_parameters = {\n","    \"num_runs\" : 30,                     # The number of times we run the experiment\n","    \"num_max_steps\" : 6000,              # The number of steps per experiment\n","}\n","\n","# Environment parameters\n","environment_parameters = { \n","    \"discount\": 0.95,\n","    \"change_at_n\": 3000\n","}\n","\n","# Agent parameters\n","agent_parameters = {  \n","    \"num_states\" : 54,\n","    \"num_actions\" : 4, \n","    \"epsilon\": 0.1, \n","    \"step_size\" : 0.5,\n","    \"planning_steps\" : [50]      \n","}\n","\n","current_env = ShortcutMazeEnvironment   # The environment\n","current_agent = DynaQPlusAgent          # The agent\n","\n","data_qplus = run_experiment_with_state_visitations(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters, \"Dyna-Q+\")"]},{"cell_type":"code","execution_count":34,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":323,"status":"ok","timestamp":1673189638764,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"NTIiCdBhxnkr","nbgrader":{"cell_type":"code","checksum":"56f9182c13c40b6647f53e95d2a89302","grade":false,"grade_id":"cell-b17bc044f6e4e020","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def plot_cumulative_reward_comparison(data1, data2):\n","\n","    cum_reward_q = data1['cum_reward_all'][2]\n","    cum_reward_qPlus = data2['cum_reward_all'][0]\n","\n","    plt.plot(np.mean(cum_reward_qPlus, axis=0), label='Dyna-Q+')\n","    plt.plot(np.mean(cum_reward_q, axis=0), label='Dyna-Q')\n","\n","    plt.axvline(x=3000, linestyle='--', color='grey', alpha=0.4)\n","    plt.xlabel('Timesteps')\n","    plt.ylabel('Cumulative\\nreward', rotation=0, labelpad=60)\n","    plt.legend(loc='upper left')\n","    plt.title('Average performance of Dyna-Q and Dyna-Q+ agents in the Shortcut Maze\\n')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"hnkEQuCExnks","nbgrader":{"cell_type":"code","checksum":"74b2b53a88c98b3a41f4ccdf24c585bf","grade":false,"grade_id":"cell-bff6a7315a81ba36","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["plot_cumulative_reward_comparison(dataq, data_qplus)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"o9tZHKuXxnkt","nbgrader":{"cell_type":"code","checksum":"02a92b5dfca164799531bfbfc51b2947","grade":false,"grade_id":"cell-30b40e125c10f4a1","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["plot_state_visitations(data_qplus, ['Dyna-Q+ : State visitations before the env changes', 'Dyna-Q+ : State visitations after the env changes'], 0)"]}],"metadata":{"colab":{"provenance":[]},"coursera":{"course_slug":"sample-based-learning-methods","graded_item_id":"trR7Z","launcher_item_id":"edrCE"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
