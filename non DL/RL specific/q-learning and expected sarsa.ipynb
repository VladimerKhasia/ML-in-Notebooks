{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"nrnHAERsa91z","nbgrader":{"cell_type":"markdown","checksum":"9fe4bed7f12954da5406e29bacc33d29","grade":false,"grade_id":"cell-8752ebba5eba6908","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Q-Learning and Expected Sarsa"]},{"cell_type":"code","execution_count":10,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":323,"status":"ok","timestamp":1673187310114,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"GZi6s5KWa918","nbgrader":{"cell_type":"code","checksum":"a2ce19695c4ca93e0cdc6bb23fd07486","grade":false,"grade_id":"cell-88a8c00170d9648d","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from scipy.stats import sem\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":11,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":552,"status":"ok","timestamp":1673187310981,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ITQWpmq9a91-","nbgrader":{"cell_type":"code","checksum":"8f97fc33c742328ff58b1032dc9619ef","grade":false,"grade_id":"cell-daf33c237c54cae3","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["plt.rcParams.update({'font.size': 15})\n","plt.rcParams.update({'figure.figsize': [10,5]})"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673187310982,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"0QwM_EbVbCCu"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":13,"metadata":{"cellView":"form","executionInfo":{"elapsed":28,"status":"ok","timestamp":1673187310983,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"6ZRgQUxHbgFH"},"outputs":[],"source":["#@title Clifworld environment\n","\n","class Environment(BaseEnvironment):\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    # def __init__(self):\n","\n","\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","        self.rows = 4\n","        self.cols = 12\n","        self.start = [0,0]\n","        self.goal = [0,11]\n","        self.current_state = None\n","\n","    def env_start(self):\n","        \"\"\"The first method called when the episode starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","        self.current_state = self.start  # An empty NumPy array\n","\n","        self.reward_obs_term = (0.0, self.observation(self.current_state), False)\n","\n","        return self.reward_obs_term[1]\n","\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","        new_state = deepcopy(self.current_state)\n","\n","        if action == 0: #right\n","            new_state[1] = min(new_state[1]+1, self.cols-1)\n","        elif action == 1: #down\n","            new_state[0] = max(new_state[0]-1, 0)\n","        elif action == 2: #left\n","            new_state[1] = max(new_state[1]-1, 0)\n","        elif action == 3: #up\n","            new_state[0] = min(new_state[0]+1, self.rows-1)\n","        else:\n","            raise Exception(\"Invalid action.\")\n","        self.current_state = new_state\n","\n","        reward = -1.0\n","        is_terminal = False\n","        if self.current_state[0] == 0 and self.current_state[1] > 0:\n","            if self.current_state[1] < self.cols - 1:\n","                reward = -100.0\n","                self.current_state = deepcopy(self.start)\n","            else:\n","                is_terminal = True\n","\n","        self.reward_obs_term = (reward, self.observation(self.current_state), is_terminal)\n","\n","        return self.reward_obs_term\n","\n","    def observation(self, state):\n","        return state[0] * self.cols + state[1] \n","\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","        pass\n","\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message (string): the message passed to the environment\n","\n","        Returns:\n","            string: the response (or answer) to the message\n","        \"\"\"\n","        if message == \"what is the current reward?\":\n","            return \"{}\".format(self.reward_obs_term[0])\n","\n","        # else\n","        return \"I don't know how to respond to your message\"\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"e0m_BR4ea91_","nbgrader":{"cell_type":"markdown","checksum":"ae04ccbdacc68b749425b9cac219bcc3","grade":false,"grade_id":"cell-31f453dda88f470a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["## Q-Learning"]},{"cell_type":"code","execution_count":14,"metadata":{"deletable":false,"executionInfo":{"elapsed":27,"status":"ok","timestamp":1673187310984,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"I008ehi6a92C","nbgrader":{"cell_type":"code","checksum":"c315b49c7a6c4295794c2df75d93d656","grade":false,"grade_id":"cell-8db23776199c2dbc","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class QLearningAgent(BaseAgent):\n","    def agent_init(self, agent_init_info):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","        \n","        Args:\n","        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n","        {\n","            num_states (int): The number of states,\n","            num_actions (int): The number of actions,\n","            epsilon (float): The epsilon parameter for exploration,\n","            step_size (float): The step-size,\n","            discount (float): The discount factor,\n","        }\n","        \n","        \"\"\"\n","        # Store the parameters provided in agent_init_info.\n","        self.num_actions = agent_init_info[\"num_actions\"]\n","        self.num_states = agent_init_info[\"num_states\"]\n","        self.epsilon = agent_init_info[\"epsilon\"]\n","        self.step_size = agent_init_info[\"step_size\"]\n","        self.discount = agent_init_info[\"discount\"]\n","        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n","        \n","        # Create an array for action-value estimates and initialize it to zero.\n","        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n","\n","        \n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the episode starts, called after\n","        the environment starts.\n","        Args:\n","            observation (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            action (int): the first action the agent takes.\n","        \"\"\"\n","        \n","        # Choose action using epsilon greedy.\n","        state = observation\n","        current_q = self.q[state,:]\n","        if self.rand_generator.rand() < self.epsilon:\n","            action = self.rand_generator.randint(self.num_actions)\n","        else:\n","            action = self.argmax(current_q)\n","        self.prev_state = state\n","        self.prev_action = action\n","        return action\n","    \n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","        \n","        # Choose action using epsilon greedy.\n","        state = observation\n","        current_q = self.q[state, :]\n","        if self.rand_generator.rand() < self.epsilon:\n","            action = self.rand_generator.randint(self.num_actions)\n","        else:\n","            action = self.argmax(current_q)\n","        self.q[self.prev_state, self.prev_action] += self.step_size * (reward + self.discount * np.max(self.q[state, :]) - self.q[self.prev_state, self.prev_action])\n","        \n","        self.prev_state = state\n","        self.prev_action = action\n","        return action\n","    \n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","        # Perform the last update in the episode\n","        self.q[self.prev_state, self.prev_action] += self.step_size * (reward- self.q[self.prev_state, self.prev_action])\n","        \n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","        top = float(\"-inf\")\n","        ties = []\n","\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return self.rand_generator.choice(ties)"]},{"cell_type":"code","execution_count":15,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":27,"status":"ok","timestamp":1673187310986,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"9NAnn4pfa92E","nbgrader":{"cell_type":"code","checksum":"d85a79701d766e72d852512c3f579914","grade":true,"grade_id":"cell-11f37433c8ca54c8","locked":true,"points":20,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n","agent = QLearningAgent()\n","agent.agent_init(agent_info)\n","action = agent.agent_start(0)\n","\n","expected_values = np.array([\n","    [0, 0, 0, 0],\n","    [0, 0, 0, 0],\n","    [0, 0, 0, 0],\n","])\n","\n","assert np.all(agent.q == expected_values)\n","assert action == 1\n","\n","# reset the agent\n","agent.agent_init(agent_info)\n","\n","action = agent.agent_start(0)\n","assert action == 1\n","\n","action = agent.agent_step(2, 1)\n","assert action == 3\n","\n","action = agent.agent_step(0, 0)\n","assert action == 1\n","\n","expected_values = np.array([\n","    [0.,  0.2,  0.,  0.  ],\n","    [0.,  0.,   0.,  0.02],\n","    [0.,  0.,   0.,  0.  ],\n","])\n","assert np.all(np.isclose(agent.q, expected_values))\n","\n","# reset the agent\n","agent.agent_init(agent_info)\n","\n","action = agent.agent_start(0)\n","assert action == 1\n","\n","action = agent.agent_step(2, 1)\n","assert action == 3\n","\n","agent.agent_end(1)\n","\n","expected_values = np.array([\n","    [0.,  0.2, 0.,  0. ],\n","    [0.,  0.,  0.,  0.1],\n","    [0.,  0.,  0.,  0. ],\n","])\n","assert np.all(np.isclose(agent.q, expected_values))\n","\n","# Run a few more tests to ensure the epsilon-random action is not picked in the update\n","expected_values = np.array([\n","    [0.,         0.2,        0.,         0.        ],\n","    [5.97824336, 5.75000715, 5.79372928, 6.69483878],\n","    [0.,         0.,         0.,         0.        ],\n","])\n","agent.epsilon = 1.0  # Set epsilon high so that there is a larger chance to catch the errors\n","for _ in range(100):\n","    agent.agent_step(2, 1)\n","assert np.all(np.isclose(agent.q, expected_values))"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"qf31uX1La92F","nbgrader":{"cell_type":"markdown","checksum":"e9ab41c00758baeb958ad2d9406d7bfd","grade":false,"grade_id":"cell-c9fb7428a7449328","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Expected Sarsa"]},{"cell_type":"code","execution_count":16,"metadata":{"deletable":false,"executionInfo":{"elapsed":26,"status":"ok","timestamp":1673187310987,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Dt7kc8fMa92G","nbgrader":{"cell_type":"code","checksum":"ee482f620e4d5f62099cf873e42c21eb","grade":false,"grade_id":"cell-5d6d92b752b28869","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class ExpectedSarsaAgent(BaseAgent):\n","    def agent_init(self, agent_init_info):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","        \n","        Args:\n","        agent_init_info (dict), the parameters used to initialize the agent. The dictionary contains:\n","        {\n","            num_states (int): The number of states,\n","            num_actions (int): The number of actions,\n","            epsilon (float): The epsilon parameter for exploration,\n","            step_size (float): The step-size,\n","            discount (float): The discount factor,\n","        }\n","        \n","        \"\"\"\n","        # Store the parameters provided in agent_init_info.\n","        self.num_actions = agent_init_info[\"num_actions\"]\n","        self.num_states = agent_init_info[\"num_states\"]\n","        self.epsilon = agent_init_info[\"epsilon\"]\n","        self.step_size = agent_init_info[\"step_size\"]\n","        self.discount = agent_init_info[\"discount\"]\n","        self.rand_generator = np.random.RandomState(agent_info[\"seed\"])\n","        \n","        # Create an array for action-value estimates and initialize it to zero.\n","        self.q = np.zeros((self.num_states, self.num_actions)) # The array of action-value estimates.\n","\n","        \n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the episode starts, called after\n","        the environment starts.\n","        Args:\n","            observation (int): the state observation from the\n","                environment's evn_start function.\n","        Returns:\n","            action (int): the first action the agent takes.\n","        \"\"\"\n","        \n","        # Choose action using epsilon greedy.\n","        state = observation\n","        current_q = self.q[state, :]\n","        if self.rand_generator.rand() < self.epsilon:\n","            action = self.rand_generator.randint(self.num_actions)\n","        else:\n","            action = self.argmax(current_q)\n","        self.prev_state = state\n","        self.prev_action = action\n","        return action\n","    \n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (int): the state observation from the\n","                environment's step based on where the agent ended up after the\n","                last step.\n","        Returns:\n","            action (int): the action the agent is taking.\n","        \"\"\"\n","        \n","        # Choose action using epsilon greedy.\n","        state = observation\n","        current_q = self.q[state,:]\n","        if self.rand_generator.rand() < self.epsilon:\n","            action = self.rand_generator.randint(self.num_actions)\n","        else:\n","            action = self.argmax(current_q)\n","        \n","        # Perform an update\n","        expected_q = 0\n","        q_max = np.max(self.q[state,:])\n","        pi = np.ones(self.num_actions) * self.epsilon / self.num_actions + (self.q[state,:] == q_max) * (1 - self.epsilon) / np.sum(self.q[state,:] == q_max)\n","        expected_q = np.sum(self.q[state,:] * pi)\n","        self.q[self.prev_state, self.prev_action] += self.step_size * (reward + self.discount * expected_q  - self.q[self.prev_state, self.prev_action])        \n","        \n","        self.prev_state = state\n","        self.prev_action = action\n","        return action\n","    \n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","        # Perform the last update in the episode\n","        self.q[self.prev_state, self.prev_action] += self.step_size * (reward- self.q[self.prev_state, self.prev_action])\n","        \n","    def argmax(self, q_values):\n","        \"\"\"argmax with random tie-breaking\n","        Args:\n","            q_values (Numpy array): the array of action-values\n","        Returns:\n","            action (int): an action with the highest value\n","        \"\"\"\n","        top = float(\"-inf\")\n","        ties = []\n","\n","        for i in range(len(q_values)):\n","            if q_values[i] > top:\n","                top = q_values[i]\n","                ties = []\n","\n","            if q_values[i] == top:\n","                ties.append(i)\n","\n","        return self.rand_generator.choice(ties)"]},{"cell_type":"code","execution_count":17,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":25,"status":"ok","timestamp":1673187310988,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"_PYv6g74a92I","nbgrader":{"cell_type":"code","checksum":"f8d82d7062874dfe82f73a063cd37a19","grade":true,"grade_id":"cell-f7d7c26712ff16cb","locked":true,"points":30,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\"num_actions\": 4, \"num_states\": 3, \"epsilon\": 0.1, \"step_size\": 0.1, \"discount\": 1.0, \"seed\": 0}\n","agent = ExpectedSarsaAgent()\n","agent.agent_init(agent_info)\n","\n","action = agent.agent_start(0)\n","assert action == 1\n","\n","expected_values = np.array([\n","    [0, 0, 0, 0],\n","    [0, 0, 0, 0],\n","    [0, 0, 0, 0],\n","])\n","assert np.all(agent.q == expected_values)\n","\n","# ---------------\n","# test agent step\n","# ---------------\n","\n","action = agent.agent_step(2, 1)\n","assert action == 3\n","\n","action = agent.agent_step(0, 0)\n","assert action == 1\n","\n","expected_values = np.array([\n","    [0, 0.2, 0, 0],\n","    [0, 0, 0, 0.0185],\n","    [0, 0, 0, 0],\n","])\n","assert np.all(np.isclose(agent.q, expected_values))\n","\n","# --------------\n","# test agent end\n","# --------------\n","\n","agent.agent_end(1)\n","\n","expected_values = np.array([\n","    [0, 0.28, 0, 0],\n","    [0, 0, 0, 0.0185],\n","    [0, 0, 0, 0],\n","])\n","assert np.all(np.isclose(agent.q, expected_values))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"deletable":false,"editable":false,"executionInfo":{"elapsed":81511,"status":"ok","timestamp":1673187392476,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"FNi5XCWIa92K","nbgrader":{"cell_type":"code","checksum":"1860389f9ea51bb49f21a170dd60311c","grade":false,"grade_id":"cell-def1e29f8484e2bb","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"5c1d5f5a-53d5-488a-d8aa-dfbb24f280f6"},"outputs":[],"source":["np.random.seed(0)\n","\n","agents = {\n","    \"Q-learning\": QLearningAgent,\n","    \"Expected Sarsa\": ExpectedSarsaAgent\n","}\n","env = Environment\n","all_reward_sums = {} # Contains sum of rewards during episode\n","all_state_visits = {} # Contains state visit counts during the last 10 episodes\n","agent_info = {\"num_actions\": 4, \"num_states\": 48, \"epsilon\": 0.1, \"step_size\": 0.5, \"discount\": 1.0}\n","env_info = {}\n","num_runs = 100 # The number of runs\n","num_episodes = 200 # The number of episodes in each run\n","\n","for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n","    all_reward_sums[algorithm] = []\n","    all_state_visits[algorithm] = []\n","    for run in tqdm(range(num_runs)):\n","        agent_info[\"seed\"] = run\n","        rl_glue = RLGlue(env, agents[algorithm])\n","        rl_glue.rl_init(agent_info, env_info)\n","\n","        reward_sums = []\n","        state_visits = np.zeros(48)\n","        for episode in range(num_episodes):\n","            if episode < num_episodes - 10:\n","                # Runs an episode\n","                rl_glue.rl_episode(10000) \n","            else: \n","                # Runs an episode while keeping track of visited states\n","                state, action = rl_glue.rl_start()\n","                state_visits[state] += 1\n","                is_terminal = False\n","                while not is_terminal:\n","                    reward, state, action, is_terminal = rl_glue.rl_step()\n","                    state_visits[state] += 1\n","                \n","            reward_sums.append(rl_glue.rl_return())\n","            \n","        all_reward_sums[algorithm].append(reward_sums)\n","        all_state_visits[algorithm].append(state_visits)\n","\n","# plot results\n","for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n","    plt.plot(np.mean(all_reward_sums[algorithm], axis=0), label=algorithm)\n","plt.xlabel(\"Episodes\")\n","plt.ylabel(\"Sum of\\n rewards\\n during\\n episode\",rotation=0, labelpad=40)\n","plt.ylim(-100,0)\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":466},"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673187392477,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"LaHik6rba92K","nbgrader":{"cell_type":"code","checksum":"fa1344355fb2cb8cdeddbaa877831d97","grade":false,"grade_id":"cell-f3926cb72105b801","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"9f6c936f-bd94-421e-a5f6-0730f5ba6230"},"outputs":[],"source":["for algorithm, position in [(\"Q-learning\", 211), (\"Expected Sarsa\", 212)]:\n","    plt.subplot(position)\n","    average_state_visits = np.array(all_state_visits[algorithm]).mean(axis=0)\n","    grid_state_visits = average_state_visits.reshape((4,12))\n","    grid_state_visits[0,1:-1] = np.nan\n","    plt.pcolormesh(grid_state_visits, edgecolors='gray', linewidth=2)\n","    plt.title(algorithm)\n","    plt.axis('off')\n","    cm = plt.get_cmap()\n","    cm.set_bad('gray')\n","\n","    plt.subplots_adjust(bottom=0.0, right=0.7, top=1.0)\n","    cax = plt.axes([0.85, 0.0, 0.075, 1.])\n","    \n","cbar = plt.colorbar(cax=cax)\n","cbar.ax.set_ylabel(\"Visits during\\n the last 10\\n episodes\", rotation=0, labelpad=70)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"deletable":false,"editable":false,"executionInfo":{"elapsed":189069,"status":"ok","timestamp":1673187581533,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"dwsZKCXVa92L","nbgrader":{"cell_type":"code","checksum":"f458141112450c336f6c71e15bcf116e","grade":false,"grade_id":"cell-232036ab548ecf06","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"40aa9d39-84db-498a-8949-089329a06d73"},"outputs":[],"source":["from itertools import product\n","\n","agents = {\n","    \"Q-learning\": QLearningAgent,\n","    \"Expected Sarsa\": ExpectedSarsaAgent\n","}\n","env = Environment\n","all_reward_sums = {}\n","step_sizes = np.linspace(0.1,1.0,10)\n","agent_info = {\"num_actions\": 4, \"num_states\": 48, \"epsilon\": 0.1, \"discount\": 1.0}\n","env_info = {}\n","num_runs = 30\n","num_episodes = 100\n","all_reward_sums = {}\n","\n","algorithms = [\"Q-learning\", \"Expected Sarsa\"]\n","cross_product = list(product(algorithms, step_sizes, range(num_runs)))\n","for algorithm, step_size, run in tqdm(cross_product):\n","    if (algorithm, step_size) not in all_reward_sums:\n","        all_reward_sums[(algorithm, step_size)] = []\n","\n","    agent_info[\"step_size\"] = step_size\n","    agent_info[\"seed\"] = run\n","    rl_glue = RLGlue(env, agents[algorithm])\n","    rl_glue.rl_init(agent_info, env_info)\n","\n","    last_episode_total_reward = 0\n","    for episode in range(num_episodes):\n","        rl_glue.rl_episode(0)\n","    all_reward_sums[(algorithm, step_size)].append(rl_glue.rl_return()/num_episodes)\n","        \n","\n","for algorithm in [\"Q-learning\", \"Expected Sarsa\"]:\n","    algorithm_means = np.array([np.mean(all_reward_sums[(algorithm, step_size)]) for step_size in step_sizes])\n","    algorithm_stds = np.array([sem(all_reward_sums[(algorithm, step_size)]) for step_size in step_sizes])\n","    plt.plot(step_sizes, algorithm_means, marker='o', linestyle='solid', label=algorithm)\n","    plt.fill_between(step_sizes, algorithm_means + algorithm_stds, algorithm_means - algorithm_stds, alpha=0.2)\n","\n","plt.legend()\n","plt.xlabel(\"Step-size\")\n","plt.ylabel(\"Sum of\\n rewards\\n per episode\",rotation=0, labelpad=50)\n","plt.xticks(step_sizes)\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
