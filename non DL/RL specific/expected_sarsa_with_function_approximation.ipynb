{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pXgXGreXNMEb"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import shutil\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "s2k2Zpf7KqK_",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "91bab923ff51b8c4cc2db62df96dceba",
          "grade": false,
          "grade_id": "cell-728287ea719cc025",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# Build environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "id": "m3qGjyQ5KqLL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "193356706350cbad3ee2e968e1f33fce",
          "grade": false,
          "grade_id": "cell-b5475cc072c387ff",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "#@title Lunar Lander Environment\n",
        "\n",
        "from __future__ import print_function\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "# environment transition dinamics\n",
        "def get_velocity(action):\n",
        "    if action == 1:\n",
        "        return 12, 15\n",
        "    if action == 2:\n",
        "        return 10, 10\n",
        "    if action == 3:\n",
        "        return 0, 0\n",
        "    if action == 4:\n",
        "        return 12, 15\n",
        "    if action == 5:\n",
        "        return 0, 0\n",
        "\n",
        "def get_angle(action):\n",
        "    if action == 1:\n",
        "        return 0\n",
        "    if action == 2:\n",
        "        return 15\n",
        "    if action == 3:\n",
        "        return 0\n",
        "    if action == 4:\n",
        "        return 0\n",
        "    if action == 5:\n",
        "        return 0\n",
        "\n",
        "def get_position(action):\n",
        "    if action == 1:\n",
        "        return 10, 100\n",
        "    if action == 2:\n",
        "        return 50, 0\n",
        "    if action == 3:\n",
        "        return 50, 0\n",
        "    if action == 4:\n",
        "        return 50, 20\n",
        "    if action == 5:\n",
        "        return 2, 0\n",
        "\n",
        "def get_landing_zone():\n",
        "    return 50, 0\n",
        "\n",
        "def get_fuel(action):\n",
        "    if action == 1:\n",
        "        return 10\n",
        "    if action == 2:\n",
        "        return 20\n",
        "    if action == 3:\n",
        "        return 5\n",
        "    if action == 4:\n",
        "        return 0\n",
        "    if action == 5:\n",
        "        return 10\n",
        "\n",
        "def tests(LunarLander, test_number):\n",
        "    ll = LunarLander()\n",
        "    ll.env_start()\n",
        "    reward, obs, term = ll.env_step(test_number)\n",
        "    print(\"Reward: {}, Terminal: {}\".format(reward, term))\n",
        "\n",
        "\n",
        "\n",
        "# Abstract environment base class for environment\n",
        "\n",
        "class BaseEnvironment:\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message: the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            the response (or answer) to the message\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Lunar Lander Environment\n",
        "\n",
        "get_landing_zone()\n",
        "class LunarLanderEnvironment(BaseEnvironment):\n",
        "    def __init__(self):\n",
        "        self.current_state = None\n",
        "        self.count = 0\n",
        "    \n",
        "    def env_init(self, env_info):\n",
        "        # users set this up\n",
        "        self.state = np.zeros(6) # velocity x, y, angle, distance to ground, landing zone x, y\n",
        "    \n",
        "    def env_start(self):\n",
        "        land_x, land_y = get_landing_zone() # gets the x, y coordinate of the landing zone\n",
        "        # At the start we initialize the agent to the top left hand corner (100, 20) with 0 velocity \n",
        "        # in either any direction. The agent's angle is set to 0 and the landing zone is retrieved and set.\n",
        "        # The lander starts with fuel of 100.\n",
        "        # (vel_x, vel_y, angle, pos_x, pos_y, land_x, land_y, fuel)\n",
        "        self.current_state = (0, 0, 0, 100, 20, land_x, land_y, 100)\n",
        "        return self.current_state\n",
        "    \n",
        "    def env_step(self, action):\n",
        "        \n",
        "        land_x, land_y = get_landing_zone() # gets the x, y coordinate of the landing zone\n",
        "        vel_x, vel_y = get_velocity(action) # gets the x, y velocity of the lander\n",
        "        angle = get_angle(action) # gets the angle the lander is positioned in\n",
        "        pos_x, pos_y = get_position(action) # gets the x, y position of the lander\n",
        "        fuel = get_fuel(action) # get the amount of fuel remaining for the lander\n",
        "        \n",
        "        terminal = False\n",
        "        reward = 0.0\n",
        "        observation = (vel_x, vel_y, angle, pos_x, pos_y, land_x, land_y, fuel)\n",
        "        \n",
        "        if pos_y <= land_y:\n",
        "            terminal = True\n",
        "            # crashes\n",
        "            if vel_y < -3 or abs(vel_x) > 10 or 5 < angle < 355 or pos_x != land_x:\n",
        "                reward -= 10000\n",
        "            # does not crash\n",
        "            else:\n",
        "                # saves fuel\n",
        "                reward += fuel\n",
        "        # lands\n",
        "        else:\n",
        "            # runs out of the fuel\n",
        "            if fuel <= 0:\n",
        "                terminal = True\n",
        "                reward -= 10000        \n",
        "        \n",
        "        self.reward_obs_term = (reward, observation, terminal)\n",
        "        return self.reward_obs_term\n",
        "    \n",
        "    def env_cleanup(self):\n",
        "        return None\n",
        "    \n",
        "    def env_message(self):\n",
        "        return None\n",
        "\n",
        "### same with gym\n",
        "# import gym\n",
        "\n",
        "# class LunarLanderEnvironment(BaseEnvironment):\n",
        "#     def env_init(self, env_info={}):\n",
        "#         \"\"\"\n",
        "#         Setup for the environment called when the experiment first starts.\n",
        "#         \"\"\"\n",
        "#         self.env = gym.make(\"LunarLander-v2\")\n",
        "#         self.env.seed(0)\n",
        "\n",
        "#     def env_start(self):\n",
        "#         \"\"\"\n",
        "#         The first method called when the experiment starts, called before the\n",
        "#         agent starts.\n",
        "\n",
        "#         Returns:\n",
        "#             The first state observation from the environment.\n",
        "#         \"\"\"        \n",
        "        \n",
        "#         reward = 0.0\n",
        "#         observation = self.env.reset()\n",
        "#         is_terminal = False\n",
        "                \n",
        "#         self.reward_obs_term = (reward, observation, is_terminal)\n",
        "        \n",
        "#         # return first state observation from the environment\n",
        "#         return self.reward_obs_term[1]\n",
        "        \n",
        "#     def env_step(self, action):\n",
        "#         \"\"\"A step taken by the environment.\n",
        "\n",
        "#         Args:\n",
        "#             action: The action taken by the agent\n",
        "\n",
        "#         Returns:\n",
        "#             (float, state, Boolean): a tuple of the reward, state observation,\n",
        "#                 and boolean indicating if it's terminal.\n",
        "#         \"\"\"\n",
        "\n",
        "#         last_state = self.reward_obs_term[1]\n",
        "#         current_state, reward, is_terminal, _ = self.env.step(action)\n",
        "        \n",
        "#         self.reward_obs_term = (reward, current_state, is_terminal)\n",
        "        \n",
        "#         return self.reward_obs_term"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "QEAjkkKKKqLP",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "09b03ba3c243b29134f5bc04dcdc10f4",
          "grade": true,
          "grade_id": "cell-99abd81376335339",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "e841563c-8427-4a40-ef7c-48ac641204a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward: 0.0, Terminal: False\n"
          ]
        }
      ],
      "source": [
        "tests(LunarLanderEnvironment, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "ROY9ZLl5KqLR",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5f07c252a2f60904323ad2fb02675221",
          "grade": true,
          "grade_id": "cell-9b3900153803f78e",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "5b956fdf-b3d1-4e22-9f2e-028117954d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward: -10000.0, Terminal: True\n"
          ]
        }
      ],
      "source": [
        "tests(LunarLanderEnvironment, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "HeUoM802KqLT",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b2013e0485824e18311b02cfda0c857b",
          "grade": true,
          "grade_id": "cell-6a53769313d85b0b",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "52f06a24-8aa4-4449-c302-9aeb30644642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward: 5.0, Terminal: True\n"
          ]
        }
      ],
      "source": [
        "tests(LunarLanderEnvironment, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "8pD8Zv5BKqLU",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4ca9f4cf5b7d498584d814a0ec14fbbb",
          "grade": true,
          "grade_id": "cell-86ece2998b73491a",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "8ec50a62-3efd-4435-f4c0-45fc5657dea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward: -10000.0, Terminal: True\n"
          ]
        }
      ],
      "source": [
        "tests(LunarLanderEnvironment, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "1jjffPGhKqLV",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4198a382067136602a638dd1690df307",
          "grade": true,
          "grade_id": "cell-b7cee5ebb3ab91bf",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "90a535ca-c2c8-4cf7-dc87-26bdad91491b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward: -10000.0, Terminal: True\n"
          ]
        }
      ],
      "source": [
        "tests(LunarLanderEnvironment, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1pKvzphSAZC"
      },
      "source": [
        "# Implement the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "deletable": false,
        "id": "ZT5x4ZnHRlMi",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d10feeabf000214a0f53c5dfc5812437",
          "grade": false,
          "grade_id": "cell-e6d82e74c686dbf5",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class ActionValueNetwork:\n",
        " \n",
        "    def __init__(self, network_config):\n",
        "        self.state_dim = network_config.get(\"state_dim\")\n",
        "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
        "        self.num_actions = network_config.get(\"num_actions\")\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
        "\n",
        "        self.layer_sizes = [self.state_dim, self.num_hidden_units, self.num_actions]\n",
        "        \n",
        "        # Initialize the weights of the neural network\n",
        "        # self.weights is an array of dictionaries with each dictionary corresponding to \n",
        "        # the weights from one layer to the next. Each dictionary includes W and b\n",
        "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
        "        for i in range(0, len(self.layer_sizes) - 1):\n",
        "            self.weights[i]['W'] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
        "            self.weights[i]['b'] = np.zeros((1, self.layer_sizes[i + 1]))\n",
        "    \n",
        "    def get_action_values(self, s):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (Numpy array): The state.\n",
        "        Returns:\n",
        "            The action-values (Numpy array) calculated using the network's weights.\n",
        "        \"\"\"\n",
        "        \n",
        "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
        "        psi = np.dot(s, W0) + b0\n",
        "        x = np.maximum(psi, 0)\n",
        "        \n",
        "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
        "        q_vals = np.dot(x, W1) + b1\n",
        "\n",
        "        return q_vals\n",
        "    \n",
        "    def get_TD_update(self, s, delta_mat):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            s (Numpy array): The state.\n",
        "            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
        "            correspond to one state in the batch. Each row has only one non-zero element \n",
        "            which is the TD-error corresponding to the action taken.\n",
        "        Returns:\n",
        "            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
        "        \"\"\"\n",
        "\n",
        "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
        "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
        "        \n",
        "        psi = np.dot(s, W0) + b0\n",
        "        x = np.maximum(psi, 0)\n",
        "        dx = (psi > 0).astype(float)\n",
        "\n",
        "        # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
        "        # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
        "        # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
        "        td_update = [dict() for i in range(len(self.weights))]\n",
        "         \n",
        "        v = delta_mat\n",
        "        td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
        "        td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
        "        \n",
        "        v = np.dot(v, W1.T) * dx\n",
        "        td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
        "        td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
        "                \n",
        "        return td_update\n",
        "    \n",
        "    # Work Required: No. You may wish to read the relevant paper for more information on this weight initialization\n",
        "    # (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)\n",
        "    def init_saxe(self, rows, cols):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            rows (int): number of input units for layer.\n",
        "            cols (int): number of output units for layer.\n",
        "        Returns:\n",
        "            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.\n",
        "        \"\"\"\n",
        "        tensor = self.rand_generator.normal(0, 1, (rows, cols))\n",
        "        if rows < cols:\n",
        "            tensor = tensor.T\n",
        "        tensor, r = np.linalg.qr(tensor)\n",
        "        d = np.diag(r, 0)\n",
        "        ph = np.sign(d)\n",
        "        tensor *= ph\n",
        "\n",
        "        if rows < cols:\n",
        "            tensor = tensor.T\n",
        "        return tensor\n",
        "    \n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Returns: \n",
        "            A copy of the current weights of this network.\n",
        "        \"\"\"\n",
        "        return deepcopy(self.weights)\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
        "        \"\"\"\n",
        "        self.weights = deepcopy(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "deletable": false,
        "id": "Hrd7CKqnRlMo",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "798d4618ba32342f63eb237947151a4a",
          "grade": false,
          "grade_id": "cell-585fd403a17cf660",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class Adam():\n",
        " \n",
        "    def __init__(self, layer_sizes, \n",
        "                 optimizer_info):\n",
        "        self.layer_sizes = layer_sizes\n",
        "\n",
        "        # Specify Adam algorithm's hyper parameters\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
        "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
        "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
        "        \n",
        "        # Initialize Adam algorithm's m and v\n",
        "        self.m = [dict() for i in range(1, len(self.layer_sizes))]\n",
        "        self.v = [dict() for i in range(1, len(self.layer_sizes))]\n",
        "        \n",
        "        for i in range(0, len(self.layer_sizes) - 1):\n",
        "            \n",
        "            self.m[i][\"W\"] = None\n",
        "            self.m[i][\"b\"] = None\n",
        "            self.v[i][\"W\"] = None\n",
        "            self.v[i][\"b\"] = None\n",
        "        \n",
        "            self.m[i][\"W\"] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "            self.m[i][\"b\"] = np.zeros((1, self.layer_sizes[i+1]))\n",
        "            self.v[i][\"W\"] = np.zeros((self.layer_sizes[i], self.layer_sizes[i+1]))\n",
        "            self.v[i][\"b\"] = np.zeros((1, self.layer_sizes[i+1]))            \n",
        "            \n",
        " \n",
        "        self.beta_m_product = self.beta_m\n",
        "        self.beta_v_product = self.beta_v\n",
        "    \n",
        " \n",
        "    def update_weights(self, weights, td_errors_times_gradients):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            weights (Array of dictionaries): The weights of the neural network.\n",
        "            td_errors_times_gradients (Array of dictionaries): The gradient of the \n",
        "            action-values with respect to the network's weights times the TD-error\n",
        "        Returns:\n",
        "            The updated weights (Array of dictionaries).\n",
        "        \"\"\"\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "\n",
        "                weight_update = None\n",
        "                \n",
        "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * td_errors_times_gradients[i][param]\n",
        "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * td_errors_times_gradients[i][param] ** 2\n",
        "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
        "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
        "                weight_update = self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)                \n",
        "                \n",
        "                weights[i][param] = weights[i][param] + weight_update\n",
        "                \n",
        "        self.beta_m_product *= self.beta_m\n",
        "        self.beta_v_product *= self.beta_v\n",
        "        \n",
        "        return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "eNCHit5tRlMs",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dd216bf2169746f6331d6a5fbd79d605",
          "grade": false,
          "grade_id": "cell-1e1aaa0d442eb015",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, size, minibatch_size, seed):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size (integer): The size of the replay buffer.              \n",
        "            minibatch_size (integer): The sample size.\n",
        "            seed (integer): The seed for the random number generator. \n",
        "        \"\"\"\n",
        "        self.buffer = []\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.rand_generator = np.random.RandomState(seed)\n",
        "        self.max_size = size\n",
        "\n",
        "    def append(self, state, action, reward, terminal, next_state):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): The state.              \n",
        "            action (integer): The action.\n",
        "            reward (float): The reward.\n",
        "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
        "            next_state (Numpy array): The next state.           \n",
        "        \"\"\"\n",
        "        if len(self.buffer) == self.max_size:\n",
        "            del self.buffer[0]\n",
        "        self.buffer.append([state, action, reward, terminal, next_state])\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
        "        \"\"\"\n",
        "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
        "        return [self.buffer[idx] for idx in idxs]\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "deletable": false,
        "id": "OS3JGm_hRlMt",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0bc082ff0d5b933fb88fa1936f2057d3",
          "grade": false,
          "grade_id": "cell-b32ebbeb60c5b9f7",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def softmax(action_values, tau=1.0):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
        "                       The action-values computed by an action-value network.              \n",
        "        tau (float): The temperature parameter scalar.\n",
        "    Returns:\n",
        "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
        "        the actions representing the policy.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
        "    # Compute the maximum preference across the actions\n",
        "    preferences = action_values / tau\n",
        "    max_preference = np.max(preferences, axis=1)\n",
        "    \n",
        "    \n",
        "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
        "    # when subtracting the maximum preference from the preference of each action.\n",
        "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
        "    \n",
        "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
        "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
        "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
        "    sum_of_exp_preferences = np.sum(exp_preferences, axis=1)    \n",
        "    \n",
        "    \n",
        "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
        "    # when dividing the numerator by the denominator.\n",
        "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
        "    \n",
        "    # Compute the action probabilities according to the equation in the previous cell.\n",
        "    action_probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
        "    \n",
        "    \n",
        "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
        "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
        "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
        "    action_probs = action_probs.squeeze()\n",
        "    return action_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "deletable": false,
        "id": "Y1clpZ8LRlMv",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7e8ff7f160ca26a7639acc062ae6b29a",
          "grade": false,
          "grade_id": "cell-f370691c828efad9",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
        "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
        "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
        "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
        "        discount (float): The discount factor.\n",
        "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
        "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
        "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
        "                                        and particularly, the action-values at the next-states.\n",
        "    Returns:\n",
        "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
        "    \"\"\"\n",
        "    \n",
        "    q_next_mat = current_q.get_action_values(next_states)\n",
        "    \n",
        "    probs_mat = softmax(q_next_mat, tau)\n",
        "    \n",
        "    v_next_vec = np.sum(q_next_mat * probs_mat, axis=1) * (1-terminals)\n",
        "    \n",
        "    target_vec = rewards + discount * v_next_vec\n",
        "\n",
        "    q_mat = network.get_action_values(states)\n",
        "    \n",
        "    batch_indices = np.arange(q_mat.shape[0])\n",
        "\n",
        "    q_vec = q_mat[batch_indices, actions]\n",
        "\n",
        "    delta_vec = target_vec - q_vec\n",
        "    \n",
        "    return delta_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "deletable": false,
        "id": "1CPmR3NQRlMx",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5772dc09af7d47867f70baa8580057cf",
          "grade": false,
          "grade_id": "cell-2b9714cb6ee933de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
        "                                   rewards, terminals, and next_states.\n",
        "        discount (float): The discount factor.\n",
        "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
        "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
        "                                        and particularly, the action-values at the next-states.\n",
        "    \"\"\"\n",
        "    \n",
        "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
        "    states = np.concatenate(states)\n",
        "    next_states = np.concatenate(next_states)\n",
        "    rewards = np.array(rewards)\n",
        "    terminals = np.array(terminals)\n",
        "    batch_size = states.shape[0]\n",
        "\n",
        "    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
        "\n",
        "    batch_indices = np.arange(batch_size)\n",
        "\n",
        "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
        "    delta_mat[batch_indices, actions] = delta_vec\n",
        "\n",
        "    td_update = network.get_TD_update(states,delta_mat)\n",
        "\n",
        "    weights = optimizer.update_weights(network.get_weights(), td_update)\n",
        "    \n",
        "    network.set_weights(weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "deletable": false,
        "id": "weG0JTCxRlMz",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1d9134ac89ad8c86157599044f5dbc8e",
          "grade": false,
          "grade_id": "cell-54b5db480295424c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#@title Agent implementation\n",
        "\n",
        "\n",
        "### An abstract class that specifies the Agent API\n",
        "from __future__ import print_function\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Implements the agent for an RL-Glue environment.\n",
        "    Note:\n",
        "        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n",
        "        agent_message are required methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_init(self, agent_info= {}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the environment's env_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_cleanup(self):\n",
        "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_message(self, message):\n",
        "        \"\"\"A function used to pass information from the agent to the experiment.\n",
        "        Args:\n",
        "            message: The message passed to the agent.\n",
        "        Returns:\n",
        "            The response (or answer) to the message.\n",
        "        \"\"\"\n",
        "\n",
        "### Agent\n",
        "    #Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines).\n",
        "class Agent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"expected_sarsa_agent\"\n",
        "        \n",
        "    # Work Required: No.\n",
        "    def agent_init(self, agent_config):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the agent.\n",
        "\n",
        "        Assume agent_config dict contains:\n",
        "        {\n",
        "            network_config: dictionary,\n",
        "            optimizer_config: dictionary,\n",
        "            replay_buffer_size: integer,\n",
        "            minibatch_sz: integer, \n",
        "            num_replay_updates_per_step: float\n",
        "            discount_factor: float,\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
        "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
        "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
        "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
        "        self.num_actions = agent_config['network_config']['num_actions']\n",
        "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
        "        self.discount = agent_config['gamma']\n",
        "        self.tau = agent_config['tau']\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
        "        \n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "        \n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "\n",
        "    # Work Required: No.\n",
        "    def policy(self, state):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): the state.\n",
        "        Returns:\n",
        "            the action. \n",
        "        \"\"\"\n",
        "        action_values = self.network.get_action_values(state)\n",
        "        probs_batch = softmax(action_values, self.tau)\n",
        "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
        "        return action\n",
        "\n",
        "    # Work Required: No.\n",
        "    def agent_start(self, state):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "        self.last_state = np.array([state])\n",
        "        self.last_action = self.policy(self.last_state)\n",
        "        return self.last_action\n",
        "\n",
        "    # Work Required: Yes. Fill in the action selection, replay-buffer update, \n",
        "    # weights update using optimize_network, and updating last_state and last_action (~5 lines).\n",
        "    def agent_step(self, reward, state):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        state = np.array([state])\n",
        "        action = self.policy(state)\n",
        "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
        "        \n",
        "        # Perform replay steps:\n",
        "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
        "            current_q = deepcopy(self.network)\n",
        "            for _ in range(self.num_replay):\n",
        "                experiences = self.replay_buffer.sample()\n",
        "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
        "        self.last_state = state\n",
        "        self.last_action = action        \n",
        "        \n",
        "        return action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        state = np.zeros_like(self.last_state)\n",
        "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
        "        \n",
        "        # Perform replay steps:\n",
        "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
        "            current_q = deepcopy(self.network)\n",
        "            for _ in range(self.num_replay):\n",
        "                experiences = self.replay_buffer.sample()\n",
        "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
        "                \n",
        "    def agent_message(self, message):\n",
        "        if message == \"get_sum_reward\":\n",
        "            return self.sum_rewards\n",
        "        else:\n",
        "            raise Exception(\"Unrecognized Message!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "cellView": "form",
        "id": "LSrR594Ha5Os"
      },
      "outputs": [],
      "source": [
        "#@title connect agent to environment so that interaction and thus also experiment can be performed\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "class RLGlue:\n",
        "    \"\"\"RLGlue class\n",
        "\n",
        "    args:\n",
        "        env_name (string): the name of the module where the Environment class can be found\n",
        "        agent_name (string): the name of the module where the Agent class can be found\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_class, agent_class):\n",
        "        self.environment = env_class()\n",
        "        self.agent = agent_class()\n",
        "\n",
        "        self.total_reward = None\n",
        "        self.last_action = None\n",
        "        self.num_steps = None\n",
        "        self.num_episodes = None\n",
        "\n",
        "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
        "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
        "        self.environment.env_init(env_init_info)\n",
        "        self.agent.agent_init(agent_init_info)\n",
        "\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 0\n",
        "        self.num_episodes = 0\n",
        "\n",
        "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
        "        \"\"\"Starts RLGlue experiment\n",
        "\n",
        "        Returns:\n",
        "            tuple: (state, action)\n",
        "        \"\"\"\n",
        "        \n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        last_state = self.environment.env_start()\n",
        "        self.last_action = self.agent.agent_start(last_state)\n",
        "\n",
        "        observation = (last_state, self.last_action)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def rl_agent_start(self, observation):\n",
        "        \"\"\"Starts the agent.\n",
        "\n",
        "        Args:\n",
        "            observation: The first observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_start(observation)\n",
        "\n",
        "    def rl_agent_step(self, reward, observation):\n",
        "        \"\"\"Step taken by the agent\n",
        "\n",
        "        Args:\n",
        "            reward (float): the last reward the agent received for taking the\n",
        "                last action.\n",
        "            observation : the state observation the agent receives from the\n",
        "                environment.\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_step(reward, observation)\n",
        "\n",
        "    def rl_agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates\n",
        "\n",
        "        Args:\n",
        "            reward (float): the reward the agent received when terminating\n",
        "        \"\"\"\n",
        "        self.agent.agent_end(reward)\n",
        "\n",
        "    def rl_env_start(self):\n",
        "        \"\"\"Starts RL-Glue environment.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination\n",
        "        \"\"\"\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        this_observation = self.environment.env_start()\n",
        "\n",
        "        return this_observation\n",
        "\n",
        "    def rl_env_step(self, action):\n",
        "        \"\"\"Step taken by the environment based on action from agent\n",
        "\n",
        "        Args:\n",
        "            action: Action taken by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination.\n",
        "        \"\"\"\n",
        "        ro = self.environment.env_step(action)\n",
        "        (this_reward, _, terminal) = ro\n",
        "\n",
        "        self.total_reward += this_reward\n",
        "\n",
        "        if terminal:\n",
        "            self.num_episodes += 1\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "\n",
        "        return ro\n",
        "\n",
        "    def rl_step(self):\n",
        "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
        "            end by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, action, Boolean): reward, last state observation,\n",
        "                last action, boolean indicating termination\n",
        "        \"\"\"\n",
        "\n",
        "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
        "\n",
        "        self.total_reward += reward;\n",
        "\n",
        "        if term:\n",
        "            self.num_episodes += 1\n",
        "            self.agent.agent_end(reward)\n",
        "            roat = (reward, last_state, None, term)\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "            self.last_action = self.agent.agent_step(reward, last_state)\n",
        "            roat = (reward, last_state, self.last_action, term)\n",
        "\n",
        "        return roat\n",
        "\n",
        "    def rl_cleanup(self):\n",
        "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
        "        self.environment.env_cleanup()\n",
        "        self.agent.agent_cleanup()\n",
        "\n",
        "    def rl_agent_message(self, message):\n",
        "        \"\"\"Message passed to communicate with agent during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the agent\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the agent\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.agent.agent_message(message)\n",
        "\n",
        "    def rl_env_message(self, message):\n",
        "        \"\"\"Message passed to communicate with environment during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the environment\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the environment\n",
        "\n",
        "        \"\"\"\n",
        "        return self.environment.env_message(message)\n",
        "\n",
        "    def rl_episode(self, max_steps_this_episode):\n",
        "        \"\"\"Runs an RLGlue episode\n",
        "\n",
        "        Args:\n",
        "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
        "\n",
        "        Returns:\n",
        "            Boolean: if the episode should terminate\n",
        "        \"\"\"\n",
        "        is_terminal = False\n",
        "\n",
        "        self.rl_start()\n",
        "\n",
        "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
        "                                     (self.num_steps < max_steps_this_episode)):\n",
        "            rl_step_result = self.rl_step()\n",
        "            is_terminal = rl_step_result[3]\n",
        "\n",
        "        return is_terminal\n",
        "\n",
        "    def rl_return(self):\n",
        "        \"\"\"The total reward\n",
        "\n",
        "        Returns:\n",
        "            float: the total reward\n",
        "        \"\"\"\n",
        "        return self.total_reward\n",
        "\n",
        "    def rl_num_steps(self):\n",
        "        \"\"\"The total number of steps taken\n",
        "\n",
        "        Returns:\n",
        "            Int: the total number of steps taken\n",
        "        \"\"\"\n",
        "        return self.num_steps\n",
        "\n",
        "    def rl_num_episodes(self):\n",
        "        \"\"\"The number of episodes\n",
        "\n",
        "        Returns\n",
        "            Int: the total number of episodes\n",
        "\n",
        "        \"\"\"\n",
        "        return self.num_episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "WLnM46ANRlM2",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e192cd7f474ff57861f6f8a3e3ab188c",
          "grade": false,
          "grade_id": "cell-0defecc3f69370dc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "44b03c05-bf94-4bba-e9a4-72115b711975"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 300/300 [00:00<00:00, 360.17it/s]\n"
          ]
        }
      ],
      "source": [
        "#@title performe an experiment\n",
        "\n",
        "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
        "    \n",
        "    rl_glue = RLGlue(environment, agent)\n",
        "        \n",
        "    # save sum of reward at the end of each episode\n",
        "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
        "                                 experiment_parameters[\"num_episodes\"]))\n",
        "\n",
        "    env_info = {}\n",
        "\n",
        "    agent_info = agent_parameters\n",
        "\n",
        "    # one agent setting\n",
        "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
        "        agent_info[\"seed\"] = run\n",
        "        agent_info[\"network_config\"][\"seed\"] = run\n",
        "        env_info[\"seed\"] = run\n",
        "\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        \n",
        "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
        "            # run episode\n",
        "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
        "            \n",
        "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
        "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
        "    save_name = \"{}\".format(rl_glue.agent.name)\n",
        "    if not os.path.exists('./results'):\n",
        "        os.makedirs('./results')\n",
        "    np.save(\"./results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
        "    shutil.make_archive('results', 'zip', './results')\n",
        "\n",
        "# Run Experiment\n",
        "\n",
        "# Experiment parameters\n",
        "experiment_parameters = {\n",
        "    \"num_runs\" : 1,\n",
        "    \"num_episodes\" : 300,\n",
        "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
        "    # some number of timesteps. Here we use the default of 500.\n",
        "    \"timeout\" : 500\n",
        "}\n",
        "\n",
        "# Environment parameters\n",
        "environment_parameters = {}\n",
        "\n",
        "current_env = LunarLanderEnvironment\n",
        "\n",
        "# Agent parameters\n",
        "agent_parameters = {\n",
        "    'network_config': {\n",
        "        'state_dim': 8,\n",
        "        'num_hidden_units': 256,\n",
        "        'num_actions': 4\n",
        "    },\n",
        "    'optimizer_config': {\n",
        "        'step_size': 1e-3,\n",
        "        'beta_m': 0.9, \n",
        "        'beta_v': 0.999,\n",
        "        'epsilon': 1e-8\n",
        "    },\n",
        "    'replay_buffer_size': 50000,\n",
        "    'minibatch_sz': 8,\n",
        "    'num_replay_updates_per_step': 4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 0.001\n",
        "}\n",
        "current_agent = Agent\n",
        "\n",
        "# run experiment\n",
        "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "hEcdphR0brBr"
      },
      "outputs": [],
      "source": [
        "#@title helper for visualization\n",
        "\n",
        "\n",
        "plt_legend_dict = {\"expected_sarsa_agent\": \"Expected SARSA with neural network\",\n",
        "                   \"random_agent\": \"Random\"}\n",
        "path_dict = {\"expected_sarsa_agent\": \"./results/\",\n",
        "             \"random_agent\": \"./results\"}\n",
        "\n",
        "plt_label_dict = {\"expected_sarsa_agent\": \"Sum of\\nreward\\nduring\\nepisode\"}\n",
        "\n",
        "def smooth(data, k):\n",
        "    num_episodes = data.shape[1]\n",
        "    num_runs = data.shape[0]\n",
        "\n",
        "    smoothed_data = np.zeros((num_runs, num_episodes))\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        if i < k:\n",
        "            smoothed_data[:, i] = np.mean(data[:, :i+1], axis = 1)   \n",
        "        else:\n",
        "            smoothed_data[:, i] = np.mean(data[:, i-k:i+1], axis = 1)    \n",
        "        \n",
        "\n",
        "    return smoothed_data\n",
        "\n",
        "# Function to plot result\n",
        "def plot_result(data_name_array):\n",
        "    plt_agent_sweeps = []\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "    \n",
        "    for data_name in data_name_array:\n",
        "        \n",
        "        # load data\n",
        "        filename = 'sum_reward_{}'.format(data_name).replace('.','')\n",
        "        sum_reward_data = np.load('{}/{}.npy'.format(path_dict[data_name], filename))\n",
        "\n",
        "        # smooth data\n",
        "        smoothed_sum_reward = smooth(data = sum_reward_data, k = 100)\n",
        "        \n",
        "        mean_smoothed_sum_reward = np.mean(smoothed_sum_reward, axis = 0)\n",
        "\n",
        "        plot_x_range = np.arange(0, mean_smoothed_sum_reward.shape[0])\n",
        "        graph_current_agent_sum_reward, = ax.plot(plot_x_range, mean_smoothed_sum_reward[:], label=plt_legend_dict[data_name])\n",
        "        plt_agent_sweeps.append(graph_current_agent_sum_reward)\n",
        "    \n",
        "    ax.legend(handles=plt_agent_sweeps, fontsize = 13)\n",
        "    ax.set_title(\"Learning Curve\", fontsize = 15)\n",
        "    ax.set_xlabel('Episodes', fontsize = 14)\n",
        "    ax.set_ylabel(plt_label_dict[data_name_array[0]], rotation=0, labelpad=40, fontsize = 14)\n",
        "    ax.set_ylim([-300, 300])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()     \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "deletable": false,
        "editable": false,
        "id": "h0qnjTgERlM3",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3132510fde7c06020276a6c6f272eccd",
          "grade": false,
          "grade_id": "cell-337be142123eb81f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "07e61fcb-fa4b-441c-c5b9-603f95fb042f"
      },
      "outputs": [],
      "source": [
        "#plot_result([\"expected_sarsa_agent\", \"random_agent\"])  # use this when you experiment with random agent as baseline\n",
        "plot_result([\"expected_sarsa_agent\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5_qhDYCjX32"
      },
      "source": [
        "# Parameter Study\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cGvqd7Vli6Jf",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "501d238ebcf7d6e116e6849af15dbb07",
          "grade": false,
          "grade_id": "cell-e55836e566b8c01d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "#@title dummy environment and dummy agent\n",
        "\n",
        "\n",
        "### dummy environment\n",
        "\n",
        "class DummyEnvironment(BaseEnvironment):\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"\n",
        "        Setup for the environment called when the experiment first starts.\n",
        "        \"\"\"\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"\n",
        "        The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"        \n",
        "        \n",
        "        reward = 0.0\n",
        "        observation = None\n",
        "        is_terminal = False\n",
        "                \n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "        \n",
        "        # return first state observation from the environment\n",
        "        return self.reward_obs_term[1]\n",
        "        \n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0.0\n",
        "        is_terminal = True\n",
        "        current_state = None\n",
        "        \n",
        "        self.reward_obs_term = (reward, current_state, is_terminal)\n",
        "        \n",
        "        return self.reward_obs_term\n",
        "\n",
        "\n",
        "\n",
        "### dummy agent\n",
        "\n",
        "class DummyAgent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"dummy_agent\"\n",
        "        self.step_size = None\n",
        "        self.discount_factor = None\n",
        "        pass\n",
        "       \n",
        "    def agent_init(self, agent_info={}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the semi-gradient TD(0) with Neural Network.\n",
        "\n",
        "        Assume agent_info dict contains: TODO\n",
        "        {\n",
        "            step_size: float, \n",
        "            discount_factor: float, \n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\")) # set random seed for each run\n",
        "\n",
        "        # save relevant info from agent_info\n",
        "        self.input_dim = agent_info.get(\"input_dim\")\n",
        "        self.num_actions = agent_info.get(\"num_actions\")\n",
        "        self.step_size = agent_info.get(\"step_size\")\n",
        "        self.discount_factor = agent_info.get(\"discount_factor\")\n",
        "        self.tau = agent_info.get(\"tau\")\n",
        "        \n",
        "        self.weights = np.zeros((self.num_actions, self.input_dim))\n",
        "\n",
        "        self.last_state = None\n",
        "        self.last_action = None\n",
        "\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        return self.rand_generator.randint(self.num_actions)\n",
        "\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.last_state = observation\n",
        "        self.last_action = self.choose_action(observation)\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "        state = observation\n",
        "\n",
        "        # choose action\n",
        "        action = self.choose_action(state)\n",
        "\n",
        "        self.last_state = observation\n",
        "        self.last_action = action\n",
        "        \n",
        "        return action\n",
        "\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "        \n",
        "    def agent_message(self, message):\n",
        "        if message == 'get_sum_reward':\n",
        "            return self.rand_generator.normal(0, 0.1) * -1 * (np.log2(self.step_size / 3e-5) ** 2 + np.log(self.tau) ** 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "deletable": false,
        "id": "xU_baTc-i6Ji",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "74fc83f73918b97072a2c3dffa909bdd",
          "grade": false,
          "grade_id": "cell-e53c85e6098a975b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
        "    \n",
        "    \"\"\"\n",
        "    Assume environment_parameters dict contains:\n",
        "    {\n",
        "        input_dim: integer,\n",
        "        num_actions: integer,\n",
        "        discount_factor: float\n",
        "    }\n",
        "    \n",
        "    Assume agent_parameters dict contains:\n",
        "    {\n",
        "        step_size: 1D numpy array of floats,\n",
        "        tau: 1D numpy array of floats\n",
        "    }\n",
        "    \n",
        "    Assume experiment_parameters dict contains:\n",
        "    {\n",
        "        num_runs: integer,\n",
        "        num_episodes: integer\n",
        "    }    \n",
        "    \"\"\"\n",
        "    \n",
        "    ### Instantiate rl_glue from RLGlue    \n",
        "    rl_glue = RLGlue(environment, agent)\n",
        "\n",
        "    os.system('sleep 1') # to prevent tqdm printing out-of-order\n",
        "    agent_sum_reward = np.zeros((len(agent_parameters[\"tau\"]), len(agent_parameters[\"step_size\"]),experiment_parameters[\"num_runs\"], experiment_parameters[\"num_episodes\"]))    \n",
        "\n",
        "    for i in tqdm(range(len(agent_parameters[\"tau\"]))):\n",
        "    \n",
        "        # for loop over different values of the step-size\n",
        "        for j in range(len(agent_parameters[\"step_size\"])): \n",
        "\n",
        "            ### Specify env_info \n",
        "            env_info = {}\n",
        "\n",
        "            ### Specify agent_info\n",
        "            agent_info = {\"num_actions\": environment_parameters[\"num_actions\"],\n",
        "                          \"input_dim\": environment_parameters[\"input_dim\"],\n",
        "                          \"discount_factor\": environment_parameters[\"discount_factor\"],\n",
        "                          \"tau\": agent_parameters[\"tau\"][i],\n",
        "                          \"step_size\": agent_parameters[\"step_size\"][j]}\n",
        "\n",
        "            # for loop over runs\n",
        "            for run in range(experiment_parameters[\"num_runs\"]): \n",
        "                \n",
        "                # Set the seed\n",
        "                agent_info[\"seed\"] = agent_parameters[\"seed\"] * experiment_parameters[\"num_runs\"] + run\n",
        "                \n",
        "                # Beginning of the run            \n",
        "                rl_glue.rl_init(agent_info, env_info)\n",
        "\n",
        "                for episode in range(experiment_parameters[\"num_episodes\"]): \n",
        "                    \n",
        "                    # Run episode\n",
        "                    rl_glue.rl_episode(0) # no step limit\n",
        "\n",
        "                    ### Store sum of reward\n",
        "                    agent_sum_reward[i, j, run, episode] = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
        "\n",
        "            if not os.path.exists('./results'):\n",
        "                    os.makedirs('./results')\n",
        "\n",
        "            save_name = \"{}\".format(rl_glue.agent.name).replace('.','')\n",
        "\n",
        "            # save sum reward\n",
        "            np.save(\"./results/sum_reward_{}\".format(save_name), agent_sum_reward)     \n",
        "   "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s2k2Zpf7KqK_",
        "u1pKvzphSAZC",
        "5fYWU6TNRlMX",
        "K5_qhDYCjX32"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
