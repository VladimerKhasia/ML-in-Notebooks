{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ROVG2Jo3A2kl",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3d7e0bf7bf37a14b2f4cc8896af5d808",
          "grade": false,
          "grade_id": "cell-c9904c1c46f57746",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# Bandits and Exploration/Exploitation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "OmFXSrRpA2kt",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fbe36b78ea23f980c0fec58209d0f136",
          "grade": false,
          "grade_id": "cell-b1f350f6be960eea",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "cZdbPQY_J5Ew"
      },
      "outputs": [],
      "source": [
        "#@title agent abstract class and agent itself\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from __future__ import print_function\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "\n",
        "### Abstract Class of Agent Starts Here\n",
        "\"\"\"An abstract class that specifies the Agent API for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Implements the agent for an RL-Glue environment.\n",
        "    Note:\n",
        "        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n",
        "        agent_message are required methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_init(self, agent_info= {}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_cleanup(self):\n",
        "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_message(self, message):\n",
        "        \"\"\"A function used to pass information from the agent to the experiment.\n",
        "        Args:\n",
        "            message: The message passed to the agent.\n",
        "        Returns:\n",
        "            The response (or answer) to the message.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "### Agent Starts Here\n",
        "\n",
        "class Agent(BaseAgent):\n",
        "    \"\"\"agent does *no* learning, selects action 0 always\"\"\"\n",
        "    def __init__(self):\n",
        "        self.last_action = None\n",
        "        self.num_actions = None\n",
        "        self.q_values = None\n",
        "        self.step_size = None\n",
        "        self.epsilon = None\n",
        "        self.initial_value = 0.0\n",
        "        self.arm_count = [0.0 for _ in range(10)]\n",
        "\n",
        "    def agent_init(self, agent_info={}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "        # if \"actions\" in agent_info:\n",
        "        #     self.num_actions = agent_info[\"actions\"]\n",
        "\n",
        "        # if \"state_array\" in agent_info:\n",
        "        #     self.q_values = agent_info[\"state_array\"]\n",
        "\n",
        "        self.num_actions = agent_info.get(\"num_actions\", 2)\n",
        "        self.initial_value = agent_info.get(\"initial_value\", 0.0)\n",
        "        self.q_values = np.ones(agent_info.get(\"num_actions\", 2)) * self.initial_value\n",
        "        self.step_size = agent_info.get(\"step_size\", 0.1)\n",
        "        self.epsilon = agent_info.get(\"epsilon\", 0.0)\n",
        "\n",
        "        self.last_action = 0\n",
        "\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        self.last_action = np.random.choice(self.num_actions)  # set first action to 0\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "        # local_action = 0  # choose the action here\n",
        "        self.last_action = np.random.choice(self.num_actions)\n",
        "\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def agent_cleanup(self):\n",
        "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def agent_message(self, message):\n",
        "        \"\"\"A function used to pass information from the agent to the experiment.\n",
        "        Args:\n",
        "            message: The message passed to the agent.\n",
        "        Returns:\n",
        "            The response (or answer) to the message.\n",
        "        \"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "soTOGkMPLSKu"
      },
      "outputs": [],
      "source": [
        "#@title environment abstract class and environment itself\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "# from __future__ import print_function\n",
        "\n",
        "# from abc import ABCMeta, abstractmethod\n",
        "\n",
        "### Environment Abstract Class Starts Here\n",
        "\"\"\"Abstract environment base class for RL-Glue-py.\n",
        "\"\"\"\n",
        "\n",
        "class BaseEnvironment:\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message: the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            the response (or answer) to the message\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "### Environment Starts Here\n",
        "\n",
        "class Environment(BaseEnvironment):\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    actions = [0]\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "        self.count = 0\n",
        "        self.arms = []\n",
        "        self.seed = None\n",
        "\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        self.arms = np.random.randn(10)#[np.random.normal(0.0, 1.0) for _ in range(10)]\n",
        "        local_observation = 0  # An empty NumPy array\n",
        "\n",
        "        self.reward_obs_term = (0.0, local_observation, False)\n",
        "\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "        return self.reward_obs_term[1]\n",
        "\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        # if action == 0:\n",
        "        #     if np.random.random() < 0.2:\n",
        "        #         reward = 14\n",
        "        #     else:\n",
        "        #         reward = 6\n",
        "\n",
        "        # if action == 1:\n",
        "        #     reward = np.random.choice(range(10,14))\n",
        "\n",
        "        # if action == 2:\n",
        "        #     if np.random.random() < 0.8:\n",
        "        #         reward = 174\n",
        "        #     else:\n",
        "        #         reward = 7\n",
        "\n",
        "        # reward = np.random.normal(self.arms[action], 1.0)\n",
        "\n",
        "        reward = self.arms[action] + np.random.randn()\n",
        "\n",
        "        obs = self.reward_obs_term[1]\n",
        "\n",
        "        self.reward_obs_term = (reward, obs, False)\n",
        "\n",
        "        return self.reward_obs_term\n",
        "\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "        pass\n",
        "\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message (string): the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            string: the response (or answer) to the message\n",
        "        \"\"\"\n",
        "        if message == \"what is the current reward?\":\n",
        "            return \"{}\".format(self.reward_obs_term[0])\n",
        "\n",
        "        # else\n",
        "        return \"I don't know how to respond to your message\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "6VCxZM3HMWhN"
      },
      "outputs": [],
      "source": [
        "#@title connect environment and agent (based on rl_glue originally designed by Adam White, Brian Tanner, and Rich Sutton)\n",
        "\n",
        "#!/usr/bin/env python\n",
        "\n",
        "#from __future__ import print_function\n",
        "\n",
        "\n",
        "class RLGlue:\n",
        "    \"\"\"RLGlue class\n",
        "\n",
        "    args:\n",
        "        env_name (string): the name of the module where the Environment class can be found\n",
        "        agent_name (string): the name of the module where the Agent class can be found\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_class, agent_class):\n",
        "        self.environment = env_class()\n",
        "        self.agent = agent_class()\n",
        "\n",
        "        self.total_reward = None\n",
        "        self.last_action = None\n",
        "        self.num_steps = None\n",
        "        self.num_episodes = None\n",
        "\n",
        "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
        "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
        "        self.environment.env_init(env_init_info)\n",
        "        self.agent.agent_init(agent_init_info)\n",
        "\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 0\n",
        "        self.num_episodes = 0\n",
        "\n",
        "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
        "        \"\"\"Starts RLGlue experiment\n",
        "\n",
        "        Returns:\n",
        "            tuple: (state, action)\n",
        "        \"\"\"\n",
        "\n",
        "        last_state = self.environment.env_start()\n",
        "        self.last_action = self.agent.agent_start(last_state)\n",
        "\n",
        "        observation = (last_state, self.last_action)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def rl_agent_start(self, observation):\n",
        "        \"\"\"Starts the agent.\n",
        "\n",
        "        Args:\n",
        "            observation: The first observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_start(observation)\n",
        "\n",
        "    def rl_agent_step(self, reward, observation):\n",
        "        \"\"\"Step taken by the agent\n",
        "\n",
        "        Args:\n",
        "            reward (float): the last reward the agent received for taking the\n",
        "                last action.\n",
        "            observation : the state observation the agent receives from the\n",
        "                environment.\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_step(reward, observation)\n",
        "\n",
        "    def rl_agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates\n",
        "\n",
        "        Args:\n",
        "            reward (float): the reward the agent received when terminating\n",
        "        \"\"\"\n",
        "        self.agent.agent_end(reward)\n",
        "\n",
        "    def rl_env_start(self):\n",
        "        \"\"\"Starts RL-Glue environment.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination\n",
        "        \"\"\"\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        this_observation = self.environment.env_start()\n",
        "\n",
        "        return this_observation\n",
        "\n",
        "    def rl_env_step(self, action):\n",
        "        \"\"\"Step taken by the environment based on action from agent\n",
        "\n",
        "        Args:\n",
        "            action: Action taken by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination.\n",
        "        \"\"\"\n",
        "        ro = self.environment.env_step(action)\n",
        "        (this_reward, _, terminal) = ro\n",
        "\n",
        "        self.total_reward += this_reward\n",
        "\n",
        "        if terminal:\n",
        "            self.num_episodes += 1\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "\n",
        "        return ro\n",
        "\n",
        "    def rl_step(self):\n",
        "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
        "            end by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, action, Boolean): reward, last state observation,\n",
        "                last action, boolean indicating termination\n",
        "        \"\"\"\n",
        "\n",
        "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
        "\n",
        "        self.total_reward += reward\n",
        "\n",
        "        if term:\n",
        "            self.num_episodes += 1\n",
        "            self.agent.agent_end(reward)\n",
        "            roat = (reward, last_state, None, term)\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "            self.last_action = self.agent.agent_step(reward, last_state)\n",
        "            roat = (reward, last_state, self.last_action, term)\n",
        "\n",
        "        return roat\n",
        "\n",
        "    def rl_cleanup(self):\n",
        "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
        "        self.environment.env_cleanup()\n",
        "        self.agent.agent_cleanup()\n",
        "\n",
        "    def rl_agent_message(self, message):\n",
        "        \"\"\"Message passed to communicate with agent during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the agent\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the agent\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.agent.agent_message(message)\n",
        "\n",
        "    def rl_env_message(self, message):\n",
        "        \"\"\"Message passed to communicate with environment during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the environment\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the environment\n",
        "\n",
        "        \"\"\"\n",
        "        return self.environment.env_message(message)\n",
        "\n",
        "    def rl_episode(self, max_steps_this_episode):\n",
        "        \"\"\"Runs an RLGlue episode\n",
        "\n",
        "        Args:\n",
        "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
        "\n",
        "        Returns:\n",
        "            Boolean: if the episode should terminate\n",
        "        \"\"\"\n",
        "        is_terminal = False\n",
        "\n",
        "        self.rl_start()\n",
        "\n",
        "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
        "                                     (self.num_steps < max_steps_this_episode)):\n",
        "            rl_step_result = self.rl_step()\n",
        "            is_terminal = rl_step_result[3]\n",
        "\n",
        "        return is_terminal\n",
        "\n",
        "    def rl_return(self):\n",
        "        \"\"\"The total reward\n",
        "\n",
        "        Returns:\n",
        "            float: the total reward\n",
        "        \"\"\"\n",
        "        return self.total_reward\n",
        "\n",
        "    def rl_num_steps(self):\n",
        "        \"\"\"The total number of steps taken\n",
        "\n",
        "        Returns:\n",
        "            Int: the total number of steps taken\n",
        "        \"\"\"\n",
        "        return self.num_steps\n",
        "\n",
        "    def rl_num_episodes(self):\n",
        "        \"\"\"The number of episodes\n",
        "\n",
        "        Returns\n",
        "            Int: the total number of episodes\n",
        "\n",
        "        \"\"\"\n",
        "        return self.num_episodes\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "tDv_HjcjA2kw",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6dfc1a07738ba7ef428ad0d6045b194d",
          "grade": false,
          "grade_id": "cell-753cb03c956b611e",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Greedy Agent "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "id": "H3Q6ZL7LA2ky",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7891047f2d6f4137ad3a82d2a4390c88",
          "grade": false,
          "grade_id": "cell-00a70af9534c45cb",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def argmax(q_values):\n",
        "    \"\"\"\n",
        "    Takes in a list of q_values and returns the index of the item \n",
        "    with the highest value. Breaks ties randomly.\n",
        "    returns: int - the index of the highest value in q_values\n",
        "    \"\"\"\n",
        "    top_value = float(\"-inf\")\n",
        "    ties = []\n",
        "    \n",
        "    for i in range(len(q_values)):\n",
        "        # if a value in q_values is greater than the highest value update top and reset ties to zero\n",
        "        # if a value is equal to top value add the index to ties\n",
        "        # return a random selection from ties.\n",
        "        # YOUR CODE HERE\n",
        "        ## raise NotImplementedError()\n",
        "        if q_values[i] > top_value:\n",
        "            top_value, ties = q_values[i], [i]\n",
        "        elif q_values[i] == top_value:\n",
        "            ties.append(i)\n",
        "    return np.random.choice(ties)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "deletable": false,
        "id": "j22qlG2BA2k1",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a6f8d5ef9b1437998b78c7e73ec75ac1",
          "grade": false,
          "grade_id": "cell-582d9e7f86d07eb6",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "class GreedyAgent(Agent):\n",
        "    def agent_step(self, reward, observation=None):\n",
        "        \"\"\"\n",
        "        Takes one step for the agent. It takes in a reward and observation and \n",
        "        returns the action the agent chooses at that time step.\n",
        "        \n",
        "        Arguments:\n",
        "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
        "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
        "                              until future lessons\n",
        "        Returns:\n",
        "        current_action -- int, the action chosen by the agent at the current time step.\n",
        "        \"\"\"\n",
        "\n",
        "        current_action = argmax(self.q_values)\n",
        "\n",
        "        self.arm_count[self.last_action] += 1\n",
        "        step_size = 1 / self.arm_count[self.last_action]\n",
        "        old_estimate = self.q_values[self.last_action]\n",
        "        self.q_values[self.last_action] = old_estimate + step_size * (reward - old_estimate)        \n",
        "    \n",
        "        self.last_action = current_action\n",
        "        \n",
        "        return current_action\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "deletable": false,
        "editable": false,
        "id": "LqQNmRcHA2k3",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "085d0577b0263d3f40d731383d9ad2a4",
          "grade": false,
          "grade_id": "cell-13bf4a5ec5402a22",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "aefb9105-f17a-453e-a356-46fdc65eb639"
      },
      "outputs": [],
      "source": [
        "num_runs = 200                    # The number of times we run the experiment\n",
        "num_steps = 1000                  # The number of pulls of each arm the agent takes\n",
        "env = Environment                 # We set what environment we want to use to test\n",
        "agent = GreedyAgent               # We choose what agent we want to use\n",
        "agent_info = {\"num_actions\": 10}  # We pass the agent the information it needs. Here how many arms there are.\n",
        "env_info = {}                     # We pass the environment the information it needs. In this case nothing.\n",
        "\n",
        "rewards = np.zeros((num_runs, num_steps))\n",
        "average_best = 0\n",
        "for run in tqdm(range(num_runs)):           # tqdm is what creates the progress bar below\n",
        "    np.random.seed(run)\n",
        "    \n",
        "    rl_glue = RLGlue(env, agent)          # Creates a new RLGlue experiment with the env and agent we chose above\n",
        "    rl_glue.rl_init(agent_info, env_info) # We pass RLGlue what it needs to initialize the agent and environment\n",
        "    rl_glue.rl_start()                    # We start the experiment\n",
        "\n",
        "    average_best += np.max(rl_glue.environment.arms)\n",
        "    \n",
        "    for i in range(num_steps):\n",
        "        reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return\n",
        "                                                 # the reward, and action taken.\n",
        "        rewards[run, i] = reward\n",
        "\n",
        "greedy_scores = np.mean(rewards, axis=0)\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([average_best / num_runs for _ in range(num_steps)], linestyle=\"--\")\n",
        "plt.plot(greedy_scores)\n",
        "plt.legend([\"Best Possible\", \"Greedy\"])\n",
        "plt.title(\"Average Reward of Greedy Agent\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f2yZn0MAA2k4",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e097e83bdc2cb91f5f54d2ed0a80c79e",
          "grade": false,
          "grade_id": "cell-ca7a4ae176f250d1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## Epsilon-Greedy Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "knhOO6UxA2k4",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9130c5614d2ba27c32e5fe6173a54af7",
          "grade": false,
          "grade_id": "cell-04a8bd103b7af798",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "We learned about [another way for an agent to operate](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/lecture/tHDck/what-is-the-trade-off), where it does not always take the greedy action. Instead, sometimes it takes an exploratory action. It does this so that it can find out what the best action really is. If we always choose what we think is the current best action is, we may miss out on taking the true best action, because we haven't explored enough times to find that best action.\n",
        "\n",
        "Implement an epsilon-greedy agent below. Hint: we are implementing the algorithm from [section 2.4](http://www.incompleteideas.net/book/RLbook2018.pdf#page=52) of the textbook. You may want to use your greedy code from above and look at [np.random.random](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random.html), as well as [np.random.randint](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.randint.html), to help you select random actions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "deletable": false,
        "id": "MCf964-aA2k5",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d8598190c401a85561155c94f1b7e24d",
          "grade": false,
          "grade_id": "cell-6862cb5ef5702d22",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyAgent(Agent):\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"\n",
        "        Takes one step for the agent. It takes in a reward and observation and \n",
        "        returns the action the agent chooses at that time step.\n",
        "        \n",
        "        Arguments:\n",
        "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
        "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
        "                              until future lessons\n",
        "        Returns:\n",
        "        current_action -- int, the action chosen by the agent at the current time step.\n",
        "        \"\"\"\n",
        "     \n",
        "        if np.random.random() < self.epsilon:\n",
        "            current_action = np.random.randint(0, len(self.q_values))\n",
        "        else:\n",
        "            current_action = argmax(self.q_values)\n",
        "        \n",
        "\n",
        "        self.arm_count[self.last_action] += 1\n",
        "        step_size = 1 / self.arm_count[self.last_action]\n",
        "        old_estimate = self.q_values[self.last_action]\n",
        "        self.q_values[self.last_action] = old_estimate + step_size * (reward - old_estimate)\n",
        "        \n",
        "        self.last_action = current_action\n",
        "        \n",
        "        return current_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "deletable": false,
        "editable": false,
        "id": "M7fk76SqA2k6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "8ef12f07e52b4cb86feada569e684744",
          "grade": false,
          "grade_id": "cell-2f6cef9d3ecdace7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "3f5dabd2-9509-4cd2-c601-871c5b4501a6"
      },
      "outputs": [],
      "source": [
        "# Plot Epsilon greedy results and greedy results\n",
        "num_runs = 200\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "agent = EpsilonGreedyAgent\n",
        "env = Environment\n",
        "agent_info = {\"num_actions\": 10, \"epsilon\": epsilon}\n",
        "env_info = {}\n",
        "all_rewards = np.zeros((num_runs, num_steps))\n",
        "\n",
        "for run in tqdm(range(num_runs)):\n",
        "    np.random.seed(run)\n",
        "    \n",
        "    rl_glue = RLGlue(env, agent)\n",
        "    rl_glue.rl_init(agent_info, env_info)\n",
        "    rl_glue.rl_start()\n",
        "\n",
        "    for i in range(num_steps):\n",
        "        reward, _, action, _ = rl_glue.rl_step() # The environment and agent take a step and return\n",
        "                                                 # the reward, and action taken.\n",
        "        all_rewards[run, i] = reward\n",
        "\n",
        "# take the mean over runs\n",
        "scores = np.mean(all_rewards, axis=0)\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n",
        "plt.plot(greedy_scores)\n",
        "plt.title(\"Average Reward of Greedy Agent vs. E-Greedy Agent\")\n",
        "plt.plot(scores)\n",
        "plt.legend((\"Best Possible\", \"Greedy\", \"Epsilon: 0.1\"))\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "deletable": false,
        "editable": false,
        "id": "jW1DyM25A2k8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9896340e89e8cd11bb0a90ef048e9084",
          "grade": false,
          "grade_id": "cell-69d62e83fc1d91bc",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "6b8e4735-1994-4e43-fef1-569c402db04c"
      },
      "outputs": [],
      "source": [
        "# Averaging Multiple Runs\n",
        "\n",
        "# Plot runs of e-greedy agent\n",
        "agent = EpsilonGreedyAgent\n",
        "env = Environment\n",
        "agent_info = {\"num_actions\": 10, \"epsilon\": 0.1}\n",
        "env_info = {}\n",
        "all_averages = []\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "num_steps = 1000\n",
        "\n",
        "for run in (0, 1):\n",
        "    np.random.seed(run) # Here we set the seed so that we can compare two different runs\n",
        "    averages = []\n",
        "    rl_glue = RLGlue(env, agent)\n",
        "    rl_glue.rl_init(agent_info, env_info)\n",
        "    rl_glue.rl_start()\n",
        "\n",
        "    scores = [0]\n",
        "    for i in range(num_steps):\n",
        "        reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "        scores.append(scores[-1] + reward)\n",
        "        averages.append(scores[-1] / (i + 1))\n",
        "    \n",
        "    plt.plot(averages)\n",
        "\n",
        "plt.title(\"Comparing two independent runs\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "bijj_hjMA2k8",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "27f087a7a6f92b6ba2461d66dfc64779",
          "grade": false,
          "grade_id": "cell-a6e9ef699d799240",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "2d778512-2240-48c0-ee90-1b8bdea2018e"
      },
      "outputs": [],
      "source": [
        "print(\"Random Seed 1\")\n",
        "np.random.seed(1)\n",
        "for _ in range(15):\n",
        "    if np.random.random() < 0.1:\n",
        "        print(\"Exploratory Action\")\n",
        "    \n",
        "\n",
        "print()\n",
        "print()\n",
        "\n",
        "print(\"Random Seed 2\")\n",
        "np.random.seed(2)\n",
        "for _ in range(15):\n",
        "    if np.random.random() < 0.1:\n",
        "        print(\"Exploratory Action\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "deletable": false,
        "editable": false,
        "id": "PWgcrj9hA2k9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e1e772b65c6e29c699f2fb141c37df73",
          "grade": false,
          "grade_id": "cell-4c9881740ba46656",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "b1c986ad-f048-4251-e7f5-2529f0586b09"
      },
      "outputs": [],
      "source": [
        "# Comparing values of epsilon\n",
        "\n",
        "# Experiment code for different e-greedy\n",
        "epsilons = [0.0, 0.01, 0.1, 0.4]\n",
        "\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n",
        "\n",
        "n_q_values = []\n",
        "n_averages = []\n",
        "n_best_actions = []\n",
        "\n",
        "num_runs = 200\n",
        "\n",
        "for epsilon in epsilons:\n",
        "    all_averages = []\n",
        "    for run in tqdm(range(num_runs)):\n",
        "        agent = EpsilonGreedyAgent\n",
        "        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon}\n",
        "        env_info = {\"random_seed\": run}\n",
        "\n",
        "        rl_glue = RLGlue(env, agent)\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        rl_glue.rl_start()\n",
        "        \n",
        "        best_arm = np.argmax(rl_glue.environment.arms)\n",
        "\n",
        "        scores = [0]\n",
        "        averages = []\n",
        "        best_action_chosen = []\n",
        "        \n",
        "        for i in range(num_steps):\n",
        "            reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "            scores.append(scores[-1] + reward)\n",
        "            averages.append(scores[-1] / (i + 1))\n",
        "            if action == best_arm:\n",
        "                best_action_chosen.append(1)\n",
        "            else:\n",
        "                best_action_chosen.append(0)\n",
        "            if epsilon == 0.1 and run == 0:\n",
        "                n_q_values.append(np.copy(rl_glue.agent.q_values))\n",
        "        if epsilon == 0.1:\n",
        "            n_averages.append(averages)\n",
        "            n_best_actions.append(best_action_chosen)\n",
        "        all_averages.append(averages)\n",
        "        \n",
        "    plt.plot(np.mean(all_averages, axis=0))\n",
        "\n",
        "plt.legend([\"Best Possible\"] + epsilons)\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "deletable": false,
        "id": "aZl8W28CA2k_",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "706014b629a7e271074f562e910272ae",
          "grade": false,
          "grade_id": "cell-fe26903228ef0c50",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyAgentConstantStepsize(Agent):\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"\n",
        "        Takes one step for the agent. It takes in a reward and observation and \n",
        "        returns the action the agent chooses at that time step.\n",
        "        \n",
        "        Arguments:\n",
        "        reward -- float, the reward the agent recieved from the environment after taking the last action.\n",
        "        observation -- float, the observed state the agent is in. Do not worry about this as you will not use it\n",
        "                              until future lessons\n",
        "        Returns:\n",
        "        current_action -- int, the action chosen by the agent at the current time step.\n",
        "        \"\"\"\n",
        "        \n",
        "        if np.random.random() < self.epsilon:\n",
        "            current_action = np.random.randint(0, len(self.q_values))\n",
        "        else:\n",
        "            current_action = argmax(self.q_values)        \n",
        "        \n",
        "        self.arm_count[self.last_action] += 1\n",
        "        old_estimate = self.q_values[self.last_action]\n",
        "        self.q_values[self.last_action] = old_estimate + self.step_size * (reward - old_estimate)        \n",
        "        \n",
        "        self.last_action = current_action\n",
        "        \n",
        "        return current_action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "deletable": false,
        "editable": false,
        "id": "mZgvTUPDA2lA",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bef121f61b9cf786c92f5bd2af961cb9",
          "grade": false,
          "grade_id": "cell-a5d327f4d52578e6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "3b76737c-f789-411a-aafe-eadc34f61d4e"
      },
      "outputs": [],
      "source": [
        "# Experiment code for different step sizes\n",
        "step_sizes = [0.01, 0.1, 0.5, 1.0, '1/N(A)']\n",
        "\n",
        "epsilon = 0.1\n",
        "num_steps = 1000\n",
        "num_runs = 200\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "\n",
        "q_values = {step_size: [] for step_size in step_sizes}\n",
        "true_values = {step_size: None for step_size in step_sizes}\n",
        "best_actions = {step_size: [] for step_size in step_sizes}\n",
        "\n",
        "for step_size in step_sizes:\n",
        "    all_averages = []\n",
        "    for run in tqdm(range(num_runs)):\n",
        "        np.random.seed(run)\n",
        "        agent = EpsilonGreedyAgentConstantStepsize if step_size != '1/N(A)' else EpsilonGreedyAgent\n",
        "        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon, \"step_size\": step_size, \"initial_value\": 0.0}\n",
        "        env_info = {}\n",
        "\n",
        "        rl_glue = RLGlue(env, agent)\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        rl_glue.rl_start()\n",
        "        \n",
        "        best_arm = np.argmax(rl_glue.environment.arms)\n",
        "\n",
        "        if run == 0:\n",
        "            true_values[step_size] = np.copy(rl_glue.environment.arms)\n",
        "            \n",
        "        best_action_chosen = []\n",
        "        for i in range(num_steps):\n",
        "            reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "            if action == best_arm:\n",
        "                best_action_chosen.append(1)\n",
        "            else:\n",
        "                best_action_chosen.append(0)\n",
        "            if run == 0:\n",
        "                q_values[step_size].append(np.copy(rl_glue.agent.q_values))\n",
        "        best_actions[step_size].append(best_action_chosen)\n",
        "    ax.plot(np.mean(best_actions[step_size], axis=0))\n",
        "\n",
        "plt.legend(step_sizes)\n",
        "plt.title(\"% Best Arm Pulled\")\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"% Best Arm Pulled\")\n",
        "vals = ax.get_yticks()\n",
        "ax.set_yticklabels(['{:,.2%}'.format(x) for x in vals])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "deletable": false,
        "editable": false,
        "id": "NrGvJ_CDA2lA",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c9192f306586b80bc3cf978e3a99d6c0",
          "grade": false,
          "grade_id": "cell-cd92b14ec8825d38",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "outputId": "830afff2-f958-4e47-cf1e-fb7ffd102cc3"
      },
      "outputs": [],
      "source": [
        "largest = 0\n",
        "num_steps = 1000\n",
        "for step_size in step_sizes:\n",
        "    plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "    largest = np.argmax(true_values[step_size])\n",
        "    plt.plot([true_values[step_size][largest] for _ in range(num_steps)], linestyle=\"--\")\n",
        "    plt.title(\"Step Size: {}\".format(step_size))\n",
        "    plt.plot(np.array(q_values[step_size])[:, largest])\n",
        "    plt.legend([\"True Expected Value\", \"Estimated Value\"])\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "deletable": false,
        "editable": false,
        "id": "pAUKX8aSA2lC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4db5be8e659415ba13ceb258c9b1ceb9",
          "grade": false,
          "grade_id": "cell-55536f4ac923ab96",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "c66db7f0-5f0d-46b0-fe15-5565d4df5ae2"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1\n",
        "num_steps = 2000\n",
        "num_runs = 500\n",
        "step_size = 0.1\n",
        "\n",
        "plt.figure(figsize=(15, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
        "plt.plot([1.55 for _ in range(num_steps)], linestyle=\"--\")\n",
        "\n",
        "for agent in [EpsilonGreedyAgent, EpsilonGreedyAgentConstantStepsize]:\n",
        "    rewards = np.zeros((num_runs, num_steps))\n",
        "    for run in tqdm(range(num_runs)):\n",
        "        agent_info = {\"num_actions\": 10, \"epsilon\": epsilon, \"step_size\": step_size}\n",
        "        np.random.seed(run)\n",
        "        \n",
        "        rl_glue = RLGlue(env, agent)\n",
        "        rl_glue.rl_init(agent_info, env_info)\n",
        "        rl_glue.rl_start()\n",
        "\n",
        "        for i in range(num_steps):\n",
        "            reward, state, action, is_terminal = rl_glue.rl_step()\n",
        "            rewards[run, i] = reward\n",
        "            if i == 1000:\n",
        "                rl_glue.environment.arms = np.random.randn(10)\n",
        "        \n",
        "    plt.plot(np.mean(rewards, axis=0))\n",
        "plt.legend([\"Best Possible\", \"1/N(A)\", \"0.1\"])\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": null,
      "lastKernelId": null
    },
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "fundamentals-of-reinforcement-learning",
      "graded_item_id": "QpLYg",
      "launcher_item_id": "9ldtk"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
