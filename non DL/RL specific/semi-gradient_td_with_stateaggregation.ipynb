{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"lWy-MVmo4yys","nbgrader":{"cell_type":"markdown","checksum":"cf073b8339c6caf337498b984d0678f8","grade":false,"grade_id":"cell-4492106616b7ddd6","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# TD with State Aggregation"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3839,"status":"ok","timestamp":1673197078546,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"e7mgWqru5uO3"},"outputs":[],"source":["!pip install -q jdc"]},{"cell_type":"code","execution_count":7,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":19,"status":"ok","timestamp":1673197078547,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"_N2by_i64yy3","nbgrader":{"cell_type":"code","checksum":"33ccd5de59ab1974ad1b867fd105c664","grade":false,"grade_id":"cell-a4c4b2e807873209","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import jdc\n","from tqdm import tqdm\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":8,"metadata":{"cellView":"form","executionInfo":{"elapsed":18,"status":"ok","timestamp":1673197078547,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"1J8mGjVo5Uvo"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kJqd6sqo5jAH"},"outputs":[],"source":["#@title random walk policy evaluation - create true_V.npy file\n","\n","\n","def compute_value_function(num_states, action_range):\n","    \"\"\"\n","    Computes the value function for the 1000 state random walk as described in Sutton and Barto (2017).\n","    :return: The value function for states 1 to 1000. Index 0 is not used in this array (i.e. should remain 0).\n","    \"\"\"\n","    state_prob = 0.5 / float(action_range)\n","    gamma = 1\n","    theta = 0.000001\n","\n","    V = np.zeros(num_states+1)\n","\n","    delta = np.infty\n","    i = 0\n","    while delta > theta:\n","        i += 1\n","        delta = 0.0\n","        for s in range(1, num_states+1):\n","            v = V[s]\n","            value_sum = 0.0\n","            for transition in range(1, action_range+1):\n","                right = s + transition\n","                right_reward = 0\n","                if right > num_states:\n","                    right_reward = 1\n","                    right = 0\n","\n","                left = s - transition\n","                left_reward = 0\n","                if left < 1:\n","                    left_reward = -1\n","                    left = 0\n","\n","                value_sum += state_prob * ((right_reward + gamma * V[right]) + (left_reward + gamma * V[left]))\n","\n","            V[s] = value_sum\n","            delta = max(delta, np.abs(v-V[s]))\n","    \n","    np.save(\"./true_V\", V[1:])\n","    return V[1:]\n","\n","### if __name__ == '__main__':\n","num_states = 500\n","action_range = 100\n","V = compute_value_function(num_states, action_range)\n","\n","np.save(\"./true_V\", V)\n","plt.plot(range(1, num_states), V[1:])\n","plt.xlabel('State')\n","plt.ylabel('Value')\n","plt.show()\n","\n","### data1 = np.load('./test.npy') \n","### data2 = np.load('./true_V.npy')\n","### if (data1 == data2).all:\n","###   print('all right')\n","### np.save('./true_V.npy', np.array('here goes the array'))"]},{"cell_type":"code","execution_count":10,"metadata":{"cellView":"form","executionInfo":{"elapsed":31,"status":"ok","timestamp":1673197084247,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"DHCk-Ah8LVbR"},"outputs":[],"source":["#@title we have written known actual state distribution - in state_distribution.npy file\n","\n","# data = np.load('./state_distribution.npy') \n","# data\n","\n","np.save('./state_distribution.npy', np.array([0.00050342, 0.00051537, 0.00051983, 0.00052783, 0.00054043,\n","       0.00054095, 0.00055063, 0.0005632 , 0.0005723 , 0.00057897,\n","       0.00058859, 0.00059669, 0.00060697, 0.00060958, 0.00062441,\n","       0.00062788, 0.00064452, 0.00065015, 0.00065469, 0.00066292,\n","       0.00067739, 0.00068159, 0.00069353, 0.00069643, 0.00071412,\n","       0.00071868, 0.00072884, 0.00075045, 0.00074827, 0.0007581 ,\n","       0.00077134, 0.00077444, 0.00079118, 0.00080066, 0.00080477,\n","       0.00081647, 0.00082221, 0.00083824, 0.00085436, 0.00085694,\n","       0.00085677, 0.00088564, 0.00088572, 0.00089417, 0.00090236,\n","       0.00092084, 0.00093196, 0.00093285, 0.0009458 , 0.00096468,\n","       0.00097549, 0.00098674, 0.00099336, 0.00100658, 0.0010176 ,\n","       0.00103931, 0.00103993, 0.00105593, 0.00106808, 0.00108118,\n","       0.00109397, 0.00110813, 0.00111901, 0.0011267 , 0.0011282 ,\n","       0.00114946, 0.00116087, 0.00118098, 0.00118296, 0.00120058,\n","       0.00121217, 0.00122346, 0.00123674, 0.00125523, 0.00125821,\n","       0.00128223, 0.00128554, 0.00130661, 0.00132195, 0.00132658,\n","       0.001337  , 0.00134872, 0.00136579, 0.00136663, 0.00139404,\n","       0.00139848, 0.001419  , 0.00142815, 0.00144499, 0.00146277,\n","       0.00147143, 0.00148438, 0.00149689, 0.00150875, 0.00151946,\n","       0.00152384, 0.00154264, 0.00156307, 0.00157091, 0.00158682,\n","       0.00161182, 0.001613  , 0.0016272 , 0.00163835, 0.00164645,\n","       0.00165212, 0.00165297, 0.00168245, 0.00168208, 0.00170092,\n","       0.0017003 , 0.00172867, 0.0017209 , 0.0017406 , 0.00175318,\n","       0.00177206, 0.00177385, 0.00179813, 0.00179566, 0.00181001,\n","       0.00180087, 0.00182957, 0.00183724, 0.00185188, 0.00185671,\n","       0.00187875, 0.00187678, 0.00189374, 0.00189855, 0.00192199,\n","       0.00193324, 0.0019392 , 0.00193704, 0.00194829, 0.00196719,\n","       0.00198096, 0.00197397, 0.00200054, 0.00200993, 0.00201649,\n","       0.00201641, 0.00202741, 0.00204063, 0.00205622, 0.00206629,\n","       0.00208476, 0.00208844, 0.00209567, 0.00210024, 0.00231221,\n","       0.00232935, 0.00233713, 0.00234784, 0.00236371, 0.00235567,\n","       0.00238131, 0.00238263, 0.00238604, 0.00239073, 0.00240666,\n","       0.00240008, 0.00241935, 0.00243903, 0.00243995, 0.0024497 ,\n","       0.00246551, 0.002471  , 0.00247267, 0.00248375, 0.00249487,\n","       0.00250789, 0.00250768, 0.00251874, 0.00251356, 0.00254146,\n","       0.00255166, 0.00254615, 0.00255837, 0.00256789, 0.0025661 ,\n","       0.00258586, 0.0025787 , 0.0025969 , 0.00259521, 0.00259499,\n","       0.00261569, 0.0026059 , 0.00263882, 0.00262394, 0.00263241,\n","       0.0026314 , 0.00264764, 0.00263598, 0.00266424, 0.00267746,\n","       0.00266757, 0.00268087, 0.00269125, 0.00268402, 0.00269576,\n","       0.00270081, 0.00270205, 0.00270602, 0.00272495, 0.00273038,\n","       0.00272314, 0.00273873, 0.00273984, 0.00273815, 0.00275035,\n","       0.0027457 , 0.00276034, 0.00276651, 0.00276281, 0.00278392,\n","       0.00277704, 0.00278361, 0.00279305, 0.00280185, 0.00278799,\n","       0.00279638, 0.00278878, 0.0028044 , 0.00280381, 0.00281524,\n","       0.00279908, 0.00281635, 0.00282225, 0.00281668, 0.00283035,\n","       0.00282996, 0.00282696, 0.00283561, 0.00283615, 0.00283524,\n","       0.00283788, 0.00285564, 0.00285225, 0.0028593 , 0.00284709,\n","       0.0028474 , 0.00285042, 0.00286586, 0.00285936, 0.00286968,\n","       0.00286822, 0.00285015, 0.00285607, 0.00285977, 0.04377746,\n","       0.00286391, 0.00286415, 0.00286389, 0.00285858, 0.00285776,\n","       0.00284733, 0.00284927, 0.00285587, 0.00284777, 0.00285377,\n","       0.00284738, 0.00286175, 0.00284187, 0.00284762, 0.00284123,\n","       0.00283183, 0.00283757, 0.00284797, 0.00282714, 0.00281623,\n","       0.00282297, 0.00280843, 0.00280987, 0.0028102 , 0.00280245,\n","       0.00280249, 0.00281425, 0.0027992 , 0.00279464, 0.0027992 ,\n","       0.00279439, 0.0027763 , 0.00277307, 0.00277019, 0.00278148,\n","       0.00277169, 0.00275489, 0.00276468, 0.00275734, 0.00274451,\n","       0.00274257, 0.00274218, 0.00275072, 0.00273704, 0.00271091,\n","       0.00270943, 0.00271375, 0.00271696, 0.00271064, 0.00270924,\n","       0.00269711, 0.00269526, 0.00267912, 0.00268229, 0.00267865,\n","       0.00266763, 0.00264579, 0.00264951, 0.00264949, 0.00263432,\n","       0.00261388, 0.00262694, 0.00263121, 0.00261701, 0.00261951,\n","       0.00260331, 0.00259392, 0.00259246, 0.00258876, 0.00258065,\n","       0.00256513, 0.00256525, 0.00255693, 0.00253721, 0.00253501,\n","       0.00253614, 0.00252541, 0.00251543, 0.00250009, 0.00248048,\n","       0.00248539, 0.00249251, 0.002474  , 0.002468  , 0.00243952,\n","       0.00243311, 0.00244324, 0.00243121, 0.00242437, 0.00242182,\n","       0.00241158, 0.00237987, 0.00238674, 0.00237942, 0.00236205,\n","       0.00235565, 0.00234366, 0.00234175, 0.00232925, 0.00232238,\n","       0.00209493, 0.00210441, 0.00208087, 0.00207885, 0.00207061,\n","       0.00206829, 0.00204614, 0.00204075, 0.00202141, 0.00202227,\n","       0.00201676, 0.00199749, 0.00198952, 0.00198503, 0.0019751 ,\n","       0.00196309, 0.00193906, 0.00193038, 0.00192386, 0.00192063,\n","       0.00191264, 0.00188757, 0.00187937, 0.00187628, 0.00185891,\n","       0.00184988, 0.00184497, 0.00183835, 0.00182716, 0.00181147,\n","       0.00181871, 0.00177915, 0.00178643, 0.00177317, 0.00176385,\n","       0.00175148, 0.00174691, 0.00173135, 0.00171385, 0.00170172,\n","       0.00170096, 0.00167898, 0.00167445, 0.00165476, 0.00165786,\n","       0.0016416 , 0.00163549, 0.00162126, 0.00160757, 0.00159714,\n","       0.00157578, 0.00156311, 0.00156005, 0.00154052, 0.00152658,\n","       0.00152329, 0.00150449, 0.00149567, 0.00147009, 0.00146162,\n","       0.00146175, 0.00142883, 0.00142605, 0.00140757, 0.00139762,\n","       0.00139032, 0.00137479, 0.00136601, 0.00134701, 0.00133754,\n","       0.00132832, 0.00131237, 0.00129362, 0.00127879, 0.0012761 ,\n","       0.00126337, 0.00125122, 0.00124542, 0.00122344, 0.00120191,\n","       0.00119003, 0.00118888, 0.00116669, 0.0011473 , 0.00114438,\n","       0.00113435, 0.00112177, 0.00110673, 0.00109323, 0.00109464,\n","       0.00107809, 0.00106341, 0.00104904, 0.00102916, 0.00102669,\n","       0.00101378, 0.00100317, 0.00098875, 0.00098067, 0.00096445,\n","       0.00095721, 0.00094804, 0.00094091, 0.00091977, 0.00092059,\n","       0.00090069, 0.00089598, 0.00088747, 0.00087217, 0.00086631,\n","       0.00085375, 0.00084779, 0.00083345, 0.00082541, 0.00081696,\n","       0.00079618, 0.00078721, 0.00077798, 0.00077422, 0.00076572,\n","       0.00075419, 0.00074821, 0.0007377 , 0.00072146, 0.0007267 ,\n","       0.00070789, 0.00069964, 0.00069625, 0.00067577, 0.00067271,\n","       0.00066238, 0.00065126, 0.00064614, 0.00063833, 0.00062539,\n","       0.00061828, 0.00060413, 0.00060095, 0.00059326, 0.00058701,\n","       0.00057868, 0.000573  , 0.00056377, 0.0005522 , 0.00054181,\n","       0.00053344, 0.00052228, 0.00051599, 0.00051122, 0.00050052]))"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673197084248,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"x6ojYjF86GT3"},"outputs":[],"source":["#@title helpers for visualization\n","\n","# Function to plot result\n","def plot_result(agent_parameters, directory):\n","    \n","    true_V = np.load('./true_V.npy')\n","\n","    for num_g in agent_parameters[\"num_groups\"]:\n","        plt1_agent_sweeps = []\n","        plt2_agent_sweeps = []\n","        \n","        # two plots: learned state-value and learning curve (RMSVE)\n","        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n","            \n","        for step_size in agent_parameters[\"step_size\"]:\n","            \n","            # plot1\n","            filename = 'V_TD_agent_agg_states_{}_step_size_{}'.format(num_g, step_size).replace('.','')\n","            current_agent_V = np.load('{}/{}.npy'.format(directory, filename))\n","\n","            plt1_x_legend = range(1,len(current_agent_V[:]) + 1)\n","            graph_current_agent_V, = ax[0].plot(plt1_x_legend, current_agent_V[:], label=\"approximate values: state aggregation: {}, step-size: {}\".format(num_g, step_size))\n","            plt1_agent_sweeps.append(graph_current_agent_V)\n","            \n","            # plot2\n","            filename = 'RMSVE_TD_agent_agg_states_{}_step_size_{}'.format(num_g, step_size).replace('.','')\n","            current_agent_RMSVE = np.load('{}/{}.npy'.format(directory, filename))\n","\n","            plt2_x_legend = range(1,len(current_agent_RMSVE[:]) + 1)\n","            graph_current_agent_RMSVE, = ax[1].plot(plt2_x_legend, current_agent_RMSVE[:], label=\"approximate values: state aggregation: {}, step-size: {}\".format(num_g, step_size))\n","            plt2_agent_sweeps.append(graph_current_agent_RMSVE)\n","            \n","          \n","        # plot1: \n","        # add True V\n","        plt1_x_legend = range(1,len(true_V[:]) + 1)\n","        graph_true_V, = ax[0].plot(plt1_x_legend, true_V[:], label=\"$v_\\pi$\")\n","        \n","        ax[0].legend(handles=[*plt1_agent_sweeps, graph_true_V])\n","        \n","        ax[0].set_title(\"Learned State Value after 2000 episodes\")\n","        ax[0].set_xlabel('State')\n","        ax[0].set_ylabel('Value\\n scale', rotation=0, labelpad=15)\n","\n","        plt1_xticks = [1, 100, 200, 300, 400, 500]#, 600, 700, 800, 900, 1000]\n","        plt1_yticks = [-1.0, 0.0, 1.0]\n","        ax[0].set_xticks(plt1_xticks)\n","        ax[0].set_xticklabels(plt1_xticks)\n","        ax[0].set_yticks(plt1_yticks)\n","        ax[0].set_yticklabels(plt1_yticks)\n","        \n","        \n","        # plot2:\n","        ax[1].legend(handles=plt2_agent_sweeps)\n","        \n","        ax[1].set_title(\"Learning Curve\")\n","        ax[1].set_xlabel('Episodes')\n","        ax[1].set_ylabel('RMSVE\\n averaged over 50 runs', rotation=0, labelpad=40)\n","\n","        plt2_xticks = range(0, 210, 20) # [0, 10, 20, 30, 40, 50, 60, 70, 80]\n","        plt2_xticklabels = range(0, 2100, 200) # [0, 100, 200, 300, 400, 500, 600, 700, 800]\n","        plt2_yticks = [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n","        ax[1].set_xticks(plt2_xticks)\n","        ax[1].set_xticklabels(plt2_xticklabels)\n","        ax[1].set_yticks(plt2_yticks)\n","        ax[1].set_yticklabels(plt2_yticks)\n","        \n","        plt.tight_layout()\n","        plt.suptitle(\"{}-State Aggregation\".format(num_g),fontsize=16, fontweight='bold', y=1.03)\n","        plt.show()      \n"]},{"cell_type":"code","execution_count":12,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673197084250,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"YO5heri34yy7","nbgrader":{"cell_type":"code","checksum":"a39f70fc723d242c51a64de70ddba135","grade":false,"grade_id":"cell-931189bd60e2d453","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["class RandomWalkEnvironment(BaseEnvironment):\n","    def env_init(self, env_info={}):\n","        \"\"\"\n","        Setup for the environment called when the experiment first starts.\n","        \n","        Set parameters needed to setup the 500-state random walk environment.\n","        \n","        Assume env_info dict contains:\n","        {\n","            num_states: 500 [int],\n","            start_state: 250 [int],\n","            left_terminal_state: 0 [int],\n","            right_terminal_state: 501 [int],\n","            seed: int\n","        }\n","        \"\"\"\n","        \n","        # set random seed for each run\n","        self.rand_generator = np.random.RandomState(env_info.get(\"seed\")) \n","        \n","        # set each class attribute\n","        self.num_states = env_info[\"num_states\"] \n","        self.start_state = env_info[\"start_state\"] \n","        self.left_terminal_state = env_info[\"left_terminal_state\"] \n","        self.right_terminal_state = env_info[\"right_terminal_state\"]\n","\n","    def env_start(self):\n","        \"\"\"\n","        The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state from the environment.\n","        \"\"\"\n","\n","        # set self.reward_state_term tuple\n","        reward = 0.0\n","        state = self.start_state\n","        is_terminal = False\n","                \n","        self.reward_state_term = (reward, state, is_terminal)\n","        \n","        # return first state from the environment\n","        return self.reward_state_term[1]\n","        \n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","        \n","        last_state = self.reward_state_term[1]\n","        \n","        # set reward, current_state, and is_terminal\n","        #\n","        # action: specifies direction of movement - 0 (indicating left) or 1 (indicating right)  [int]\n","        # current state: next state after taking action from the last state [int]\n","        # reward: -1 if terminated left, 1 if terminated right, 0 otherwise [float]\n","        # is_terminal: indicates whether the episode terminated [boolean]\n","        #\n","        # Given action (direction of movement), determine how much to move in that direction from last_state\n","        # All transitions beyond the terminal state are absorbed into the terminal state.\n","        \n","        if action == 0: # left\n","            current_state = max(self.left_terminal_state, last_state + self.rand_generator.choice(range(-100,0)))\n","        elif action == 1: # right\n","            current_state = min(self.right_terminal_state, last_state + self.rand_generator.choice(range(1,101)))\n","        else: \n","            raise ValueError(\"Wrong action value\")\n","        \n","        # terminate left\n","        if current_state == self.left_terminal_state: \n","            reward = -1.0\n","            is_terminal = True\n","        \n","        # terminate right\n","        elif current_state == self.right_terminal_state:\n","            reward = 1.0\n","            is_terminal = True\n","        \n","        else:\n","            reward = 0.0\n","            is_terminal = False\n","        \n","        self.reward_state_term = (reward, current_state, is_terminal)\n","        \n","        return self.reward_state_term\n","        "]},{"cell_type":"code","execution_count":13,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673197084251,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"jW9cSMMP4yy_","nbgrader":{"cell_type":"code","checksum":"03bb830eea52c7388af728e97748adfe","grade":false,"grade_id":"cell-70fd3462c0981100","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["def agent_policy(rand_generator, state):\n","    \"\"\"\n","    Given random number generator and state, returns an action according to the agent's policy.\n","    \n","    Args:\n","        rand_generator: Random number generator\n","\n","    Returns:\n","        chosen action [int]\n","    \"\"\"\n","    \n","    # set chosen_action as 0 or 1 with equal probability\n","    # state is unnecessary for this agent policy\n","    chosen_action = rand_generator.choice([0,1])\n","    \n","    return chosen_action"]},{"cell_type":"code","execution_count":14,"metadata":{"deletable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673197084252,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"xSIeuSk94yzA","nbgrader":{"cell_type":"code","checksum":"13d3aaef058d482e6f4e630d0975504a","grade":false,"grade_id":"cell-0c3f9e12091de241","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def get_state_feature(num_states_in_group, num_groups, state):\n","    \"\"\"\n","    Given state, return the feature of that state\n","    \n","    Args:\n","        num_states_in_group [int]\n","        num_groups [int] \n","        state [int] : 1~500\n","\n","    Returns:\n","        one_hot_vector [numpy array]\n","    \"\"\"\n","    \n","    ### Generate state feature (2~4 lines)\n","    # Create one_hot_vector with size of the num_groups, according to state\n","    # For simplicity, assume num_states is always perfectly divisible by num_groups\n","    # Note that states start from index 1, not 0!\n","    \n","    # Example:\n","    # If num_states = 100, num_states_in_group = 20, num_groups = 5,\n","    # one_hot_vector would be of size 5.\n","    # For states 1~20, one_hot_vector would be: [1, 0, 0, 0, 0]\n","    one_hot_vector = np.zeros(num_groups)\n","    one_hot_vector[(state - 1) // num_states_in_group] = 1    \n","    \n","    return one_hot_vector\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673197084252,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"XNE_XX2o4yzB","nbgrader":{"cell_type":"code","checksum":"222c94a1ddaa58d6318d97e182519542","grade":true,"grade_id":"cell-f03905f4b2fe4cd2","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"outputId":"538ae799-7db7-4eb6-bce8-ad47327bbe78"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","# Given that num_states = 10 and num_groups = 5, test get_state_feature()\n","# There are states 1~10, and the state feature vector would be of size 5.\n","# Only one element would be active for any state feature vector.\n","\n","# get_state_feature() should support various values of num_states, num_groups, not just this example\n","# For simplicity, assume num_states will always be perfectly divisible by num_groups\n","num_states = 10\n","num_groups = 5\n","num_states_in_group = int(num_states / num_groups)\n","\n","# Test 1st group, state = 1\n","state = 1\n","features = get_state_feature(num_states_in_group, num_groups, state)\n","print(\"1st group: {}\".format(features))\n","\n","assert np.all(features == [1, 0, 0, 0, 0])\n","\n","# Test 2nd group, state = 3\n","state = 3\n","features = get_state_feature(num_states_in_group, num_groups, state)\n","print(\"2nd group: {}\".format(features))\n","\n","assert np.all(features == [0, 1, 0, 0, 0])\n","\n","# Test 3rd group, state = 6\n","state = 6\n","features = get_state_feature(num_states_in_group, num_groups, state)\n","print(\"3rd group: {}\".format(features))\n","\n","assert np.all(features == [0, 0, 1, 0, 0])\n","\n","# Test 4th group, state = 7\n","state = 7\n","features = get_state_feature(num_states_in_group, num_groups, state)\n","print(\"4th group: {}\".format(features))\n","\n","assert np.all(features == [0, 0, 0, 1, 0])\n","\n","# Test 5th group, state = 10\n","state = 10\n","features = get_state_feature(num_states_in_group, num_groups, state)\n","print(\"5th group: {}\".format(features))\n","\n","assert np.all(features == [0, 0, 0, 0, 1])"]},{"cell_type":"code","execution_count":16,"metadata":{"deletable":false,"executionInfo":{"elapsed":949,"status":"ok","timestamp":1673197085179,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"v9a_H_UM4yzC","nbgrader":{"cell_type":"code","checksum":"e1f3a8172c159929cde4ace3e57b850a","grade":false,"grade_id":"cell-5f5dd660fd400570","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Create TDAgent\n","class TDAgent(BaseAgent):\n","    def __init__(self):\n","        self.num_states = None\n","        self.num_groups = None\n","        self.step_size = None\n","        self.discount_factor = None\n","        \n","    def agent_init(self, agent_info={}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","\n","        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.\n","\n","        Assume agent_info dict contains:\n","        {\n","            num_states: 500 [int],\n","            num_groups: int, \n","            step_size: float, \n","            discount_factor: float,\n","            seed: int\n","        }\n","        \"\"\"\n","\n","        # set random seed for each run\n","        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\")) \n","\n","        # set class attributes\n","        self.num_states = agent_info.get(\"num_states\")\n","        self.num_groups = agent_info.get(\"num_groups\")\n","        self.step_size = agent_info.get(\"step_size\")\n","        self.discount_factor = agent_info.get(\"discount_factor\")\n","\n","        # pre-compute all observable features\n","        num_states_in_group = int(self.num_states / self.num_groups)\n","        self.all_state_features = np.array([get_state_feature(num_states_in_group, self.num_groups, state) for state in range(1, self.num_states + 1)])\n","\n","        # initialize all weights to zero using numpy array with correct size\n","        self.weights = np.zeros(self.num_groups)\n","\n","        self.last_state = None\n","        self.last_action = None\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state [int]: the state from the\n","                environment's evn_start function.\n","        Returns:\n","            self.last_action [int] : The first action the agent takes.\n","        \"\"\"\n","\n","        ### select action given state (using agent_policy), and save current state and action\n","        # Use self.rand_generator for agent_policy\n","        self.last_state = state\n","        self.last_action = agent_policy(self.rand_generator, state)        \n","\n","        return self.last_action\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward [float]: the reward received for taking the last action taken\n","            state [int]: the state from the environment's step, where the agent ended up after the last step\n","        Returns:\n","            self.last_action [int] : The action the agent is taking.\n","        \"\"\"\n","        \n","        # get relevant feature\n","        current_state_feature = self.all_state_features[state-1] \n","        last_state_feature = self.all_state_features[self.last_state-1] \n","        \n","        ### update weights and select action\n","        # (Hint: np.dot method is useful!)\n","        #\n","        # Update weights:\n","        #     use self.weights, current_state_feature, and last_state_feature\n","        #\n","        # Select action:\n","        #     use self.rand_generator for agent_policy\n","        #\n","        # Current state and selected action should be saved to self.last_state and self.last_action at the end\n","        last_value = last_state_feature @ self.weights\n","        value = current_state_feature @ self.weights\n","        delta = reward + self.discount_factor * value - last_value\n","        self.weights += self.step_size * delta * last_state_feature\n","        self.last_state = state\n","        self.last_action = agent_policy(self.rand_generator, state)        \n","\n","        return self.last_action\n","\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the\n","                terminal state.\n","        \"\"\"\n","\n","        # get relevant feature\n","        last_state_feature = self.all_state_features[self.last_state-1]\n","        \n","        ### update weights\n","        last_value = last_state_feature @ self.weights\n","        delta = reward - last_value\n","        self.weights += self.step_size * delta * last_state_feature        \n","        \n","        return\n","        \n","    def agent_message(self, message):\n","        # We will implement this method later\n","        raise NotImplementedError\n"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"J4xDDxsR4yzD","nbgrader":{"cell_type":"markdown","checksum":"06b8089562061cb16a442f14afa90bff","grade":false,"grade_id":"cell-5cde0a383d7244f6","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["\n","Run the following code to verify `agent_init()`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":50,"status":"ok","timestamp":1673197085180,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"dHtBZkTX4yzD","nbgrader":{"cell_type":"code","checksum":"bb1d63d54f56543a7b3eb2866b15b7b6","grade":true,"grade_id":"cell-51482f87693745b0","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"outputId":"a713f0ba-d64b-43c0-d26b-c181f104d90c"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\n","    \"num_states\": 500,\n","    \"num_groups\": 10,\n","    \"step_size\": 0.1,\n","    \"discount_factor\": 1.0,\n","    \"seed\": 1,\n","}\n","\n","agent = TDAgent()\n","agent.agent_init(agent_info)\n","\n","assert np.all(agent.weights == 0)\n","assert agent.weights.shape == (10,)\n","\n","# check attributes\n","print(\"num_states: {}\".format(agent.num_states))\n","print(\"num_groups: {}\".format(agent.num_groups))\n","print(\"step_size: {}\".format(agent.step_size))\n","print(\"discount_factor: {}\".format(agent.discount_factor))\n","\n","print(\"weights shape: {}\".format(agent.weights.shape))\n","print(\"weights init. value: {}\".format(agent.weights))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":44,"status":"ok","timestamp":1673197085180,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"dxqSBmN84yzE","nbgrader":{"cell_type":"code","checksum":"8be0a3ee9c6a85cd19446cc7484fc483","grade":true,"grade_id":"cell-b33fdd3efc9215da","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"outputId":"767a0103-4117-438d-ddd8-2e798419ea56"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\n","    \"num_states\": 500,\n","    \"num_groups\": 10,\n","    \"step_size\": 0.1,\n","    \"discount_factor\": 1.0,\n","    \"seed\": 1,\n","}\n","\n","# Suppose state = 250\n","state = 250\n","\n","agent = TDAgent()\n","agent.agent_init(agent_info)\n","action = agent.agent_start(state)\n","\n","assert action == 1\n","assert agent.last_state == 250\n","assert agent.last_action == 1\n","\n","print(\"Agent state: {}\".format(agent.last_state))\n","print(\"Agent selected action: {}\".format(agent.last_action))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":34,"status":"ok","timestamp":1673197085181,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"0M927le34yzF","nbgrader":{"cell_type":"code","checksum":"3c3420b0c2fb9120a9bee034ad9f6987","grade":true,"grade_id":"cell-f06dfcb8e4b952c9","locked":true,"points":20,"schema_version":3,"solution":false,"task":false},"outputId":"13873dd8-f28d-4be8-dcb9-d55cce411e12"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\n","    \"num_states\": 500,\n","    \"num_groups\": 10,\n","    \"step_size\": 0.1,\n","    \"discount_factor\": 0.9,\n","    \"seed\": 1,\n","}\n","\n","agent = TDAgent()\n","agent.agent_init(agent_info)\n","\n","# Initializing the weights to arbitrary values to verify the correctness of weight update\n","agent.weights = np.array([-1.5, 0.5, 1., -0.5, 1.5, -0.5, 1.5, 0.0, -0.5, -1.0])\n","\n","# Assume the agent started at State 50\n","start_state = 50\n","action = agent.agent_start(start_state)\n","\n","assert action == 1\n","\n","# Assume the reward was 10.0 and the next state observed was State 120\n","reward = 10.0\n","next_state = 120\n","action = agent.agent_step(reward, next_state)\n","\n","assert action == 1\n","\n","print(\"Updated weights: {}\".format(agent.weights))\n","assert np.allclose(agent.weights, [-0.26, 0.5, 1., -0.5, 1.5, -0.5, 1.5, 0., -0.5, -1.])\n","\n","assert agent.last_state == 120\n","assert agent.last_action == 1\n","\n","print(\"last state: {}\".format(agent.last_state))\n","print(\"last action: {}\".format(agent.last_action))\n","\n","# let's do another\n","reward = -22\n","next_state = 222\n","action = agent.agent_step(reward, next_state)\n","\n","assert action == 0\n","\n","assert np.allclose(agent.weights, [-0.26, 0.5, -1.165, -0.5, 1.5, -0.5, 1.5, 0, -0.5, -1])\n","assert agent.last_state == 222\n","assert agent.last_action == 0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":29,"status":"ok","timestamp":1673197085181,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"tDBjoVx94yzF","nbgrader":{"cell_type":"code","checksum":"4d298a9af62ff0f53efd137439b2b45a","grade":true,"grade_id":"cell-3449ab4cb6f74b31","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"outputId":"cd9fd161-a7d0-49ae-ec7f-96a2ff95d203"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\n","    \"num_states\": 500,\n","    \"num_groups\": 10,\n","    \"step_size\": 0.1,\n","    \"discount_factor\": 0.9,\n","    \"seed\": 1,\n","}\n","\n","agent = TDAgent()\n","agent.agent_init(agent_info)\n","\n","# Initializing the weights to arbitrary values to verify the correctness of weight update\n","agent.weights = np.array([-1.5, 0.5, 1., -0.5, 1.5, -0.5, 1.5, 0.0, -0.5, -1.0])\n","\n","# Assume the agent started at State 50\n","start_state = 50\n","action = agent.agent_start(start_state)\n","\n","assert action == 1\n","\n","# Assume the reward was 10.0 and reached the terminal state\n","agent.agent_end(10.0)\n","print(\"Updated weights: {}\".format(agent.weights))\n","\n","assert np.allclose(agent.weights, [-0.35, 0.5, 1., -0.5, 1.5, -0.5, 1.5, 0., -0.5, -1.])"]},{"cell_type":"code","execution_count":21,"metadata":{"deletable":false,"executionInfo":{"elapsed":25,"status":"ok","timestamp":1673197085182,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"xamMsCsZ4yzG","nbgrader":{"cell_type":"code","checksum":"eb78c449ca2c4313477fa2279290046b","grade":false,"grade_id":"cell-71fa8b08ea15ff38","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_message(self, message):\n","    if message == 'get state value':\n","        \n","        ### return state_value\n","        state_value = self.all_state_features @ self.weights\n","        \n","        return state_value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":24,"status":"ok","timestamp":1673197085182,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"10CSOxKs4yzH","nbgrader":{"cell_type":"code","checksum":"cbcd8f8f3120bbe709a8b53fb4d57d15","grade":true,"grade_id":"cell-f8d18149b05b7c49","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"outputId":"2fe94545-b700-4b42-e223-ac88b3b20ee7"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\n","    \"num_states\": 20,\n","    \"num_groups\": 5,\n","    \"step_size\": 0.1,\n","    \"discount_factor\": 1.0,\n","}\n","\n","agent = TDAgent()\n","agent.agent_init(agent_info)\n","test_state_val = agent.agent_message('get state value')\n","\n","assert test_state_val.shape == (20,)\n","assert np.all(test_state_val == 0)\n","\n","print(\"State value shape: {}\".format(test_state_val.shape))\n","print(\"Initial State value for all states: {}\".format(test_state_val))"]},{"cell_type":"code","execution_count":23,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":20,"status":"ok","timestamp":1673197085183,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"FIevguxs4yzI","nbgrader":{"cell_type":"code","checksum":"2fa55c90fc3679615a86df3dac8c8522","grade":false,"grade_id":"cell-f974bf03a35b3975","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Here we provide you with the true state value and state distribution\n","true_state_val = np.load('./true_V.npy')    \n","state_distribution = np.load('./state_distribution.npy')\n","\n","def calc_RMSVE(learned_state_val):\n","    assert(len(true_state_val) == len(learned_state_val) == len(state_distribution))\n","    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))\n","    RMSVE = np.sqrt(MSVE)\n","    return RMSVE"]},{"cell_type":"code","execution_count":24,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":19,"status":"ok","timestamp":1673197085183,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ZMuut9Hg4yzI","nbgrader":{"cell_type":"code","checksum":"b1ae63f2204e736a41d57fc2674260b7","grade":false,"grade_id":"cell-2aec1de29d4beb79","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import os\n","\n","# Define function to run experiment\n","def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n","\n","    rl_glue = RLGlue(environment, agent)\n","    \n","    # Sweep Agent parameters\n","    for num_agg_states in agent_parameters[\"num_groups\"]:\n","        for step_size in agent_parameters[\"step_size\"]:\n","            \n","            # save rmsve at the end of each evaluation episode\n","            # size: num_episode / episode_eval_frequency + 1 (includes evaluation at the beginning of training)\n","            agent_rmsve = np.zeros(int(experiment_parameters[\"num_episodes\"]/experiment_parameters[\"episode_eval_frequency\"]) + 1)\n","            \n","            # save learned state value at the end of each run\n","            agent_state_val = np.zeros(environment_parameters[\"num_states\"])\n","\n","            env_info = {\"num_states\": environment_parameters[\"num_states\"],\n","                        \"start_state\": environment_parameters[\"start_state\"],\n","                        \"left_terminal_state\": environment_parameters[\"left_terminal_state\"],\n","                        \"right_terminal_state\": environment_parameters[\"right_terminal_state\"]}\n","\n","            agent_info = {\"num_states\": environment_parameters[\"num_states\"],\n","                          \"num_groups\": num_agg_states,\n","                          \"step_size\": step_size,\n","                          \"discount_factor\": environment_parameters[\"discount_factor\"]}\n","\n","            print('Setting - num. agg. states: {}, step_size: {}'.format(num_agg_states, step_size))\n","            os.system('sleep 0.2')\n","            \n","            # one agent setting\n","            for run in tqdm(range(1, experiment_parameters[\"num_runs\"]+1)):\n","                env_info[\"seed\"] = run\n","                agent_info[\"seed\"] = run\n","                rl_glue.rl_init(agent_info, env_info)\n","                \n","                # Compute initial RMSVE before training\n","                current_V = rl_glue.rl_agent_message(\"get state value\")\n","                agent_rmsve[0] += calc_RMSVE(current_V)\n","                    \n","                for episode in range(1, experiment_parameters[\"num_episodes\"]+1):\n","                    # run episode\n","                    rl_glue.rl_episode(0) # no step limit\n","                    \n","                    if episode % experiment_parameters[\"episode_eval_frequency\"] == 0:\n","                        current_V = rl_glue.rl_agent_message(\"get state value\")\n","                        agent_rmsve[int(episode/experiment_parameters[\"episode_eval_frequency\"])] += calc_RMSVE(current_V)\n","                        \n","                # store only one run of state value\n","                if run == 50:\n","                    agent_state_val = rl_glue.rl_agent_message(\"get state value\")\n","            \n","            # rmsve averaged over runs\n","            agent_rmsve /= experiment_parameters[\"num_runs\"]\n","            \n","            save_name = \"{}_agg_states_{}_step_size_{}\".format('TD_agent', num_agg_states, step_size).replace('.','')\n","            \n","            if not os.path.exists('results'):\n","                os.makedirs('results')\n","    \n","            # save avg. state value\n","            np.save(\"results/V_{}\".format(save_name), agent_state_val)\n","\n","            # save avg. rmsve\n","            np.save(\"results/RMSVE_{}\".format(save_name), agent_rmsve)"]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"qwan-Kpt4yzJ","nbgrader":{"cell_type":"code","checksum":"621dbc134b486d99d34f8e63fd3b2468","grade":false,"grade_id":"cell-0c0267acbc817952","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["#### Run Experiment\n","\n","# Experiment parameters\n","experiment_parameters = {\n","    \"num_runs\" : 50,\n","    \"num_episodes\" : 2000,\n","    \"episode_eval_frequency\" : 10 # evaluate every 10 episodes\n","}\n","\n","# Environment parameters\n","environment_parameters = {\n","    \"num_states\" : 500, \n","    \"start_state\" : 250,\n","    \"left_terminal_state\" : 0,\n","    \"right_terminal_state\" : 501, \n","    \"discount_factor\" : 1.0\n","}\n","\n","# Agent parameters\n","# Each element is an array because we will be later sweeping over multiple values\n","agent_parameters = {\n","    \"num_groups\": [10],\n","    \"step_size\": [0.01, 0.05, 0.1]\n","}\n","\n","current_env = RandomWalkEnvironment\n","current_agent = TDAgent\n","\n","run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n","plot_result(agent_parameters, './results')"]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
