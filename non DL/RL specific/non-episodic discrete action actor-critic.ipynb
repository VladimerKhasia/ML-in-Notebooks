{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"Ug57WbsRv3mG","nbgrader":{"cell_type":"markdown","checksum":"7784c6aa8890d8f94e327df0e2046c06","grade":false,"grade_id":"cell-772b7b8774cc890e","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Average Reward Softmax Actor-Critic"]},{"cell_type":"code","execution_count":1,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673257985395,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"vjSLOqN1v3mQ","nbgrader":{"cell_type":"code","checksum":"8074478613373060bc01819879f799e1","grade":false,"grade_id":"cell-67bdb1b8185106d1","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import os\n","import itertools\n","from tqdm import tqdm\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","executionInfo":{"elapsed":29,"status":"ok","timestamp":1673257985396,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"u8y8dhQKxsk3"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL-Glue environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","        \n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward;\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","executionInfo":{"elapsed":28,"status":"ok","timestamp":1673257985397,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"nOkDlhRiyJUd"},"outputs":[],"source":["#@title Pendelum Environment\n","\n","class PendulumEnvironment(BaseEnvironment):\n","    \n","    def __init__(self):\n","        self.rand_generator = None\n","        self.ang_velocity_range = None\n","        self.dt = None\n","        self.viewer = None\n","        self.gravity = None\n","        self.mass = None\n","        self.length = None\n","        \n","        self.valid_actions = None\n","        self.actions = None\n","        \n","    \n","    def env_init(self, env_info={}):\n","        \"\"\"\n","        Setup for the environment called when the experiment first starts.\n","        \n","        Set parameters needed to setup the pendulum swing-up environment.\n","        \"\"\"\n","        # set random seed for each run\n","        self.rand_generator = np.random.RandomState(env_info.get(\"seed\"))     \n","        \n","        self.ang_velocity_range = [-2 * np.pi, 2 * np.pi]\n","        self.dt = 0.05\n","        self.viewer = None\n","        self.gravity = 9.8\n","        self.mass = float(1./3.)\n","        self.length = float(3./2.)\n","        \n","        self.valid_actions = (0,1,2)\n","        self.actions = [-1,0,1]\n","        \n","        self.last_action = None\n","    \n","    def env_start(self):\n","        \"\"\"\n","        The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","        ### set self.reward_obs_term tuple accordingly (3~5 lines)\n","        # Angle starts at -pi or pi, and Angular velocity at 0.\n","        \n","        beta = -np.pi\n","        betadot = 0.\n","        \n","        reward = 0.0\n","        observation = np.array([beta, betadot])\n","        is_terminal = False\n","        \n","        self.reward_obs_term = (reward, observation, is_terminal)\n","        \n","        # return first state observation from the environment\n","        return self.reward_obs_term[1]\n","        \n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","        \n","        ### set reward, observation, and is_terminal correctly (10~12 lines)\n","        # Update the state according to the transition dynamics\n","        # Remember to normalize the angle so that it is always between -pi and pi.\n","        # If the angular velocity exceeds the bound, reset the state to the resting position\n","        # Compute reward according to the new state, and is_terminal should always be False\n","\n","        # Check if action is valid\n","        assert(action in self.valid_actions)\n","        \n","        last_state = self.reward_obs_term[1]\n","        last_beta, last_betadot = last_state        \n","        self.last_action = action\n","        \n","        betadot = last_betadot + 0.75 * (self.actions[action] + self.mass * self.length * self.gravity * np.sin(last_beta)) / (self.mass * self.length**2) * self.dt\n","\n","        beta = last_beta + betadot * self.dt\n","\n","        # normalize angle\n","        beta = ((beta + np.pi) % (2*np.pi)) - np.pi\n","        \n","        # reset if out of bound\n","        if betadot < self.ang_velocity_range[0] or betadot > self.ang_velocity_range[1]:\n","            beta = -np.pi\n","            betadot = 0.\n","        \n","        # compute reward\n","        reward = -(np.abs(((beta+np.pi) % (2 * np.pi)) - np.pi))\n","        observation = np.array([beta, betadot])\n","        is_terminal = False\n","\n","        \n","        self.reward_obs_term = (reward, observation, is_terminal)\n","        \n","        return self.reward_obs_term"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","executionInfo":{"elapsed":27,"status":"ok","timestamp":1673257985398,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Das4brUJyXbF"},"outputs":[],"source":["#@title We use tile coding which helps us increase generalization and discrimination at the same time - better than raw coarse coding\n","\n","\"\"\"\n","Tile Coding Software version 3.0beta\n","by Rich Sutton\n","based on a program created by Steph Schaeffer and others\n","External documentation and recommendations on the use of this code is available in the \n","reinforcement learning textbook by Sutton and Barto, and on the web.\n","These need to be understood before this code is.\n","\n","This software is for Python 3 or more.\n","\n","This is an implementation of grid-style tile codings, based originally on\n","the UNH CMAC code (see http://www.ece.unh.edu/robots/cmac.htm), but by now highly changed. \n","Here we provide a function, \"tiles\", that maps floating and integer\n","variables to a list of tiles, and a second function \"tiles-wrap\" that does the same while\n","wrapping some floats to provided widths (the lower wrap value is always 0).\n","\n","The float variables will be gridded at unit intervals, so generalization\n","will be by approximately 1 in each direction, and any scaling will have \n","to be done externally before calling tiles.\n","\n","Num-tilings should be a power of 2, e.g., 16. To make the offsetting work properly, it should\n","also be greater than or equal to four times the number of floats.\n","\n","The first argument is either an index hash table of a given size (created by (make-iht size)), \n","an integer \"size\" (range of the indices from 0), or nil (for testing, indicating that the tile \n","coordinates are to be returned without being converted to indices).\n","\"\"\"\n","\n","basehash = hash\n","\n","class IHT:\n","    \"Structure to handle collisions\"\n","    def __init__(self, sizeval):\n","        self.size = sizeval                        \n","        self.overfullCount = 0\n","        self.dictionary = {}\n","\n","    def __str__(self):\n","        \"Prepares a string for printing whenever this object is printed\"\n","        return \"Collision table:\" + \\\n","               \" size:\" + str(self.size) + \\\n","               \" overfullCount:\" + str(self.overfullCount) + \\\n","               \" dictionary:\" + str(len(self.dictionary)) + \" items\"\n","\n","    def count (self):\n","        return len(self.dictionary)\n","    \n","    def fullp (self):\n","        return len(self.dictionary) >= self.size\n","    \n","    def getindex (self, obj, readonly=False):\n","        d = self.dictionary\n","        if obj in d: return d[obj]\n","        elif readonly: return None\n","        size = self.size\n","        count = self.count()\n","        if count >= size:\n","            if self.overfullCount==0: print('IHT full, starting to allow collisions')\n","            self.overfullCount += 1\n","            return basehash(obj) % self.size\n","        else:\n","            d[obj] = count\n","            return count\n","\n","def hashcoords(coordinates, m, readonly=False):\n","    if type(m)==IHT: return m.getindex(tuple(coordinates), readonly)\n","    if type(m)==int: return basehash(tuple(coordinates)) % m\n","    if m==None: return coordinates\n","\n","from math import floor, log\n","from itertools import zip_longest\n","\n","def tiles (ihtORsize, numtilings, floats, ints=[], readonly=False):\n","    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n","    qfloats = [floor(f*numtilings) for f in floats]\n","    Tiles = []\n","    for tiling in range(numtilings):\n","        tilingX2 = tiling*2\n","        coords = [tiling]\n","        b = tiling\n","        for q in qfloats:\n","            coords.append( (q + b) // numtilings )\n","            b += tilingX2\n","        coords.extend(ints)\n","        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n","    return Tiles\n","\n","def tileswrap (ihtORsize, numtilings, floats, wrapwidths, ints=[], readonly=False):\n","    \"\"\"returns num-tilings tile indices corresponding to the floats and ints, wrapping some floats\"\"\"\n","    qfloats = [floor(f*numtilings) for f in floats]\n","    Tiles = []\n","    for tiling in range(numtilings):\n","        tilingX2 = tiling*2\n","        coords = [tiling]\n","        b = tiling\n","        for q, width in zip_longest(qfloats, wrapwidths):\n","            c = (q + b%numtilings) // numtilings\n","            coords.append(c%width if width else c)\n","            b += tilingX2\n","        coords.extend(ints)\n","        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n","    return Tiles\n"]},{"cell_type":"code","execution_count":5,"metadata":{"cellView":"form","executionInfo":{"elapsed":26,"status":"ok","timestamp":1673257985399,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Bo-0YY7X0HGz"},"outputs":[],"source":["#@title helpers for visualization\n","\n","\n","# Function to plot result\n","def plot_result(agent_parameters, directory):\n","    \n","    plt1_agent_sweeps = []\n","    plt2_agent_sweeps = []\n","    \n","    x_range = 20000\n","    plt_xticks = [0, 4999, 9999, 14999, 19999]\n","    plt_xlabels = [1, 5000, 10000, 15000, 20000]\n","    plt1_yticks = range(0, -6001, -2000)\n","    plt2_yticks = range(-3, 1, 1)\n","    \n","        \n","    # single plots: Exp Avg reward \n","    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12,14))\n","\n","    for num_tilings in agent_parameters[\"num_tilings\"]:\n","        for num_tiles in agent_parameters[\"num_tiles\"]:\n","            for actor_ss in agent_parameters[\"actor_step_size\"]:\n","                for critic_ss in agent_parameters[\"critic_step_size\"]:\n","                    for avg_reward_ss in agent_parameters[\"avg_reward_step_size\"]:\n","\n","                        load_name = 'ActorCriticSoftmax_tilings_{}_tiledim_{}_actor_ss_{}_critic_ss_{}_avg_reward_ss_{}'.format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)\n","                        \n","                        ### plot1\n","                        file_type1 = \"total_return\"\n","                        data = np.load('{}/{}_{}.npy'.format(directory, load_name, file_type1))\n","\n","                        data_mean = np.mean(data, axis=0)\n","                        data_std_err = np.std(data, axis=0)/np.sqrt(len(data))\n","\n","                        data_mean = data_mean[:x_range]\n","                        data_std_err = data_std_err[:x_range]\n","\n","                        plt_x_legend = range(0,len(data_mean))[:x_range]\n","\n","                        ax[0].fill_between(plt_x_legend, data_mean - data_std_err, data_mean + data_std_err, alpha = 0.2)\n","                        graph_current_data, = ax[0].plot(plt_x_legend, data_mean, linewidth=1.0, label=\"actor_ss: {}/32, critic_ss: {}/32, avg reward step_size: {}\".format(actor_ss, critic_ss, avg_reward_ss))\n","                        plt1_agent_sweeps.append(graph_current_data)\n","\n","    \n","                        ### plot2\n","                        file_type2 = \"exp_avg_reward\"\n","                        data = np.load('{}/{}_{}.npy'.format(directory, load_name, file_type2))\n","\n","                        data_mean = np.mean(data, axis=0)\n","                        data_std_err = np.std(data, axis=0)/np.sqrt(len(data))\n","\n","                        data_mean = data_mean[:x_range]\n","                        data_std_err = data_std_err[:x_range]\n","\n","                        plt_x_legend = range(1,len(data_mean) + 1)[:x_range]\n","\n","                        ax[1].fill_between(plt_x_legend, data_mean - data_std_err, data_mean + data_std_err, alpha = 0.2)\n","                        graph_current_data, = ax[1].plot(plt_x_legend, data_mean, linewidth=1.0, label=\"actor: {}/32, critic: {}/32, avg reward: {}\".format(actor_ss, critic_ss, avg_reward_ss))\n","                        plt2_agent_sweeps.append(graph_current_data)\n","\n","    # plot 1\n","    ax[0].legend(handles=[*plt1_agent_sweeps])\n","    ax[0].set_xticks(plt_xticks)\n","    ax[0].set_yticks(plt1_yticks)\n","    ax[0].set_xticklabels(plt_xlabels)\n","    ax[0].set_yticklabels(plt1_yticks)\n","                        \n","    ax[0].set_title(\"Return per Step\")\n","    ax[0].set_xlabel('Training steps')\n","    ax[0].set_ylabel('Total Return', rotation=90)\n","    ax[0].set_xlim([0,20000])\n","    \n","    # plot 2\n","    ax[1].legend(handles=[*plt2_agent_sweeps])\n","    ax[1].set_xticks(plt_xticks)\n","    ax[1].set_yticks(plt2_yticks)\n","\n","    ax[1].set_title(\"Exponential Average Reward per Step\")\n","    ax[1].set_xlabel('Training steps')\n","    ax[1].set_ylabel('Exponential Average Reward', rotation=90)\n","    ax[1].set_xticklabels(plt_xlabels)\n","    ax[1].set_yticklabels(plt2_yticks)\n","    ax[1].set_xlim([0,20000])\n","    ax[1].set_ylim([-3, 0.16])\n","\n","    plt.suptitle(\"Average Reward Softmax Actor-Critic ({} Runs)\".format(len(data)),fontsize=16, fontweight='bold', y=1.03)\n","                    \n","    # ax[1].legend(handles=plt2_agent_sweeps)\n","\n","    # ax[1].set_title(\"Softmax policy Actor-Critic: Average Reward per Step ({} Runs)\".format(len(avg_reward)))\n","    # ax[1].set_xlabel('Training steps')\n","    # ax[1].set_ylabel('Average Reward', rotation=0, labelpad=40)\n","    # ax[1].set_xticklabels(plt_xticks)\n","    # ax[1].set_yticklabels(plt_yticks)\n","    # ax.axhline(y=0.1, linestyle='dashed', linewidth=1.0, color='black')\n","\n","    plt.tight_layout()\n","    # plt.suptitle(\"{}-State Aggregation\".format(num_agg_states),fontsize=16, fontweight='bold', y=1.03)\n","    # plt.suptitle(\"Average Reward ActorCritic\",fontsize=16, fontweight='bold', y=1.03)\n","    plt.show()   \n","        \n","def plot_sweep_result(directory):\n","    \n","    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(12,7))\n","    \n","    plt_agent_sweeps = []\n","    \n","    x_range = 20000\n","    plt_xticks = [0, 4999, 9999, 14999, 19999]\n","    plt_xlabels = [1, 5000, 10000, 15000, 20000]\n","    plt2_yticks = range(-3, 1, 1)\n","\n","    top_results = [{\"actor_ss\": 0.25, \"critic_ss\": 2, \"avg_reward_ss\": 0.03125},\n","                  {\"actor_ss\": 0.25, \"critic_ss\": 2, \"avg_reward_ss\": 0.015625},\n","                  {\"actor_ss\": 0.5, \"critic_ss\": 2, \"avg_reward_ss\": 0.0625},\n","                  {\"actor_ss\": 1, \"critic_ss\": 2, \"avg_reward_ss\": 0.0625},\n","                  {\"actor_ss\": 0.25, \"critic_ss\": 1, \"avg_reward_ss\": 0.015625}]\n","    \n","    for setting in top_results:\n","        \n","        num_tilings = 32\n","        num_tiles = 8\n","        actor_ss = setting[\"actor_ss\"]\n","        critic_ss = setting[\"critic_ss\"]\n","        avg_reward_ss = setting[\"avg_reward_ss\"]\n","        \n","        load_name = 'ActorCriticSoftmax_tilings_{}_tiledim_{}_actor_ss_{}_critic_ss_{}_avg_reward_ss_{}'.format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)\n","\n","        file_type2 = \"exp_avg_reward\"\n","        data = np.load('{}/{}_{}.npy'.format(directory, load_name, file_type2))\n","\n","        data_mean = np.mean(data, axis=0)\n","        data_std_err = np.std(data, axis=0)/np.sqrt(len(data))\n","\n","        data_mean = data_mean[:x_range]\n","        data_std_err = data_std_err[:x_range]\n","\n","        plt_x_legend = range(1,len(data_mean) + 1)[:x_range]\n","\n","        ax.fill_between(plt_x_legend, data_mean - data_std_err, data_mean + data_std_err, alpha = 0.2)\n","        graph_current_data, = ax.plot(plt_x_legend, data_mean, linewidth=1.0, label=\"actor: {}/32, critic: {}/32, avg reward: {}\".format(actor_ss, critic_ss, avg_reward_ss))\n","        plt_agent_sweeps.append(graph_current_data)\n","        \n","    ax.legend(handles=[*plt_agent_sweeps])\n","    ax.set_xticks(plt_xticks)\n","    ax.set_yticks(plt2_yticks)\n","\n","    ax.set_title(\"Exponential Average Reward per Step ({} Runs)\".format(len(data)))\n","    ax.set_xlabel('Training steps')\n","    ax.set_ylabel('Exponential Average Reward', rotation=90)\n","    ax.set_xticklabels(plt_xlabels)\n","    ax.set_yticklabels(plt2_yticks)\n","    ax.set_xlim([0, 20000])\n","    ax.set_ylim([-3.5, 0.16])\n","    "]},{"cell_type":"code","execution_count":6,"metadata":{"deletable":false,"executionInfo":{"elapsed":26,"status":"ok","timestamp":1673257985400,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"J-25fNt4v3mT","nbgrader":{"cell_type":"code","checksum":"c7798475f36f714545d25cc19324b8d6","grade":false,"grade_id":"cell-0aea119ff5ad24aa","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class PendulumTileCoder:\n","    def __init__(self, iht_size=4096, num_tilings=32, num_tiles=8):\n","        \"\"\"\n","        Initializes the MountainCar Tile Coder\n","        Initializers:\n","        iht_size -- int, the size of the index hash table, typically a power of 2\n","        num_tilings -- int, the number of tilings\n","        num_tiles -- int, the number of tiles. Here both the width and height of the tiles are the same\n","                            \n","        Class Variables:\n","        self.iht -- IHT, the index hash table that the tile coder will use\n","        self.num_tilings -- int, the number of tilings the tile coder will use\n","        self.num_tiles -- int, the number of tiles the tile coder will use\n","        \"\"\"\n","        \n","        self.num_tilings = num_tilings\n","        self.num_tiles = num_tiles \n","        self.iht = IHT(iht_size)\n","    \n","    def get_tiles(self, angle, ang_vel):\n","        \"\"\"\n","        Takes in an angle and angular velocity from the pendulum environment\n","        and returns a numpy array of active tiles.\n","        \n","        Arguments:\n","        angle -- float, the angle of the pendulum between -np.pi and np.pi\n","        ang_vel -- float, the angular velocity of the agent between -2*np.pi and 2*np.pi\n","        \n","        returns:\n","        tiles -- np.array, active tiles\n","        \n","        \"\"\"\n","        \n","        ### Use the ranges above and scale the angle and angular velocity between [0, 1]\n","        # then multiply by the number of tiles so they are scaled between [0, self.num_tiles]\n","        min_angle = - np.pi\n","        max_angle = np.pi\n","        ang_vel_min = - 2 * np.pi\n","        ang_vel_max = 2 * np.pi\n","\n","        angle_norm = (angle - min_angle) / (max_angle - min_angle)\n","        ang_vel_norm = (ang_vel - ang_vel_min) / (ang_vel_max - ang_vel_min) \n","                \n","        angle_scaled = self.num_tiles * angle_norm\n","        ang_vel_scaled = self.num_tiles * ang_vel_norm               \n","        \n","        # Get tiles by calling tileswrap method\n","        # wrapwidths specify which dimension to wrap over and its wrapwidth\n","        tiles = tileswrap(self.iht, self.num_tilings, [angle_scaled, ang_vel_scaled], wrapwidths=[self.num_tiles, False])\n","                    \n","        return np.array(tiles)"]},{"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"3I2X5fPUv3mU","nbgrader":{"cell_type":"markdown","checksum":"7a4fa9bdb9dc9e4475a08b429a512fa1","grade":false,"grade_id":"cell-d7ec813a92c7a24a","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["Run the following code to verify `PendulumTilecoder`"]},{"cell_type":"code","execution_count":7,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":24,"status":"ok","timestamp":1673257985400,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"xGzNwXaGv3mV","nbgrader":{"cell_type":"code","checksum":"3dec4aa9b5d293b1d3f1809f58e68625","grade":true,"grade_id":"cell-25f642d5c07d3914","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","## Test Code for PendulumTileCoder ##\n","angles = np.linspace(-np.pi, np.pi, num=5)\n","vels = np.linspace(-2 * np.pi, 2 * np.pi, num=5)\n","test_obs = list(itertools.product(angles, vels))\n","\n","pdtc = PendulumTileCoder(iht_size=4096, num_tilings=8, num_tiles=2)\n","\n","result=[]\n","for obs in test_obs:\n","    angle, ang_vel = obs\n","    tiles = pdtc.get_tiles(angle=angle, ang_vel=ang_vel)\n","    result.append(tiles)\n","    \n","expected = np.array([\n","    [0, 1, 2, 3, 4, 5, 6, 7],\n","    [0, 1, 8, 3, 9, 10, 6, 11],\n","    [12, 13, 8, 14, 9, 10, 15, 11],\n","    [12, 13, 16, 14, 17, 18, 15, 19],\n","    [20, 21, 16, 22, 17, 18, 23, 19],\n","    [0, 1, 2, 3, 24, 25, 26, 27],\n","    [0, 1, 8, 3, 28, 29, 26, 30],\n","    [12, 13, 8, 14, 28, 29, 31, 30],\n","    [12, 13, 16, 14, 32, 33, 31, 34],\n","    [20, 21, 16, 22, 32, 33, 35, 34],\n","    [36, 37, 38, 39, 24, 25, 26, 27],\n","    [36, 37, 40, 39, 28, 29, 26, 30],\n","    [41, 42, 40, 43, 28, 29, 31, 30],\n","    [41, 42, 44, 43, 32, 33, 31, 34],\n","    [45, 46, 44, 47, 32, 33, 35, 34],\n","    [36, 37, 38, 39, 4, 5, 6, 7],\n","    [36, 37, 40, 39, 9, 10, 6, 11],\n","    [41, 42, 40, 43, 9, 10, 15, 11],\n","    [41, 42, 44, 43, 17, 18, 15, 19],\n","    [45, 46, 44, 47, 17, 18, 23, 19],\n","    [0, 1, 2, 3, 4, 5, 6, 7],\n","    [0, 1, 8, 3, 9, 10, 6, 11],\n","    [12, 13, 8, 14, 9, 10, 15, 11],\n","    [12, 13, 16, 14, 17, 18, 15, 19],\n","    [20, 21, 16, 22, 17, 18, 23, 19],\n","])\n","\n","assert np.all(expected == np.array(result))"]},{"cell_type":"code","execution_count":8,"metadata":{"deletable":false,"executionInfo":{"elapsed":23,"status":"ok","timestamp":1673257985401,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Ol-WAOYbv3mY","nbgrader":{"cell_type":"code","checksum":"9359abf8f5989b07767b67e08a051cf8","grade":false,"grade_id":"cell-8e687ecd856dcacd","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["def compute_softmax_prob(actor_w, tiles):\n","    \"\"\"\n","    Computes softmax probability for all actions\n","    \n","    Args:\n","    actor_w - np.array, an array of actor weights\n","    tiles - np.array, an array of active tiles\n","    \n","    Returns:\n","    softmax_prob - np.array, an array of size equal to num. actions, and sums to 1.\n","    \"\"\"\n","    \n","    # First compute the list of state-action preferences (1~2 lines)\n","    state_action_preferences = []\n","\n","    state_action_preferences = actor_w[:, tiles].sum(axis=1)\n","\n","    # Set the constant c by finding the maximum of state-action preferences (use np.max) (1 line)\n","    c = np.max(state_action_preferences)\n","\n","    # Compute the numerator by subtracting c from state-action preferences and exponentiating it (use np.exp) (1 line)\n","    numerator = np.exp(state_action_preferences - c)\n","    \n","    # Next compute the denominator by summing the values in the numerator (use np.sum) (1 line)\n","    denominator = np.sum(numerator)\n","\n","    # Create a probability array by dividing each element in numerator array by denominator (1 line)\n","    # We will store this probability array in self.softmax_prob as it will be useful later when updating the Actor\n","    softmax_prob = numerator / denominator\n","\n","    \n","    return softmax_prob"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":23,"status":"ok","timestamp":1673257985402,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Q9QMbZVEv3mZ","nbgrader":{"cell_type":"code","checksum":"c029cf63786e01653c60f7b2130fbba2","grade":true,"grade_id":"cell-b27241568a857869","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"outputId":"c54c4ac0-5757-412e-f6ff-6c9246ae0843"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","# set tile-coder\n","iht_size = 4096\n","num_tilings = 8\n","num_tiles = 8\n","test_tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)\n","\n","num_actions = 3\n","actions = list(range(num_actions))\n","actor_w = np.zeros((len(actions), iht_size))\n","\n","# setting actor weights such that state-action preferences are always [-1, 1, 2]\n","actor_w[0] = -1./num_tilings\n","actor_w[1] = 1./num_tilings\n","actor_w[2] = 2./num_tilings\n","\n","# obtain active_tiles from state\n","state = [-np.pi, 0.]\n","angle, ang_vel = state\n","active_tiles = test_tc.get_tiles(angle, ang_vel)\n","\n","# compute softmax probability\n","softmax_prob = compute_softmax_prob(actor_w, active_tiles) \n","print('softmax probability: {}'.format(softmax_prob))\n","\n","assert np.allclose(softmax_prob, [0.03511903, 0.25949646, 0.70538451])"]},{"cell_type":"code","execution_count":10,"metadata":{"deletable":false,"executionInfo":{"elapsed":16,"status":"ok","timestamp":1673257985403,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ncMBBNIqv3ma","nbgrader":{"cell_type":"code","checksum":"7d2aa6794604c259643d6bad07cc6b6f","grade":false,"grade_id":"cell-1a5afc4aed047144","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["class ActorCriticSoftmaxAgent(BaseAgent): \n","    def __init__(self):\n","        self.rand_generator = None\n","\n","        self.actor_step_size = None\n","        self.critic_step_size = None\n","        self.avg_reward_step_size = None\n","\n","        self.tc = None\n","\n","        self.avg_reward = None\n","        self.critic_w = None\n","        self.actor_w = None\n","\n","        self.actions = None\n","\n","        self.softmax_prob = None\n","        self.prev_tiles = None\n","        self.last_action = None\n","    \n","    def agent_init(self, agent_info={}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\n","\n","        Set parameters needed to setup the semi-gradient TD(0) state aggregation agent.\n","\n","        Assume agent_info dict contains:\n","        {\n","            \"iht_size\": int\n","            \"num_tilings\": int,\n","            \"num_tiles\": int,\n","            \"actor_step_size\": float,\n","            \"critic_step_size\": float,\n","            \"avg_reward_step_size\": float,\n","            \"num_actions\": int,\n","            \"seed\": int\n","        }\n","        \"\"\"\n","\n","        # set random seed for each run\n","        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\")) \n","\n","        iht_size = agent_info.get(\"iht_size\")\n","        num_tilings = agent_info.get(\"num_tilings\")\n","        num_tiles = agent_info.get(\"num_tiles\")\n","\n","        # initialize self.tc to the tile coder we created\n","        self.tc = PendulumTileCoder(iht_size=iht_size, num_tilings=num_tilings, num_tiles=num_tiles)\n","\n","        # set step-size accordingly (we normally divide actor and critic step-size by num. tilings (p.217-218 of textbook))\n","        self.actor_step_size = agent_info.get(\"actor_step_size\")/num_tilings\n","        self.critic_step_size = agent_info.get(\"critic_step_size\")/num_tilings\n","        self.avg_reward_step_size = agent_info.get(\"avg_reward_step_size\")\n","\n","        self.actions = list(range(agent_info.get(\"num_actions\")))\n","\n","        # Set initial values of average reward, actor weights, and critic weights\n","        # We initialize actor weights to three times the iht_size. \n","        # Recall this is because we need to have one set of weights for each of the three actions.\n","        self.avg_reward = 0.0\n","        self.actor_w = np.zeros((len(self.actions), iht_size))\n","        self.critic_w = np.zeros(iht_size)\n","\n","        self.softmax_prob = None\n","        self.prev_tiles = None\n","        self.last_action = None\n","    \n","    def agent_policy(self, active_tiles):\n","        \"\"\" policy of the agent\n","        Args:\n","            active_tiles (Numpy array): active tiles returned by tile coder\n","            \n","        Returns:\n","            The action selected according to the policy\n","        \"\"\"\n","        \n","        # compute softmax probability\n","        softmax_prob = compute_softmax_prob(self.actor_w, active_tiles)\n","        \n","        # Sample action from the softmax probability array\n","        # self.rand_generator.choice() selects an element from the array with the specified probability\n","        chosen_action = self.rand_generator.choice(self.actions, p=softmax_prob)\n","        \n","        # save softmax_prob as it will be useful later when updating the Actor\n","        self.softmax_prob = softmax_prob\n","        \n","        return chosen_action\n","\n","    def agent_start(self, state):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            state (Numpy array): the state from the environment's env_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","        angle, ang_vel = state\n","\n","        ### Use self.tc to get active_tiles using angle and ang_vel (2 lines)\n","        # set current_action by calling self.agent_policy with active_tiles\n","        active_tiles = self.tc.get_tiles(angle, ang_vel)\n","        current_action = self.agent_policy(active_tiles)        \n","\n","        self.last_action = current_action\n","        self.prev_tiles = np.copy(active_tiles)\n","\n","        return self.last_action\n","\n","\n","    def agent_step(self, reward, state):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            state (Numpy array): the state from the environment's step based on \n","                                where the agent ended up after the\n","                                last step.\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","        angle, ang_vel = state\n","\n","        ### Use self.tc to get active_tiles using angle and ang_vel (1 line)\n","        active_tiles = self.tc.get_tiles(angle, ang_vel)\n","\n","        ### Compute delta using Equation (1) (1 line)\n","        delta = reward - self.avg_reward + self.critic_w[active_tiles].sum() - self.critic_w[self.prev_tiles].sum()\n","\n","        ### update average reward using Equation (2) (1 line)\n","        self.avg_reward += self.avg_reward_step_size * delta\n","\n","        # update critic weights using Equation (3) and (5) (1 line)\n","        self.critic_w[self.prev_tiles] += self.critic_step_size * delta\n","\n","        # update actor weights using Equation (4) and (6)\n","        # We use self.softmax_prob saved from the previous timestep\n","        # We leave it as an exercise to verify that the code below corresponds to the equation.\n","        for a in self.actions:\n","            if a == self.last_action:\n","                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (1 - self.softmax_prob[a])\n","            else:\n","                self.actor_w[a][self.prev_tiles] += self.actor_step_size * delta * (0 - self.softmax_prob[a])\n","\n","        ### set current_action by calling self.agent_policy with active_tiles (1 line)\n","        current_action = self.agent_policy(active_tiles)\n","\n","        self.prev_tiles = active_tiles\n","        self.last_action = current_action\n","\n","        return self.last_action\n","\n","\n","    def agent_message(self, message):\n","        if message == 'get avg reward':\n","            return self.avg_reward\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":335,"status":"ok","timestamp":1673257985723,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"wIqKi6mJv3mb","nbgrader":{"cell_type":"code","checksum":"bc38580596ba40adffc54281e60f990b","grade":true,"grade_id":"cell-a7d7210fe9299556","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"outputId":"a295ad69-5fbc-4067-cad8-019bcdc4211c"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent_info = {\n","    \"iht_size\": 4096,\n","    \"num_tilings\": 8,\n","    \"num_tiles\": 8,\n","    \"actor_step_size\": 1e-1,\n","    \"critic_step_size\": 1e-0,\n","    \"avg_reward_step_size\": 1e-2,\n","    \"num_actions\": 3,\n","    \"seed\": 99,\n","}\n","\n","test_agent = ActorCriticSoftmaxAgent()\n","test_agent.agent_init(agent_info)\n","\n","state = [-np.pi, 0.]\n","\n","test_agent.agent_start(state)\n","\n","assert np.all(test_agent.prev_tiles == [0, 1, 2, 3, 4, 5, 6, 7])\n","assert test_agent.last_action == 2\n","\n","print(\"agent active_tiles: {}\".format(test_agent.prev_tiles))\n","print(\"agent selected action: {}\".format(test_agent.last_action))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"deletable":false,"editable":false,"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673257985724,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Ka_Lm0h8v3mc","nbgrader":{"cell_type":"code","checksum":"331d10b308ac8e4820239ada569bbd29","grade":true,"grade_id":"cell-f53c4c744932d219","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"outputId":"b33ee516-7310-42d6-fbd5-a1350d33a0f5"},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","# Make sure agent_start() and agent_policy() are working correctly first.\n","# agent_step() should work correctly for other arbitrary state transitions in addition to this test case.\n","\n","env_info = {\"seed\": 99}\n","agent_info = {\n","    \"iht_size\": 4096,\n","    \"num_tilings\": 8,\n","    \"num_tiles\": 8,\n","    \"actor_step_size\": 1e-1,\n","    \"critic_step_size\": 1e-0,\n","    \"avg_reward_step_size\": 1e-2,\n","    \"num_actions\": 3,\n","    \"seed\": 99,\n","}\n","\n","rl_glue = RLGlue(PendulumEnvironment, ActorCriticSoftmaxAgent)\n","rl_glue.rl_init(agent_info, env_info)\n","\n","# start env/agent\n","rl_glue.rl_start()\n","rl_glue.rl_step()\n","\n","# simple alias\n","agent = rl_glue.agent\n","\n","print(\"agent next_action: {}\".format(agent.last_action))\n","print(\"agent avg reward: {}\\n\".format(agent.avg_reward))\n","\n","assert agent.last_action == 1\n","assert agent.avg_reward == -0.03139092653589793\n","\n","print(\"agent first 10 values of actor weights[0]: \\n{}\\n\".format(agent.actor_w[0][:10]))\n","print(\"agent first 10 values of actor weights[1]: \\n{}\\n\".format(agent.actor_w[1][:10]))\n","print(\"agent first 10 values of actor weights[2]: \\n{}\\n\".format(agent.actor_w[2][:10]))\n","print(\"agent first 10 values of critic weights: \\n{}\".format(agent.critic_w[:10]))\n","\n","assert np.allclose(agent.actor_w[0][:10], [0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0., 0.])\n","assert np.allclose(agent.actor_w[1][:10], [0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0.01307955, 0., 0.])\n","assert np.allclose(agent.actor_w[2][:10], [-0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, -0.02615911, 0., 0.])\n","\n","assert np.allclose(agent.critic_w[:10], [-0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, -0.39238658, 0., 0.])"]},{"cell_type":"code","execution_count":13,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673257985724,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"xP35DzECv3md","nbgrader":{"cell_type":"code","checksum":"f5091c85ef2c69a7dc5992f593acc4d9","grade":false,"grade_id":"cell-8d55fd86b9fc3769","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Define function to run experiment\n","def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n","\n","    rl_glue = RLGlue(environment, agent)\n","            \n","    # sweep agent parameters\n","    for num_tilings in agent_parameters['num_tilings']:\n","        for num_tiles in agent_parameters[\"num_tiles\"]:\n","            for actor_ss in agent_parameters[\"actor_step_size\"]:\n","                for critic_ss in agent_parameters[\"critic_step_size\"]:\n","                    for avg_reward_ss in agent_parameters[\"avg_reward_step_size\"]:\n","                        \n","                        env_info = {}\n","                        agent_info = {\"num_tilings\": num_tilings,\n","                                      \"num_tiles\": num_tiles,\n","                                      \"actor_step_size\": actor_ss,\n","                                      \"critic_step_size\": critic_ss,\n","                                      \"avg_reward_step_size\": avg_reward_ss,\n","                                      \"num_actions\": agent_parameters[\"num_actions\"],\n","                                      \"iht_size\": agent_parameters[\"iht_size\"]}            \n","            \n","                        # results to save\n","                        return_per_step = np.zeros((experiment_parameters[\"num_runs\"], experiment_parameters[\"max_steps\"]))\n","                        exp_avg_reward_per_step = np.zeros((experiment_parameters[\"num_runs\"], experiment_parameters[\"max_steps\"]))\n","\n","                        # using tqdm we visualize progress bars \n","                        for run in tqdm(range(1, experiment_parameters[\"num_runs\"]+1)):\n","                            env_info[\"seed\"] = run\n","                            agent_info[\"seed\"] = run\n","                \n","                            rl_glue.rl_init(agent_info, env_info)\n","                            rl_glue.rl_start()\n","\n","                            num_steps = 0\n","                            total_return = 0.\n","                            return_arr = []\n","\n","                            # exponential average reward without initial bias\n","                            exp_avg_reward = 0.0\n","                            exp_avg_reward_ss = 0.01\n","                            exp_avg_reward_normalizer = 0\n","\n","                            while num_steps < experiment_parameters['max_steps']:\n","                                num_steps += 1\n","                                \n","                                rl_step_result = rl_glue.rl_step()\n","                                \n","                                reward = rl_step_result[0]\n","                                total_return += reward\n","                                return_arr.append(reward)\n","                                avg_reward = rl_glue.rl_agent_message(\"get avg reward\")\n","\n","                                exp_avg_reward_normalizer = exp_avg_reward_normalizer + exp_avg_reward_ss * (1 - exp_avg_reward_normalizer)\n","                                ss = exp_avg_reward_ss / exp_avg_reward_normalizer\n","                                exp_avg_reward += ss * (reward - exp_avg_reward)\n","                                \n","                                return_per_step[run-1][num_steps-1] = total_return\n","                                exp_avg_reward_per_step[run-1][num_steps-1] = exp_avg_reward\n","                                                        \n","                        if not os.path.exists('./results'):\n","                            os.makedirs('./results')\n","                \n","                        save_name = \"ActorCriticSoftmax_tilings_{}_tiledim_{}_actor_ss_{}_critic_ss_{}_avg_reward_ss_{}\".format(num_tilings, num_tiles, actor_ss, critic_ss, avg_reward_ss)\n","                        total_return_filename = \"./results/{}_total_return.npy\".format(save_name)\n","                        exp_avg_reward_filename = \"./results/{}_exp_avg_reward.npy\".format(save_name)\n","\n","                        np.save(total_return_filename, return_per_step)\n","                        np.save(exp_avg_reward_filename, exp_avg_reward_per_step)\n","                        "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"deletable":false,"editable":false,"executionInfo":{"elapsed":208582,"status":"ok","timestamp":1673258194298,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"sP70bA1Mv3me","nbgrader":{"cell_type":"code","checksum":"9f76e04859fa6d09d18caca6b89bae8f","grade":false,"grade_id":"cell-4d1df8adaf0d23c6","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"7b4bc761-a7ea-4bd1-d4e3-658e63534be2"},"outputs":[],"source":["#### Run Experiment\n","\n","# Experiment parameters\n","experiment_parameters = {\n","    \"max_steps\" : 20000,\n","    \"num_runs\" : 50\n","}\n","\n","# Environment parameters\n","environment_parameters = {}\n","\n","# Agent parameters\n","# Each element is an array because we will be later sweeping over multiple values\n","# actor and critic step-sizes are divided by num. tilings inside the agent\n","agent_parameters = {\n","    \"num_tilings\": [32],\n","    \"num_tiles\": [8],\n","    \"actor_step_size\": [2**(-2)],\n","    \"critic_step_size\": [2**1],\n","    \"avg_reward_step_size\": [2**(-6)],\n","    \"num_actions\": 3,\n","    \"iht_size\": 4096\n","}\n","\n","current_env = PendulumEnvironment\n","current_agent = ActorCriticSoftmaxAgent\n","\n","\n","run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n","plot_result(agent_parameters, './results')"]}],"metadata":{"colab":{"provenance":[]},"coursera":{"course_slug":"prediction-control-function-approximation","graded_item_id":"bHUHt","launcher_item_id":"Igqsy"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
