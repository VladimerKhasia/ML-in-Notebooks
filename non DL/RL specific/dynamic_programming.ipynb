{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "kpcYavMHgdHv",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3996977e7d68ec12f2c9ffcfe5ce214b",
          "grade": false,
          "grade_id": "cell-649fd0b1aa7ccb0f",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# Optimal Policies with Dynamic Programming\n",
        "\n",
        "this notebook will walk you through:\n",
        "- Policy Evaluation and Policy Improvement.\n",
        "- Value and Policy Iteration.\n",
        "- Bellman Equations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-9iK2Gp7gdH4",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2b5347a2a22c1bacff0c0453119c8d84",
          "grade": false,
          "grade_id": "cell-c11ff54faaf3fd89",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "from copy import deepcopy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, AutoMinorLocator\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x_uygTBhTIC"
      },
      "outputs": [],
      "source": [
        "#@title base classes for agent, environment and their interaction\n",
        "\n",
        "#!/usr/bin/env python                     \n",
        "\n",
        "from __future__ import print_function\n",
        "from abc import ABCMeta, abstractmethod\n",
        "\n",
        "### An abstract class that specifies the Agent API\n",
        "\n",
        "class BaseAgent:\n",
        "    \"\"\"Implements the agent for an RL environment.\n",
        "    Note:\n",
        "        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n",
        "        agent_message are required methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_init(self, agent_info= {}):\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_start(self, observation):\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            observation (Numpy array): the state observation from the environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            observation (Numpy array): the state observation from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_cleanup(self):\n",
        "        \"\"\"Cleanup done after the agent ends.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def agent_message(self, message):\n",
        "        \"\"\"A function used to pass information from the agent to the experiment.\n",
        "        Args:\n",
        "            message: The message passed to the agent.\n",
        "        Returns:\n",
        "            The response (or answer) to the message.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "### Abstract environment base class \n",
        "\n",
        "class BaseEnvironment:\n",
        "    \"\"\"Implements the environment for an RLGlue environment\n",
        "\n",
        "    Note:\n",
        "        env_init, env_start, env_step, env_cleanup, and env_message are required\n",
        "        methods.\n",
        "    \"\"\"\n",
        "\n",
        "    __metaclass__ = ABCMeta\n",
        "\n",
        "    def __init__(self):\n",
        "        reward = None\n",
        "        observation = None\n",
        "        termination = None\n",
        "        self.reward_obs_term = (reward, observation, termination)\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"Setup for the environment called when the experiment first starts.\n",
        "\n",
        "        Note:\n",
        "            Initialize a tuple with the reward, first state observation, boolean\n",
        "            indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_start(self):\n",
        "        \"\"\"The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_cleanup(self):\n",
        "        \"\"\"Cleanup done after the environment ends\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def env_message(self, message):\n",
        "        \"\"\"A message asking the environment for information\n",
        "\n",
        "        Args:\n",
        "            message: the message passed to the environment\n",
        "\n",
        "        Returns:\n",
        "            the response (or answer) to the message\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "### connects together an experiment, agent, and environment.\n",
        "\n",
        "class RLGlue:\n",
        "    \"\"\"RLGlue class\n",
        "\n",
        "    args:\n",
        "        env_name (string): the name of the module where the Environment class can be found\n",
        "        agent_name (string): the name of the module where the Agent class can be found\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env_class, agent_class):\n",
        "        self.environment = env_class()\n",
        "        self.agent = agent_class()\n",
        "\n",
        "        self.total_reward = None\n",
        "        self.last_action = None\n",
        "        self.num_steps = None\n",
        "        self.num_episodes = None\n",
        "\n",
        "    def rl_init(self, agent_init_info={}, env_init_info={}):\n",
        "        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n",
        "        self.environment.env_init(env_init_info)\n",
        "        self.agent.agent_init(agent_init_info)\n",
        "\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 0\n",
        "        self.num_episodes = 0\n",
        "\n",
        "    def rl_start(self, agent_start_info={}, env_start_info={}):\n",
        "        \"\"\"Starts RLGlue experiment\n",
        "\n",
        "        Returns:\n",
        "            tuple: (state, action)\n",
        "        \"\"\"\n",
        "\n",
        "        last_state = self.environment.env_start()\n",
        "        self.last_action = self.agent.agent_start(last_state)\n",
        "\n",
        "        observation = (last_state, self.last_action)\n",
        "\n",
        "        return observation\n",
        "\n",
        "    def rl_agent_start(self, observation):\n",
        "        \"\"\"Starts the agent.\n",
        "\n",
        "        Args:\n",
        "            observation: The first observation from the environment\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_start(observation)\n",
        "\n",
        "    def rl_agent_step(self, reward, observation):\n",
        "        \"\"\"Step taken by the agent\n",
        "\n",
        "        Args:\n",
        "            reward (float): the last reward the agent received for taking the\n",
        "                last action.\n",
        "            observation : the state observation the agent receives from the\n",
        "                environment.\n",
        "\n",
        "        Returns:\n",
        "            The action taken by the agent.\n",
        "        \"\"\"\n",
        "        return self.agent.agent_step(reward, observation)\n",
        "\n",
        "    def rl_agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates\n",
        "\n",
        "        Args:\n",
        "            reward (float): the reward the agent received when terminating\n",
        "        \"\"\"\n",
        "        self.agent.agent_end(reward)\n",
        "\n",
        "    def rl_env_start(self):\n",
        "        \"\"\"Starts RL-Glue environment.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination\n",
        "        \"\"\"\n",
        "        self.total_reward = 0.0\n",
        "        self.num_steps = 1\n",
        "\n",
        "        this_observation = self.environment.env_start()\n",
        "\n",
        "        return this_observation\n",
        "\n",
        "    def rl_env_step(self, action):\n",
        "        \"\"\"Step taken by the environment based on action from agent\n",
        "\n",
        "        Args:\n",
        "            action: Action taken by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): reward, state observation, boolean\n",
        "                indicating termination.\n",
        "        \"\"\"\n",
        "        ro = self.environment.env_step(action)\n",
        "        (this_reward, _, terminal) = ro\n",
        "\n",
        "        self.total_reward += this_reward\n",
        "\n",
        "        if terminal:\n",
        "            self.num_episodes += 1\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "\n",
        "        return ro\n",
        "\n",
        "    def rl_step(self):\n",
        "        \"\"\"Step taken by RLGlue, takes environment step and either step or\n",
        "            end by agent.\n",
        "\n",
        "        Returns:\n",
        "            (float, state, action, Boolean): reward, last state observation,\n",
        "                last action, boolean indicating termination\n",
        "        \"\"\"\n",
        "\n",
        "        (reward, last_state, term) = self.environment.env_step(self.last_action)\n",
        "\n",
        "        self.total_reward += reward\n",
        "\n",
        "        if term:\n",
        "            self.num_episodes += 1\n",
        "            self.agent.agent_end(reward)\n",
        "            roat = (reward, last_state, None, term)\n",
        "        else:\n",
        "            self.num_steps += 1\n",
        "            self.last_action = self.agent.agent_step(reward, last_state)\n",
        "            roat = (reward, last_state, self.last_action, term)\n",
        "\n",
        "        return roat\n",
        "\n",
        "    def rl_cleanup(self):\n",
        "        \"\"\"Cleanup done at end of experiment.\"\"\"\n",
        "        self.environment.env_cleanup()\n",
        "        self.agent.agent_cleanup()\n",
        "\n",
        "    def rl_agent_message(self, message):\n",
        "        \"\"\"Message passed to communicate with agent during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the agent\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the agent\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return self.agent.agent_message(message)\n",
        "\n",
        "    def rl_env_message(self, message):\n",
        "        \"\"\"Message passed to communicate with environment during experiment\n",
        "\n",
        "        Args:\n",
        "            message: the message (or question) to send to the environment\n",
        "\n",
        "        Returns:\n",
        "            The message back (or answer) from the environment\n",
        "\n",
        "        \"\"\"\n",
        "        return self.environment.env_message(message)\n",
        "\n",
        "    def rl_episode(self, max_steps_this_episode):\n",
        "        \"\"\"Runs an RLGlue episode\n",
        "\n",
        "        Args:\n",
        "            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n",
        "\n",
        "        Returns:\n",
        "            Boolean: if the episode should terminate\n",
        "        \"\"\"\n",
        "        is_terminal = False\n",
        "\n",
        "        self.rl_start()\n",
        "\n",
        "        while (not is_terminal) and ((max_steps_this_episode == 0) or\n",
        "                                     (self.num_steps < max_steps_this_episode)):\n",
        "            rl_step_result = self.rl_step()\n",
        "            is_terminal = rl_step_result[3]\n",
        "\n",
        "        return is_terminal\n",
        "\n",
        "    def rl_return(self):\n",
        "        \"\"\"The total reward\n",
        "\n",
        "        Returns:\n",
        "            float: the total reward\n",
        "        \"\"\"\n",
        "        return self.total_reward\n",
        "\n",
        "    def rl_num_steps(self):\n",
        "        \"\"\"The total number of steps taken\n",
        "\n",
        "        Returns:\n",
        "            Int: the total number of steps taken\n",
        "        \"\"\"\n",
        "        return self.num_steps\n",
        "\n",
        "    def rl_num_episodes(self):\n",
        "        \"\"\"The number of episodes\n",
        "\n",
        "        Returns\n",
        "            Int: the total number of episodes\n",
        "\n",
        "        \"\"\"\n",
        "        return self.num_episodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "56j-jUQaiyHt",
        "outputId": "014c9e42-f9a4-4f27-ad2b-fcd5a21701ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title parking grid-world environment\n",
        "\n",
        "plt.rc('font', size=30)  # controls default text sizes\n",
        "plt.rc('axes', titlesize=25)  # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=25)  # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=17)  # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=17)  # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=20)  # legend fontsize\n",
        "plt.rc('figure', titlesize=30)\n",
        "plt.tight_layout()\n",
        "\n",
        "\n",
        "def plot(V, pi):\n",
        "    # plot value\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12.5, 5))\n",
        "    ax1.axis('on')\n",
        "    ax1.cla()\n",
        "    states = np.arange(V.shape[0])\n",
        "    ax1.bar(states, V, edgecolor='none')\n",
        "    ax1.set_xlabel('State')\n",
        "    ax1.set_ylabel('Value', rotation='horizontal', ha='right')\n",
        "    ax1.set_title('Value Function')\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True, nbins=6))\n",
        "    ax1.yaxis.grid()\n",
        "    ax1.set_ylim(bottom=V.min())\n",
        "    # plot policy\n",
        "    ax2.axis('on')\n",
        "    ax2.cla()\n",
        "    im = ax2.imshow(pi.T, cmap='Greys', vmin=0, vmax=1, aspect='auto')\n",
        "    ax2.invert_yaxis()\n",
        "    ax2.set_xlabel('State')\n",
        "    ax2.set_ylabel('Action', rotation='horizontal', ha='right')\n",
        "    ax2.set_title('Policy')\n",
        "    start, end = ax2.get_xlim()\n",
        "    ax2.xaxis.set_ticks(np.arange(start, end), minor=True)\n",
        "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True, nbins=6))\n",
        "    ax2.yaxis.set_major_locator(MaxNLocator(integer=True, nbins=6))\n",
        "    start, end = ax2.get_ylim()\n",
        "    ax2.yaxis.set_ticks(np.arange(start, end), minor=True)\n",
        "    ax2.grid(which='minor')\n",
        "    divider = make_axes_locatable(ax2)\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.20)\n",
        "    cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    cbar.set_label('Probability', rotation=0, ha='left')\n",
        "    fig.subplots_adjust(wspace=0.5)\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(fig)\n",
        "    time.sleep(0.001)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "class ParkingWorld:\n",
        "    def __init__(self,\n",
        "                 num_spaces=10,\n",
        "                 num_prices=4,\n",
        "                 price_factor=0.1,\n",
        "                 occupants_factor=1.0,\n",
        "                 null_factor=1 / 3):\n",
        "        self.__num_spaces = num_spaces\n",
        "        self.__num_prices = num_prices\n",
        "        self.__occupants_factor = occupants_factor\n",
        "        self.__price_factor = price_factor\n",
        "        self.__null_factor = null_factor\n",
        "        self.__S = [num_occupied for num_occupied in range(num_spaces + 1)]\n",
        "        self.__A = list(range(num_prices))\n",
        "\n",
        "\n",
        "    def transitions(self, s, a):\n",
        "        return np.array([[r, self.p(s_, r, s, a)] for s_, r in self.support(s, a)])\n",
        "\n",
        "    def support(self, s, a):\n",
        "        return [(s_, self.reward(s, s_)) for s_ in self.__S]\n",
        "\n",
        "    def p(self, s_, r, s, a):\n",
        "        if r != self.reward(s, s_):\n",
        "            return 0\n",
        "        else:\n",
        "            center = (1 - self.__price_factor\n",
        "                      ) * s + self.__price_factor * self.__num_spaces * (\n",
        "                          1 - a / self.__num_prices)\n",
        "            emphasis = np.exp(\n",
        "                -abs(np.arange(2 * self.__num_spaces) - center) / 5)\n",
        "            if s_ == self.__num_spaces:\n",
        "                return sum(emphasis[s_:]) / sum(emphasis)\n",
        "            return emphasis[s_] / sum(emphasis)\n",
        "\n",
        "    def reward(self, s, s_):\n",
        "        return self.state_reward(s) + self.state_reward(s_)\n",
        "\n",
        "    def state_reward(self, s):\n",
        "        if s == self.__num_spaces:\n",
        "            return self.__null_factor * s * self.__occupants_factor\n",
        "        else:\n",
        "            return s * self.__occupants_factor\n",
        "\n",
        "    def random_state(self):\n",
        "        return np.random.randint(self.__num_prices)\n",
        "\n",
        "    def step(self, s, a):\n",
        "        probabilities = [\n",
        "            self.p(s_, self.reward(s, s_), s, a) for s_ in self.__S\n",
        "        ]\n",
        "        return np.random.choice(self.__S, p=probabilities)\n",
        "\n",
        "    @property\n",
        "    def A(self):\n",
        "        return list(self.__A)\n",
        "\n",
        "    @property\n",
        "    def num_spaces(self):\n",
        "        return self.__num_spaces\n",
        "\n",
        "    @property\n",
        "    def num_prices(self):\n",
        "        return self.num_prices\n",
        "\n",
        "    @property\n",
        "    def S(self):\n",
        "        return list(self.__S)\n",
        "\n",
        "\n",
        "class Transitions(list):\n",
        "    def __init__(self, transitions):\n",
        "        self.__transitions = transitions\n",
        "        super().__init__(transitions)\n",
        "\n",
        "    def __repr__(self):\n",
        "        repr = '{:<14} {:<10} {:<10}'.format('Next State', 'Reward',\n",
        "                                             'Probability')\n",
        "        repr += '\\n'\n",
        "        for i, (s, r, p) in enumerate(self.__transitions):\n",
        "            repr += '{:<14} {:<10} {:<10}'.format(s, round(r, 2), round(p, 2))\n",
        "            if i != len(self.__transitions) - 1:\n",
        "                repr += '\\n'\n",
        "        return repr\n",
        "\n",
        "\n",
        "def near(arr1, arr2, thresh):\n",
        "    arr1 = np.array(arr1)\n",
        "    arr2 = np.array(arr2)\n",
        "    return np.all(np.abs(arr1 - arr2) < thresh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8Lnu-0q5gdH6",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4f51ef4bc98021f90e701f76ed48f676",
          "grade": false,
          "grade_id": "cell-d25d06a8bafc4c26",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "num_spaces = 3\n",
        "num_prices = 3\n",
        "env = ParkingWorld(num_spaces, num_prices)\n",
        "V = np.zeros(num_spaces + 1)\n",
        "pi = np.ones((num_spaces + 1, num_prices)) / num_prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "n7UKcHBFgdH7",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0813b0f481e1f2f90e12f38456781410",
          "grade": false,
          "grade_id": "cell-57212e031233c500",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "The value function is a one-dimensional array where the $i$-th entry gives the value of $i$ spaces being occupied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "wLfkvc4QgdH7",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6e59c4a32939d9211dfc0f8fdd939780",
          "grade": false,
          "grade_id": "cell-c5f693a5ff49a888",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "fa86b35d-a283-4706-cfc6-03395426c47d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0.])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4pgLynccgdH8",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "eadfa7b22b07f8c457cf09a78eb23c8c",
          "grade": false,
          "grade_id": "cell-57154206afc97770",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "We can represent the policy as a two-dimensional array where the $(i, j)$-th entry gives the probability of taking action $j$ in state $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "yvl53kehgdH9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d732d93b6545408fa819526c2e52a0cf",
          "grade": false,
          "grade_id": "cell-85c017bb1e6fe4df",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "51304279-d30d-412b-e156-8f90dffcdf60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333333, 0.33333333, 0.33333333],\n",
              "       [0.33333333, 0.33333333, 0.33333333]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "GcUfQpW4gdH9",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "96d9ee84404cf38466f1c8c93b4aca9a",
          "grade": false,
          "grade_id": "cell-d7d514ba81bc686c",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "f1578a89-a7df-4b4e-871f-194827fc5f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pi(A=0|S=0) = 0.75    pi(A=1|S=0) = 0.11    pi(A=2|S=0) = 0.14    \n",
            "pi(A=0|S=1) = 0.33    pi(A=1|S=1) = 0.33    pi(A=2|S=1) = 0.33    \n",
            "pi(A=0|S=2) = 0.33    pi(A=1|S=2) = 0.33    pi(A=2|S=2) = 0.33    \n",
            "pi(A=0|S=3) = 0.33    pi(A=1|S=3) = 0.33    pi(A=2|S=3) = 0.33    \n"
          ]
        }
      ],
      "source": [
        "pi[0] = [0.75, 0.11, 0.14]\n",
        "\n",
        "for s, pi_s in enumerate(pi):\n",
        "    for a, p in enumerate(pi_s):\n",
        "        print(f'pi(A={a}|S={s}) = {p.round(2)}    ', end='')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "deletable": false,
        "editable": false,
        "id": "bkRHNB76gdH-",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "faf7aa91a9f54e17835b6e5c3e28b4bf",
          "grade": false,
          "grade_id": "cell-46b46b0dc80c68c7",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "a142edf9-e669-45c1-b158-6f1ef1669f7e"
      },
      "outputs": [],
      "source": [
        "V[0] = 1\n",
        "\n",
        "plot(V, pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "c7wPiT5-gdIA",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4fafc756424773278069199ff876300e",
          "grade": false,
          "grade_id": "cell-94d868709c1a9eba",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "c2a8181e-9e91-4494-ba3f-4859ca84f09a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2, 3]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.S"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "-ckL6FhVgdIA",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "dc72712f4890361c35c0b19f0df5befd",
          "grade": false,
          "grade_id": "cell-6f16d9e8ebf01b60",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "75668e04-80db-4376-93bd-07f1af352f48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "zvdy5rQPgdIB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4d32e329bafe53f2061e6b577751f291",
          "grade": false,
          "grade_id": "cell-4185982b1a21cd04",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "f02a117c-b94f-4935-858b-2e51bb725456"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.12390437],\n",
              "       [2.        , 0.15133714],\n",
              "       [3.        , 0.1848436 ],\n",
              "       [2.        , 0.53991488]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = 3\n",
        "action = 1\n",
        "transitions = env.transitions(state, action)\n",
        "transitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "editable": false,
        "id": "mb2HSrkkgdIB",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "25cbea37d8b8c6404081852743cbc6bd",
          "grade": false,
          "grade_id": "cell-379fdb797cae3afb",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "d5050701-a7e9-4813-ef03-c352a960d475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p(S'=0, R=1.0 | S=3, A=1) = 0.12\n",
            "p(S'=1, R=2.0 | S=3, A=1) = 0.15\n",
            "p(S'=2, R=3.0 | S=3, A=1) = 0.18\n",
            "p(S'=3, R=2.0 | S=3, A=1) = 0.54\n"
          ]
        }
      ],
      "source": [
        "for sp, (r, p) in enumerate(transitions):\n",
        "    print(f'p(S\\'={sp}, R={r} | S={state}, A={action}) = {p.round(2)}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ascSCMjRgdIC",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0869f6736a9ab680b0c82dccf72ba11c",
          "grade": false,
          "grade_id": "cell-141d4e3806427283",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Policy Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UruoXfb6gdIC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d940b556433954baa2dfa76c55918ca1",
          "grade": false,
          "grade_id": "cell-8d04cf6f6f397e17",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "# lock\n",
        "def evaluate_policy(env, V, pi, gamma, theta):\n",
        "    delta = float('inf')\n",
        "    while delta > theta:\n",
        "        delta = 0\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            bellman_update(env, V, pi, s, gamma)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "            \n",
        "    return V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Y7QfGxQZgdIC",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3d9220b9c5e2dad29041be432e1898de",
          "grade": false,
          "grade_id": "cell-4113388a5f8401b6",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def bellman_update(env, V, pi, s, gamma):\n",
        "    \"\"\"Mutate ``V`` according to the Bellman update equation.\"\"\"\n",
        "\n",
        "    v = 0\n",
        "    for a in env.A:\n",
        "        transitions = env.transitions(s, a)\n",
        "        for s_, (r, p) in enumerate(transitions):\n",
        "            v += pi[s][a] * p * (r + gamma * V[s_])\n",
        "    V[s] = v    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "i9Xl5BgEgdID",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4d9639225bc3d57f1079ceab1d57d411",
          "grade": false,
          "grade_id": "cell-5c1f3ff4b0e1b0bf",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "The cell below uses the policy evaluation algorithm to evaluate the city's policy, which charges a constant price of one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LTlD6eqgdID",
        "outputId": "be4cbc37-20a9-4e63-e7fc-68bbd5b5854f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[80.04173399 81.65532303 83.37394007 85.12975566 86.87174913 88.55589131\n",
            " 90.14020422 91.58180605 92.81929841 93.78915889 87.77792991]\n"
          ]
        }
      ],
      "source": [
        "# --------------\n",
        "# Debugging Cell\n",
        "# --------------\n",
        "\n",
        "# set up test environment\n",
        "num_spaces = 10\n",
        "num_prices = 4\n",
        "env = ParkingWorld(num_spaces, num_prices)\n",
        "\n",
        "# build test policy\n",
        "city_policy = np.zeros((num_spaces + 1, num_prices))\n",
        "city_policy[:, 1] = 1\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "\n",
        "V = np.zeros(num_spaces + 1)\n",
        "V = evaluate_policy(env, V, city_policy, gamma, theta)\n",
        "\n",
        "print(V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "R6a7MSiMgdIE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe42a26382eb7ed41954597ed701513c",
          "grade": true,
          "grade_id": "cell-104d8ba132c36b33",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "\n",
        "# set up test environment\n",
        "num_spaces = 10\n",
        "num_prices = 4\n",
        "env = ParkingWorld(num_spaces, num_prices)\n",
        "\n",
        "# build test policy\n",
        "city_policy = np.zeros((num_spaces + 1, num_prices))\n",
        "city_policy[:, 1] = 1\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "\n",
        "V = np.zeros(num_spaces + 1)\n",
        "V = evaluate_policy(env, V, city_policy, gamma, theta)\n",
        "\n",
        "# test the value function\n",
        "answer = [80.04, 81.65, 83.37, 85.12, 86.87, 88.55, 90.14, 91.58, 92.81, 93.78, 87.77]\n",
        "\n",
        "# make sure the value function is within 2 decimal places of the correct answer\n",
        "assert near(V, answer, 1e-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "B5tP7arJgdIE",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0f19b2dc70097c6425bbc3bd25a2a500",
          "grade": false,
          "grade_id": "cell-b612ffe570dd7e29",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "You can use the ``plot`` function to visualize the final value function and policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "deletable": false,
        "editable": false,
        "id": "3whgAgaqgdIE",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ff7b2798b5af88f6444a49099599a06a",
          "grade": false,
          "grade_id": "cell-fe5cf61a03a028fc",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "1c4c17a8-54b2-4589-87cd-509a12c141b7"
      },
      "outputs": [],
      "source": [
        "# lock\n",
        "plot(V, city_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wjbdY6_JgdIF",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cd767646e891985960e2d9c9b3b76ae1",
          "grade": false,
          "grade_id": "cell-15ec36bbf7a6fdc6",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "def improve_policy(env, V, pi, gamma):\n",
        "    policy_stable = True\n",
        "    for s in env.S:\n",
        "        old = pi[s].copy()\n",
        "        q_greedify_policy(env, V, pi, s, gamma)\n",
        "        \n",
        "        if not np.array_equal(pi[s], old):\n",
        "            policy_stable = False\n",
        "            \n",
        "    return pi, policy_stable\n",
        "\n",
        "def policy_iteration(env, gamma, theta):\n",
        "    V = np.zeros(len(env.S))\n",
        "    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n",
        "    policy_stable = False\n",
        "    \n",
        "    while not policy_stable:\n",
        "        V = evaluate_policy(env, V, pi, gamma, theta)\n",
        "        pi, policy_stable = improve_policy(env, V, pi, gamma)\n",
        "        \n",
        "    return V, pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "TJN5zbzXgdIG",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "03f887ab5a215f3e89ba751895d7e9f8",
          "grade": false,
          "grade_id": "cell-43cadb209544e857",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def q_greedify_policy(env, V, pi, s, gamma):\n",
        "    \"\"\"Mutate ``pi`` to be greedy with respect to the q-values induced by ``V``.\"\"\"\n",
        "\n",
        "    G = np.zeros_like(env.A, dtype=float)\n",
        "    for a in env.A:\n",
        "        transitions = env.transitions(s, a)\n",
        "        for s_, (r, p) in enumerate(transitions):\n",
        "            G[a] += p * (r + gamma * V[s_])\n",
        "            \n",
        "    greed_actions = np.argwhere(G == np.amax(G))\n",
        "    for a in env.A:\n",
        "        if a in greed_actions:\n",
        "            pi[s, a] = 1 / len(greed_actions)\n",
        "        else:\n",
        "            pi[s, a] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1CdyU8BgdIG"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# Debugging Cell\n",
        "# --------------\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "env = ParkingWorld(num_spaces=6, num_prices=4)\n",
        "\n",
        "V = np.array([7, 6, 5, 4, 3, 2, 1])\n",
        "pi = np.ones((7, 4)) / 4\n",
        "\n",
        "new_pi, stable = improve_policy(env, V, pi, gamma)\n",
        "\n",
        "# expect first call to greedify policy\n",
        "expected_pi = np.array([\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "])\n",
        "assert np.all(new_pi == expected_pi)\n",
        "assert stable == False\n",
        "\n",
        "# the value function has not changed, so the greedy policy should not change\n",
        "new_pi, stable = improve_policy(env, V, new_pi, gamma)\n",
        "\n",
        "assert np.all(new_pi == expected_pi)\n",
        "assert stable == True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "YKnPUQAWgdIG",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "9b8b71edde57f8aab6356de9a38527e3",
          "grade": true,
          "grade_id": "cell-468eb92e69c7f65e",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
        "\n",
        "V, pi = policy_iteration(env, gamma, theta)\n",
        "\n",
        "V_answer = [81.60, 83.28, 85.03, 86.79, 88.51, 90.16, 91.70, 93.08, 94.25, 95.25, 89.45]\n",
        "pi_answer = [\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [1, 0, 0, 0],\n",
        "    [0, 0, 0, 1],\n",
        "    [0, 0, 0, 1],\n",
        "]\n",
        "\n",
        "# make sure value function is within 2 decimal places of answer\n",
        "assert near(V, V_answer, 1e-2)\n",
        "# make sure policy is exactly correct\n",
        "assert np.all(pi == pi_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "A9-V_5mdgdIH",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aeedaa745e6dc30ebbc6b822c670c9b3",
          "grade": false,
          "grade_id": "cell-6939985ef9ad58a3",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "V, pi = policy_iteration(env, gamma, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "deletable": false,
        "editable": false,
        "id": "pmeJHP-ZgdII",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "da17cf77a51f4fabd0ce3a93e2803af8",
          "grade": false,
          "grade_id": "cell-73a1da64ca84a151",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "9b8774db-afed-4fe2-e7c2-9dc80a80dbd0"
      },
      "outputs": [],
      "source": [
        "plot(V, pi)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "VK3Po-VLgdII",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e59b175ca7605a8002c2040043f7b1af",
          "grade": false,
          "grade_id": "cell-e7628124eafb2fc2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gao6qWu7gdIJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3743399285b929801497af405783d06e",
          "grade": false,
          "grade_id": "cell-75baf962376afa7c",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma, theta):\n",
        "    V = np.zeros(len(env.S))\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            bellman_optimality_update(env, V, s, gamma)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n",
        "    for s in env.S:\n",
        "        q_greedify_policy(env, V, pi, s, gamma)\n",
        "    return V, pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "Z5QtmAxXgdIJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5b8c52e5f66f45467b7f7262fc98e89b",
          "grade": false,
          "grade_id": "cell-f2c6a183cc0923fb",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def bellman_optimality_update(env, V, s, gamma):\n",
        "    \"\"\"Mutate ``V`` according to the Bellman optimality update equation.\"\"\"\n",
        "\n",
        "    vmax = - float('inf')\n",
        "    for a in env.A:\n",
        "        transitions = env.transitions(s, a)\n",
        "        va = 0\n",
        "        for s_, (r, p) in enumerate(transitions):\n",
        "            va += p * (r + gamma * V[s_])\n",
        "        vmax = max(va, vmax)\n",
        "    V[s] = vmax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClB0iADDgdIJ"
      },
      "outputs": [],
      "source": [
        "# --------------\n",
        "# Debugging Cell\n",
        "# --------------\n",
        "\n",
        "gamma = 0.9\n",
        "env = ParkingWorld(num_spaces=6, num_prices=4)\n",
        "\n",
        "V = np.array([7, 6, 5, 4, 3, 2, 1])\n",
        "\n",
        "# only state 0 updated\n",
        "bellman_optimality_update(env, V, 0, gamma)\n",
        "assert list(V) == [5, 6, 5, 4, 3, 2, 1]\n",
        "\n",
        "# only state 2 updated\n",
        "bellman_optimality_update(env, V, 2, gamma)\n",
        "assert list(V) == [5, 6, 7, 4, 3, 2, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "C6ttMAbegdIJ",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "bdcf2724450c1e103db4ded3ade9a97b",
          "grade": true,
          "grade_id": "cell-92e457ed985823c3",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# -----------\n",
        "# Tested Cell\n",
        "# -----------\n",
        "\n",
        "gamma = 0.9\n",
        "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
        "\n",
        "V = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])\n",
        "\n",
        "for _ in range(10):\n",
        "    for s in env.S:\n",
        "        bellman_optimality_update(env, V, s, gamma)\n",
        "\n",
        "# make sure value function is exactly correct\n",
        "answer = [61, 63, 65, 67, 69, 71, 72, 74, 75, 76, 71]\n",
        "assert np.all(V == answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HRUYunvPgdIK",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cd8be31ddef5580d095a7e861e52a479",
          "grade": false,
          "grade_id": "cell-f609be2c58adc3e2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "V, pi = value_iteration(env, gamma, theta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "deletable": false,
        "editable": false,
        "id": "IO4VsMDVgdIK",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d18a2592a3bac43de72e18cb54357ac9",
          "grade": false,
          "grade_id": "cell-086e26bfb519a017",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "cd7dcf47-f3f4-4fd6-b400-af5c388ab812"
      },
      "outputs": [],
      "source": [
        "plot(V, pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "QkKImo4zgdIL",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "335160bd36744265e1ac43bd4305766b",
          "grade": false,
          "grade_id": "cell-e7940cfb801649be",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "def value_iteration2(env, gamma, theta):\n",
        "    V = np.zeros(len(env.S))\n",
        "    pi = np.ones((len(env.S), len(env.A))) / len(env.A)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in env.S:\n",
        "            v = V[s]\n",
        "            q_greedify_policy(env, V, pi, s, gamma)\n",
        "            bellman_update(env, V, pi, s, gamma)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V, pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "deletable": false,
        "editable": false,
        "id": "7MZ4cPRmgdIM",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "09b1fda9c335946b52cae6c8a55e80fb",
          "grade": false,
          "grade_id": "cell-2ace3a0ae8ee2e72",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "outputId": "d3a5c5b3-0896-477d-d791-6d1854e6ef64"
      },
      "outputs": [],
      "source": [
        "env = ParkingWorld(num_spaces=10, num_prices=4)\n",
        "gamma = 0.9\n",
        "theta = 0.1\n",
        "V, pi = value_iteration2(env, gamma, theta)\n",
        "plot(V, pi)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "fundamentals-of-reinforcement-learning",
      "graded_item_id": "fvXSL",
      "launcher_item_id": "5z8bz"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
