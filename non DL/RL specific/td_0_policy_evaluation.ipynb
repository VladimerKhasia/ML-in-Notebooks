{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"deletable":false,"editable":false,"id":"sckoJ_tlA1-i","nbgrader":{"cell_type":"markdown","checksum":"4f046aa616adb1b5cff666e8a39e2da6","grade":false,"grade_id":"cell-41f41eebcd73cca6","locked":true,"schema_version":3,"solution":false,"task":false}},"source":["# Policy Evaluation in Cliff Walking Environment"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":4738,"status":"ok","timestamp":1673182433919,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"B3RR4Dz5q0dI"},"outputs":[],"source":["!pip install -q jdc"]},{"cell_type":"code","execution_count":2,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":37,"status":"ok","timestamp":1673182433920,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"A-ep5FyzA1-r","nbgrader":{"cell_type":"code","checksum":"961301eba64117ca89220f6130255741","grade":false,"grade_id":"cell-880b4e1ed6e705b9","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["import jdc             ## please remember it may not work properly with google colab\n","\n","from itertools import product\n","from tqdm import tqdm\n","import numpy as np\n","import matplotlib\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","executionInfo":{"elapsed":36,"status":"ok","timestamp":1673182433921,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"yELcqaUTBX75"},"outputs":[],"source":["#@title base classes for agent, environment and their interaction\n","\n","#!/usr/bin/env python\n","\n","from __future__ import print_function\n","from abc import ABCMeta, abstractmethod\n","\n","### An abstract class that specifies the Agent API\n","\n","class BaseAgent:\n","    \"\"\"Implements the agent for an RL-Glue environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info= {}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's env_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\"\n","\n","\n","### Abstract environment base class \n","\n","class BaseEnvironment:\n","    \"\"\"Implements the environment for an RLGlue environment\n","\n","    Note:\n","        env_init, env_start, env_step, env_cleanup, and env_message are required\n","        methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        reward = None\n","        observation = None\n","        termination = None\n","        self.reward_obs_term = (reward, observation, termination)\n","\n","    @abstractmethod\n","    def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","\n","        Note:\n","            Initialize a tuple with the reward, first state observation, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_start(self):\n","        \"\"\"The first method called when the experiment starts, called before the\n","        agent starts.\n","\n","        Returns:\n","            The first state observation from the environment.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_step(self, action):\n","        \"\"\"A step taken by the environment.\n","\n","        Args:\n","            action: The action taken by the agent\n","\n","        Returns:\n","            (float, state, Boolean): a tuple of the reward, state observation,\n","                and boolean indicating if it's terminal.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def env_cleanup(self):\n","        \"\"\"Cleanup done after the environment ends\"\"\"\n","\n","    @abstractmethod\n","    def env_message(self, message):\n","        \"\"\"A message asking the environment for information\n","\n","        Args:\n","            message: the message passed to the environment\n","\n","        Returns:\n","            the response (or answer) to the message\n","        \"\"\"\n","\n","\n","### connects together an experiment, agent, and environment.\n","\n","class RLGlue:\n","    \"\"\"RLGlue class\n","\n","    args:\n","        env_name (string): the name of the module where the Environment class can be found\n","        agent_name (string): the name of the module where the Agent class can be found\n","    \"\"\"\n","\n","    def __init__(self, env_class, agent_class):\n","        self.environment = env_class()\n","        self.agent = agent_class()\n","\n","        self.total_reward = None\n","        self.last_action = None\n","        self.num_steps = None\n","        self.num_episodes = None\n","\n","    def rl_init(self, agent_init_info={}, env_init_info={}):\n","        \"\"\"Initial method called when RLGlue experiment is created\"\"\"\n","        self.environment.env_init(env_init_info)\n","        self.agent.agent_init(agent_init_info)\n","\n","        self.total_reward = 0.0\n","        self.num_steps = 0\n","        self.num_episodes = 0\n","\n","    def rl_start(self, agent_start_info={}, env_start_info={}):\n","        \"\"\"Starts RLGlue experiment\n","\n","        Returns:\n","            tuple: (state, action)\n","        \"\"\"\n","\n","        last_state = self.environment.env_start()\n","        self.last_action = self.agent.agent_start(last_state)\n","\n","        observation = (last_state, self.last_action)\n","\n","        return observation\n","\n","    def rl_agent_start(self, observation):\n","        \"\"\"Starts the agent.\n","\n","        Args:\n","            observation: The first observation from the environment\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_start(observation)\n","\n","    def rl_agent_step(self, reward, observation):\n","        \"\"\"Step taken by the agent\n","\n","        Args:\n","            reward (float): the last reward the agent received for taking the\n","                last action.\n","            observation : the state observation the agent receives from the\n","                environment.\n","\n","        Returns:\n","            The action taken by the agent.\n","        \"\"\"\n","        return self.agent.agent_step(reward, observation)\n","\n","    def rl_agent_end(self, reward):\n","        \"\"\"Run when the agent terminates\n","\n","        Args:\n","            reward (float): the reward the agent received when terminating\n","        \"\"\"\n","        self.agent.agent_end(reward)\n","\n","    def rl_env_start(self):\n","        \"\"\"Starts RL-Glue environment.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination\n","        \"\"\"\n","        self.total_reward = 0.0\n","        self.num_steps = 1\n","\n","        this_observation = self.environment.env_start()\n","\n","        return this_observation\n","\n","    def rl_env_step(self, action):\n","        \"\"\"Step taken by the environment based on action from agent\n","\n","        Args:\n","            action: Action taken by agent.\n","\n","        Returns:\n","            (float, state, Boolean): reward, state observation, boolean\n","                indicating termination.\n","        \"\"\"\n","        ro = self.environment.env_step(action)\n","        (this_reward, _, terminal) = ro\n","\n","        self.total_reward += this_reward\n","\n","        if terminal:\n","            self.num_episodes += 1\n","        else:\n","            self.num_steps += 1\n","\n","        return ro\n","\n","    def rl_step(self):\n","        \"\"\"Step taken by RLGlue, takes environment step and either step or\n","            end by agent.\n","\n","        Returns:\n","            (float, state, action, Boolean): reward, last state observation,\n","                last action, boolean indicating termination\n","        \"\"\"\n","\n","        (reward, last_state, term) = self.environment.env_step(self.last_action)\n","\n","        self.total_reward += reward;\n","\n","        if term:\n","            self.num_episodes += 1\n","            self.agent.agent_end(reward)\n","            roat = (reward, last_state, None, term)\n","        else:\n","            self.num_steps += 1\n","            self.last_action = self.agent.agent_step(reward, last_state)\n","            roat = (reward, last_state, self.last_action, term)\n","\n","        return roat\n","\n","    def rl_cleanup(self):\n","        \"\"\"Cleanup done at end of experiment.\"\"\"\n","        self.environment.env_cleanup()\n","        self.agent.agent_cleanup()\n","\n","    def rl_agent_message(self, message):\n","        \"\"\"Message passed to communicate with agent during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the agent\n","\n","        Returns:\n","            The message back (or answer) from the agent\n","\n","        \"\"\"\n","\n","        return self.agent.agent_message(message)\n","\n","    def rl_env_message(self, message):\n","        \"\"\"Message passed to communicate with environment during experiment\n","\n","        Args:\n","            message: the message (or question) to send to the environment\n","\n","        Returns:\n","            The message back (or answer) from the environment\n","\n","        \"\"\"\n","        return self.environment.env_message(message)\n","\n","    def rl_episode(self, max_steps_this_episode):\n","        \"\"\"Runs an RLGlue episode\n","\n","        Args:\n","            max_steps_this_episode (Int): the maximum steps for the experiment to run in an episode\n","\n","        Returns:\n","            Boolean: if the episode should terminate\n","        \"\"\"\n","        is_terminal = False\n","\n","        self.rl_start()\n","\n","        while (not is_terminal) and ((max_steps_this_episode == 0) or\n","                                     (self.num_steps < max_steps_this_episode)):\n","            rl_step_result = self.rl_step()\n","            is_terminal = rl_step_result[3]\n","\n","        return is_terminal\n","\n","    def rl_return(self):\n","        \"\"\"The total reward\n","\n","        Returns:\n","            float: the total reward\n","        \"\"\"\n","        return self.total_reward\n","\n","    def rl_num_steps(self):\n","        \"\"\"The total number of steps taken\n","\n","        Returns:\n","            Int: the total number of steps taken\n","        \"\"\"\n","        return self.num_steps\n","\n","    def rl_num_episodes(self):\n","        \"\"\"The number of episodes\n","\n","        Returns\n","            Int: the total number of episodes\n","\n","        \"\"\"\n","        return self.num_episodes\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1673182433922,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"QgRJBrLrB3Vn"},"outputs":[],"source":["#@title helper for visualization \n","\n","class Manager:\n","    def __init__(self, env_info, agent_info, true_values_file=None, experiment_name=None):\n","        self.experiment_name = experiment_name\n","        \n","        self.grid_h, self.grid_w = env_info[\"grid_height\"], env_info[\"grid_width\"]\n","        self.cmap = matplotlib.cm.viridis\n","        self.cmap.set_bad('black', 1.)\n","            \n","        self.values_table = None\n","        self.policy = agent_info[\"policy\"]\n","        \n","        self.true_values_file = true_values_file\n","    \n","    def compute_values_table(self, values):\n","        self.values_table = np.empty((self.grid_h, self.grid_w))\n","        self.values_table.fill(np.nan)\n","        for state in range(len(values)):\n","            self.values_table[np.unravel_index(state, (self.grid_h, self.grid_w))] = values[state]\n","                    \n","    def compute_RMSVE(self):\n","        return (np.sqrt(np.nanmean((self.values_table - self.true_values) ** 2)))\n","    \n","    def visualize(self, values, episode_num):\n","        if not hasattr(self, \"fig\"):\n","            self.fig = plt.figure(figsize=(10, 20))\n","            plt.ion()\n","            \n","            if self.true_values_file is not None:\n","                self.cmap_VE = matplotlib.cm.Reds\n","                self.cmap_VE.set_bad('black', 1.)\n","                self.ax = self.fig.add_subplot(311)\n","                self.RMSVE_LOG = []\n","                self.true_values = np.load(self.true_values_file)\n","            else:\n","                self.true_values = None\n","\n","        self.fig.clear()\n","        if self.true_values is not None:\n","            plt.subplot(311)\n","        self.compute_values_table(values)\n","        plt.xticks([])\n","        plt.yticks([])\n","        im = plt.imshow(self.values_table, cmap=self.cmap, interpolation='nearest', origin='upper')\n","        \n","        for state in range(self.policy.shape[0]):\n","            for action in range(self.policy.shape[1]):\n","                y, x = np.unravel_index(state, (self.grid_h, self.grid_w))\n","                pi = self.policy[state][action]\n","                if pi == 0:\n","                    continue\n","                if action == 0:\n","                    plt.arrow(x, y, 0,  -0.5 * pi, fill=False, length_includes_head=True, head_width=0.1, \n","                              alpha=0.5)\n","                if action == 1: \n","                    plt.arrow(x, y, -0.5 * pi, 0, fill=False, length_includes_head=True, head_width=0.1, \n","                              alpha=0.5)\n","                if action == 2:\n","                    plt.arrow(x, y, 0, 0.5 * pi, fill=False, length_includes_head=True, head_width=0.1, \n","                              alpha=0.5)\n","                if action == 3:\n","                    plt.arrow(x, y, 0.5 * pi, 0, fill=False, length_includes_head=True, head_width=0.1, \n","                              alpha=0.5)\n","        \n","        plt.title(((\"\" or self.experiment_name) + \"\\n\") + \"Predicted Values, Episode: %d\" % episode_num)\n","        plt.colorbar(im, orientation='horizontal')\n","        \n","        if self.true_values is not None:\n","            plt.subplot(312)\n","            plt.xticks([])\n","            plt.yticks([])\n","            im = plt.imshow((self.values_table - self.true_values) ** 2, origin='upper', cmap=self.cmap_VE)\n","            plt.title('Squared Value Error: $(v_{\\pi}(S) - \\hat{v}(S))^2$')\n","            plt.colorbar(im, orientation='horizontal')\n","            self.RMSVE_LOG.append((episode_num, self.compute_RMSVE()))\n","            \n","            plt.subplot(313)\n","            plt.plot([x[0] for x in self.RMSVE_LOG], [x[1] for x in self.RMSVE_LOG])\n","            plt.xlabel(\"Episode\")\n","            plt.ylabel(\"RMSVE\", rotation=0, labelpad=20)\n","            plt.title(\"Root Mean Squared Value Error\")\n","        self.fig.canvas.draw()\n","    \n","    def run_tests(self, values, RMSVE_threshold):\n","        assert self.true_values is not None, \"This function can only be called once the true values are given during \" +\\\n","               \"runtime.\"\n","        self.compute_values_table(values)\n","        mask = ~(np.isnan(self.values_table) | np.isnan(self.true_values))\n","        if self.compute_RMSVE() < RMSVE_threshold and np.allclose(self.true_values[mask], self.values_table[mask]):\n","            pass\n","        else:\n","            assert False\n","            \n","    def __del__(self):\n","        pass #plt.close()"]},{"cell_type":"code","execution_count":5,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673182433922,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"UBi9Wge1A1-u","nbgrader":{"cell_type":"code","checksum":"70a921fedaf47f9cd58bcc38dfff2079","grade":false,"grade_id":"cell-61ab5d6bc73499aa","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Create empty CliffWalkEnvironment class.\n","# These methods will be filled in later cells.\n","class CliffWalkEnvironment(BaseEnvironment):\n","    def env_init(self, env_info={}):\n","        raise NotImplementedError\n","\n","    def env_start(self):\n","        raise NotImplementedError\n","\n","    def env_step(self, action):\n","        raise NotImplementedError\n","\n","    def env_cleanup(self):\n","        raise NotImplementedError\n","    \n","    # helper method\n","    def state(self, loc):\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":6,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673182433923,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"aMMpNNc_A1-v","nbgrader":{"cell_type":"code","checksum":"9628a884b34ab00f14b560fa3eddc294","grade":false,"grade_id":"cell-e15c46384ae349b7","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","\n","def env_init(self, env_info={}):\n","        \"\"\"Setup for the environment called when the experiment first starts.\n","        Note:\n","            Initialize a tuple with the reward, first state, boolean\n","            indicating if it's terminal.\n","        \"\"\"\n","        \n","        # Note, we can setup the following variables later, in env_start() as it is equivalent. \n","        # Code is left here to adhere to the note above, but these variables are initialized once more\n","        # in env_start() [See the env_start() function below.]\n","        \n","        reward = None\n","        state = None # See Aside\n","        termination = None\n","        self.reward_state_term = (reward, state, termination)\n","        \n","        # AN ASIDE: Observation is a general term used in the RL-Glue files that can be interachangeably \n","        # used with the term \"state\" for our purposes and for this assignment in particular. \n","        # A difference arises in the use of the terms when we have what is called Partial Observability where \n","        # the environment may return states that may not fully represent all the information needed to \n","        # predict values or make decisions (i.e., the environment is non-Markovian.)\n","        \n","        # Set the default height to 4 and width to 12 (as in the diagram given above)\n","        self.grid_h = env_info.get(\"grid_height\", 4) \n","        self.grid_w = env_info.get(\"grid_width\", 12)\n","        \n","        # Now, we can define a frame of reference. Let positive x be towards the direction down and \n","        # positive y be towards the direction right (following the row-major NumPy convention.)\n","        # Then, keeping with the usual convention that arrays are 0-indexed, max x is then grid_h - 1 \n","        # and max y is then grid_w - 1. So, we have:\n","        # Starting location of agent is the bottom-left corner, (max x, min y). \n","        self.start_loc = (self.grid_h - 1, 0)\n","        # Goal location is the bottom-right corner. (max x, max y).\n","        self.goal_loc = (self.grid_h - 1, self.grid_w - 1)\n","        \n","        # The cliff will contain all the cells between the start_loc and goal_loc.\n","        self.cliff = [(self.grid_h - 1, i) for i in range(1, (self.grid_w - 1))]\n","        \n","        # Take a look at the annotated environment diagram given in the above Jupyter Notebook cell to \n","        # verify that your understanding of the above code is correct for the default case, i.e., where \n","        # height = 4 and width = 12."]},{"cell_type":"code","execution_count":7,"metadata":{"deletable":false,"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673182433923,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"T3UlH7JMA1-y","nbgrader":{"cell_type":"code","checksum":"d8812eea241d83eb6a6e102019b543e4","grade":false,"grade_id":"cell-41a1c253613aa0b0","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","# Modify the return statement of this function to return a correct single index as \n","# the state (see the logic for this in the previous cell.)\n","def state(self, loc):\n","    return loc[0] * self.grid_w + loc[1]    "]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":35,"status":"ok","timestamp":1673182433924,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"06I9vjWNA1-z"},"outputs":[],"source":["# --------------\n","# Debugging Cell\n","# --------------\n","\n","env = CliffWalkEnvironment()\n","env.env_init({ \"grid_height\": 4, \"grid_width\": 12 })\n","\n","coords = [(0, 0), (0, 11), (1, 5), (3, 0), (3, 9), (3, 11)]\n","correct_outputs = [0, 11, 17, 36, 45, 47]\n","\n","got = [env.state(s) for s in coords]\n","assert got == correct_outputs"]},{"cell_type":"code","execution_count":9,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":34,"status":"ok","timestamp":1673182433924,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"pGG-O1OAA1-z","nbgrader":{"cell_type":"code","checksum":"f405849cc734485507aaaf0a6c3de361","grade":true,"grade_id":"cell-af9c8dc2ad3b0918","locked":true,"points":5,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","env = CliffWalkEnvironment()\n","for n in range(100):\n","    # make a gridworld of random size and shape\n","    height = np.random.randint(2, 100)\n","    width = np.random.randint(2, 100)\n","    env.env_init({ \"grid_height\": height, \"grid_width\": width })\n","    \n","    # generate some random coordinates within the grid\n","    idx_h = np.random.randint(height)\n","    idx_w = np.random.randint(width)\n","    \n","    # check that the state index is correct\n","    state = env.state((idx_h, idx_w))\n","    correct_state = width * idx_h + idx_w\n","    \n","    assert state == correct_state"]},{"cell_type":"code","execution_count":10,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":32,"status":"ok","timestamp":1673182433925,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"QhFG49Z2A1-1","nbgrader":{"cell_type":"code","checksum":"9d9555e6777c4238253214ee338d1120","grade":false,"grade_id":"cell-d4a47fc700dc1702","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","def env_start(self):\n","    \"\"\"The first method called when the episode starts, called before the\n","    agent starts.\n","\n","    Returns:\n","        The first state from the environment.\n","    \"\"\"\n","    reward = 0\n","    # agent_loc will hold the current location of the agent\n","    self.agent_loc = self.start_loc\n","    # state is the one dimensional state representation of the agent location.\n","    state = self.state(self.agent_loc)\n","    termination = False\n","    self.reward_state_term = (reward, state, termination)\n","\n","    return self.reward_state_term[1]"]},{"cell_type":"code","execution_count":11,"metadata":{"deletable":false,"executionInfo":{"elapsed":32,"status":"ok","timestamp":1673182433926,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"-7-T1bLwA1-2","nbgrader":{"cell_type":"code","checksum":"93c59f644f0a24823d6fd512465b83b8","grade":false,"grade_id":"cell-47a55053538ed113","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","\n","def isInBounds(x, y, width, height):\n","    ## x, y = self.agent_loc\n","    ## width, height = self.grid_w, self.grid_h\n","    if ((width-1) >= y >= 0) and ((height-1) >= x >= 0) : \n","      return True\n","\n","\n","# Fill in the code for action UP and implement the logic for reward and termination.\n","def env_step(self, action):\n","    \"\"\"A step taken by the environment.\n","\n","    Args:\n","        action: The action taken by the agent\n","\n","    Returns:\n","        (float, state, Boolean): a tuple of the reward, state,\n","            and boolean indicating if it's terminal.\n","    \"\"\"\n","    \n","    x, y = self.agent_loc\n","\n","    # UP\n","    if action == 0:\n","        # Hint: Look at the code given for the other actions and think about the logic in them.\n","\n","        x = x - 1\n","        \n","    # LEFT\n","    elif action == 1:\n","        y = y - 1\n","        \n","    # DOWN\n","    elif action == 2:\n","        x = x + 1\n","        \n","    # RIGHT\n","    elif action == 3:\n","        y = y + 1\n","        \n","    # Uh-oh\n","    else: \n","        raise Exception(str(action) + \" not in recognized actions [0: Up, 1: Left, 2: Down, 3: Right]!\")\n","\n","    # if the action takes the agent out-of-bounds\n","    # then the agent stays in the same state\n","    if not isInBounds(x, y, self.grid_w, self.grid_h):\n","        x, y = self.agent_loc\n","        \n","    # assign the new location to the environment object\n","    self.agent_loc = (x, y)\n","    \n","    # by default, assume -1 reward per step and that we did not terminate\n","    reward = -1\n","    terminal = False\n","    \n","    # assign the reward and terminal variables \n","    # - if the agent falls off the cliff (don't forget to reset agent location!)\n","    # - if the agent reaches the goal state\n","    if self.agent_loc == self.goal_loc: # Reached Goal!\n","        terminal = True\n","    elif self.agent_loc in self.cliff: # Fell into the cliff!\n","        reward = -100\n","        self.agent_loc = self.start_loc    \n","    \n","    self.reward_state_term = (reward, self.state(self.agent_loc), terminal)\n","    return self.reward_state_term"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673182433926,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"QVknRNFOA1-2"},"outputs":[],"source":["# --------------\n","# Debugging Cell\n","# --------------\n","\n","def test_action_up():\n","    env = CliffWalkEnvironment()\n","    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n","    env.agent_loc = (0, 0)\n","    env.env_step(0)\n","    assert(env.agent_loc == (0, 0))\n","    \n","    env.agent_loc = (1, 0)\n","    env.env_step(0)\n","    assert(env.agent_loc == (0, 0))\n","    \n","def test_reward():\n","    env = CliffWalkEnvironment()\n","    env.env_init({\"grid_height\": 4, \"grid_width\": 12})\n","    env.agent_loc = (0, 0)\n","    reward_state_term = env.env_step(0)\n","    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((0, 0)) and\n","           reward_state_term[2] == False)\n","    \n","    env.agent_loc = (3, 1)\n","    reward_state_term = env.env_step(2)\n","    assert(reward_state_term[0] == -100 and reward_state_term[1] == env.state((3, 0)) and\n","           reward_state_term[2] == False)\n","    \n","    env.agent_loc = (2, 11)\n","    reward_state_term = env.env_step(2)\n","    assert(reward_state_term[0] == -1 and reward_state_term[1] == env.state((3, 11)) and\n","           reward_state_term[2] == True)\n","    \n","test_action_up()\n","test_reward()"]},{"cell_type":"code","execution_count":13,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":32,"status":"ok","timestamp":1673182433927,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"b6G-xMaXA1-3","nbgrader":{"cell_type":"code","checksum":"930e2f004e7f5bc157d4ade45c186185","grade":true,"grade_id":"cell-cc53e967d108b33c","locked":true,"points":5,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","env = CliffWalkEnvironment()\n","for n in range(100):\n","    # create a cliff world of random size\n","    height = np.random.randint(2, 100)\n","    width = np.random.randint(2, 100)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    \n","    # start the agent in a random location\n","    idx_h = 0 if np.random.random() < 0.5 else np.random.randint(height)\n","    idx_w = np.random.randint(width)\n","    env.agent_loc = (idx_h, idx_w)\n","    \n","    env.env_step(0)\n","    assert(env.agent_loc == (0 if idx_h == 0 else idx_h - 1, idx_w))"]},{"cell_type":"code","execution_count":14,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673182433927,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"lqGCMDUgA1-3","nbgrader":{"cell_type":"code","checksum":"085778d845e6e8259013f384c9c71fc7","grade":true,"grade_id":"cell-a883fea60cb72354","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","np.random.seed(0)\n","\n","env = CliffWalkEnvironment()\n","for n in range(100):\n","    # create a cliff world of random size\n","    height = np.random.randint(4, 10)\n","    width = np.random.randint(4, 10)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    env.env_start()\n","    \n","    # start the agent near the cliff\n","    idx_h = height - 2\n","    idx_w = np.random.randint(1, width - 2)\n","    env.agent_loc = (idx_h, idx_w)\n","    \n","    r, sp, term = env.env_step(2)\n","    assert(r == -100 and sp == (height - 1) * width and term == False)\n","\n","for n in range(100):\n","    # create a cliff world of random size\n","    height = np.random.randint(4, 10)\n","    width = np.random.randint(4, 10)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    env.env_start()\n","    \n","    # start the agent near the goal\n","    idx_h = height - 2\n","    idx_w = width - 1\n","    env.agent_loc = (idx_h, idx_w)\n","    \n","    r, sp, term = env.env_step(2)\n","    assert(r == -1 and sp == (height - 1) * width + (width - 1) and term == True)\n","\n","for n in range(100):\n","    # create a cliff world of random size\n","    height = np.random.randint(4, 10)\n","    width = np.random.randint(4, 10)\n","    env.env_init({\"grid_height\": height, \"grid_width\": width})\n","    env.env_start()\n","    \n","    # start the agent in a random location\n","    idx_h = np.random.randint(0, height - 3)\n","    idx_w = np.random.randint(0, width - 1)\n","    env.agent_loc = (idx_h, idx_w)\n","    \n","    r, sp, term = env.env_step(2)\n","    assert(r == -1 and term == False)"]},{"cell_type":"code","execution_count":15,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673182433928,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"JPSvPwC2A1-4","nbgrader":{"cell_type":"code","checksum":"2239673e0e1e0ce4adb6f2d4a6a703e3","grade":false,"grade_id":"cell-d5ff562d7edc277d","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to CliffWalkEnvironment\n","\n","def env_cleanup(self):\n","    \"\"\"Cleanup done after the environment ends\"\"\"\n","    self.agent_loc = self.start_loc"]},{"cell_type":"code","execution_count":16,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673182433928,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"rW2CxjA1A1-5","nbgrader":{"cell_type":"code","checksum":"2ecf4ec36550b6e56088c40cb4c6cf63","grade":false,"grade_id":"cell-8dcd58cfaa9b1391","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# Create empty TDAgent class.\n","# These methods will be filled in later cells.\n","\n","class TDAgent(BaseAgent):\n","    def agent_init(self, agent_info={}):\n","        raise NotImplementedError\n","        \n","    def agent_start(self, state):\n","        raise NotImplementedError\n","\n","    def agent_step(self, reward, state):\n","        raise NotImplementedError\n","\n","    def agent_end(self, reward):\n","        raise NotImplementedError\n","\n","    def agent_cleanup(self):        \n","        raise NotImplementedError\n","        \n","    def agent_message(self, message):\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":17,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673182433929,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"vicFzVtlA1-6","nbgrader":{"cell_type":"code","checksum":"7b2bd8f7421b260e24fb2ffea9831ee3","grade":false,"grade_id":"cell-7e6fb425c17b2222","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_init(self, agent_info={}):\n","    \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    # Create a random number generator with the provided seed to seed the agent for reproducibility.\n","    self.rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n","\n","    # Policy will be given, recall that the goal is to accurately estimate its corresponding value function. \n","    self.policy = agent_info.get(\"policy\")\n","    # Discount factor (gamma) to use in the updates.\n","    self.discount = agent_info.get(\"discount\")\n","    # The learning rate or step size parameter (alpha) to use in updates.\n","    self.step_size = agent_info.get(\"step_size\")\n","\n","    # Initialize an array of zeros that will hold the values.\n","    # Recall that the policy can be represented as a (# States, # Actions) array. With the \n","    # assumption that this is the case, we can use the first dimension of the policy to\n","    # initialize the array for values.\n","    self.values = np.zeros((self.policy.shape[0],))"]},{"cell_type":"code","execution_count":18,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673182433929,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"z8IH2b0vA1-6","nbgrader":{"cell_type":"code","checksum":"6f2f7c78f1156b50a122f009d908a1f3","grade":false,"grade_id":"cell-b7179e38fe75348d","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_start(self, state):\n","    \"\"\"The first method called when the episode starts, called after\n","    the environment starts.\n","    Args:\n","        state (Numpy array): the state from the environment's env_start function.\n","    Returns:\n","        The first action the agent takes.\n","    \"\"\"\n","    # The policy can be represented as a (# States, # Actions) array. So, we can use \n","    # the second dimension here when choosing an action.\n","    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n","    self.last_state = state\n","    return action"]},{"cell_type":"code","execution_count":19,"metadata":{"deletable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673182433930,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"APL35eOsA1-7","nbgrader":{"cell_type":"code","checksum":"bc6bb81efeca49f2fbbe857b53c911c1","grade":false,"grade_id":"cell-54079abb73dd10b4","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_step(self, reward, state):\n","    \"\"\"A step taken by the agent.\n","    Args:\n","        reward (float): the reward received for taking the last action taken\n","        state (Numpy array): the state from the\n","            environment's step after the last action, i.e., where the agent ended up after the\n","            last action\n","    Returns:\n","        The action the agent is taking.\n","    \"\"\"\n","    \n","    # We should perform an update with the last state given that we now have the reward and\n","    # next state. We break this into two steps. Recall for example that the Monte-Carlo update \n","    # had the form: V[S_t] = V[S_t] + alpha * (target - V[S_t]), where the target was the return, G_t.\n","    target = reward + self.discount * self.values[state]\n","    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])\n","\n","    # Having updated the value for the last state, we now act based on the current \n","    # state, and set the last state to be current one as we will next be making an \n","    # update with it when agent_step is called next once the action we return from this function \n","    # is executed in the environment.\n","\n","    action = self.rand_generator.choice(range(self.policy.shape[1]), p=self.policy[state])\n","    self.last_state = state\n","\n","    return action"]},{"cell_type":"code","execution_count":20,"metadata":{"deletable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673182433931,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Pswz3Rj9A1-7","nbgrader":{"cell_type":"code","checksum":"21b0563eedd718ae37cc451cfe50423a","grade":false,"grade_id":"cell-f47463c94223a7fa","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_end(self, reward):\n","    \"\"\"Run when the agent terminates.\n","    Args:\n","        reward (float): the reward the agent received for entering the terminal state.\n","    \"\"\"\n","    \n","    # we should perform an update with the last state given that we now have the \n","    # reward. Note that in this case, the action led to termination. Once more, we break this into \n","    # two steps, computing the target and the update itself that uses the target and the \n","    # current value estimate for the state whose value we are updating.\n","\n","    target = reward\n","    self.values[self.last_state] = self.values[self.last_state] + self.step_size * (target - self.values[self.last_state])    "]},{"cell_type":"code","execution_count":21,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":29,"status":"ok","timestamp":1673182433931,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"2P5EmTwxA1-8","nbgrader":{"cell_type":"code","checksum":"5ddea78d8e68fcbc0210e88d35eb84dd","grade":false,"grade_id":"cell-35c6057f39236706","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_cleanup(self):\n","    \"\"\"Cleanup done after the agent ends.\"\"\"\n","    self.last_state = None"]},{"cell_type":"code","execution_count":22,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673182433932,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"VXja7zV_A1-9","nbgrader":{"cell_type":"code","checksum":"661f3a352859f22e5581f2953a6738ec","grade":false,"grade_id":"cell-8bac91a36a2cab6f","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%%add_to TDAgent\n","\n","def agent_message(self, message):\n","    \"\"\"A function used to pass information from the agent to the experiment.\n","    Args:\n","        message: The message passed to the agent.\n","    Returns:\n","        The response (or answer) to the message.\n","    \"\"\"\n","    if message == \"get_values\":\n","        return self.values\n","    else:\n","        raise Exception(\"TDAgent.agent_message(): Message not understood!\")"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1673182433932,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"zAnMdHXLA1-9"},"outputs":[],"source":["# --------------\n","# Debugging Cell\n","# --------------\n","# The following test checks that the TD check works for a case where the transition \n","# garners reward -1 and does not lead to a terminal state. This is in a simple two state setting \n","# where there is only one action. The first state's current value estimate is 0 while the second is 1.\n","# Note the discount and step size if you are debugging this test.\n","agent = TDAgent()\n","policy_list = np.array([[1.], [1.]])\n","agent.agent_init({\"policy\": np.array(policy_list), \"discount\": 0.99, \"step_size\": 0.1})\n","agent.values = np.array([0., 1.])\n","agent.agent_start(0)\n","\n","reward = -1\n","next_state = 1\n","agent.agent_step(reward, next_state)\n","\n","assert(np.isclose(agent.values[0], -0.001) and np.isclose(agent.values[1], 1.))\n","\n","# The following test checks that the TD check works for a case where the transition \n","# garners reward -100 and lead to a terminal state. This is in a simple one state setting \n","# where there is only one action. The state's current value estimate is 0.\n","# Note the discount and step size if you are debugging this test.\n","agent = TDAgent()\n","policy_list = np.array([[1.]])\n","agent.agent_init({\"policy\": np.array(policy_list), \"discount\": 0.99, \"step_size\": 0.1})\n","agent.values = np.array([0.])\n","agent.agent_start(0)\n","\n","reward = -100\n","next_state = 0\n","agent.agent_end(reward)\n","\n","assert(np.isclose(agent.values[0], -10))"]},{"cell_type":"code","execution_count":24,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":401,"status":"ok","timestamp":1673182434305,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"9AZ53xKdA1-9","nbgrader":{"cell_type":"code","checksum":"6afbe1c030cc5d88088b8baf8055d3a5","grade":true,"grade_id":"cell-63cd42d5248e7e84","locked":true,"points":20,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","agent = TDAgent()\n","policy_list = [np.random.dirichlet(np.ones(10), size=1).squeeze() for _ in range(100)]\n","\n","for n in range(100):\n","    gamma = np.random.random()\n","    alpha = np.random.random()\n","    agent.agent_init({\"policy\": np.array(policy_list), \"discount\": gamma, \"step_size\": alpha})\n","    agent.values = np.random.randn(*agent.values.shape)\n","    state = np.random.randint(100)\n","    agent.agent_start(state)\n","    \n","    for _ in range(100):\n","        prev_agent_vals = agent.values.copy()\n","        reward = np.random.random()\n","        if np.random.random() > 0.1:\n","            next_state = np.random.randint(100)\n","            agent.agent_step(reward, next_state)\n","            prev_agent_vals[state] = prev_agent_vals[state] + alpha * (reward + gamma * prev_agent_vals[next_state] - prev_agent_vals[state])\n","            assert(np.allclose(prev_agent_vals, agent.values))\n","            state = next_state\n","        else:\n","            agent.agent_end(reward)\n","            prev_agent_vals[state] = prev_agent_vals[state] + alpha * (reward - prev_agent_vals[state])\n","            assert(np.allclose(prev_agent_vals, agent.values))\n","            break"]},{"cell_type":"code","execution_count":25,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1673182434306,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"ZNkF87QPA1--","nbgrader":{"cell_type":"code","checksum":"7dd158868b6da83ed2d8af5393fa19e7","grade":false,"grade_id":"cell-aa30396771314085","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["%matplotlib notebook\n","\n","def run_experiment(env_info, agent_info,num_episodes=5000, experiment_name=None, plot_freq=100, true_values_file=None, value_error_threshold=1e-8):\n","    env = CliffWalkEnvironment\n","    agent = TDAgent\n","    rl_glue = RLGlue(env, agent)\n","\n","    rl_glue.rl_init(agent_info, env_info)\n","\n","    manager = Manager(env_info, agent_info, true_values_file=true_values_file, experiment_name=experiment_name)\n","    for episode in range(1, num_episodes + 1):\n","        rl_glue.rl_episode(0) # no step limit\n","        if episode % plot_freq == 0:\n","            values = rl_glue.agent.agent_message(\"get_values\")\n","            manager.visualize(values, episode)\n","\n","    values = rl_glue.agent.agent_message(\"get_values\")\n","    return values"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1673182434306,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"KLF2hbjseQ5W"},"outputs":[],"source":["## we write actual optimal value function in file optimal_policy.npy \n","\n","# data = np.load('./optimal_policy_valuefunction.npy') \n","# print(data)\n","\n","nan = np.NAN\n","np.save('./optimal_policy_valuefunction.npy', np.array([[ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n","         nan],\n","       [ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n","         nan],\n","       [-12., -11., -10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,\n","         -1.],\n","       [-13.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n","         nan]]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"deletable":false,"editable":false,"executionInfo":{"elapsed":10584,"status":"ok","timestamp":1673182444885,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"j2OqfBSEA1-_","nbgrader":{"cell_type":"code","checksum":"c9935c904c8dac62540410b7dc0eb740","grade":false,"grade_id":"cell-2a4f093ff50135b8","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"0159c019-fff0-433a-f03d-f486aaba5e85"},"outputs":[],"source":["env_info = {\"grid_height\": 4, \"grid_width\": 12, \"seed\": 0}\n","agent_info = {\"discount\": 1, \"step_size\": 0.01, \"seed\": 0}\n","\n","# The Optimal Policy that strides just along the cliff\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","policy[36] = [1, 0, 0, 0]\n","for i in range(24, 35):\n","    policy[i] = [0, 0, 0, 1]\n","policy[35] = [0, 0, 1, 0]\n","\n","agent_info.update({\"policy\": policy})\n","\n","true_values_file = \"optimal_policy_valuefunction.npy\"\n","_ = run_experiment(env_info, agent_info, num_episodes=5000, experiment_name=\"Policy Evaluation on Optimal Policy\",\n","                   plot_freq=500, true_values_file=true_values_file)"]},{"cell_type":"code","execution_count":28,"metadata":{"deletable":false,"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673182444887,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"SvBiLQk5A1-_","nbgrader":{"cell_type":"code","checksum":"d288407bc4dd094efc56faebad142e5f","grade":false,"grade_id":"cell-41ea2c684571e6e8","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# The Safe Policy\n","# Hint: Fill in the array below (as done in the previous cell) based on the safe policy illustration \n","# in the environment diagram. This is the policy that strides as far as possible away from the cliff. \n","# We call it a \"safe\" policy because if the environment has any stochasticity, this policy would do a good job in \n","# keeping the agent from falling into the cliff (in contrast to the optimal policy shown before). \n","\n","# build a uniform random policy\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","\n","# build an example environment\n","env = CliffWalkEnvironment()\n","env.env_init(env_info)\n","\n","# modify the uniform random policy\n","for h in range(env_info['grid_height']):\n","    for w in range(env_info['grid_width']):\n","        state = h * env_info['grid_width'] + w\n","        action = [1, 0, 0, 0]\n","        if h == 0:\n","            action = [0, 0, 0, 1]\n","        if w == env_info['grid_width'] - 1:\n","            action = [0, 0, 1, 0]\n","        policy[state] = action"]},{"cell_type":"code","execution_count":29,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673182444888,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"s-MQAnqRA1-_","nbgrader":{"cell_type":"code","checksum":"7d6c1c46ebd8a20c291b6c485601a7da","grade":true,"grade_id":"cell-747a2fdf30624260","locked":true,"points":10,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# -----------\n","# Tested Cell\n","# -----------\n","\n","width = env_info['grid_width']\n","height = env_info['grid_height']\n","\n","# left side of space\n","for x in range(1, height):\n","    s = env.state((x, 0))\n","    \n","    # go up\n","    assert np.all(policy[s] == [1, 0, 0, 0])\n","\n","# top of space\n","for y in range(width - 1):\n","    s = env.state((0, y))\n","\n","    # go right\n","    assert np.all(policy[s] == [0, 0, 0, 1])\n","    \n","# right side of space\n","for x in range(height - 1):\n","    s = env.state((x, width - 1))\n","    \n","    # go down\n","    assert np.all(policy[s] == [0, 0, 1, 0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"deletable":false,"editable":false,"executionInfo":{"elapsed":6108,"status":"ok","timestamp":1673182450988,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"rtbzVAWPA1_A","nbgrader":{"cell_type":"code","checksum":"f61945e3fa9693a71c67308e390831f2","grade":false,"grade_id":"cell-4cff4aa8310f733e","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"2294c893-d45c-4349-e161-e1da887a5ff1"},"outputs":[],"source":["agent_info.update({\"policy\": policy})\n","v = run_experiment(env_info, agent_info, experiment_name=\"Policy Evaluation On Safe Policy\", num_episodes=5000, plot_freq=500)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"deletable":false,"editable":false,"executionInfo":{"elapsed":13,"status":"ok","timestamp":1673182450990,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"nZDF3ZX9A1_A","nbgrader":{"cell_type":"code","checksum":"fa33604922343541caab97a67b3d0b97","grade":false,"grade_id":"cell-e1cf8e11c672a07e","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["# A Near Optimal Stochastic Policy\n","# Now, we try a stochastic policy that deviates a little from the optimal policy seen above. \n","# This means we can get different results due to randomness.\n","# We will thus average the value function estimates we get over multiple runs. \n","# This can take some time, upto about 5 minutes from previous testing. \n","# NOTE: The autograder will compare . Re-run this cell upon making any changes.\n","\n","env_info = {\"grid_height\": 4, \"grid_width\": 12}\n","agent_info = {\"discount\": 1, \"step_size\": 0.01}\n","\n","policy = np.ones(shape=(env_info['grid_width'] * env_info['grid_height'], 4)) * 0.25\n","policy[36] = [0.9, 0.1/3., 0.1/3., 0.1/3.]\n","for i in range(24, 35):\n","    policy[i] = [0.1/3., 0.1/3., 0.1/3., 0.9]\n","policy[35] = [0.1/3., 0.1/3., 0.9, 0.1/3.]\n","agent_info.update({\"policy\": policy})\n","agent_info.update({\"step_size\": 0.01})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"deletable":false,"editable":false,"executionInfo":{"elapsed":25611,"status":"ok","timestamp":1673182476588,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"tWoYeTxCA1_A","nbgrader":{"cell_type":"code","checksum":"b3888898a45dcda0c5420f24132be6d4","grade":false,"grade_id":"cell-6038bf336a6d7fd2","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"6f60d217-3749-4891-9563-dcd293e252fb"},"outputs":[],"source":["env_info['seed'] = 0\n","agent_info['seed'] = 0\n","v = run_experiment(env_info, agent_info,\n","               experiment_name=\"Policy Evaluation On Optimal Policy\",\n","               num_episodes=5000, plot_freq=100)"]}],"metadata":{"colab":{"provenance":[]},"coursera":{"course_slug":"sample-based-learning-methods","graded_item_id":"P4k5f","launcher_item_id":"OwIbv"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
