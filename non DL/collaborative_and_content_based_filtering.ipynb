{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzk7iX_CodX6",
        "tags": []
      },
      "source": [
        "# Here we use collaborative filtering for recommender systems\n",
        "\n",
        "### Download data here: \n",
        "\n",
        "https://1drv.ms/u/s!AqisPMhP9M9zgQAKy1wP4AE5PfKO?e=ku4MgN \n",
        "\n",
        "[\"ml-latest-small\"](https://grouplens.org/datasets/movielens/latest/) dataset.   \n",
        "[The MovieLens Datasets: History and Context by F. Maxwell Harper and Joseph A. Konstan. 2015. ACM Transactions on Interactive Intelligent Systems <https://doi.org/10.1145/2827872>]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyZ0r_ZxeP9S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import loadtxt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax3UcTJtelNf"
      },
      "outputs": [],
      "source": [
        "# some helper functions for the loading and handling data\n",
        "\n",
        "def normalizeRatings(Y, R):\n",
        "    \"\"\"\n",
        "    Preprocess data by subtracting mean rating for every movie (every row).\n",
        "    Only include real ratings R(i,j)=1.\n",
        "    [Ynorm, Ymean] = normalizeRatings(Y, R) normalized Y so that each movie\n",
        "    has a rating of 0 on average. Unrated moves then have a mean rating (0)\n",
        "    Returns the mean rating in Ymean.\n",
        "    \"\"\"\n",
        "    Ymean = (np.sum(Y*R,axis=1)/(np.sum(R, axis=1)+1e-12)).reshape(-1,1)\n",
        "    Ynorm = Y - np.multiply(Ymean, R) \n",
        "    return(Ynorm, Ymean)\n",
        "\n",
        "def load_precalc_params_small():\n",
        "\n",
        "    file = open('./small_movies_X.csv', 'rb')\n",
        "    X = loadtxt(file, delimiter = \",\")\n",
        "\n",
        "    file = open('./small_movies_W.csv', 'rb')\n",
        "    W = loadtxt(file,delimiter = \",\")\n",
        "\n",
        "    file = open('./small_movies_b.csv', 'rb')\n",
        "    b = loadtxt(file,delimiter = \",\")\n",
        "    b = b.reshape(1,-1)\n",
        "    num_movies, num_features = X.shape\n",
        "    num_users,_ = W.shape\n",
        "    return(X, W, b, num_movies, num_features, num_users)\n",
        "    \n",
        "def load_ratings_small():\n",
        "    file = open('./small_movies_Y.csv', 'rb')\n",
        "    Y = loadtxt(file,delimiter = \",\")\n",
        "\n",
        "    file = open('./small_movies_R.csv', 'rb')\n",
        "    R = loadtxt(file,delimiter = \",\")\n",
        "    return(Y,R)\n",
        "\n",
        "def load_Movie_List_pd():\n",
        "    \"\"\" returns df with and index of movies in the order they are in in the Y matrix \"\"\"\n",
        "    df = pd.read_csv('./small_movie_list.csv', header=0, index_col=0,  delimiter=',', quotechar='\"')\n",
        "    mlist = df[\"title\"].to_list()\n",
        "    return(mlist, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxMSp2HXeP9d"
      },
      "outputs": [],
      "source": [
        "#Load data\n",
        "X, W, b, num_movies, num_features, num_users = load_precalc_params_small()\n",
        "Y, R = load_ratings_small()\n",
        "\n",
        "print(\"Y\", Y.shape, \"R\", R.shape)\n",
        "print(\"X\", X.shape)\n",
        "print(\"W\", W.shape)\n",
        "print(\"b\", b.shape)\n",
        "print(\"num_features\", num_features)\n",
        "print(\"num_movies\",   num_movies)\n",
        "print(\"num_users\",    num_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxm1O_wbodYF"
      },
      "outputs": [],
      "source": [
        "#  From the matrix, we can compute statistics like average rating.\n",
        "tsmean =  np.mean(Y[0, R[0, :].astype(bool)])\n",
        "print(f\"Average rating for movie 1 : {tsmean:0.3f} / 5\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADaLmFPBeP9k"
      },
      "outputs": [],
      "source": [
        "# Reduce the data set size so that this runs faster\n",
        "num_users_r = 4\n",
        "num_movies_r = 5 \n",
        "num_features_r = 3\n",
        "\n",
        "X_r = X[:num_movies_r, :num_features_r]\n",
        "W_r = W[:num_users_r,  :num_features_r]\n",
        "b_r = b[0, :num_users_r].reshape(1,-1)\n",
        "Y_r = Y[:num_movies_r, :num_users_r]\n",
        "R_r = R[:num_movies_r, :num_users_r]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDaEGSIweP9i"
      },
      "outputs": [],
      "source": [
        "# cost function\n",
        "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
        "    \"\"\"\n",
        "    Returns the cost for the content-based filtering\n",
        "    Args:\n",
        "      X (ndarray (num_movies,num_features)): matrix of item features\n",
        "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
        "      b (ndarray (1, num_users)            : vector of user parameters\n",
        "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
        "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
        "      lambda_ (float): regularization parameter\n",
        "    Returns:\n",
        "      J (float) : Cost\n",
        "    \"\"\"\n",
        "    nm, nu = Y.shape\n",
        "    J = 0\n",
        "    for j in range(nu):\n",
        "        w = W[j,:]\n",
        "        b_j = b[0,j]\n",
        "        for i in range(nm):\n",
        "            x = X[i,:]\n",
        "            y = Y[i,j]\n",
        "            r = R[i,j]\n",
        "            J += np.square(r * (np.dot(w,x) + b_j - y ) )\n",
        "    J = J/2\n",
        "    J += (lambda_/2) * (np.sum(np.square(W)) + np.sum(np.square(X))) #regularization\n",
        "\n",
        "    return J\n",
        "\n",
        "# Evaluate cost function\n",
        "J = cofi_cost_func(X_r, W_r, b_r, Y_r, R_r, 0);\n",
        "print(f\"Cost: {J:0.2f}\")\n",
        "\n",
        "# Evaluate cost function with regularization \n",
        "J = cofi_cost_func(X_r, W_r, b_r, Y_r, R_r, 1.5);\n",
        "print(f\"Cost (with regularization): {J:0.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6lmUSezeP9o"
      },
      "outputs": [],
      "source": [
        "def cofi_cost_func_v(X, W, b, Y, R, lambda_):\n",
        "    \"\"\"\n",
        "    Returns the cost for the content-based filtering\n",
        "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
        "    Args:\n",
        "      X (ndarray (num_movies,num_features)): matrix of item features\n",
        "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
        "      b (ndarray (1, num_users)            : vector of user parameters\n",
        "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
        "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
        "      lambda_ (float): regularization parameter\n",
        "    Returns:\n",
        "      J (float) : Cost\n",
        "    \"\"\"\n",
        "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
        "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
        "    return J\n",
        "\n",
        "\n",
        "# Evaluate cost function\n",
        "J = cofi_cost_func_v(X_r, W_r, b_r, Y_r, R_r, 0);\n",
        "print(f\"Cost: {J:0.2f}\")\n",
        "\n",
        "# Evaluate cost function with regularization \n",
        "J = cofi_cost_func_v(X_r, W_r, b_r, Y_r, R_r, 1.5);\n",
        "print(f\"Cost (with regularization): {J:0.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJO8Jr0UodYR"
      },
      "outputs": [],
      "source": [
        "movieList, movieList_df = load_Movie_List_pd()\n",
        "\n",
        "my_ratings = np.zeros(num_movies)          #  Initialize my ratings\n",
        "\n",
        "# Check the file small_movie_list.csv for id of each movie in our dataset\n",
        "# For example, Toy Story 3 (2010) has ID 2700, so to rate it \"5\", you can set\n",
        "my_ratings[2700] = 5 \n",
        "\n",
        "#Or suppose you did not enjoy Persuasion (2007), you can set\n",
        "my_ratings[2609] = 2;\n",
        "\n",
        "# We have selected a few movies we liked / did not like and the ratings we\n",
        "# gave are as follows:\n",
        "my_ratings[929]  = 5   # Lord of the Rings: The Return of the King, The\n",
        "my_ratings[246]  = 5   # Shrek (2001)\n",
        "my_ratings[2716] = 3   # Inception\n",
        "my_ratings[1150] = 5   # Incredibles, The (2004)\n",
        "my_ratings[382]  = 2   # Amelie (Fabuleux destin d'Amélie Poulain, Le)\n",
        "my_ratings[366]  = 5   # Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\n",
        "my_ratings[622]  = 5   # Harry Potter and the Chamber of Secrets (2002)\n",
        "my_ratings[988]  = 3   # Eternal Sunshine of the Spotless Mind (2004)\n",
        "my_ratings[2925] = 1   # Louis Theroux: Law & Disorder (2008)\n",
        "my_ratings[2937] = 1   # Nothing to Declare (Rien à déclarer)\n",
        "my_ratings[793]  = 5   # Pirates of the Caribbean: The Curse of the Black Pearl (2003)\n",
        "my_rated = [i for i in range(len(my_ratings)) if my_ratings[i] > 0]\n",
        "\n",
        "print('\\nNew user ratings:\\n')\n",
        "for i in range(len(my_ratings)):\n",
        "    if my_ratings[i] > 0 :\n",
        "        print(f'Rated {my_ratings[i]} for  {movieList_df.loc[i,\"title\"]}');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MgksyGteP9s"
      },
      "outputs": [],
      "source": [
        "# Reload ratings\n",
        "Y, R = load_ratings_small()\n",
        "\n",
        "# Add new user ratings to Y\n",
        "Y = np.c_[my_ratings, Y]\n",
        "\n",
        "# Add new user indicator matrix to R\n",
        "R = np.c_[(my_ratings != 0).astype(int), R]\n",
        "\n",
        "# Normalize the Dataset\n",
        "Ynorm, Ymean = normalizeRatings(Y, R)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iuuqhe8eP9s",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#  Useful Values\n",
        "num_movies, num_users = Y.shape\n",
        "num_features = 100\n",
        "\n",
        "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
        "tf.random.set_seed(1234) # for consistent results\n",
        "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
        "X = tf.Variable(tf.random.normal((num_movies, num_features),dtype=tf.float64),  name='X')\n",
        "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWj2tIzMeP9t"
      },
      "outputs": [],
      "source": [
        "iterations = 200\n",
        "lambda_ = 1\n",
        "for iter in range(iterations):\n",
        "    # Use TensorFlow’s GradientTape\n",
        "    # to record the operations used to compute the cost \n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        # Compute the cost (forward pass included in cost)\n",
        "        cost_value = cofi_cost_func_v(X, W, b, Ynorm, R, lambda_)\n",
        "\n",
        "    # Use the gradient tape to automatically retrieve\n",
        "    # the gradients of the trainable variables with respect to the loss\n",
        "    grads = tape.gradient( cost_value, [X,W,b] )\n",
        "\n",
        "    # Run one step of gradient descent by updating\n",
        "    # the value of the variables to minimize the loss.\n",
        "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
        "\n",
        "    # Log periodically.\n",
        "    if iter % 20 == 0:\n",
        "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ns266wKtodYT"
      },
      "outputs": [],
      "source": [
        "# Make a prediction using trained weights and biases\n",
        "p = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
        "\n",
        "#restore the mean\n",
        "pm = p + Ymean\n",
        "\n",
        "my_predictions = pm[:,0]\n",
        "\n",
        "# sort predictions\n",
        "ix = tf.argsort(my_predictions, direction='DESCENDING')\n",
        "\n",
        "for i in range(17):\n",
        "    j = ix[i]\n",
        "    if j not in my_rated:\n",
        "        print(f'Predicting rating {my_predictions[j]:0.2f} for movie {movieList[j]}')\n",
        "\n",
        "print('\\n\\nOriginal vs Predicted ratings:\\n')\n",
        "for i in range(len(my_ratings)):\n",
        "    if my_ratings[i] > 0:\n",
        "        print(f'Original {my_ratings[i]}, Predicted {my_predictions[i]:0.2f} for {movieList[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i76tLWWkeP_X"
      },
      "outputs": [],
      "source": [
        "filter=(movieList_df[\"number of ratings\"] > 20)\n",
        "movieList_df[\"pred\"] = my_predictions\n",
        "movieList_df = movieList_df.reindex(columns=[\"pred\", \"mean rating\", \"number of ratings\", \"title\"])\n",
        "movieList_df.loc[ix[:300]].loc[filter].sort_values(\"mean rating\", ascending=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cgFnB4qdOVsU",
        "tags": []
      },
      "source": [
        "# Here we use content-based filtering for recommender systems\n",
        "\n",
        "### Download data here:\n",
        "\n",
        "https://1drv.ms/u/s!AqisPMhP9M9zgQf57Fh6Gk2dKS03?e=Qr7T7s\n",
        "\n",
        "## Reference: \n",
        "\n",
        "[MovieLens ml-latest-small](https://grouplens.org/datasets/movielens/latest/) dataset. \n",
        "\n",
        "[The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems F. Maxwell Harper and Joseph A. Konstan. 2015. <https://doi.org/10.1145/2827872>]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu-w_RmNwCV5"
      },
      "outputs": [],
      "source": [
        "!pip install -qq pickle5\n",
        "\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pickle5 as pickle\n",
        "import tabulate\n",
        "\n",
        "pd.set_option(\"display.precision\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Js44cKNYl9Rz"
      },
      "outputs": [],
      "source": [
        "# helper functions\n",
        "\n",
        "def load_data():\n",
        "    ''' called to load preprepared data for the lab '''\n",
        "    item_train = genfromtxt('./content_item_train.csv', delimiter=',')\n",
        "    user_train = genfromtxt('./content_user_train.csv', delimiter=',')\n",
        "    y_train    = genfromtxt('./content_y_train.csv', delimiter=',')\n",
        "    with open('./content_item_train_header.txt', newline='') as f:    #csv reader handles quoted strings better\n",
        "        item_features = list(csv.reader(f))[0]\n",
        "    with open('./content_user_train_header.txt', newline='') as f:\n",
        "        user_features = list(csv.reader(f))[0]\n",
        "    item_vecs = genfromtxt('./content_item_vecs.csv', delimiter=',')\n",
        "\n",
        "    movie_dict = defaultdict(dict)\n",
        "    count = 0\n",
        "#    with open('./movies.csv', newline='') as csvfile:\n",
        "    with open('./content_movie_list.csv', newline='') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "        for line in reader:\n",
        "            if count == 0:\n",
        "                count += 1  #skip header\n",
        "                #print(line) print\n",
        "            else:\n",
        "                count += 1\n",
        "                movie_id = int(line[0])\n",
        "                movie_dict[movie_id][\"title\"] = line[1]\n",
        "                movie_dict[movie_id][\"genres\"] = line[2]\n",
        "\n",
        "    with open('./content_user_to_genre.pickle', 'rb') as f:\n",
        "        user_to_genre = pickle.load(f)\n",
        "\n",
        "    return(item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre)\n",
        "\n",
        "def pprint_train(x_train, features, vs, u_s, maxcount=5, user=True):\n",
        "    \"\"\" Prints user_train or item_train nicely \"\"\"\n",
        "    if user:\n",
        "        flist = [\".0f\", \".0f\", \".1f\",\n",
        "                 \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\", \".1f\"]\n",
        "    else:\n",
        "        flist = [\".0f\", \".0f\", \".1f\", \n",
        "                 \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\", \".0f\"]\n",
        "\n",
        "    head = features[:vs]\n",
        "    if vs < u_s: print(\"error, vector start {vs} should be greater then user start {u_s}\")\n",
        "    for i in range(u_s):\n",
        "        head[i] = \"[\" + head[i] + \"]\"\n",
        "    genres = features[vs:]\n",
        "    hdr = head + genres\n",
        "    disp = [split_str(hdr, 5)]\n",
        "    count = 0\n",
        "    for i in range(0, x_train.shape[0]):\n",
        "        if count == maxcount: break\n",
        "        count += 1\n",
        "        disp.append([x_train[i, 0].astype(int),\n",
        "                     x_train[i, 1].astype(int),\n",
        "                     x_train[i, 2].astype(float),\n",
        "                     *x_train[i, 3:].astype(float)\n",
        "                    ])\n",
        "    table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\", floatfmt=flist, numalign='center')\n",
        "    return table\n",
        "\n",
        "\n",
        "def split_str(ifeatures, smax):\n",
        "    ''' split the feature name strings to tables fit '''\n",
        "    ofeatures = []\n",
        "    for s in ifeatures:\n",
        "        if not ' ' in s:  # skip string that already have a space\n",
        "            if len(s) > smax:\n",
        "                mid = int(len(s)/2)\n",
        "                s = s[:mid] + \" \" + s[mid:]\n",
        "        ofeatures.append(s)\n",
        "    return ofeatures\n",
        "    \n",
        "\n",
        "def print_pred_movies(y_p, item, movie_dict, maxcount=10):\n",
        "    \"\"\" print results of prediction of a new user. inputs are expected to be in\n",
        "        sorted order, unscaled. \"\"\"\n",
        "    count = 0\n",
        "    disp = [[\"y_p\", \"movie id\", \"rating ave\", \"title\", \"genres\"]]\n",
        "\n",
        "    for i in range(0, y_p.shape[0]):\n",
        "        if count == maxcount:\n",
        "            break\n",
        "        count += 1\n",
        "        movie_id = item[i, 0].astype(int)\n",
        "        disp.append([np.around(y_p[i, 0], 1), item[i, 0].astype(int), np.around(item[i, 2].astype(float), 1),\n",
        "                     movie_dict[movie_id]['title'], movie_dict[movie_id]['genres']])\n",
        "\n",
        "    table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "    return table\n",
        "\n",
        "def gen_user_vecs(user_vec, num_items):\n",
        "    \"\"\" given a user vector return:\n",
        "        user predict maxtrix to match the size of item_vecs \"\"\"\n",
        "    user_vecs = np.tile(user_vec, (num_items, 1))\n",
        "    return user_vecs\n",
        "\n",
        "# predict on  everything, filter on print/use\n",
        "def predict_uservec(user_vecs, item_vecs, model, u_s, i_s, scaler):\n",
        "    \"\"\" given a scaled user vector, does the prediction on all movies in scaled print_item_vecs returns\n",
        "        an array predictions sorted by predicted rating,\n",
        "        arrays of user and item, sorted by predicted rating sorting index\n",
        "    \"\"\"\n",
        "    y_p = model.predict([user_vecs[:, u_s:], item_vecs[:, i_s:]])\n",
        "    y_pu = scaler.inverse_transform(y_p)\n",
        "\n",
        "    if np.any(y_pu < 0):\n",
        "        print(\"Error, expected all positive predictions\")\n",
        "    sorted_index = np.argsort(-y_pu, axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "    sorted_ypu   = y_pu[sorted_index]\n",
        "    sorted_items = item_vecs[sorted_index]\n",
        "    sorted_user  = user_vecs[sorted_index]\n",
        "    return(sorted_index, sorted_ypu, sorted_items, sorted_user)\n",
        "                \n",
        "def get_user_vecs(user_id, user_train, item_vecs, user_to_genre):\n",
        "    \"\"\" given a user_id, return:\n",
        "        user train/predict matrix to match the size of item_vecs\n",
        "        y vector with ratings for all rated movies and 0 for others of size item_vecs \"\"\"\n",
        "\n",
        "    if not user_id in user_to_genre:\n",
        "        print(\"error: unknown user id\")\n",
        "        return None\n",
        "    else:\n",
        "        user_vec_found = False\n",
        "        for i in range(len(user_train)):\n",
        "            if user_train[i, 0] == user_id:\n",
        "                user_vec = user_train[i]\n",
        "                user_vec_found = True\n",
        "                break\n",
        "        if not user_vec_found:\n",
        "            print(\"error in get_user_vecs, did not find uid in user_train\")\n",
        "        num_items = len(item_vecs)\n",
        "        user_vecs = np.tile(user_vec, (num_items, 1))\n",
        "\n",
        "        y = np.zeros(num_items)\n",
        "        for i in range(num_items):  # walk through movies in item_vecs and get the movies, see if user has rated them\n",
        "            movie_id = item_vecs[i, 0]\n",
        "            if movie_id in user_to_genre[user_id]['movies']:\n",
        "                rating = user_to_genre[user_id]['movies'][movie_id]\n",
        "            else:\n",
        "                rating = 0\n",
        "            y[i] = rating\n",
        "    return(user_vecs, y)\n",
        "\n",
        "def get_item_genres(item_gvec, genre_features):\n",
        "    ''' takes in the item's genre vector and list of genre names\n",
        "    returns the feature names where gvec was 1 '''\n",
        "    offsets = np.nonzero(item_gvec)[0]\n",
        "    genres = [genre_features[i] for i in offsets]\n",
        "    return genres\n",
        "\n",
        "\n",
        "def print_existing_user(y_p, y, user, items, ivs, uvs, movie_dict, maxcount=10):\n",
        "    \"\"\" print results of prediction for a user who was in the database.\n",
        "        Inputs are expected to be in sorted order, unscaled.\n",
        "    \"\"\"\n",
        "    count = 0\n",
        "    disp = [[\"y_p\", \"y\", \"user\", \"user genre ave\", \"movie rating ave\", \"movie id\", \"title\", \"genres\"]]\n",
        "    count = 0\n",
        "    for i in range(0, y.shape[0]):\n",
        "        if y[i, 0] != 0:  # zero means not rated\n",
        "            if count == maxcount:\n",
        "                break\n",
        "            count += 1\n",
        "            movie_id = items[i, 0].astype(int)\n",
        "\n",
        "            offsets = np.nonzero(items[i, ivs:] == 1)[0]\n",
        "            genre_ratings = user[i, uvs + offsets]\n",
        "            disp.append([y_p[i, 0], y[i, 0],\n",
        "                         user[i, 0].astype(int),      # userid\n",
        "                         np.array2string(genre_ratings, \n",
        "                                         formatter={'float_kind':lambda x: \"%.1f\" % x},\n",
        "                                         separator=',', suppress_small=True),\n",
        "                         items[i, 2].astype(float),    # movie average rating\n",
        "                         movie_id,\n",
        "                         movie_dict[movie_id]['title'],\n",
        "                         movie_dict[movie_id]['genres']])\n",
        "\n",
        "    table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\", floatfmt=[\".1f\", \".1f\", \".0f\", \".2f\", \".1f\"])\n",
        "    return table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb0ZlCq1l0WR"
      },
      "outputs": [],
      "source": [
        "top10_df = pd.read_csv(\"./content_top10_df.csv\")\n",
        "bygenre_df = pd.read_csv(\"./content_bygenre_df.csv\")\n",
        "top10_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axmnsHrwl0WU"
      },
      "outputs": [],
      "source": [
        "bygenre_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5gfMLYgxCD1"
      },
      "outputs": [],
      "source": [
        "# Load Data, set configuration variables\n",
        "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
        "\n",
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
        "uvs = 3  # user genre vector start\n",
        "ivs = 3  # item genre vector start\n",
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 1  # start of columns to use in training, items\n",
        "print(f\"Number of training vectors: {len(item_train)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yibh8bp9l0Wa"
      },
      "outputs": [],
      "source": [
        "pprint_train(user_train, user_features, uvs,  u_s, maxcount=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgcTwG4Kl0Wc"
      },
      "outputs": [],
      "source": [
        "pprint_train(item_train, item_features, ivs, i_s, maxcount=5, user=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWRRqXIel0We"
      },
      "outputs": [],
      "source": [
        "print(f\"y_train[:5]: {y_train[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8j4_3Ivxl0Wg"
      },
      "outputs": [],
      "source": [
        "# scale training data\n",
        "item_train_unscaled = item_train\n",
        "user_train_unscaled = user_train\n",
        "y_train_unscaled    = y_train\n",
        "\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
        "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCxItU3Ul0Wh"
      },
      "outputs": [],
      "source": [
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
        "print(f\"movie/item training data shape: {item_train.shape}\")\n",
        "print(f\"movie/item test data shape: {item_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2oALgxol0Wi"
      },
      "outputs": [],
      "source": [
        "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBjZ2HhRwpa0"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_outputs = 32\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([   \n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(num_outputs),\n",
        "])\n",
        "\n",
        "# create the user input and point to the base network\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "# create the item input and point to the base network\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
        "\n",
        "# compute the dot product of the two vectors vu and vm\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGK5MEUowxN4"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(1)\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=cost_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zHf7eASw0tN"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(1)\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xf9Z_xNYl0Wl"
      },
      "outputs": [],
      "source": [
        "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_7nZyPiVJ4r"
      },
      "outputs": [],
      "source": [
        "new_user_id = 5000\n",
        "new_rating_ave = 0.0\n",
        "new_action = 0.0\n",
        "new_adventure = 5.0\n",
        "new_animation = 0.0\n",
        "new_childrens = 0.0\n",
        "new_comedy = 0.0\n",
        "new_crime = 0.0\n",
        "new_documentary = 0.0\n",
        "new_drama = 0.0\n",
        "new_fantasy = 5.0\n",
        "new_horror = 0.0\n",
        "new_mystery = 0.0\n",
        "new_romance = 0.0\n",
        "new_scifi = 0.0\n",
        "new_thriller = 0.0\n",
        "new_rating_count = 3\n",
        "\n",
        "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
        "                      new_action, new_adventure, new_animation, new_childrens,\n",
        "                      new_comedy, new_crime, new_documentary,\n",
        "                      new_drama, new_fantasy, new_horror, new_mystery,\n",
        "                      new_romance, new_scifi, new_thriller]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaPbxVtul0Wm"
      },
      "outputs": [],
      "source": [
        "# generate and replicate the user vector to match the number movies in the data set.\n",
        "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "\n",
        "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIcNhoEHl0Wn"
      },
      "outputs": [],
      "source": [
        "uid = 2 \n",
        "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
        "user_vecs, y_vecs = get_user_vecs(uid, user_train_unscaled, item_vecs, user_to_genre)\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sitem_vecs = scalerItem.transform(item_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction \n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display\n",
        "sorted_user  = user_vecs[sorted_index]\n",
        "sorted_y     = y_vecs[sorted_index]\n",
        "\n",
        "#print sorted predictions for movies rated by the user\n",
        "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, ivs, uvs, movie_dict, maxcount = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySEnf3A1l0Wo"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sq_dist(a,b):\n",
        "    \"\"\"\n",
        "    Returns the squared distance between two vectors\n",
        "    Args:\n",
        "      a (ndarray (n,)): vector with n features\n",
        "      b (ndarray (n,)): vector with n features\n",
        "    Returns:\n",
        "      d (float) : distance\n",
        "    \"\"\"\n",
        "    d =  np.sum(np.square(a-b))  \n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik2otjPHl0Wo"
      },
      "outputs": [],
      "source": [
        "a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
        "a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
        "a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
        "print(f\"squared distance between a1 and b1: {sq_dist(a1, b1):0.3f}\")\n",
        "print(f\"squared distance between a2 and b2: {sq_dist(a2, b2):0.3f}\")\n",
        "print(f\"squared distance between a3 and b3: {sq_dist(a3, b3):0.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW56v9EGl0Wq"
      },
      "outputs": [],
      "source": [
        "input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
        "vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
        "vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
        "model_m = tf.keras.Model(input_item_m, vm_m)                                \n",
        "model_m.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Lq3Mx0l0Wq"
      },
      "outputs": [],
      "source": [
        "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
        "vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
        "print(f\"size of all predicted movie feature vectors: {vms.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWYlYqk6l0Wr"
      },
      "outputs": [],
      "source": [
        "count = 50  # number of movies to display\n",
        "dim = len(vms)\n",
        "dist = np.zeros((dim,dim))\n",
        "\n",
        "for i in range(dim):\n",
        "    for j in range(dim):\n",
        "        dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
        "        \n",
        "m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal\n",
        "\n",
        "disp = [[\"movie1\", \"genres\", \"movie2\", \"genres\"]]\n",
        "for i in range(count):\n",
        "    min_idx = np.argmin(m_dist[i])\n",
        "    movie1_id = int(item_vecs[i,0])\n",
        "    movie2_id = int(item_vecs[min_idx,0])\n",
        "    disp.append( [movie_dict[movie1_id]['title'], movie_dict[movie1_id]['genres'],\n",
        "                  movie_dict[movie2_id]['title'], movie_dict[movie1_id]['genres']]\n",
        "               )\n",
        "table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "table"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Lzk7iX_CodX6"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
