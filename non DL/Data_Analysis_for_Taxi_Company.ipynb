{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkUIItlQ5Nn2"
      },
      "source": [
        "Project for advanced analysics\n",
        "\n",
        "Data: https://drive.google.com/file/d/1LsnAFkBMBLwbVZ2j6ZuOBcu_tN7K-IBF/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "h8dnkSYpQVcK"
      },
      "outputs": [],
      "source": [
        "# Packages for numerics + dataframes\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Packages for date conversions for calculating trip durations\n",
        "import datetime as dt\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "from datetime import timedelta\n",
        "\n",
        "# Packages for OLS, MLR, confusion matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "\n",
        "# Packages for Random Forest and XGBoost\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score,\\\n",
        "f1_score, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Packages for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from xgboost import plot_importance\n",
        "\n",
        "# Other\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvoElNMkRObi"
      },
      "source": [
        "# 1. Exploratory data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Q-qReuOSFUW"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('./2017_Yellow_Taxi_Trip_Data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz4y1iFCSmd3"
      },
      "outputs": [],
      "source": [
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8233O2OSq-6"
      },
      "outputs": [],
      "source": [
        "df.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PPolIP8StLi"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1xwM32DSvbB"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9CLx0JLTSyN6"
      },
      "outputs": [],
      "source": [
        "# Convert data columns to datetime\n",
        "df['tpep_pickup_datetime']=pd.to_datetime(df['tpep_pickup_datetime'])\n",
        "df['tpep_dropoff_datetime']=pd.to_datetime(df['tpep_dropoff_datetime'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2cz_B8uS23Q"
      },
      "outputs": [],
      "source": [
        "# Create box plot of trip_distance\n",
        "plt.figure(figsize=(7,2))\n",
        "plt.title('trip_distance')\n",
        "sns.boxplot(data=None, x=df['trip_distance'], fliersize=1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvLkbWWMS7IU"
      },
      "outputs": [],
      "source": [
        "# Create histogram of trip_distance\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['trip_distance'], bins=range(0,26,1))\n",
        "plt.title('Trip distance histogram');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3uA28Z6S8F4"
      },
      "outputs": [],
      "source": [
        "# Create box plot of total_amount\n",
        "plt.figure(figsize=(7,2))\n",
        "plt.title('total_amount')\n",
        "sns.boxplot(x=df['total_amount'], fliersize=1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLtMTQFzS-8u"
      },
      "outputs": [],
      "source": [
        "# Create histogram of total_amount\n",
        "plt.figure(figsize=(12,6))\n",
        "ax = sns.histplot(df['total_amount'], bins=range(-10,101,5))\n",
        "ax.set_xticks(range(-10,101,5))\n",
        "ax.set_xticklabels(range(-10,101,5))\n",
        "plt.title('Total amount histogram');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxyoVtf4TCeq"
      },
      "outputs": [],
      "source": [
        "# Create box plot of tip_amount\n",
        "plt.figure(figsize=(7,2))\n",
        "plt.title('tip_amount')\n",
        "sns.boxplot(x=df['tip_amount'], fliersize=1);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_rMzwsMTGBG"
      },
      "outputs": [],
      "source": [
        "# Create histogram of tip_amount\n",
        "plt.figure(figsize=(12,6))\n",
        "ax = sns.histplot(df['tip_amount'], bins=range(0,21,1))\n",
        "ax.set_xticks(range(0,21,2))\n",
        "ax.set_xticklabels(range(0,21,2))\n",
        "plt.title('Tip amount histogram');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQWGIgClTK_B"
      },
      "outputs": [],
      "source": [
        "# Create histogram of tip_amount by vendor\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.histplot(data=df, x='tip_amount', bins=range(0,21,1),\n",
        "                  hue='VendorID',\n",
        "                  multiple='stack',\n",
        "                  palette='pastel')\n",
        "ax.set_xticks(range(0,21,1))\n",
        "ax.set_xticklabels(range(0,21,1))\n",
        "plt.title('Tip amount by vendor histogram');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeD3_GM7TMZ9"
      },
      "outputs": [],
      "source": [
        "# Create histogram of tip_amount by vendor for tips > $10\n",
        "tips_over_ten = df[df['tip_amount'] > 10]\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.histplot(data=tips_over_ten, x='tip_amount', bins=range(10,21,1),\n",
        "                  hue='VendorID',\n",
        "                  multiple='stack',\n",
        "                  palette='pastel')\n",
        "ax.set_xticks(range(10,21,1))\n",
        "ax.set_xticklabels(range(10,21,1))\n",
        "plt.title('Tip amount by vendor histogram');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJOZ3yhaTQMb"
      },
      "outputs": [],
      "source": [
        "df['passenger_count'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NughIVQPTSx_"
      },
      "outputs": [],
      "source": [
        "# Calculate mean tips by passenger_count\n",
        "mean_tips_by_passenger_count = df.groupby(['passenger_count']).mean()[['tip_amount']]\n",
        "mean_tips_by_passenger_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfsPe_bETViS"
      },
      "outputs": [],
      "source": [
        "# Create bar plot for mean tips by passenger count\n",
        "data = mean_tips_by_passenger_count.tail(-1)\n",
        "pal = sns.color_palette(\"Blues_d\", len(data))\n",
        "rank = data['tip_amount'].argsort().argsort()\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.barplot(x=data.index,\n",
        "            y=data['tip_amount'],\n",
        "            hue=data.index,\n",
        "            legend=False,\n",
        "            palette=list(np.array(pal[::-1])[rank]))\n",
        "ax.axhline(df['tip_amount'].mean(), ls='--', color='red', label='global mean')\n",
        "ax.legend()\n",
        "plt.title('Mean tip amount by passenger count', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Z8LW4zVRTYRE"
      },
      "outputs": [],
      "source": [
        "# Create a month column\n",
        "df['month'] = df['tpep_pickup_datetime'].dt.month_name()\n",
        "# Create a day column\n",
        "df['day'] = df['tpep_pickup_datetime'].dt.day_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnCjnUwLTp26"
      },
      "outputs": [],
      "source": [
        "# Get total number of rides for each month\n",
        "monthly_rides = df['month'].value_counts()\n",
        "monthly_rides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zleZNyMTtIo"
      },
      "outputs": [],
      "source": [
        "# Reorder the monthly ride list so months go in order\n",
        "month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\n",
        "         'August', 'September', 'October', 'November', 'December']\n",
        "\n",
        "monthly_rides = monthly_rides.reindex(index=month_order)\n",
        "monthly_rides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rugvl-0TwL6"
      },
      "outputs": [],
      "source": [
        "monthly_rides.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5KxB_uZUCAC"
      },
      "outputs": [],
      "source": [
        "# Create a bar plot of total rides per month\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.barplot(x=monthly_rides.index, y=monthly_rides)\n",
        "ax.set_xticklabels(month_order)\n",
        "plt.title('Ride count by month', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INi6mTTuUEjT"
      },
      "outputs": [],
      "source": [
        "# Repeat the above process, this time for rides by day\n",
        "daily_rides = df['day'].value_counts()\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "daily_rides = daily_rides.reindex(index=day_order)\n",
        "daily_rides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntkFun_-UHqJ"
      },
      "outputs": [],
      "source": [
        "# Create bar plot for ride count by day\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.barplot(x=daily_rides.index, y=daily_rides)\n",
        "ax.set_xticklabels(day_order)\n",
        "ax.set_ylabel('Count')\n",
        "plt.title('Ride count by day', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NY1DotMUKRX"
      },
      "outputs": [],
      "source": [
        "# Repeat the process, this time for total revenue by day\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "total_amount_day = df.groupby('day').sum()[['total_amount']]\n",
        "total_amount_day = total_amount_day.reindex(index=day_order)\n",
        "total_amount_day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boondtkqUOBs"
      },
      "outputs": [],
      "source": [
        "# Create bar plot of total revenue by day\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.barplot(x=total_amount_day.index, y=total_amount_day['total_amount'])\n",
        "ax.set_xticklabels(day_order)\n",
        "ax.set_ylabel('Revenue (USD)')\n",
        "plt.title('Total revenue by day', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLG3vNM2UQ26"
      },
      "outputs": [],
      "source": [
        "# Repeat the process, this time for total revenue by month\n",
        "total_amount_month = df.groupby('month').sum()[['total_amount']]\n",
        "total_amount_month = total_amount_month.reindex(index=month_order)\n",
        "total_amount_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut6NifliUUHw"
      },
      "outputs": [],
      "source": [
        "# Create a bar plot of total revenue by month\n",
        "plt.figure(figsize=(12,7))\n",
        "ax = sns.barplot(x=total_amount_month.index, y=total_amount_month['total_amount'])\n",
        "plt.title('Total revenue by month', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3KEKZYxUWIg"
      },
      "outputs": [],
      "source": [
        "# Get number of unique drop-off location IDs\n",
        "df['DOLocationID'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucCOqrpdUZbp"
      },
      "outputs": [],
      "source": [
        "# Calculate the mean trip distance for each drop-off location\n",
        "distance_by_dropoff = df.groupby('DOLocationID').mean()[['trip_distance']]\n",
        "\n",
        "# Sort the results in descending order by mean trip distance\n",
        "distance_by_dropoff = distance_by_dropoff.sort_values(by='trip_distance')\n",
        "distance_by_dropoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr_FlpUZUcAp"
      },
      "outputs": [],
      "source": [
        "# Create a bar plot of mean trip distances by drop-off location in ascending order by distance\n",
        "plt.figure(figsize=(14,6))\n",
        "ax = sns.barplot(x=distance_by_dropoff.index,\n",
        "                 y=distance_by_dropoff['trip_distance'],\n",
        "                 order=distance_by_dropoff.index)\n",
        "ax.set_xticklabels([])\n",
        "ax.set_xticks([])\n",
        "plt.title('Mean trip distance by drop-off location', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpxzNuVkUedm"
      },
      "outputs": [],
      "source": [
        "# 1. Generate random points on a 2D plane from a normal distribution\n",
        "test = np.round(np.random.normal(10, 5, (3000, 2)), 1)\n",
        "midway = int(len(test)/2)  # Calculate midpoint of the array of coordinates\n",
        "start = test[:midway]      # Isolate first half of array (\"pick-up locations\")\n",
        "end = test[midway:]        # Isolate second half of array (\"drop-off locations\")\n",
        "\n",
        "# 2. Calculate Euclidean distances between points in first half and second half of array\n",
        "distances = (start - end)**2\n",
        "distances = distances.sum(axis=-1)\n",
        "distances = np.sqrt(distances)\n",
        "\n",
        "# 3. Group the coordinates by \"drop-off location\", compute mean distance\n",
        "test_df = pd.DataFrame({'start': [tuple(x) for x in start.tolist()],\n",
        "                   'end': [tuple(x) for x in end.tolist()],\n",
        "                   'distance': distances})\n",
        "data = test_df[['end', 'distance']].groupby('end').mean().reset_index()\n",
        "data = data.sort_values(by='distance')\n",
        "\n",
        "# 4. Plot the mean distance between each endpoint (\"drop-off location\") and all points it connected to\n",
        "plt.figure(figsize=(14,6))\n",
        "ax = sns.barplot(x=data.index,\n",
        "                 y=data['distance'],\n",
        "                 order=data.index)\n",
        "ax.set_xticklabels([])\n",
        "ax.set_xticks([])\n",
        "ax.set_xlabel('Endpoint')\n",
        "ax.set_ylabel('Mean distance to all other points')\n",
        "ax.set_title('Mean distance between points taken randomly from normal distribution');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw9jOqf5Uk7H"
      },
      "outputs": [],
      "source": [
        "# Check if all drop-off locations are consecutively numbered\n",
        "df['DOLocationID'].max() - len(set(df['DOLocationID']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbUq_9bXUlsw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,4))\n",
        "# DOLocationID column is numeric, so sort in ascending order\n",
        "sorted_dropoffs = df['DOLocationID'].sort_values()\n",
        "# Convert to string\n",
        "sorted_dropoffs = sorted_dropoffs.astype('str')\n",
        "# Plot\n",
        "sns.histplot(sorted_dropoffs, bins=range(0, df['DOLocationID'].max()+1, 1))\n",
        "plt.xticks([])\n",
        "plt.xlabel('Drop-off locations')\n",
        "plt.title('Histogram of rides by drop-off location', fontsize=16);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaHY6DuaUoNF"
      },
      "outputs": [],
      "source": [
        "df['trip_duration'] = (df['tpep_dropoff_datetime']-df['tpep_pickup_datetime'])\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBlSyoTZZmgx"
      },
      "source": [
        "# 2. A/B and hypothesis testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Ew4c-0x1Z2u4"
      },
      "outputs": [],
      "source": [
        "taxi_data = pd.read_csv(\"./2017_Yellow_Taxi_Trip_Data.csv\", index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--bJarazabyD"
      },
      "outputs": [],
      "source": [
        "# descriptive stats code for EDA\n",
        "taxi_data.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr7Kyox1adND"
      },
      "outputs": [],
      "source": [
        "taxi_data.groupby('payment_type')['fare_amount'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "956pPYgOa2PN"
      },
      "source": [
        "goal in this step is to conduct a two-sample t-test.\n",
        "\n",
        "1.   State the null hypothesis and the alternative hypothesis\n",
        "2.   Choose a signficance level\n",
        "3.   Find the p-value\n",
        "4.   Reject or fail to reject the null hypothesis\n",
        "\n",
        "$H_0$: There is no difference in the average fare amount between customers who use credit cards and customers who use cash.\n",
        "\n",
        "$H_A$: There is a difference in the average fare amount between customers who use credit cards and customers who use cash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ4AEG28afyK",
        "outputId": "c5075b7b-ff43-4b9f-a356-2f610fdbdf1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TtestResult(statistic=6.866800855655372, pvalue=6.797387473030518e-12, df=16675.48547403633)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#hypothesis test, A/B test\n",
        "#significance level\n",
        "\n",
        "credit_card = taxi_data[taxi_data['payment_type'] == 1]['fare_amount']\n",
        "cash = taxi_data[taxi_data['payment_type'] == 2]['fare_amount']\n",
        "stats.ttest_ind(a=credit_card, b=cash, equal_var=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih8mUd77bWZt"
      },
      "source": [
        "Since p value smaller than significance level 0.05 we reject $H_0$\n",
        "\n",
        "1.   The key business insight is that encouraging customers to pay with credit cards can generate more revenue for taxi cab drivers.\n",
        "\n",
        "2.   This project requires an assumption that passengers were forced to pay one way or the other, and that once informed of this requirement, they always complied with it. The data was not collected this way; so, an assumption had to be made to randomly group data entries to perform an A/B test. This dataset does not account for other likely explanations. For example, riders might not carry lots of cash, so it's easier to pay for longer/farther trips with a credit card. In other words, it's far more likely that fare amount determines payment type, rather than vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSQskg9XcNpn"
      },
      "source": [
        "# 3. Multiple linear regression for predicting the taxi fare amounts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpKQ-Z0-ppPQ"
      },
      "source": [
        "Please Note:\n",
        "\n",
        "To keep things in this notebook not repetitive and simple there are some things that differ from best practice or from how tasks are typically performed.\n",
        "\n",
        "1.  When the `mean_distance` and `mean_duration` columns were computed, the means were calculated from the entire dataset. These same columns were then used to train a model that was used to predict on a test set. A test set is supposed to represent entirely new data that the model has not seen before, but in this case, some of its predictor variables were derived using data that *was* in the test set.</br></br>\n",
        "This is known as **data leakage**. Data leakage is when information from your training data contaminates the test data. If your model has unexpectedly high scores, there is a good chance that there was some data leakage.\n",
        "</br></br>\n",
        "To avoid data leakage in this modeling process, it would be best to compute the means using only the training set and then copy those into the test set, thus preventing values from the test set from being included in the computation of the means. This would have created some problems because it's very likely that some combinations of pickup-dropoff locations would only appear in the test data (not the train data). This means that there would be NaNs in the test data, and further steps would be required to address this.\n",
        "</br></br>\n",
        "In this case, the data leakage improved the R<sup>2</sup> score by ~0.03.\n",
        "</br></br>\n",
        "2. Imputing the fare amount for `RatecodeID 2` after training the model and then calculating model performance metrics on the post-imputed data is not best practice. It would be better to separate the rides that did *not* have rate codes of 2, train the model on that data specifically, and then add the `RatecodeID 2` data (and its imputed rates) *after*. This would prevent training the model on data that you don't need a model for, and would likely result in a better final model. However, the steps were combined for simplicity.\n",
        "</br></br>\n",
        "3. Models that predict values to be used in another downstream model are common in data science workflows. When models are deployed, the data cleaning, imputations, splits, predictions, etc. are done using modeling pipelines. Pandas was used here to granularize and explain the concepts of certain steps, but this process would be streamlined by machine learning engineers. The ideas are the same, but the implementation would differ. Once a modeling workflow has been validated, the entire process can be automated, often with no need for pandas and no need to examine outputs at each step. This entire process would be reduced to a page of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyFVkmZldIcH"
      },
      "outputs": [],
      "source": [
        "# Keep `df0` as the original dataframe and create a copy (df) where changes will go\n",
        "# Can revert `df` to `df0` if needed down the line\n",
        "df0 = df.copy()\n",
        "\n",
        "# Display the dataset's shape\n",
        "print(df.shape)\n",
        "\n",
        "# Display basic info about the dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLBa6B6Ve3I1"
      },
      "outputs": [],
      "source": [
        "# Check for missing data and duplicates using .isna() and .drop_duplicates()\n",
        "# Check for duplicates\n",
        "print('Shape of dataframe:', df.shape)\n",
        "print('Shape of dataframe with duplicates dropped:', df.drop_duplicates().shape)\n",
        "\n",
        "# Check for missing values in dataframe\n",
        "print('Total count of missing values:', df.isna().sum().sum())\n",
        "\n",
        "# Display missing values per column in dataframe\n",
        "print('Missing values per column:')\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMoqjHrAe6g5"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1dvUHVnfBxG"
      },
      "outputs": [],
      "source": [
        "# Check the format of the data\n",
        "df['tpep_dropoff_datetime'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOjhrH9DfDip"
      },
      "outputs": [],
      "source": [
        "# Convert datetime columns to datetime\n",
        "# Display data types of `tpep_pickup_datetime`, `tpep_dropoff_datetime`\n",
        "print('Data type of tpep_pickup_datetime:', df['tpep_pickup_datetime'].dtype)\n",
        "print('Data type of tpep_dropoff_datetime:', df['tpep_dropoff_datetime'].dtype)\n",
        "\n",
        "# Convert `tpep_pickup_datetime` to datetime format\n",
        "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "\n",
        "# Convert `tpep_dropoff_datetime` to datetime format\n",
        "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "\n",
        "# Display data types of `tpep_pickup_datetime`, `tpep_dropoff_datetime`\n",
        "print('Data type of tpep_pickup_datetime:', df['tpep_pickup_datetime'].dtype)\n",
        "print('Data type of tpep_dropoff_datetime:', df['tpep_dropoff_datetime'].dtype)\n",
        "\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ZS0Mxd7ofLI2"
      },
      "outputs": [],
      "source": [
        "# Create `duration` column\n",
        "df['duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime'])/np.timedelta64(1,'m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYE9-iB1fPOx"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiTkwvvMfTTl"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(15, 2))\n",
        "fig.suptitle('Boxplots for outlier detection')\n",
        "sns.boxplot(ax=axes[0], x=df['trip_distance'])\n",
        "sns.boxplot(ax=axes[1], x=df['fare_amount'])\n",
        "sns.boxplot(ax=axes[2], x=df['duration'])\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQuDqYIKfbqt"
      },
      "outputs": [],
      "source": [
        "# Are trip distances of 0 bad data or very short trips rounded down?\n",
        "sorted(set(df['trip_distance']))[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5ji2XLnfflR"
      },
      "outputs": [],
      "source": [
        "sum(df['trip_distance']==0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF74CnARfi5i"
      },
      "outputs": [],
      "source": [
        "df['fare_amount'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_vIhJKQfn7V"
      },
      "outputs": [],
      "source": [
        "# Impute values less than $0 with 0\n",
        "df.loc[df['fare_amount'] < 0, 'fare_amount'] = 0\n",
        "df['fare_amount'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "hOG0ib19frjm"
      },
      "outputs": [],
      "source": [
        "def outlier_imputer(column_list, iqr_factor):\n",
        "    '''\n",
        "    Impute upper-limit values in specified columns based on their interquartile range.\n",
        "\n",
        "    Arguments:\n",
        "        column_list: A list of columns to iterate over\n",
        "        iqr_factor: A number representing x in the formula:\n",
        "                    Q3 + (x * IQR). Used to determine maximum threshold,\n",
        "                    beyond which a point is considered an outlier.\n",
        "\n",
        "    The IQR is computed for each column in column_list and values exceeding\n",
        "    the upper threshold for each column are imputed with the upper threshold value.\n",
        "    '''\n",
        "    for col in column_list:\n",
        "        # Reassign minimum to zero\n",
        "        df.loc[df[col] < 0, col] = 0\n",
        "\n",
        "        # Calculate upper threshold\n",
        "        q1 = df[col].quantile(0.25)\n",
        "        q3 = df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        upper_threshold = q3 + (iqr_factor * iqr)\n",
        "        print(col)\n",
        "        print('q3:', q3)\n",
        "        print('upper_threshold:', upper_threshold)\n",
        "\n",
        "        # Reassign values > threshold to threshold\n",
        "        df.loc[df[col] > upper_threshold, col] = upper_threshold\n",
        "        print(df[col].describe())\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1d3FcUEfvlh"
      },
      "outputs": [],
      "source": [
        "outlier_imputer(['fare_amount'], 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0psEnzX1f2ku"
      },
      "outputs": [],
      "source": [
        "df['duration'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RGeP7GWf7Ux"
      },
      "outputs": [],
      "source": [
        "# Impute a 0 for any negative values\n",
        "df.loc[df['duration'] < 0, 'duration'] = 0\n",
        "df['duration'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHRLDcsMf75v"
      },
      "outputs": [],
      "source": [
        "# Impute the high outliers\n",
        "outlier_imputer(['duration'], 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiCyZ8d0gLxu"
      },
      "source": [
        "#### Create `mean_distance` column\n",
        "\n",
        "When deployed, the model will not know the duration of a trip until after the trip occurs, so you cannot train a model that uses this feature. However, you can use the statistics of trips you *do* know to generalize about ones you do not know.\n",
        "\n",
        "In this step, create a column called `mean_distance` that captures the mean distance for each group of trips that share pickup and dropoff points.\n",
        "\n",
        "For example, if your data were:\n",
        "\n",
        "|Trip|Start|End|Distance|\n",
        "|--: |:---:|:-:|    |\n",
        "| 1  | A   | B | 1  |\n",
        "| 2  | C   | D | 2  |\n",
        "| 3  | A   | B |1.5 |\n",
        "| 4  | D   | C | 3  |\n",
        "\n",
        "The results should be:\n",
        "```\n",
        "A -> B: 1.25 miles\n",
        "C -> D: 2 miles\n",
        "D -> C: 3 miles\n",
        "```\n",
        "\n",
        "Notice that C -> D is not the same as D -> C. All trips that share a unique pair of start and end points get grouped and averaged.\n",
        "\n",
        "Then, a new column `mean_distance` will be added where the value at each row is the average for all trips with those pickup and dropoff locations:\n",
        "\n",
        "|Trip|Start|End|Distance|mean_distance|\n",
        "|--: |:---:|:-:|  :--   |:--   |\n",
        "| 1  | A   | B | 1      | 1.25 |\n",
        "| 2  | C   | D | 2      | 2    |\n",
        "| 3  | A   | B |1.5     | 1.25 |\n",
        "| 4  | D   | C | 3      | 3    |\n",
        "\n",
        "\n",
        "Begin by creating a helper column called `pickup_dropoff`, which contains the unique combination of pickup and dropoff location IDs for each row.\n",
        "\n",
        "One way to do this is to convert the pickup and dropoff location IDs to strings and join them, separated by a space. The space is to ensure that, for example, a trip with pickup/dropoff points of 12 & 151 gets encoded differently than a trip with points 121 & 51.\n",
        "\n",
        "So, the new column would look like this:\n",
        "\n",
        "|Trip|Start|End|pickup_dropoff|\n",
        "|--: |:---:|:-:|  :--         |\n",
        "| 1  | A   | B | 'A B'        |\n",
        "| 2  | C   | D | 'C D'        |\n",
        "| 3  | A   | B | 'A B'        |\n",
        "| 4  | D   | C | 'D C'        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cA3ffb0gASf"
      },
      "outputs": [],
      "source": [
        "# Create `pickup_dropoff` column\n",
        "df['pickup_dropoff'] = df['PULocationID'].astype(str) + ' ' + df['DOLocationID'].astype(str)\n",
        "df['pickup_dropoff'].head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEw9MEmxgImA"
      },
      "outputs": [],
      "source": [
        "grouped = df.groupby('pickup_dropoff').mean(numeric_only=True)[['trip_distance']]\n",
        "grouped[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "OejhXOicgWkX"
      },
      "outputs": [],
      "source": [
        "# 1. Convert `grouped` to a dictionary\n",
        "grouped_dict = grouped.to_dict()\n",
        "\n",
        "# 2. Reassign to only contain the inner dictionary\n",
        "grouped_dict = grouped_dict['trip_distance']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dghbIkIlgf3q"
      },
      "source": [
        "1. Create a `mean_distance` column that is a copy of the `pickup_dropoff` helper column.\n",
        "\n",
        "2. Use the [`map()`](https://pandas.pydata.org/docs/reference/api/pandas.Series.map.html#pandas-series-map) method on the `mean_distance` series. Pass `grouped_dict` as its argument. Reassign the result back to the `mean_distance` series.\n",
        "</br></br>\n",
        "When you pass a dictionary to the `Series.map()` method, it will replace the data in the series where that data matches the dictionary's keys. The values that get imputed are the values of the dictionary.\n",
        "\n",
        "```\n",
        "Example:\n",
        "df['mean_distance']\n",
        "```\n",
        "\n",
        "|mean_distance |\n",
        "|  :-:         |\n",
        "| 'A B'        |\n",
        "| 'C D'        |\n",
        "| 'A B'        |\n",
        "| 'D C'        |\n",
        "| 'E F'        |\n",
        "\n",
        "```\n",
        "grouped_dict = {'A B': 1.25, 'C D': 2, 'D C': 3}\n",
        "df['mean_distance`] = df['mean_distance'].map(grouped_dict)\n",
        "df['mean_distance']\n",
        "```\n",
        "\n",
        "|mean_distance |\n",
        "|  :-:         |\n",
        "| 1.25         |\n",
        "| 2            |\n",
        "| 1.25         |\n",
        "| 3            |\n",
        "| NaN          |\n",
        "\n",
        "When used this way, the `map()` `Series` method is very similar to `replace()`, however, note that `map()` will impute `NaN` for any values in the series that do not have a corresponding key in the mapping dictionary, so be careful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl977gicgbKx"
      },
      "outputs": [],
      "source": [
        "# 1. Create a mean_distance column that is a copy of the pickup_dropoff helper column\n",
        "df['mean_distance'] = df['pickup_dropoff']\n",
        "\n",
        "# 2. Map `grouped_dict` to the `mean_distance` column\n",
        "df['mean_distance'] = df['mean_distance'].map(grouped_dict)\n",
        "\n",
        "# Confirm that it worked\n",
        "df[(df['PULocationID']==100) & (df['DOLocationID']==231)][['mean_distance']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLzT-ri0gnVZ"
      },
      "outputs": [],
      "source": [
        "grouped = df.groupby('pickup_dropoff').mean(numeric_only=True)[['duration']]\n",
        "grouped\n",
        "\n",
        "# Create a dictionary where keys are unique pickup_dropoffs and values are\n",
        "# mean trip duration for all trips with those pickup_dropoff combos\n",
        "grouped_dict = grouped.to_dict()\n",
        "grouped_dict = grouped_dict['duration']\n",
        "\n",
        "df['mean_duration'] = df['pickup_dropoff']\n",
        "df['mean_duration'] = df['mean_duration'].map(grouped_dict)\n",
        "\n",
        "# Confirm that it worked\n",
        "df[(df['PULocationID']==100) & (df['DOLocationID']==231)][['mean_duration']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "vRr36nN5gsJY"
      },
      "outputs": [],
      "source": [
        "# Create 'day' col\n",
        "df['day'] = df['tpep_pickup_datetime'].dt.day_name().str.lower()\n",
        "\n",
        "# Create 'month' col\n",
        "df['month'] = df['tpep_pickup_datetime'].dt.strftime('%b').str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "y8BrWkZZgvNp"
      },
      "outputs": [],
      "source": [
        "# Create 'rush_hour' col\n",
        "df['rush_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
        "\n",
        "# If day is Saturday or Sunday, impute 0 in `rush_hour` column\n",
        "df.loc[df['day'].isin(['saturday', 'sunday']), 'rush_hour'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "_t_ZWDNGgyRn"
      },
      "outputs": [],
      "source": [
        "def rush_hourizer(hour):\n",
        "    if 6 <= hour['rush_hour'] < 10:\n",
        "        val = 1\n",
        "    elif 16 <= hour['rush_hour'] < 20:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np3D3_DUg0rQ"
      },
      "outputs": [],
      "source": [
        "# Apply the `rush_hourizer()` function to the new column\n",
        "df.loc[(df.day != 'saturday') & (df.day != 'sunday'), 'rush_hour'] = df.apply(rush_hourizer, axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_-Tr4Mag3gq"
      },
      "outputs": [],
      "source": [
        "# Create a scatter plot of duration and trip_distance, with a line of best fit\n",
        "sns.set(style='whitegrid')\n",
        "f = plt.figure()\n",
        "f.set_figwidth(5)\n",
        "f.set_figheight(5)\n",
        "sns.regplot(x=df['mean_duration'], y=df['fare_amount'],\n",
        "            scatter_kws={'alpha':0.5, 's':5},\n",
        "            line_kws={'color':'red'})\n",
        "plt.ylim(0, 70)\n",
        "plt.xlim(0, 70)\n",
        "plt.title('Mean duration x fare amount')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDTE2APyg8TN"
      },
      "outputs": [],
      "source": [
        "df[df['fare_amount'] > 50]['fare_amount'].value_counts().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO8OcrXGhCk8"
      },
      "outputs": [],
      "source": [
        "# Set pandas to display all columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "df[df['fare_amount']==52].head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jf2iDLpdhEJ4"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9I-mMoUhHzb"
      },
      "outputs": [],
      "source": [
        "df2 = df.copy()\n",
        "df2 = df2.drop(['Unnamed: 0', 'tpep_dropoff_datetime', 'tpep_pickup_datetime',\n",
        "               'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID',\n",
        "               'payment_type', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
        "               'total_amount', 'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'duration',\n",
        "               'pickup_dropoff', 'day', 'month'\n",
        "               ], axis=1)\n",
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbNylPefhN37"
      },
      "outputs": [],
      "source": [
        "# Create a pairplot to visualize pairwise relationships between variables in the data\n",
        "sns.pairplot(df2[['fare_amount', 'mean_duration', 'mean_distance']],\n",
        "             plot_kws={'alpha':0.4, 'size':5},\n",
        "             );"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OG-cW1DthXbs"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix containing pairwise correlation of columns, using pearson correlation coefficient\n",
        "df2.corr(method='pearson')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MrYnRpuhb2c"
      },
      "outputs": [],
      "source": [
        "# Create correlation heatmap\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(df2.corr(method='pearson', numeric_only=True), annot=True, cmap='Reds')\n",
        "plt.title('Correlation heatmap', fontsize=18)\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BKFMQMBiStU"
      },
      "outputs": [],
      "source": [
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNIAJg4WiWtB"
      },
      "outputs": [],
      "source": [
        "# Remove the target column from the features\n",
        "X = df2.drop(columns=['fare_amount'])\n",
        "\n",
        "# Set y variable\n",
        "y = df2[['fare_amount']]\n",
        "\n",
        "# Display first few rows\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaJYlPcFiaX5"
      },
      "outputs": [],
      "source": [
        "# Convert VendorID to string\n",
        "X['VendorID'] = X['VendorID'].astype(str)\n",
        "\n",
        "# Get dummies\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "9j8pvwjRidfx"
      },
      "outputs": [],
      "source": [
        "# Create training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkU0CLQ7laWV"
      },
      "outputs": [],
      "source": [
        "# We must convert nanosecond to second format only if in our environment StandardScaler().fit(X_train) raises an error.\n",
        "# There are other ways for fixing this issue.\n",
        "print(X_train.dtypes)\n",
        "X_train['trip_duration'] = X_train['trip_duration'].astype('timedelta64[s]')\n",
        "X_test['trip_duration'] = X_test['trip_duration'].astype('timedelta64[s]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiuoRrcMilzE"
      },
      "outputs": [],
      "source": [
        "# Standardize the X variables\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "print('X_train scaled:', X_train_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0z8o0X00nlsq"
      },
      "outputs": [],
      "source": [
        "# Fit your model to the training data\n",
        "lr=LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR4b55DInrMf"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model performance on the training data\n",
        "r_sq = lr.score(X_train_scaled, y_train)\n",
        "print('Coefficient of determination:', r_sq)\n",
        "y_pred_train = lr.predict(X_train_scaled)\n",
        "print('R^2:', r2_score(y_train, y_pred_train))\n",
        "print('MAE:', mean_absolute_error(y_train, y_pred_train))\n",
        "print('MSE:', mean_squared_error(y_train, y_pred_train))\n",
        "print('RMSE:',np.sqrt(mean_squared_error(y_train, y_pred_train)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "csV6iT44nvQI"
      },
      "outputs": [],
      "source": [
        "# Scale the X_test data\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Do60Bp42n-eq"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model performance on the testing data\n",
        "r_sq_test = lr.score(X_test_scaled, y_test)\n",
        "print('Coefficient of determination:', r_sq_test)\n",
        "y_pred_test = lr.predict(X_test_scaled)\n",
        "print('R^2:', r2_score(y_test, y_pred_test))\n",
        "print('MAE:', mean_absolute_error(y_test,y_pred_test))\n",
        "print('MSE:', mean_squared_error(y_test, y_pred_test))\n",
        "print('RMSE:',np.sqrt(mean_squared_error(y_test, y_pred_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8n4w9TCoByl"
      },
      "outputs": [],
      "source": [
        "# Create a `results` dataframe\n",
        "results = pd.DataFrame(data={'actual': y_test['fare_amount'],\n",
        "                             'predicted': y_pred_test.ravel()})\n",
        "results['residual'] = results['actual'] - results['predicted']\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBea6O82oH6I"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot to visualize `predicted` over `actual`\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "sns.set(style='whitegrid')\n",
        "sns.scatterplot(x='actual',\n",
        "                y='predicted',\n",
        "                data=results,\n",
        "                s=20,\n",
        "                alpha=0.5,\n",
        "                ax=ax\n",
        ")\n",
        "# Draw an x=y line to show what the results would be if the model were perfect\n",
        "plt.plot([0,60], [0,60], c='red', linewidth=2)\n",
        "plt.title('Actual vs. predicted');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98zEzv1foOnk"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of the `residuals`\n",
        "sns.histplot(results['residual'], bins=np.arange(-15,15.5,0.5))\n",
        "plt.title('Distribution of the residuals')\n",
        "plt.xlabel('residual value')\n",
        "plt.ylabel('count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtjc_DkXoS9S"
      },
      "outputs": [],
      "source": [
        "results['residual'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNYyGjS2oUx5"
      },
      "outputs": [],
      "source": [
        "# Create a scatterplot of `residuals` over `predicted`\n",
        "\n",
        "sns.scatterplot(x='predicted', y='residual', data=results)\n",
        "plt.axhline(0, c='red')\n",
        "plt.title('Scatterplot of residuals over predicted values')\n",
        "plt.xlabel('predicted value')\n",
        "plt.ylabel('residual value')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7ES1_11oZU8"
      },
      "outputs": [],
      "source": [
        "# Get model coefficients\n",
        "coefficients = pd.DataFrame(lr.coef_, columns=X.columns)\n",
        "coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yDPPyUPogfJ"
      },
      "outputs": [],
      "source": [
        "# 1. Calculate SD of `mean_distance` in X_train data\n",
        "print(X_train['mean_distance'].std())\n",
        "\n",
        "# 2. Divide the model coefficient by the standard deviation\n",
        "print(7.133867 / X_train['mean_distance'].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "WD1wDxI0olBy"
      },
      "outputs": [],
      "source": [
        "# Now predict on full dataset\n",
        "\n",
        "X['trip_duration'] = X['trip_duration'].astype('timedelta64[s]')\n",
        "X_scaled = scaler.transform(X)\n",
        "y_preds_full = lr.predict(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqBM4iDKpMTT"
      },
      "outputs": [],
      "source": [
        "# Create a new df containing just the RatecodeID col from the whole dataset\n",
        "final_preds = df[['RatecodeID']].copy()\n",
        "\n",
        "# Add a column containing all the predictions\n",
        "final_preds['y_preds_full'] = y_preds_full\n",
        "\n",
        "# Impute a prediction of 52 at all rows where RatecodeID == 2\n",
        "final_preds.loc[final_preds['RatecodeID']==2, 'y_preds_full'] = 52\n",
        "\n",
        "# Check that it worked\n",
        "final_preds[final_preds['RatecodeID']==2].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUpf-Oj4pX3z"
      },
      "outputs": [],
      "source": [
        "final_preds = final_preds['y_preds_full']\n",
        "print('R^2:', r2_score(y, final_preds))\n",
        "print('MAE:', mean_absolute_error(y, final_preds))\n",
        "print('MSE:', mean_squared_error(y, final_preds))\n",
        "print('RMSE:',np.sqrt(mean_squared_error(y, final_preds)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef3XPP-Lpcn1"
      },
      "outputs": [],
      "source": [
        "# Combine means columns with predictions column\n",
        "nyc_preds_means = df[['mean_duration', 'mean_distance']].copy()\n",
        "nyc_preds_means['predicted_fare'] = final_preds\n",
        "\n",
        "nyc_preds_means.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "2FkKVADcwK3R"
      },
      "outputs": [],
      "source": [
        "nyc_preds_means.to_csv('nyc_preds_means.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li4KLodJro5J"
      },
      "source": [
        "# 4. predict if a customer will not leave a tip - Random Forest and Extreme Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "2jGmaBIBtH3f"
      },
      "outputs": [],
      "source": [
        "# This lets us see all of the columns, preventing Juptyer notebook from redacting them.\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqAeq4suuuLY"
      },
      "outputs": [],
      "source": [
        "nyc_preds_means = pd.read_csv('nyc_preds_means.csv')\n",
        "nyc_preds_means.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AVgoHxkw1e4"
      },
      "outputs": [],
      "source": [
        "df0 = pd.read_csv('./2017_Yellow_Taxi_Trip_Data.csv')\n",
        "df0.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kh_hqA-xEUp"
      },
      "outputs": [],
      "source": [
        "# Merge datasets\n",
        "df0 = df0.merge(nyc_preds_means,\n",
        "                left_index=True,\n",
        "                right_index=True)\n",
        "\n",
        "df0.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrCuva5ExNBf"
      },
      "outputs": [],
      "source": [
        "df0.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "rkBlZB7jxQ-J"
      },
      "outputs": [],
      "source": [
        "# Subset the data to isolate only customers who paid by credit card\n",
        "df1 = df0[df0['payment_type']==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZN7d0CFyNUm"
      },
      "source": [
        "Notice that there isn't a column that indicates tip percent, which is what you need to create the target variable. You'll have to engineer it.\n",
        "\n",
        "Add a `tip_percent` column to the dataframe by performing the following calculation:  \n",
        "<br/>  \n",
        "\n",
        "\n",
        "$$tip\\ percent = \\frac{tip\\ amount}{total\\ amount - tip\\ amount}$$  \n",
        "\n",
        "Round the result to three places beyond the decimal. **This is an important step.** It affects how many customers are labeled as generous tippers. In fact, without performing this step, approximately 1,800 people who do tip  20% would be labeled as not generous.\n",
        "\n",
        "To understand why, you must consider how floats work. Computers make their calculations using floating-point arithmetic (hence the word \"float\"). Floating-point arithmetic is a system that allows computers to express both very large numbers and very small numbers with a high degree of precision, encoded in binary. However, precision is limited by the number of bits used to represent a number, which is generally 32 or 64, depending on the capabilities of your operating system.\n",
        "\n",
        "This comes with limitations in that sometimes calculations that should result in clean, precise values end up being encoded as very long decimals.\n",
        "e.g. 1.1 + 2.2 gives 3.3000000000000003\n",
        "\n",
        "Notice the three that is 16 places to the right of the decimal. As a consequence, if you were to then have a step in your code that identifies values  3.3, this would not be included in the result. Therefore, whenever you perform a calculation to compute a number that is then used to make an important decision or filtration, round the number. How many degrees of precision you round to is your decision, which should be based on your use case.\n",
        "\n",
        "Refer to this [guide for more information related to floating-point arithmetic](https://floating-point-gui.de/formats/fp/).  \n",
        "Refer to this [guide for more information related to fixed-point arithmetic](https://inst.eecs.berkeley.edu/~cs61c/sp06/handout/fixedpt.html), which is an alternative to floating-point arithmetic used in certain cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtAxxbAvyMjk"
      },
      "outputs": [],
      "source": [
        "# Create tip % col\n",
        "df1['tip_percent'] = round(df1['tip_amount'] / (df1['total_amount'] - df1['tip_amount']), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdwbxrNPx4ac"
      },
      "outputs": [],
      "source": [
        "# Create 'generous' col (target)\n",
        "df1['generous'] = df1['tip_percent']\n",
        "df1['generous'] = (df1['generous'] >= 0.2)\n",
        "df1['generous'] = df1['generous'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n95_3DnAyqaa"
      },
      "outputs": [],
      "source": [
        "# Convert pickup and dropoff cols to datetime\n",
        "df1['tpep_pickup_datetime'] = pd.to_datetime(df1['tpep_pickup_datetime'], format='%m/%d/%Y %I:%M:%S %p')\n",
        "df1['tpep_dropoff_datetime'] = pd.to_datetime(df1['tpep_dropoff_datetime'], format='%m/%d/%Y %I:%M:%S %p')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3e-RoIhytTe"
      },
      "outputs": [],
      "source": [
        "# Create a 'day' col\n",
        "df1['day'] = df1['tpep_pickup_datetime'].dt.day_name().str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHEoDitmyyrJ"
      },
      "outputs": [],
      "source": [
        "# Create 'am_rush' col\n",
        "df1['am_rush'] = df1['tpep_pickup_datetime'].dt.hour\n",
        "\n",
        "# Create 'daytime' col\n",
        "df1['daytime'] = df1['tpep_pickup_datetime'].dt.hour\n",
        "\n",
        "# Create 'pm_rush' col\n",
        "df1['pm_rush'] = df1['tpep_pickup_datetime'].dt.hour\n",
        "\n",
        "# Create 'nighttime' col\n",
        "df1['nighttime'] = df1['tpep_pickup_datetime'].dt.hour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "uUGy46wpy2BT"
      },
      "outputs": [],
      "source": [
        "# Define 'am_rush()' conversion function [06:0010:00)\n",
        "def am_rush(hour):\n",
        "    if 6 <= hour['am_rush'] < 10:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNbc0mlny4xM"
      },
      "outputs": [],
      "source": [
        "# Apply 'am_rush' function to the 'am_rush' series\n",
        "df1['am_rush'] = df1.apply(am_rush, axis=1)\n",
        "df1['am_rush'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "S2RP_f3EzZUH"
      },
      "outputs": [],
      "source": [
        "# Define 'daytime()' conversion function [10:0016:00)\n",
        "def daytime(hour):\n",
        "    if 10 <= hour['daytime'] < 16:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTQ1chfOzdin"
      },
      "outputs": [],
      "source": [
        "# Apply 'daytime' function to the 'daytime' series\n",
        "df1['daytime'] = df1.apply(daytime, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "kdwqmfrJzjEm"
      },
      "outputs": [],
      "source": [
        "# Define 'pm_rush()' conversion function [16:0020:00)\n",
        "def pm_rush(hour):\n",
        "    if 16 <= hour['pm_rush'] < 20:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC50IFTvzqG2"
      },
      "outputs": [],
      "source": [
        "# Apply 'pm_rush' function to the 'pm_rush' series\n",
        "df1['pm_rush'] = df1.apply(pm_rush, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "u9DIqgglztR-"
      },
      "outputs": [],
      "source": [
        "# Define 'nighttime()' conversion function [20:0006:00)\n",
        "def nighttime(hour):\n",
        "    if 20 <= hour['nighttime'] < 24:\n",
        "        val = 1\n",
        "    elif 0 <= hour['nighttime'] < 6:\n",
        "        val = 1\n",
        "    else:\n",
        "        val = 0\n",
        "    return val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HuFsWgNzwPG"
      },
      "outputs": [],
      "source": [
        "# Apply 'nighttime' function to the 'nighttime' series\n",
        "df1['nighttime'] = df1.apply(nighttime, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIDPsBpPz2s5"
      },
      "outputs": [],
      "source": [
        "# Create 'month' col\n",
        "df1['month'] = df1['tpep_pickup_datetime'].dt.strftime('%b').str.lower()\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbiLoO1Q0Ahw"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RBQ0EXm0EPr"
      },
      "outputs": [],
      "source": [
        "# Drop columns\n",
        "drop_cols = ['Unnamed: 0_x', 'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
        "             'payment_type', 'trip_distance', 'store_and_fwd_flag', 'payment_type',\n",
        "             'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
        "             'improvement_surcharge', 'total_amount', 'tip_percent']\n",
        "\n",
        "df1 = df1.drop(drop_cols, axis=1)\n",
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "OliJKkds0U0o"
      },
      "outputs": [],
      "source": [
        "# 1. Define list of cols to convert to string\n",
        "cols_to_str = ['RatecodeID', 'PULocationID', 'DOLocationID', 'VendorID']\n",
        "\n",
        "# 2. Convert each column to string\n",
        "for col in cols_to_str:\n",
        "    df1[col] = df1[col].astype('str')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW0ZbMVu0Y-L"
      },
      "outputs": [],
      "source": [
        "# Convert categoricals to binary\n",
        "df2 = pd.get_dummies(df1, drop_first=True)\n",
        "df2.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OEpEq-50c0q"
      },
      "outputs": [],
      "source": [
        "# Get class balance of 'generous' col\n",
        "df2['generous'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "2h05uduF0hnt"
      },
      "outputs": [],
      "source": [
        "# Isolate target variable (y)\n",
        "y = df2['generous']\n",
        "\n",
        "# Isolate the features (X)\n",
        "X = df2.drop('generous', axis=1)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xohCO4_P0qqa"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "UTyuLpVR0m4a"
      },
      "outputs": [],
      "source": [
        "# 1. Instantiate the random forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 2. Create a dictionary of hyperparameters to tune\n",
        "# Note that this example only contains 1 value for each parameter for simplicity,\n",
        "# but you should assign a dictionary with ranges of values\n",
        "cv_params = {'max_depth': [None],\n",
        "             'max_features': [1.0],\n",
        "             'max_samples': [0.7],\n",
        "             'min_samples_leaf': [1],\n",
        "             'min_samples_split': [2],\n",
        "             'n_estimators': [300]\n",
        "             }\n",
        "\n",
        "# 3. Define a set of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
        "\n",
        "# 4. Instantiate the GridSearchCV object\n",
        "rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='f1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "HQ_CThUZ0xMd",
        "outputId": "f015b34a-9979-485b-d321-f611c23b0f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 5s, sys: 201 ms, total: 6min 5s\n",
            "Wall time: 6min 56s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=4, estimator=RandomForestClassifier(random_state=42),\n",
              "             param_grid={&#x27;max_depth&#x27;: [None], &#x27;max_features&#x27;: [1.0],\n",
              "                         &#x27;max_samples&#x27;: [0.7], &#x27;min_samples_leaf&#x27;: [1],\n",
              "                         &#x27;min_samples_split&#x27;: [2], &#x27;n_estimators&#x27;: [300]},\n",
              "             refit=&#x27;f1&#x27;, scoring={&#x27;accuracy&#x27;, &#x27;f1&#x27;, &#x27;recall&#x27;, &#x27;precision&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=4, estimator=RandomForestClassifier(random_state=42),\n",
              "             param_grid={&#x27;max_depth&#x27;: [None], &#x27;max_features&#x27;: [1.0],\n",
              "                         &#x27;max_samples&#x27;: [0.7], &#x27;min_samples_leaf&#x27;: [1],\n",
              "                         &#x27;min_samples_split&#x27;: [2], &#x27;n_estimators&#x27;: [300]},\n",
              "             refit=&#x27;f1&#x27;, scoring={&#x27;accuracy&#x27;, &#x27;f1&#x27;, &#x27;recall&#x27;, &#x27;precision&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=4, estimator=RandomForestClassifier(random_state=42),\n",
              "             param_grid={'max_depth': [None], 'max_features': [1.0],\n",
              "                         'max_samples': [0.7], 'min_samples_leaf': [1],\n",
              "                         'min_samples_split': [2], 'n_estimators': [300]},\n",
              "             refit='f1', scoring={'accuracy', 'f1', 'recall', 'precision'})"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "rf1.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "ZN0Gi7KW01lL"
      },
      "outputs": [],
      "source": [
        "# Define a path to the folder where you want to save the model\n",
        "path = './'\n",
        "\n",
        "# Define functions for saving and loading model as pickle file\n",
        "\n",
        "def write_pickle(path, model_object, save_name:str):\n",
        "    '''\n",
        "    save_name is a string.\n",
        "    '''\n",
        "    with open(path + save_name + '.pickle', 'wb') as to_write:\n",
        "        pickle.dump(model_object, to_write)\n",
        "\n",
        "def read_pickle(path, saved_model_name:str):\n",
        "    '''\n",
        "    saved_model_name is a string.\n",
        "    '''\n",
        "    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n",
        "        model = pickle.load(to_read)\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofTFUtkn1rEp"
      },
      "outputs": [],
      "source": [
        "rf1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYYuy5Ff1wdD"
      },
      "outputs": [],
      "source": [
        "rf1.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "hxIRvZGz1xYD"
      },
      "outputs": [],
      "source": [
        "def make_results(model_name:str, model_object, metric:str):\n",
        "    '''\n",
        "    Arguments:\n",
        "    model_name (string): what you want the model to be called in the output table\n",
        "    model_object: a fit GridSearchCV object\n",
        "    metric (string): precision, recall, f1, or accuracy\n",
        "\n",
        "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
        "    for the model with the best mean 'metric' score across all validation folds.\n",
        "    '''\n",
        "\n",
        "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
        "    metric_dict = {'precision': 'mean_test_precision',\n",
        "                 'recall': 'mean_test_recall',\n",
        "                 'f1': 'mean_test_f1',\n",
        "                 'accuracy': 'mean_test_accuracy',\n",
        "                 }\n",
        "\n",
        "    # Get all the results from the CV and put them in a df\n",
        "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
        "\n",
        "    # Isolate the row of the df with the max(metric) score\n",
        "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
        "\n",
        "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
        "    f1 = best_estimator_results.mean_test_f1\n",
        "    recall = best_estimator_results.mean_test_recall\n",
        "    precision = best_estimator_results.mean_test_precision\n",
        "    accuracy = best_estimator_results.mean_test_accuracy\n",
        "\n",
        "    # Create table of results\n",
        "    table = pd.DataFrame({'model': [model_name],\n",
        "                        'precision': [precision],\n",
        "                        'recall': [recall],\n",
        "                        'F1': [f1],\n",
        "                        'accuracy': [accuracy],\n",
        "                        },\n",
        "                       )\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X8zHxb02Dpk"
      },
      "outputs": [],
      "source": [
        "# Call 'make_results()' on the GridSearch object\n",
        "results = make_results('RF CV', rf1, 'f1')\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "ftK-pzQs2FO9"
      },
      "outputs": [],
      "source": [
        "# Get scores on test data\n",
        "rf_preds = rf1.best_estimator_.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "6vYg7McH2IR7"
      },
      "outputs": [],
      "source": [
        "def get_test_scores(model_name:str, preds, y_test_data):\n",
        "    '''\n",
        "    Generate a table of test scores.\n",
        "\n",
        "    In:\n",
        "    model_name (string): Your choice: how the model will be named in the output table\n",
        "    preds: numpy array of test predictions\n",
        "    y_test_data: numpy array of y_test data\n",
        "\n",
        "    Out:\n",
        "    table: a pandas df of precision, recall, f1, and accuracy scores for your model\n",
        "    '''\n",
        "    accuracy = accuracy_score(y_test_data, preds)\n",
        "    precision = precision_score(y_test_data, preds)\n",
        "    recall = recall_score(y_test_data, preds)\n",
        "    f1 = f1_score(y_test_data, preds)\n",
        "\n",
        "    table = pd.DataFrame({'model': [model_name],\n",
        "                        'precision': [precision],\n",
        "                        'recall': [recall],\n",
        "                        'F1': [f1],\n",
        "                        'accuracy': [accuracy]\n",
        "                        })\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzGthimM2Llf"
      },
      "outputs": [],
      "source": [
        "# Get scores on test data\n",
        "rf_test_scores = get_test_scores('RF test', rf_preds, y_test)\n",
        "results = pd.concat([results, rf_test_scores], axis=0)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qujkNMhK2RmS"
      },
      "source": [
        "XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "3NhEdO8r2Nv3"
      },
      "outputs": [],
      "source": [
        "# 1. Instantiate the XGBoost classifier\n",
        "xgb = XGBClassifier(objective='binary:logistic', random_state=0)\n",
        "\n",
        "# 2. Create a dictionary of hyperparameters to tune\n",
        "# Note that this example only contains 1 value for each parameter for simplicity,\n",
        "# but you should assign a dictionary with ranges of values\n",
        "cv_params = {'learning_rate': [0.1],\n",
        "             'max_depth': [8],\n",
        "             'min_child_weight': [2],\n",
        "             'n_estimators': [500]\n",
        "             }\n",
        "\n",
        "# 3. Define a set of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1'}\n",
        "\n",
        "# 4. Instantiate the GridSearchCV object\n",
        "xgb1 = GridSearchCV(xgb, cv_params, scoring=scoring, cv=4, refit='f1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "z9NyLHkS2WBB",
        "outputId": "8c4631dc-42b6-4e27-d590-9c6036f8f586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 53.6 s, sys: 203 ms, total: 53.8 s\n",
            "Wall time: 34.5 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=4,\n",
              "             estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None,...\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=0, ...),\n",
              "             param_grid={&#x27;learning_rate&#x27;: [0.1], &#x27;max_depth&#x27;: [8],\n",
              "                         &#x27;min_child_weight&#x27;: [2], &#x27;n_estimators&#x27;: [500]},\n",
              "             refit=&#x27;f1&#x27;, scoring={&#x27;accuracy&#x27;, &#x27;f1&#x27;, &#x27;recall&#x27;, &#x27;precision&#x27;})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=4,\n",
              "             estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None,...\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=0, ...),\n",
              "             param_grid={&#x27;learning_rate&#x27;: [0.1], &#x27;max_depth&#x27;: [8],\n",
              "                         &#x27;min_child_weight&#x27;: [2], &#x27;n_estimators&#x27;: [500]},\n",
              "             refit=&#x27;f1&#x27;, scoring={&#x27;accuracy&#x27;, &#x27;f1&#x27;, &#x27;recall&#x27;, &#x27;precision&#x27;})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=0, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
              "              gamma=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
              "              num_parallel_tree=None, random_state=0, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=4,\n",
              "             estimator=XGBClassifier(base_score=None, booster=None,\n",
              "                                     callbacks=None, colsample_bylevel=None,\n",
              "                                     colsample_bynode=None,\n",
              "                                     colsample_bytree=None, device=None,\n",
              "                                     early_stopping_rounds=None,\n",
              "                                     enable_categorical=False, eval_metric=None,\n",
              "                                     feature_types=None, gamma=None,\n",
              "                                     grow_policy=None, importance_type=None,\n",
              "                                     interaction_constraints=None,\n",
              "                                     learning_rate=None,...\n",
              "                                     max_delta_step=None, max_depth=None,\n",
              "                                     max_leaves=None, min_child_weight=None,\n",
              "                                     missing=nan, monotone_constraints=None,\n",
              "                                     multi_strategy=None, n_estimators=None,\n",
              "                                     n_jobs=None, num_parallel_tree=None,\n",
              "                                     random_state=0, ...),\n",
              "             param_grid={'learning_rate': [0.1], 'max_depth': [8],\n",
              "                         'min_child_weight': [2], 'n_estimators': [500]},\n",
              "             refit='f1', scoring={'accuracy', 'f1', 'recall', 'precision'})"
            ]
          },
          "execution_count": 198,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "xgb1.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skwmMHK82Yih",
        "outputId": "6fd8c429-bad7-4f27-9eb3-8d4bb734bc55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6944065731666975"
            ]
          },
          "execution_count": 199,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Examine best score\n",
        "xgb1.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9gsEfAk2b3Y"
      },
      "outputs": [],
      "source": [
        "# Examine best parameters\n",
        "xgb1.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQpp31y12eGS"
      },
      "outputs": [],
      "source": [
        "# Call 'make_results()' on the GridSearch object\n",
        "xgb1_cv_results = make_results('XGB CV', xgb1, 'f1')\n",
        "results = pd.concat([results, xgb1_cv_results], axis=0)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "0WuDgYKl2iEQ"
      },
      "outputs": [],
      "source": [
        "# Get scores on test data\n",
        "xgb_preds = xgb1.best_estimator_.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AgJWyac2kRN"
      },
      "outputs": [],
      "source": [
        "# Get scores on test data\n",
        "xgb_test_scores = get_test_scores('XGB test', xgb_preds, y_test)\n",
        "results = pd.concat([results, xgb_test_scores], axis=0)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgPaCG1E2m7u"
      },
      "outputs": [],
      "source": [
        "# Generate array of values for confusion matrix\n",
        "cm = confusion_matrix(y_test, rf_preds, labels=rf1.classes_)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                             display_labels=rf1.classes_,\n",
        "                             )\n",
        "disp.plot(values_format='');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBM2Q3nA2qFU"
      },
      "outputs": [],
      "source": [
        "importances = rf1.best_estimator_.feature_importances_\n",
        "rf_importances = pd.Series(importances, index=X_test.columns)\n",
        "rf_importances = rf_importances.sort_values(ascending=False)[:15]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8,5))\n",
        "rf_importances.plot.bar(ax=ax)\n",
        "ax.set_title('Feature importances')\n",
        "ax.set_ylabel('Mean decrease in impurity')\n",
        "fig.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrJwYq853Uza"
      },
      "source": [
        "1. **Why or why not use the model**  \n",
        "This model performs acceptably. Its F<sub>1</sub> score was 0.7235 and it had an overall accuracy of 0.6865. It correctly identified ~78% of the actual responders in the test set, which is 48% better than a random guess. It may be worthwhile to test the model with a select group of taxi drivers to get feedback.  \n",
        "\n",
        "\n",
        "2. **Model results**   \n",
        "Unfortunately, random forest is not the most transparent machine learning algorithm. We know that `VendorID`, `predicted_fare`, `mean_duration`, and `mean_distance` are the most important features, but we don't know how they influence tipping. This would require further exploration. It is interesting that `VendorID` is the most predictive feature. This seems to indicate that one of the two vendors tends to attract more generous customers. It may be worth performing statistical tests on the different vendors to examine this further.  \n",
        "\n",
        "\n",
        "3. **What features might be engineered that might improve model performance**  \n",
        "There are almost always additional features that can be engineered, but hopefully the most obvious ones were generated during the first round of modeling. In our case, we could try creating three new columns that indicate if the trip distance is short, medium, or far. We could also engineer a column that gives a ratio that represents (the amount of money from the fare amount to the nearest higher multiple of \\\\$5) / fare amount. For example, if the fare were \\\\$12, the value in this column would be 0.25, because \\\\$12 to the nearest higher multiple of \\\\$5 (\\\\$15) is \\\\$3, and \\\\$3 divided by \\\\$12 is 0.25. The intuition for this feature is that people might be likely to simply round up their tip, so journeys with fares with values just under a multiple of \\\\$5 may have lower tip percentages than those with fare values just over a multiple of \\\\$5. We could also do the same thing for fares to the nearest \\\\$10.\n",
        "\n",
        "$$\n",
        "round5\\_ratio = \\frac{amount\\ of\\ money\\ from\\ the\\ fare\\ amount\\ to\\ the\\ nearest\\ higher\\ multiple\\ of\\ \\$5}{fare\\ amount}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$ = \\frac{5 - (fare\\ mod\\ 5)}{fare\\ amount}$$\n",
        "\n",
        "\n",
        "4. **features that would likely improve the performance of model**   \n",
        "It would probably be very helpful to have past tipping behavior for each customer. It would also be valuable to have accurate tip values for customers who pay with cash.\n",
        "It would be helpful to have a lot more data. With enough data, we could create a unique feature for each pickup/dropoff combination."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EvoElNMkRObi",
        "oBlSyoTZZmgx",
        "rSQskg9XcNpn",
        "Li4KLodJro5J"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
