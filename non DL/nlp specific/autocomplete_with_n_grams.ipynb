{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d_x3dMc6rpEA",
      "metadata": {
        "id": "d_x3dMc6rpEA"
      },
      "source": [
        "Dataset:\n",
        "\n",
        "you can take any .txt file format, create it yourself or download it from any source, e.g. from project gutenberg https://www.gutenberg.org/\n",
        "\n",
        "I created Rustavely.txt file, which you can download here and use: https://1drv.ms/t/s!AhnVhbVlzYkKgQmnRS7FrC4QrQNM?e=800gvv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qNmorcrOTVsQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "qNmorcrOTVsQ",
        "outputId": "d164a409-69b0-4380-82eb-03536f33eff4"
      },
      "outputs": [],
      "source": [
        "with open(\"./Rustaveli.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "print(\"Data type:\", type(data))\n",
        "print(\"Number of letters:\", len(data))\n",
        "print(\"First 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[0:300])\n",
        "print(\"-------\")\n",
        "\n",
        "print(\"Last 300 letters of the data\")\n",
        "print(\"-------\")\n",
        "display(data[-300:])\n",
        "print(\"-------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0qWmTcoVTVsT",
      "metadata": {
        "id": "0qWmTcoVTVsT"
      },
      "outputs": [],
      "source": [
        "def split_to_sentences(data):\n",
        "    \"\"\"\n",
        "    Split data by linebreak \"\\n\"\n",
        "    \n",
        "    Args:\n",
        "        data: str\n",
        "    \n",
        "    Returns:\n",
        "        A list of sentences\n",
        "    \"\"\"\n",
        "    sentences = data.split(\"\\n\")\n",
        "    \n",
        "    # Additional clearning (This part is already implemented)\n",
        "    # - Remove leading and trailing spaces from each sentence\n",
        "    # - Drop sentences if they are empty strings.\n",
        "    sentences = [s.strip() for s in sentences]\n",
        "    sentences = [s for s in sentences if len(s) > 0]\n",
        "    \n",
        "    return sentences    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hSgU-8p0TVsU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSgU-8p0TVsU",
        "outputId": "9f0f7911-bf38-4b53-eedc-69eb390da9c7"
      },
      "outputs": [],
      "source": [
        "x = \"\"\"\n",
        "I have a pen.\\nI have an apple. \\nAh\\nApple pen.\\n\n",
        "\"\"\"\n",
        "print(x)\n",
        "\n",
        "split_to_sentences(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KuSIxDSaTVsW",
      "metadata": {
        "id": "KuSIxDSaTVsW"
      },
      "outputs": [],
      "source": [
        "def tokenize_sentences(sentences):\n",
        "    \"\"\"\n",
        "    Tokenize sentences into tokens (words)\n",
        "    \n",
        "    Args:\n",
        "        sentences: List of strings\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the list of lists of tokenized sentences\n",
        "    tokenized_sentences = []\n",
        "    # Go through each sentence\n",
        "    for sentence in sentences:\n",
        "        \n",
        "        # Convert to lowercase letters\n",
        "        sentence = sentence.lower()\n",
        "        \n",
        "        # Convert into a list of words\n",
        "        tokenized = nltk.word_tokenize(sentence)\n",
        "        \n",
        "        # append the list of words to the list of lists\n",
        "        tokenized_sentences.append(tokenized)\n",
        "    \n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "axR8ptHpTVsW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axR8ptHpTVsW",
        "outputId": "e2c25ce9-3c2e-4013-8259-f73ffb640bf0"
      },
      "outputs": [],
      "source": [
        "sentences = [\"Sky is blue.\", \"Leaves are green.\", \"Roses are red.\"]\n",
        "tokenize_sentences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jXUPDB-tTVsY",
      "metadata": {
        "id": "jXUPDB-tTVsY"
      },
      "outputs": [],
      "source": [
        "def get_tokenized_data(data):\n",
        "    \"\"\"\n",
        "    Make a list of tokenized sentences\n",
        "    \n",
        "    Args:\n",
        "        data: String\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of tokens\n",
        "    \"\"\"\n",
        "    # Get the sentences by splitting up the data\n",
        "    sentences = split_to_sentences(data)\n",
        "    \n",
        "    # Get the list of lists of tokens by tokenizing the sentences\n",
        "    tokenized_sentences = tokenize_sentences(sentences)\n",
        "    \n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "asKviGjeTVsZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asKviGjeTVsZ",
        "outputId": "e480923b-356b-443b-8073-4a33fd202435"
      },
      "outputs": [],
      "source": [
        "x = \"Sky is blue.\\nLeaves are green\\nRoses are red.\"\n",
        "get_tokenized_data(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ly3GMIQATVsa",
      "metadata": {
        "id": "ly3GMIQATVsa"
      },
      "outputs": [],
      "source": [
        "tokenized_data = get_tokenized_data(data)\n",
        "random.seed(87)\n",
        "random.shuffle(tokenized_data)\n",
        "\n",
        "train_size = int(len(tokenized_data) * 0.8)\n",
        "train_data = tokenized_data[0:train_size]\n",
        "test_data = tokenized_data[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5VHyEFaoTVsa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VHyEFaoTVsa",
        "outputId": "397da935-2364-4a2c-eb17-1b2cda7bfb53"
      },
      "outputs": [],
      "source": [
        "print(\"{} data are split into {} train and {} test set\".format(\n",
        "    len(tokenized_data), len(train_data), len(test_data)))\n",
        "\n",
        "print(\"First training sample:\")\n",
        "print(train_data[0])\n",
        "      \n",
        "print(\"First test sample\")\n",
        "print(test_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pDFM4WMETVsc",
      "metadata": {
        "id": "pDFM4WMETVsc"
      },
      "outputs": [],
      "source": [
        "def count_words(tokenized_sentences):\n",
        "    \"\"\"\n",
        "    Count the number of word appearence in the tokenized sentences\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of strings\n",
        "    \n",
        "    Returns:\n",
        "        dict that maps word (str) to the frequency (int)\n",
        "    \"\"\"\n",
        "            \n",
        "    word_counts = {}\n",
        "    # Loop through each sentence\n",
        "    for sentence in tokenized_sentences: # complete this line\n",
        "        \n",
        "        # Go through each token in the sentence\n",
        "        for token in sentence: # complete this line\n",
        "\n",
        "            # If the token is not in the dictionary yet, set the count to 1\n",
        "            if not token in word_counts: # complete this line\n",
        "                word_counts[token] = 1\n",
        "            \n",
        "            # If the token is already in the dictionary, increment the count by 1\n",
        "            else:\n",
        "                word_counts[token] += 1\n",
        "    \n",
        "    return word_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iC_rXaK8TVsc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC_rXaK8TVsc",
        "outputId": "5862a59a-aeef-4cea-b099-51256f22dba8"
      },
      "outputs": [],
      "source": [
        "# test your code\n",
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "count_words(tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mHv9E70vTVse",
      "metadata": {
        "id": "mHv9E70vTVse"
      },
      "outputs": [],
      "source": [
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "    \"\"\"\n",
        "    Find the words that appear N times or more\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of sentences\n",
        "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
        "    \n",
        "    Returns:\n",
        "        List of words that appear N times or more\n",
        "    \"\"\"\n",
        "    # Initialize an empty list to contain the words that\n",
        "    # appear at least 'minimum_freq' times.\n",
        "    closed_vocab = []\n",
        "    \n",
        "    # Get the word couts of the tokenized sentences\n",
        "    # Use the function that you defined earlier to count the words\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    \n",
        "    # for each word and its count\n",
        "    for word, cnt in word_counts.items(): \n",
        "        \n",
        "        # check that the word's count\n",
        "        # is at least as great as the minimum count\n",
        "        if cnt >= count_threshold:\n",
        "            \n",
        "            # append the word to the list\n",
        "            closed_vocab.append(word)\n",
        "    \n",
        "    return closed_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1pudqF2YTVse",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pudqF2YTVse",
        "outputId": "efb42920-fae4-4b14-c777-156a27d3184f"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences = [['sky', 'is', 'blue', '.'],\n",
        "                       ['leaves', 'are', 'green', '.'],\n",
        "                       ['roses', 'are', 'red', '.']]\n",
        "tmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\n",
        "print(f\"Closed vocabulary:\")\n",
        "print(tmp_closed_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8izRSSxdTVsf",
      "metadata": {
        "id": "8izRSSxdTVsf"
      },
      "outputs": [],
      "source": [
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    \"\"\"\n",
        "    Replace words not in the given vocabulary with '<unk>' token.\n",
        "    \n",
        "    Args:\n",
        "        tokenized_sentences: List of lists of strings\n",
        "        vocabulary: List of strings that we will use\n",
        "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
        "    \n",
        "    Returns:\n",
        "        List of lists of strings, with words not in the vocabulary replaced\n",
        "    \"\"\"\n",
        "    \n",
        "    # Place vocabulary into a set for faster search\n",
        "    vocabulary = set(vocabulary)\n",
        "    \n",
        "    # Initialize a list that will hold the sentences\n",
        "    # after less frequent words are replaced by the unknown token\n",
        "    replaced_tokenized_sentences = []\n",
        "    \n",
        "    # Go through each sentence\n",
        "    for sentence in tokenized_sentences:\n",
        "        \n",
        "        # Initialize the list that will contain\n",
        "        # a single sentence with \"unknown_token\" replacements\n",
        "        replaced_sentence = []\n",
        "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "\n",
        "        # for each token in the sentence\n",
        "        for token in sentence: # complete this line\n",
        "            \n",
        "            # Check if the token is in the closed vocabulary\n",
        "            if token in vocabulary: # complete this line\n",
        "                # If so, append the word to the replaced_sentence\n",
        "                replaced_sentence.append(token)\n",
        "            else:\n",
        "                # otherwise, append the unknown token instead\n",
        "                replaced_sentence.append(unknown_token)\n",
        "        \n",
        "        # Append the list of tokens to the list of lists\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "    return replaced_tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BsuLXBgyTVsg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsuLXBgyTVsg",
        "outputId": "bd6e7699-3fdb-4781-afbf-bc790a576420"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\n",
        "vocabulary = [\"dogs\", \"sleep\"]\n",
        "tmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\n",
        "print(f\"Original sentence:\")\n",
        "print(tokenized_sentences)\n",
        "print(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\n",
        "print(tmp_replaced_tokenized_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VQK6A68dTVsh",
      "metadata": {
        "id": "VQK6A68dTVsh"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\", get_words_with_nplus_frequency=get_words_with_nplus_frequency, replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
        "    \"\"\"\n",
        "    Preprocess data, i.e.,\n",
        "        - Find tokens that appear at least N times in the training data.\n",
        "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
        "    Args:\n",
        "        train_data, test_data: List of lists of strings.\n",
        "        count_threshold: Words whose count is less than this are \n",
        "                      treated as unknown.\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of\n",
        "        - training data with low frequent words replaced by \"<unk>\"\n",
        "        - test data with low frequent words replaced by \"<unk>\"\n",
        "        - vocabulary of words that appear n times or more in the training data\n",
        "    \"\"\"\n",
        "    # Get the closed vocabulary using the train data\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
        "    \n",
        "    # For the train data, replace less common words with \"<unk>\"\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
        "    \n",
        "    # For the test data, replace less common words with \"<unk>\"\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
        "\n",
        "    return train_data_replaced, test_data_replaced, vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dqwxKYd9TVsh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqwxKYd9TVsh",
        "outputId": "ed65538c-4d1a-480a-bb96-bdd48f93db03"
      },
      "outputs": [],
      "source": [
        "tmp_train = [['sky', 'is', 'blue', '.'],\n",
        "     ['leaves', 'are', 'green']]\n",
        "tmp_test = [['roses', 'are', 'red', '.']]\n",
        "\n",
        "tmp_train_repl, tmp_test_repl, tmp_vocab = preprocess_data(tmp_train, \n",
        "                                                           tmp_test, \n",
        "                                                           count_threshold = 1\n",
        "                                                          )\n",
        "\n",
        "print(\"tmp_train_repl\")\n",
        "print(tmp_train_repl)\n",
        "print()\n",
        "print(\"tmp_test_repl\")\n",
        "print(tmp_test_repl)\n",
        "print()\n",
        "print(\"tmp_vocab\")\n",
        "print(tmp_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aggj1l03TVsi",
      "metadata": {
        "id": "aggj1l03TVsi"
      },
      "outputs": [],
      "source": [
        "minimum_freq = 2\n",
        "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
        "                                                                        test_data, \n",
        "                                                                        minimum_freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jIidqWZZTVsj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIidqWZZTVsj",
        "outputId": "d2f65111-2bce-43c3-8e06-fa959f3fa8c9"
      },
      "outputs": [],
      "source": [
        "print(\"First preprocessed training sample:\")\n",
        "print(train_data_processed[0])\n",
        "print()\n",
        "print(\"First preprocessed test sample:\")\n",
        "print(test_data_processed[0])\n",
        "print()\n",
        "print(\"First 10 vocabulary:\")\n",
        "print(vocabulary[0:10])\n",
        "print()\n",
        "print(\"Size of vocabulary:\", len(vocabulary))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gCnO3A6mTVsk",
      "metadata": {
        "id": "gCnO3A6mTVsk"
      },
      "outputs": [],
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
        "    \"\"\"\n",
        "    Count all n-grams in the data\n",
        "    \n",
        "    Args:\n",
        "        data: List of lists of words\n",
        "        n: number of words in a sequence\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary that maps a tuple of n-words to its frequency\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize dictionary of n-grams and their counts\n",
        "    n_grams = {}\n",
        "\n",
        "    # Go through each sentence in the data\n",
        "    for sentence in data: # complete this line\n",
        "        \n",
        "        # prepend start token n times, and  append <e> one time\n",
        "        sentence = [start_token]*n + sentence + [end_token]\n",
        "        \n",
        "        # convert list to tuple\n",
        "        # So that the sequence of words can be used as\n",
        "        # a key in the dictionary\n",
        "        sentence = tuple(sentence)\n",
        "        \n",
        "        # Use 'i' to indicate the start of the n-gram\n",
        "        # from index 0\n",
        "        # to the last index where the end of the n-gram\n",
        "        # is within the sentence.\n",
        "        \n",
        "        for i in range(len(sentence)-n+1): # complete this line\n",
        "\n",
        "            # Get the n-gram from i to i+n\n",
        "            n_gram = sentence[i:i+n]\n",
        "\n",
        "            # check if the n-gram is in the dictionary\n",
        "            if n_gram in n_grams: # complete this line\n",
        "            \n",
        "                # Increment the count for this n-gram\n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                # Initialize this n-gram count to 1\n",
        "                n_grams[n_gram] = 1\n",
        "                \n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55Whx-zJTVsl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55Whx-zJTVsl",
        "outputId": "74a6f835-0752-4d79-f7f7-912522034983"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "print(\"Uni-gram:\")\n",
        "print(count_n_grams(sentences, 1))\n",
        "print(\"Bi-gram:\")\n",
        "print(count_n_grams(sentences, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tMy9dKt_TVsq",
      "metadata": {
        "id": "tMy9dKt_TVsq"
      },
      "outputs": [],
      "source": [
        "def estimate_probability(word, previous_n_gram, \n",
        "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of a next word using the n-gram counts with k-smoothing\n",
        "    \n",
        "    Args:\n",
        "        word: next word\n",
        "        previous_n_gram: A sequence of words of length n\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of words in the vocabulary\n",
        "        k: positive constant, smoothing parameter\n",
        "    \n",
        "    Returns:\n",
        "        A probability\n",
        "    \"\"\"\n",
        "    # convert list to tuple to use it as a dictionary key\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    \n",
        "    # Set the denominator\n",
        "    # If the previous n-gram exists in the dictionary of n-gram counts,\n",
        "    # Get its count.  Otherwise set the count to zero\n",
        "    # Use the dictionary that has counts for n-grams\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "        \n",
        "    # Calculate the denominator using the count of the previous n gram\n",
        "    # and apply k-smoothing\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "\n",
        "    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "  \n",
        "    # Set the count to the count in the dictionary,\n",
        "    # otherwise 0 if not in the dictionary\n",
        "    # use the dictionary that has counts for the n-gram plus current word\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "        \n",
        "    # Define the numerator use the count of the n-gram plus current word,\n",
        "    # and apply smoothing\n",
        "    numerator = n_plus1_gram_count + k\n",
        "\n",
        "    # Calculate the probability as the numerator divided by denominator\n",
        "    probability = numerator / denominator\n",
        "    \n",
        "    return probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ew77VqiZTVsq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew77VqiZTVsq",
        "outputId": "277b76f5-47e4-4a9a-8075-df06f168bce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n"
          ]
        }
      ],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "tmp_prob = estimate_probability(\"cat\", \"a\", unigram_counts, bigram_counts, len(unique_words), k=1)\n",
        "\n",
        "print(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qLoyBcKeTVss",
      "metadata": {
        "id": "qLoyBcKeTVss"
      },
      "outputs": [],
      "source": [
        "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
        "    \"\"\"\n",
        "    Estimate the probabilities of next words using the n-gram counts with k-smoothing\n",
        "    \n",
        "    Args:\n",
        "        previous_n_gram: A sequence of words of length n\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary: List of words\n",
        "        k: positive constant, smoothing parameter\n",
        "    \n",
        "    Returns:\n",
        "        A dictionary mapping from next words to the probability.\n",
        "    \"\"\"\n",
        "    # convert list to tuple to use it as a dictionary key\n",
        "    previous_n_gram = tuple(previous_n_gram)    \n",
        "    \n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is not needed since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
        "    vocabulary_size = len(vocabulary)    \n",
        "    \n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = estimate_probability(word, previous_n_gram, \n",
        "                                           n_gram_counts, n_plus1_gram_counts, \n",
        "                                           vocabulary_size, k=k)\n",
        "                \n",
        "        probabilities[word] = probability\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HnkhFvQzTVss",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnkhFvQzTVss",
        "outputId": "1d5fe0f0-2915-4bc2-dab3-b394505ba20d"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "estimate_probabilities(\"a\", unigram_counts, bigram_counts, unique_words, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cEVA-y2iTVss",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEVA-y2iTVss",
        "outputId": "02a97c48-da54-4423-f214-6572e4d00454"
      },
      "outputs": [],
      "source": [
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "estimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHuuHBOvTVst",
      "metadata": {
        "id": "HHuuHBOvTVst"
      },
      "outputs": [],
      "source": [
        "def make_count_matrix(n_plus1_gram_counts, vocabulary):\n",
        "    # add <e> <unk> to the vocabulary\n",
        "    # <s> is omitted since it should not appear as the next word\n",
        "    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n",
        "    \n",
        "    # obtain unique n-grams\n",
        "    n_grams = []\n",
        "    for n_plus1_gram in n_plus1_gram_counts.keys():\n",
        "        n_gram = n_plus1_gram[0:-1]        \n",
        "        n_grams.append(n_gram)\n",
        "    n_grams = list(set(n_grams))\n",
        "    \n",
        "    # mapping from n-gram to row\n",
        "    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}    \n",
        "    # mapping from next word to column\n",
        "    col_index = {word:j for j, word in enumerate(vocabulary)}    \n",
        "    \n",
        "    nrow = len(n_grams)\n",
        "    ncol = len(vocabulary)\n",
        "    count_matrix = np.zeros((nrow, ncol))\n",
        "    for n_plus1_gram, count in n_plus1_gram_counts.items():\n",
        "        n_gram = n_plus1_gram[0:-1]\n",
        "        word = n_plus1_gram[-1]\n",
        "        if word not in vocabulary:\n",
        "            continue\n",
        "        i = row_index[n_gram]\n",
        "        j = col_index[word]\n",
        "        count_matrix[i, j] = count\n",
        "    \n",
        "    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n",
        "    return count_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rt4EJlqyTVsu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "rt4EJlqyTVsu",
        "outputId": "99fbbf10-60a1-4c88-a6b6-8593f09e20bf"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "print('bigram counts')\n",
        "display(make_count_matrix(bigram_counts, unique_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lqvFUZ98TVsu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "lqvFUZ98TVsu",
        "outputId": "35256d0f-59fa-49e3-ec2e-42494973d885"
      },
      "outputs": [],
      "source": [
        "# Show trigram counts\n",
        "print('\\ntrigram counts')\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "display(make_count_matrix(trigram_counts, unique_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Uc__dqWTVsv",
      "metadata": {
        "id": "-Uc__dqWTVsv"
      },
      "outputs": [],
      "source": [
        "def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n",
        "    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n",
        "    count_matrix += k\n",
        "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
        "    return prob_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Wt7UemkTVsv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "_Wt7UemkTVsv",
        "outputId": "d790671d-83b5-4163-f755-e2fddeadecff"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "print(\"bigram probabilities\")\n",
        "display(make_probability_matrix(bigram_counts, unique_words, k=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2T6QWwGrTVsw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "2T6QWwGrTVsw",
        "outputId": "98f53755-cd76-4c17-bc81-25df2f18424a"
      },
      "outputs": [],
      "source": [
        "print(\"trigram probabilities\")\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "display(make_probability_matrix(trigram_counts, unique_words, k=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wTcItUogTVsx",
      "metadata": {
        "id": "wTcItUogTVsx"
      },
      "outputs": [],
      "source": [
        "# calculate_perplexity\n",
        "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token = '<e>', k=1.0):\n",
        "    \"\"\"\n",
        "    Calculate perplexity for a list of sentences\n",
        "    \n",
        "    Args:\n",
        "        sentence: List of strings\n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary_size: number of unique words in the vocabulary\n",
        "        k: Positive smoothing constant\n",
        "    \n",
        "    Returns:\n",
        "        Perplexity score\n",
        "    \"\"\"\n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "    \n",
        "    # prepend <s> and append <e>\n",
        "    sentence = [start_token] * n + sentence + [end_token]\n",
        "    \n",
        "    # Cast the sentence from a list to a tuple\n",
        "    sentence = tuple(sentence)\n",
        "    \n",
        "    # length of sentence (after adding <s> and <e> tokens)\n",
        "    N = len(sentence)\n",
        "    \n",
        "    # The variable p will hold the product\n",
        "    # that is calculated inside the n-root\n",
        "    # Update this in the code below\n",
        "    product_pi = 1.0\n",
        "    \n",
        "    # Index t ranges from n to N - 1, inclusive on both ends\n",
        "    for t in range(n, N): # complete this line\n",
        "\n",
        "        # get the n-gram preceding the word at position t\n",
        "        n_gram = sentence[t-n:t]\n",
        "        \n",
        "        # get the word at position t\n",
        "        word = sentence[t]\n",
        "        # Estimate the probability of the word given the n-gram\n",
        "        # using the n-gram counts, n-plus1-gram counts,\n",
        "        # vocabulary size, and smoothing constant\n",
        "        probability = estimate_probability(word,n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1)\n",
        "        # Update the product of the probabilities\n",
        "        # This 'product_pi' is a cumulative product \n",
        "        # of the (1/P) factors that are calculated in the loop\n",
        "        product_pi *= 1/probability\n",
        "\n",
        "    # Take the Nth root of the product\n",
        "    perplexity = product_pi**(1/float(N))\n",
        "    \n",
        "    return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ge2PhaM6TVsx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge2PhaM6TVsx",
        "outputId": "9fffe9e4-eeb1-42d9-88e1-e6e1aa84bf17"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "\n",
        "perplexity_train = calculate_perplexity(sentences[0],\n",
        "                                         unigram_counts, bigram_counts,\n",
        "                                         len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n",
        "\n",
        "test_sentence = ['i', 'like', 'a', 'dog']\n",
        "perplexity_test = calculate_perplexity(test_sentence,\n",
        "                                       unigram_counts, bigram_counts,\n",
        "                                       len(unique_words), k=1.0)\n",
        "print(f\"Perplexity for test sample: {perplexity_test:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S-I-gA7ETVsz",
      "metadata": {
        "id": "S-I-gA7ETVsz"
      },
      "outputs": [],
      "source": [
        "# suggest_a_word\n",
        "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
        "    \"\"\"\n",
        "    Get suggestion for the next word\n",
        "    \n",
        "    Args:\n",
        "        previous_tokens: The sentence you input where each token is a word. Must have length > n \n",
        "        n_gram_counts: Dictionary of counts of n-grams\n",
        "        n_plus1_gram_counts: Dictionary of counts of (n+1)-grams\n",
        "        vocabulary: List of words\n",
        "        k: positive constant, smoothing parameter\n",
        "        start_with: If not None, specifies the first few letters of the next word\n",
        "        \n",
        "    Returns:\n",
        "        A tuple of \n",
        "          - string of the most likely next word\n",
        "          - corresponding probability\n",
        "    \"\"\"\n",
        "    \n",
        "    # length of previous words\n",
        "    n = len(list(n_gram_counts.keys())[0]) \n",
        "    \n",
        "    # From the words that the user already typed\n",
        "    # get the most recent 'n' words as the previous n-gram\n",
        "    previous_n_gram = previous_tokens[-n:]\n",
        "\n",
        "    # Estimate the probabilities that each word in the vocabulary\n",
        "    # is the next word,\n",
        "    # given the previous n-gram, the dictionary of n-gram counts,\n",
        "    # the dictionary of n plus 1 gram counts, and the smoothing constant\n",
        "    probabilities = estimate_probabilities(previous_n_gram,\n",
        "                                           n_gram_counts, n_plus1_gram_counts,\n",
        "                                           vocabulary, k=k)\n",
        "    \n",
        "    # Initialize suggested word to None\n",
        "    # This will be set to the word with highest probability\n",
        "    suggestion = None\n",
        "    \n",
        "    # Initialize the highest word probability to 0\n",
        "    # this will be set to the highest probability \n",
        "    # of all words to be suggested\n",
        "    max_prob = 0\n",
        "    \n",
        "    # For each word and its probability in the probabilities dictionary:\n",
        "    for word, prob in probabilities.items(): # complete this line\n",
        "        \n",
        "        # If the optional start_with string is set\n",
        "        if start_with != None: # complete this line\n",
        "            \n",
        "            # Check if the beginning of word does not match with the letters in 'start_with'\n",
        "            if not word.startswith(start_with): # complete this line\n",
        "\n",
        "                # if they don't match, skip this word (move onto the next word)\n",
        "                continue # complete this line\n",
        "        \n",
        "        # Check if this word's probability\n",
        "        # is greater than the current maximum probability\n",
        "        if prob > max_prob: # complete this line\n",
        "            \n",
        "            # If so, save this word as the best suggestion (so far)\n",
        "            suggestion = word\n",
        "            \n",
        "            # Save the new maximum probability\n",
        "            max_prob = prob\n",
        "    \n",
        "    return suggestion, max_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4VS2fpC3TVsz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VS2fpC3TVsz",
        "outputId": "09ee0956-30a6-4883-df6d-e589393f870a"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
        "print(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n",
        "\n",
        "print()\n",
        "# test your code when setting the starts_with\n",
        "tmp_starts_with = 'c'\n",
        "tmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\n",
        "print(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vn2B0pfeTVs0",
      "metadata": {
        "id": "vn2B0pfeTVs0"
      },
      "outputs": [],
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    model_counts = len(n_gram_counts_list)\n",
        "    suggestions = []\n",
        "    for i in range(model_counts-1):\n",
        "        n_gram_counts = n_gram_counts_list[i]\n",
        "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
        "        \n",
        "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
        "                                    n_plus1_gram_counts, vocabulary,\n",
        "                                    k=k, start_with=start_with)\n",
        "        suggestions.append(suggestion)\n",
        "    return suggestions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDkH8cFmTVs1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "hDkH8cFmTVs1",
        "outputId": "a62322f3-1f33-42c4-c5c9-dccd81482908"
      },
      "outputs": [],
      "source": [
        "sentences = [['i', 'like', 'a', 'cat'],\n",
        "             ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "unique_words = list(set(sentences[0] + sentences[1]))\n",
        "\n",
        "unigram_counts = count_n_grams(sentences, 1)\n",
        "bigram_counts = count_n_grams(sentences, 2)\n",
        "trigram_counts = count_n_grams(sentences, 3)\n",
        "quadgram_counts = count_n_grams(sentences, 4)\n",
        "qintgram_counts = count_n_grams(sentences, 5)\n",
        "\n",
        "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
        "previous_tokens = [\"i\", \"like\"]\n",
        "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
        "\n",
        "print(f\"The previous words are 'i like', the suggestions are:\")\n",
        "display(tmp_suggest3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lgwiBM1jTVs1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgwiBM1jTVs1",
        "outputId": "7ad9589d-e2d8-41db-be6c-af9442fa3459"
      },
      "outputs": [],
      "source": [
        "n_gram_counts_list = []\n",
        "for n in range(1, 6):\n",
        "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
        "    n_model_counts = count_n_grams(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_model_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0wg-wj81TVs2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "0wg-wj81TVs2",
        "outputId": "9012c784-0919-40fa-e4f5-1a6bd2ae7e88"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"i\", \"am\", \"to\"]\n",
        "tmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UpH6JOx-TVs2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "UpH6JOx-TVs2",
        "outputId": "1f6c9e13-9658-4123-876e-068a3ed6cdbd"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\n",
        "tmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t2KHsMyeTVs2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "t2KHsMyeTVs2",
        "outputId": "7b60a1f8-37ea-471f-95a4-c9fa52cf1105"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\"]\n",
        "tmp_suggest6 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BFFtsmWVTVs2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "BFFtsmWVTVs2",
        "outputId": "c7e4cbc1-52b1-482d-d9f6-6d904515a219"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
        "tmp_suggest7 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FK8oosVcTVs3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "FK8oosVcTVs3",
        "outputId": "5a681f1d-51d3-4520-baef-3887ea1708cf"
      },
      "outputs": [],
      "source": [
        "previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\n",
        "tmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n",
        "\n",
        "print(f\"The previous words are {previous_tokens}, the suggestions are:\")\n",
        "display(tmp_suggest8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
