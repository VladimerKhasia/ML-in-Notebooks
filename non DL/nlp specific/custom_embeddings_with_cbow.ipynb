{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "YPpMrTq25gxG",
      "metadata": {
        "id": "YPpMrTq25gxG"
      },
      "source": [
        "Dataset:\n",
        "\n",
        "you can take any .txt file format, create it yourself or download it from any source, e.g. from project gutenberg https://www.gutenberg.org/\n",
        "\n",
        "I created Rustavely.txt file, which you can download here and use: https://1drv.ms/t/s!AhnVhbVlzYkKgQmnRS7FrC4QrQNM?e=800gvv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gX4AOJcyw-_w",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX4AOJcyw-_w",
        "outputId": "a92cc2b2-24cd-4ab7-90c4-07a21d85279b"
      },
      "outputs": [],
      "source": [
        "import re   \n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy import linalg\n",
        "from collections import defaultdict\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UCOjdiULzwbg",
      "metadata": {
        "cellView": "form",
        "id": "UCOjdiULzwbg"
      },
      "outputs": [],
      "source": [
        "#@title helpers\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    # sigmoid function\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def get_idx(words, word2Ind):\n",
        "    idx = []\n",
        "    for word in words:\n",
        "        idx = idx + [word2Ind[word]]\n",
        "    return idx\n",
        "\n",
        "\n",
        "def pack_idx_with_frequency(context_words, word2Ind):\n",
        "    freq_dict = defaultdict(int)\n",
        "    for word in context_words:\n",
        "        freq_dict[word] += 1\n",
        "    idxs = get_idx(context_words, word2Ind)\n",
        "    packed = []\n",
        "    for i in range(len(idxs)):\n",
        "        idx = idxs[i]\n",
        "        freq = freq_dict[context_words[i]]\n",
        "        packed.append((idx, freq))\n",
        "    return packed\n",
        "\n",
        "\n",
        "def get_vectors(data, word2Ind, V, C):\n",
        "    i = C\n",
        "    while True:\n",
        "        y = np.zeros(V)\n",
        "        x = np.zeros(V)\n",
        "        center_word = data[i]\n",
        "        y[word2Ind[center_word]] = 1\n",
        "        context_words = data[(i - C) : i] + data[(i + 1) : (i + C + 1)]\n",
        "        num_ctx_words = len(context_words)\n",
        "        for idx, freq in pack_idx_with_frequency(context_words, word2Ind):\n",
        "            x[idx] = freq / num_ctx_words\n",
        "        yield x, y\n",
        "        i += 1\n",
        "        if i >= len(data) - C:\n",
        "            print(\"i is being set to\", C)\n",
        "            i = C\n",
        "\n",
        "\n",
        "def get_batches(data, word2Ind, V, C, batch_size):\n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    for x, y in get_vectors(data, word2Ind, V, C):\n",
        "        while len(batch_x) < batch_size:\n",
        "            batch_x.append(x)\n",
        "            batch_y.append(y)\n",
        "        else:\n",
        "            yield np.array(batch_x).T, np.array(batch_y).T\n",
        "            batch_x = []\n",
        "            batch_y = []\n",
        "\n",
        "\n",
        "def compute_pca(data, n_components=2):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        data: of dimension (m,n) where each row corresponds to a word vector\n",
        "        n_components: Number of components you want to keep.\n",
        "    Output: \n",
        "        X_reduced: data transformed in 2 dims/columns + regenerated original data\n",
        "    pass in: data as 2D NumPy array\n",
        "    \"\"\"\n",
        "\n",
        "    m, n = data.shape\n",
        "\n",
        "    # mean center the data\n",
        "    data -= data.mean(axis=0)\n",
        "    # calculate the covariance matrix\n",
        "    R = np.cov(data, rowvar=False)\n",
        "    # calculate eigenvectors & eigenvalues of the covariance matrix\n",
        "    # use 'eigh' rather than 'eig' since R is symmetric,\n",
        "    # the performance gain is substantial\n",
        "    evals, evecs = linalg.eigh(R)\n",
        "    # sort eigenvalue in decreasing order\n",
        "    # this returns the corresponding indices of evals and evecs\n",
        "    idx = np.argsort(evals)[::-1]\n",
        "\n",
        "    evecs = evecs[:, idx]\n",
        "    # sort eigenvectors according to same index\n",
        "    evals = evals[idx]\n",
        "    # select the first n eigenvectors (n is desired dimension\n",
        "    # of rescaled data array, or dims_rescaled_data)\n",
        "    evecs = evecs[:, :n_components]\n",
        "\n",
        "    return np.dot(evecs.T, data.T).T\n",
        "\n",
        "\n",
        "def get_dict(data):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        K: the number of negative samples\n",
        "        data: the data you want to pull from\n",
        "        indices: a list of word indices\n",
        "    Output:\n",
        "        word_dict: a dictionary with the weighted probabilities of each word\n",
        "        word2Ind: returns dictionary mapping the word to its index\n",
        "        Ind2Word: returns dictionary mapping the index to its word\n",
        "    \"\"\"\n",
        "    #\n",
        "    #     words = nltk.word_tokenize(data)\n",
        "    words = sorted(list(set(data)))\n",
        "    n = len(words)\n",
        "    idx = 0\n",
        "    # return these correctly\n",
        "    word2Ind = {}\n",
        "    Ind2word = {}\n",
        "    for k in words:\n",
        "        word2Ind[k] = idx\n",
        "        Ind2word[idx] = k\n",
        "        idx += 1\n",
        "    return word2Ind, Ind2word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FLdPr9n6w-_y",
      "metadata": {
        "id": "FLdPr9n6w-_y"
      },
      "outputs": [],
      "source": [
        "# Download sentence tokenizer\n",
        "nltk.data.path.append('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7mnzj-ZMw-_z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mnzj-ZMw-_z",
        "outputId": "739da827-dfea-49ad-f120-1ad67403db76"
      },
      "outputs": [],
      "source": [
        "# Load, tokenize and process the data\n",
        "                                                        #  Load the Regex-modul\n",
        "with open('./Rustaveli.txt') as f:\n",
        "    data = f.read()                                                 #  Read in the data\n",
        "data = re.sub(r'[,!?;-]', '.', data)                                #  Punktuations are replaced by .\n",
        "data = nltk.word_tokenize(data)                                     #  Tokenize string to words\n",
        "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
        "print(\"Number of tokens:\", len(data),'\\n', data[:15])               #  print data sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qJyWZLMzw-_0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJyWZLMzw-_0",
        "outputId": "85d950af-8e87-4563-caa1-6fa67d800faa"
      },
      "outputs": [],
      "source": [
        "# Compute the frequency distribution of the words in the dataset (vocabulary)\n",
        "fdist = nltk.FreqDist(word for word in data)\n",
        "print(\"Size of vocabulary: \", len(fdist) )\n",
        "print(\"Most frequent tokens: \", fdist.most_common(20) ) # print the 20 most frequent words and their freq."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fa-ct5Aw-_1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fa-ct5Aw-_1",
        "outputId": "3f412cce-e754-4960-af83-52c99c2b2e92"
      },
      "outputs": [],
      "source": [
        "# get_dict creates two dictionaries, converting words to indices and viceversa.\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "print(\"Size of vocabulary: \", V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CkOaiEqtw-_2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkOaiEqtw-_2",
        "outputId": "aa79b8db-fbfd-475b-d61f-2515fde9199e"
      },
      "outputs": [],
      "source": [
        "# example of word to index mapping\n",
        "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
        "print(\"Word which has index 2743:  \",Ind2word[2743] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Bv1CItW4w-_3",
      "metadata": {
        "id": "Bv1CItW4w-_3"
      },
      "outputs": [],
      "source": [
        "def initialize_model(N,V, random_seed=1):\n",
        "    '''\n",
        "    Inputs: \n",
        "        N:  dimension of hidden vector \n",
        "        V:  dimension of vocabulary\n",
        "        random_seed: random seed for consistent results in the unit tests\n",
        "     Outputs: \n",
        "        W1, W2, b1, b2: initialized weights and biases\n",
        "    '''\n",
        "    np.random.seed(random_seed) \n",
        "    \n",
        "    # W1 has shape (N, V)\n",
        "    W1 = np.random.rand(N, V)\n",
        "    # W2 has shape (V, N)\n",
        "    W2 = np.random.rand(V, N)\n",
        "    # b1 has shape (N,1)\n",
        "    b1 = np.random.rand(N, 1)\n",
        "    # b2 has shape (V, 1)\n",
        "    b2 = np.random.rand(V, 1)\n",
        "\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aU-G6iLw-_3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aU-G6iLw-_3",
        "outputId": "597f5f30-5358-4a92-8d94-8d85b5485985"
      },
      "outputs": [],
      "source": [
        "# Test function.\n",
        "tmp_N = 4\n",
        "tmp_V = 10\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "assert tmp_W1.shape == ((tmp_N,tmp_V))\n",
        "assert tmp_W2.shape == ((tmp_V,tmp_N))\n",
        "print(f\"tmp_W1.shape: {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape: {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape: {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape: {tmp_b2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TN-_oBMQw-_5",
      "metadata": {
        "id": "TN-_oBMQw-_5"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    '''\n",
        "    Inputs: \n",
        "        z: output scores from the hidden layer\n",
        "    Outputs: \n",
        "        yhat: prediction (estimate of y)\n",
        "    '''\n",
        "    # Calculate yhat (softmax)\n",
        "    e_z = np.exp(z)\n",
        "    yhat = e_z/np.sum(e_z,axis=0)\n",
        "    \n",
        "    return yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jb98fSGTw-_6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "jb98fSGTw-_6",
        "outputId": "da1e3be0-3d3b-46cf-db4a-824d28e5a716"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "tmp = np.array([[1,2,3],\n",
        "                [1,1,1]\n",
        "               ])\n",
        "tmp_sm = softmax(tmp)\n",
        "display(tmp_sm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dFXHRNgw-_8",
      "metadata": {
        "id": "9dFXHRNgw-_8"
      },
      "outputs": [],
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    '''\n",
        "    Inputs: \n",
        "        x:  average one hot vector for the context \n",
        "        W1, W2, b1, b2:  matrices and biases to be learned\n",
        "     Outputs: \n",
        "        z:  output score vector\n",
        "        h:  output hidden vector\n",
        "    '''\n",
        "    \n",
        "    # Calculate h\n",
        "    h = np.dot(W1, x) + b1\n",
        "    \n",
        "    # Apply the relu on h (store result in h)\n",
        "    h = np.maximum(0, h)\n",
        "    \n",
        "    # Calculate z\n",
        "    z = np.dot(W2, h) + b2\n",
        "\n",
        "    return z, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S6NXICKRw-_8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6NXICKRw-_8",
        "outputId": "f3513544-2e69-4ce6-a819-261132c3d7fa"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "\n",
        "# Create some inputs\n",
        "tmp_N = 2\n",
        "tmp_V = 3\n",
        "tmp_x = np.array([[0,1,0]]).T\n",
        "#print(tmp_x)\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(N=tmp_N,V=tmp_V, random_seed=1)\n",
        "\n",
        "print(f\"x has shape {tmp_x.shape}\")\n",
        "print(f\"N is {tmp_N} and vocabulary size V is {tmp_V}\")\n",
        "\n",
        "# call function\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(\"call forward_prop\")\n",
        "print()\n",
        "# Look at output\n",
        "print(f\"z has shape {tmp_z.shape}\")\n",
        "print(\"z has values:\")\n",
        "print(tmp_z)\n",
        "\n",
        "print()\n",
        "\n",
        "print(f\"h has shape {tmp_h.shape}\")\n",
        "print(\"h has values:\")\n",
        "print(tmp_h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iaO-oAhLw-_9",
      "metadata": {
        "id": "iaO-oAhLw-_9"
      },
      "outputs": [],
      "source": [
        "# compute_cost: cross-entropy cost function\n",
        "def compute_cost(y, yhat, batch_size):\n",
        "\n",
        "    # cost function \n",
        "    logprobs = np.multiply(np.log(yhat),y) ##np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
        "    cost = - 1/batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dSpQXcCAw-_-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSpQXcCAw-_-",
        "outputId": "99e6f496-a9c6-4392-b38c-698220541981"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "tmp_C = 2\n",
        "tmp_N = 50\n",
        "tmp_batch_size = 4\n",
        "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
        "tmp_V = len(word2Ind)\n",
        "\n",
        "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
        "        \n",
        "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
        "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
        "\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "\n",
        "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
        "\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
        "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
        "\n",
        "tmp_yhat = softmax(tmp_z)\n",
        "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
        "\n",
        "tmp_cost = compute_cost(tmp_y, tmp_yhat, tmp_batch_size)\n",
        "print(\"call compute_cost\")\n",
        "print(f\"tmp_cost {tmp_cost:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "McXFLia7w-__",
      "metadata": {
        "id": "McXFLia7w-__"
      },
      "outputs": [],
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    '''\n",
        "    Inputs: \n",
        "        x:  average one hot vector for the context \n",
        "        yhat: prediction (estimate of y)\n",
        "        y:  target vector\n",
        "        h:  hidden vector (see eq. 1)\n",
        "        W1, W2, b1, b2:  matrices and biases  \n",
        "        batch_size: batch size \n",
        "     Outputs: \n",
        "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
        "    '''\n",
        "    # Re-use it whenever you see W2^T (Yhat - Y) used to compute a gradient\n",
        "    l1 = np.dot(W2.T,(yhat-y))\n",
        "    # Apply relu to l1\n",
        "    l1 = np.maximum(0, l1)      ##np.maximum() calculates elementwise maximums of two arrays, while np.max() returns maximum element from one array, column etc.\n",
        "    \n",
        "    # Compute the gradient of W1\n",
        "    grad_W1 = (1/batch_size)*np.dot(l1, x.T) \n",
        "    # Compute the gradient of W2\n",
        "    grad_W2 = (1/batch_size)*np.dot(yhat-y, h.T)\n",
        "    # Compute the gradient of b1\n",
        "    grad_b1 = (1/batch_size)*(np.dot(l1, np.ones((batch_size, 1))))\n",
        "    # Compute the gradient of b2\n",
        "    grad_b2 = (1/batch_size)*(np.dot(yhat-y, np.ones((batch_size, 1))))\n",
        "    \n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kUwfsLWpw-__",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUwfsLWpw-__",
        "outputId": "b646d70d-891d-4016-8aaf-4cae43756beb"
      },
      "outputs": [],
      "source": [
        "# Test the function\n",
        "tmp_C = 2\n",
        "tmp_N = 50\n",
        "tmp_batch_size = 4\n",
        "tmp_word2Ind, tmp_Ind2word = get_dict(data)\n",
        "tmp_V = len(word2Ind)\n",
        "\n",
        "\n",
        "# get a batch of data\n",
        "tmp_x, tmp_y = next(get_batches(data, tmp_word2Ind, tmp_V,tmp_C, tmp_batch_size))\n",
        "\n",
        "print(\"get a batch of data\")\n",
        "print(f\"tmp_x.shape {tmp_x.shape}\")\n",
        "print(f\"tmp_y.shape {tmp_y.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Initialize weights and biases\")\n",
        "tmp_W1, tmp_W2, tmp_b1, tmp_b2 = initialize_model(tmp_N,tmp_V)\n",
        "\n",
        "print(f\"tmp_W1.shape {tmp_W1.shape}\")\n",
        "print(f\"tmp_W2.shape {tmp_W2.shape}\")\n",
        "print(f\"tmp_b1.shape {tmp_b1.shape}\")\n",
        "print(f\"tmp_b2.shape {tmp_b2.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Forwad prop to get z and h\")\n",
        "tmp_z, tmp_h = forward_prop(tmp_x, tmp_W1, tmp_W2, tmp_b1, tmp_b2)\n",
        "print(f\"tmp_z.shape: {tmp_z.shape}\")\n",
        "print(f\"tmp_h.shape: {tmp_h.shape}\")\n",
        "\n",
        "print()\n",
        "print(\"Get yhat by calling softmax\")\n",
        "tmp_yhat = softmax(tmp_z)\n",
        "print(f\"tmp_yhat.shape: {tmp_yhat.shape}\")\n",
        "\n",
        "tmp_m = (2*tmp_C)\n",
        "tmp_grad_W1, tmp_grad_W2, tmp_grad_b1, tmp_grad_b2 = back_prop(tmp_x, tmp_yhat, tmp_y, tmp_h, tmp_W1, tmp_W2, tmp_b1, tmp_b2, tmp_batch_size)\n",
        "\n",
        "print()\n",
        "print(\"call back_prop\")\n",
        "print(f\"tmp_grad_W1.shape {tmp_grad_W1.shape}\")\n",
        "print(f\"tmp_grad_W2.shape {tmp_grad_W2.shape}\")\n",
        "print(f\"tmp_grad_b1.shape {tmp_grad_b1.shape}\")\n",
        "print(f\"tmp_grad_b2.shape {tmp_grad_b2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j-fdCyq-w_AA",
      "metadata": {
        "id": "j-fdCyq-w_AA"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
        "                     random_seed=282, initialize_model=initialize_model, \n",
        "                     get_batches=get_batches, forward_prop=forward_prop, \n",
        "                     softmax=softmax, compute_cost=compute_cost, \n",
        "                     back_prop=back_prop):\n",
        "    \n",
        "    '''\n",
        "    This is the gradient_descent function\n",
        "    \n",
        "      Inputs: \n",
        "        data:      text\n",
        "        word2Ind:  words to Indices\n",
        "        N:         dimension of hidden vector  \n",
        "        V:         dimension of vocabulary \n",
        "        num_iters: number of iterations  \n",
        "        random_seed: random seed to initialize the model's matrices and vectors\n",
        "        initialize_model: your implementation of the function to initialize the model\n",
        "        get_batches: function to get the data in batches\n",
        "        forward_prop: your implementation of the function to perform forward propagation\n",
        "        softmax: your implementation of the softmax function\n",
        "        compute_cost: cost function (Cross entropy)\n",
        "        back_prop: your implementation of the function to perform backward propagation\n",
        "     Outputs: \n",
        "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
        "\n",
        "    '''\n",
        "    W1, W2, b1, b2 = initialize_model(N, V, random_seed=random_seed) #W1=(N,V) and W2=(V,N)\n",
        "\n",
        "    batch_size = 128 ##512  ## when last batch is smaller than others you get altered behavior. This is why pytorch uses drop last.\n",
        "    iters = 0\n",
        "    C = 2 \n",
        "    \n",
        "    for x, y in get_batches(data, word2Ind, V, C, batch_size):\n",
        "        # Get z and h\n",
        "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
        "        # Get yhat\n",
        "        yhat = softmax(z)\n",
        "        # Get cost\n",
        "        cost = compute_cost(y, yhat, batch_size)\n",
        "        if ( (iters+1) % 10 == 0):\n",
        "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
        "        # Get gradients\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
        "        \n",
        "        # Update weights and biases\n",
        "        W1 -= alpha*grad_W1 \n",
        "        W2 -= alpha*grad_W2\n",
        "        b1 -= alpha*grad_b1\n",
        "        b2 -= alpha*grad_b2\n",
        "\n",
        "        iters +=1 \n",
        "        if iters == num_iters: \n",
        "            break\n",
        "        if iters % 100 == 0:\n",
        "            alpha *= 0.66\n",
        "            \n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zAvdecdpw_AB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAvdecdpw_AB",
        "outputId": "b86de740-24f9-4699-eb35-e8c36146c688"
      },
      "outputs": [],
      "source": [
        "# test your function\n",
        "C = 2\n",
        "N = 50\n",
        "word2Ind, Ind2word = get_dict(data)\n",
        "V = len(word2Ind)\n",
        "num_iters = 150\n",
        "print(\"Call gradient_descent\")\n",
        "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DBG4uQ1fw_AC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBG4uQ1fw_AC",
        "outputId": "f77408e8-52fc-4fc6-c937-ab561d859757"
      },
      "outputs": [],
      "source": [
        "# visualizing the word vectors here\n",
        "from matplotlib import pyplot\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "\n",
        "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
        "         'rich','happy','sad']\n",
        "\n",
        "embs = (W1.T + W2)/2.0\n",
        " \n",
        "# given a list of words and the embeddings, it returns a matrix with all the embeddings\n",
        "idx = [word2Ind[word] for word in words]\n",
        "X = embs[idx, :]\n",
        "print(X.shape, idx)  # X.shape:  Number of words of dimension N each "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gEHkekyvw_AC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "gEHkekyvw_AC",
        "outputId": "e641cfc6-10dd-435b-c519-deb97c055b48"
      },
      "outputs": [],
      "source": [
        "result= compute_pca(X, 2)\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aDwvK7Xfw_AD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "aDwvK7Xfw_AD",
        "outputId": "432edeb0-5272-4de2-9f61-2bbf46d4263e"
      },
      "outputs": [],
      "source": [
        "result= compute_pca(X, 4)\n",
        "pyplot.scatter(result[:, 3], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 3], result[i, 1]))\n",
        "pyplot.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
