{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FjXANo9Xrq5-"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cdb8eb3533884ca3bdd0aa642a39d0a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfe5b49ba2bf4f76bf2146f44483e14e",
              "IPY_MODEL_f1a99b4378fb473eb3078ac0a4573e0f",
              "IPY_MODEL_e5c38e1a3bb547c0b2f494af8fd3e9ea"
            ],
            "layout": "IPY_MODEL_996335bd5f97487a84d185261b35e2f1"
          }
        },
        "cfe5b49ba2bf4f76bf2146f44483e14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57705b226ce340bdb81a72ab3f9c3853",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e3841cc7325a4df28c98b7ec778c90ec",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "f1a99b4378fb473eb3078ac0a4573e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9a13d799f6f41ad8a34b0a02b3996ee",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5389fa8b57e648a4945ec4d65e2d6032",
            "value": 2
          }
        },
        "e5c38e1a3bb547c0b2f494af8fd3e9ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e39c9c63a8a4e6cab125b1740e4dfa2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d3f10dba4eb4627a14978607925ca70",
            "value": "â€‡2/2â€‡[00:17&lt;00:00,â€‡â€‡7.14s/it]"
          }
        },
        "996335bd5f97487a84d185261b35e2f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57705b226ce340bdb81a72ab3f9c3853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3841cc7325a4df28c98b7ec778c90ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9a13d799f6f41ad8a34b0a02b3996ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5389fa8b57e648a4945ec4d65e2d6032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e39c9c63a8a4e6cab125b1740e4dfa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d3f10dba4eb4627a14978607925ca70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "OSeL-vLZsO53"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkgFAXWVW3wm"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_community tavily-python bs4 duckduckgo-search #sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate bitsandbytes  ## accelerate is for GPU and additionally bitsandbytes is for quantization"
      ],
      "metadata": {
        "id": "NGQuLKNN3Xiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma model v1.1"
      ],
      "metadata": {
        "id": "IOE0EQIpf3SE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-1.1-2b-it\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "CdX0fVhjjZi6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ZeGdxLVltCm_",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cdb8eb3533884ca3bdd0aa642a39d0a9",
            "cfe5b49ba2bf4f76bf2146f44483e14e",
            "f1a99b4378fb473eb3078ac0a4573e0f",
            "e5c38e1a3bb547c0b2f494af8fd3e9ea",
            "996335bd5f97487a84d185261b35e2f1",
            "57705b226ce340bdb81a72ab3f9c3853",
            "e3841cc7325a4df28c98b7ec778c90ec",
            "b9a13d799f6f41ad8a34b0a02b3996ee",
            "5389fa8b57e648a4945ec4d65e2d6032",
            "2e39c9c63a8a4e6cab125b1740e4dfa2",
            "3d3f10dba4eb4627a14978607925ca70"
          ]
        },
        "outputId": "26e6609b-30cd-4263-e74b-cee37aba4aba"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb8eb3533884ca3bdd0aa642a39d0a9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title  for GPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_use_double_quant=True,\n",
        "#         bnb_4bit_quant_type=\"nf4\",\n",
        "#         bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "\n",
        "### model.save_pretrained(\"./model.\")\n",
        "### model = AutoModelForCausalLM.from_pretrained(\"./model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title for CPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16) ## if you do not use TPU torch.float16 could be better for precision safety\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZkXAAiYLu9F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    AsyncCallbackManagerForLLMRun,\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
        "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from langchain_core.runnables import run_in_executor\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class GemmaChatModel(BaseChatModel):\n",
        "    \"\"\"\n",
        "    A custom chat model powered by Gemma from Hugging Face, designed to be informative, comprehensive, and engaging.\n",
        "    See the custom model guide here: https://python.langchain.com/docs/modules/model_io/chat/custom_chat_model/\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = \"gemma_chat_model\"  # Replace with the actual Gemma model name\n",
        "    task: str = \"conversational\"  # Task for the pipeline (conversational or summarization)\n",
        "    #temperature = 0.0\n",
        "    n: int = 2048\n",
        "    model : Any = None\n",
        "    tokenizer : Any = None\n",
        "\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            messages: The list of prompt messages.\n",
        "            stop: Optional list of stop tokens.\n",
        "            run_manager: Optional callback manager.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            A ChatResult object containing the generated response.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = messages[-1].content #[: self.n]\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model.generate(**input_ids, max_new_tokens=self.n)       # , temperature=self.temperature\n",
        "        text = self.tokenizer.decode(outputs[0])\n",
        "        #text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "        start_index, end_index = text.find(\"<eos>\"), text.rfind(\"<eos>\")\n",
        "        response = text[start_index+len(\"<eos>\"):end_index].strip()\n",
        "\n",
        "        message = AIMessage(content=response, additional_kwargs={}, response_metadata={\"time_in_seconds\": 3})\n",
        "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns the type of language model used: \"gemma_chat_model\".\n",
        "        \"\"\"\n",
        "        return \"gemma_chat_model\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns a dictionary of identifying parameters for LangChain callbacks.\n",
        "        \"\"\"\n",
        "        return {\"model_name\": self.model_name, \"task\": self.task}\n",
        "\n",
        "llm = GemmaChatModel()\n",
        "llm.model = model               # This is simple but not production level way of doing things. It's just for avoiding colab run out of memory on CPU\n",
        "llm.tokenizer = tokenizer"
      ],
      "metadata": {
        "id": "lXuW_0ZC2hRc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assistant capable to search and browse the web"
      ],
      "metadata": {
        "id": "K4r49r3knYpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "REFERENCE: https://github.com/langchain-ai/langchain/blob/master/templates/research-assistant/README.md"
      ],
      "metadata": {
        "id": "Bsg3AoMNuCPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Any\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.retrievers.tavily_search_api import TavilySearchAPIRetriever\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import (\n",
        "    ConfigurableField,\n",
        "    Runnable,\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "RESULTS_PER_QUESTION = 3\n",
        "\n",
        "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
        "\n",
        "\n",
        "def scrape_text(url: str):\n",
        "    # Send a GET request to the webpage\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the content of the request with BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract all text from the webpage\n",
        "            page_text = soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "            # Print the extracted text\n",
        "            return page_text\n",
        "        else:\n",
        "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return f\"Failed to retrieve the webpage: {e}\"\n",
        "\n",
        "\n",
        "def web_search(query: str, num_results: int):\n",
        "    results = ddg_search.results(query, num_results)\n",
        "    return [r[\"link\"] for r in results]\n",
        "\n",
        "\n",
        "get_links: Runnable[Any, Any] = (\n",
        "    RunnablePassthrough()\n",
        "    | RunnableLambda(\n",
        "        lambda x: [\n",
        "            {\"url\": url, \"question\": x[\"question\"]}\n",
        "            for url in web_search(query=x[\"question\"], num_results=RESULTS_PER_QUESTION)\n",
        "        ]\n",
        "    )\n",
        ").configurable_alternatives(\n",
        "    ConfigurableField(\"search_engine\"),\n",
        "    default_key=\"duckduckgo\",\n",
        "    tavily=RunnableLambda(lambda x: x[\"question\"])\n",
        "    | RunnableParallel(\n",
        "        {\n",
        "            \"question\": RunnablePassthrough(),\n",
        "            \"results\": TavilySearchAPIRetriever(k=RESULTS_PER_QUESTION),\n",
        "        }\n",
        "    )\n",
        "    | RunnableLambda(\n",
        "        lambda x: [\n",
        "            {\"url\": result.metadata[\"source\"], \"question\": x[\"question\"]}\n",
        "            for result in x[\"results\"]\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "\n",
        "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"{agent_prompt}\"),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Write 3 google search queries to search online that form an \"\n",
        "            \"objective opinion from the following: {question}\\n\"\n",
        "            \"You must respond with a list of strings in the following format: \"\n",
        "            '[\"query 1\", \"query 2\", \"query 3\"].',\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "AUTO_AGENT_INSTRUCTIONS = \"\"\"\n",
        "This task involves researching a given topic, regardless of its complexity or the availability of a definitive answer. The research is conducted by a specific agent, defined by its type and role, with each agent requiring distinct instructions.\n",
        "Agent\n",
        "The agent is determined by the field of the topic and the specific name of the agent that could be utilized to research the topic provided. Agents are categorized by their area of expertise, and each agent type is associated with a corresponding emoji.\n",
        "\n",
        "examples:\n",
        "task: \"should I invest in apple stocks?\"\n",
        "response:\n",
        "{\n",
        "    \"agent\": \"ðŸ’° Finance Agent\",\n",
        "    \"agent_role_prompt: \"You are a seasoned finance analyst AI assistant. Your primary goal is to compose comprehensive, astute, impartial, and methodically arranged financial reports based on provided data and trends.\"\n",
        "}\n",
        "task: \"could reselling sneakers become profitable?\"\n",
        "response:\n",
        "{\n",
        "    \"agent\":  \"ðŸ“ˆ Business Analyst Agent\",\n",
        "    \"agent_role_prompt\": \"You are an experienced AI business analyst assistant. Your main objective is to produce comprehensive, insightful, impartial, and systematically structured business reports based on provided business data, market trends, and strategic analysis.\"\n",
        "}\n",
        "task: \"what are the most interesting sites in Tel Aviv?\"\n",
        "response:\n",
        "{\n",
        "    \"agent:  \"ðŸŒ Travel Agent\",\n",
        "    \"agent_role_prompt\": \"You are a world-travelled AI tour guide assistant. Your main purpose is to draft engaging, insightful, unbiased, and well-structured travel reports on given locations, including history, attractions, and cultural insights.\"\n",
        "}\n",
        "\"\"\"\n",
        "CHOOSE_AGENT_PROMPT = ChatPromptTemplate.from_messages(\n",
        "    [SystemMessage(content=AUTO_AGENT_INSTRUCTIONS), (\"user\", \"task: {task}\")]\n",
        ")\n",
        "\n",
        "SUMMARY_TEMPLATE = \"\"\"{text}\n",
        "\n",
        "-----------\n",
        "\n",
        "Using the above text, answer in short the following question:\n",
        "\n",
        "> {question}\n",
        "\n",
        "-----------\n",
        "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"  # noqa: E501\n",
        "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
        "\n",
        "scrape_and_summarize: Runnable[Any, Any] = (\n",
        "    RunnableParallel(\n",
        "        {\n",
        "            \"question\": lambda x: x[\"question\"],\n",
        "            \"text\": lambda x: scrape_text(x[\"url\"])[:10000],\n",
        "            \"url\": lambda x: x[\"url\"],\n",
        "        }\n",
        "    )\n",
        "    | RunnableParallel(\n",
        "        {\n",
        "            \"summary\": SUMMARY_PROMPT | llm | StrOutputParser(),\n",
        "            \"url\": lambda x: x[\"url\"],\n",
        "        }\n",
        "    )\n",
        "    | RunnableLambda(lambda x: f\"Source Url: {x['url']}\\nSummary: {x['summary']}\")\n",
        ")\n",
        "\n",
        "multi_search = get_links | scrape_and_summarize.map() | (lambda x: \"\\n\".join(x))\n",
        "\n",
        "\n",
        "def load_json(s):\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "\n",
        "search_query = SEARCH_PROMPT | llm | StrOutputParser() | load_json\n",
        "choose_agent = (\n",
        "    CHOOSE_AGENT_PROMPT | llm | StrOutputParser() | load_json\n",
        ")\n",
        "\n",
        "get_search_queries = (\n",
        "    RunnablePassthrough().assign(\n",
        "        agent_prompt=RunnableParallel({\"task\": lambda x: x})\n",
        "        | choose_agent\n",
        "        | (lambda x: x.get(\"agent_role_prompt\"))\n",
        "    )\n",
        "    | search_query\n",
        ")\n",
        "\n",
        "\n",
        "search_chain = (\n",
        "    get_search_queries\n",
        "    | (lambda x: [{\"question\": q} for q in x])\n",
        "    | multi_search.map()\n",
        "    | (lambda x: \"\\n\\n\".join(x))\n",
        ")"
      ],
      "metadata": {
        "id": "vX0YeqZMpY1y"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"\n",
        "\n",
        "\n",
        "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
        "RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
        "--------\n",
        "{research_summary}\n",
        "--------\n",
        "\n",
        "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
        "The report should focus on the answer to the question, should be well structured, informative, \\\n",
        "in depth, with facts and numbers if available and a minimum of 200 words.\n",
        "\n",
        "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
        "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
        "You must write the report in apa format.\"\"\"\n",
        "\n",
        "\n",
        "RESOURCE_REPORT_TEMPLATE = \"\"\"Information:\n",
        "--------\n",
        "{research_summary}\n",
        "--------\n",
        "\n",
        "Based on the above information, generate a bibliography recommendation report for the following question or topic: \"{question}\". \\\n",
        "The report should provide a detailed analysis of each recommended resource, explaining how each source can contribute to finding answers to the research question. \\\n",
        "Focus on the relevance, reliability, and significance of each source. \\\n",
        "Ensure that the report is well-structured, informative, and in-depth. \\\n",
        "Include relevant facts, figures, and numbers whenever available. \\\n",
        "The report should have a minimum length of 200 words.\"\"\"\n",
        "\n",
        "OUTLINE_REPORT_TEMPLATE = \"\"\"Information:\n",
        "--------\n",
        "{research_summary}\n",
        "--------\n",
        "\n",
        "Using the above information, generate an outline for a research report for the following question or topic: \"{question}\". \\\n",
        "The outline should provide a well-structured framework for the research report, including the main sections, subsections, and key points to be covered. \\\n",
        "The research report should be detailed, informative, in-depth, and a minimum of 200 words. \\\n",
        "Ensure that the outline is well-structured, informative, and include relevant facts, figures, and numbers whenever available.\"\"\"    #1,200 words\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
        "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
        "    ]\n",
        ").configurable_alternatives(\n",
        "    ConfigurableField(\"report_type\"),\n",
        "    default_key=\"research_report\",\n",
        "    resource_report=ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", WRITER_SYSTEM_PROMPT),\n",
        "            (\"user\", RESOURCE_REPORT_TEMPLATE),\n",
        "        ]\n",
        "    ),\n",
        "    outline_report=ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", WRITER_SYSTEM_PROMPT),\n",
        "            (\"user\", OUTLINE_REPORT_TEMPLATE),\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "writer_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "TlRi81ycwIsc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "chain_notypes = (\n",
        "    RunnablePassthrough().assign(research_summary=search_chain) | writer_chain\n",
        ")\n",
        "\n",
        "\n",
        "class InputType(BaseModel):\n",
        "    question: str\n",
        "\n",
        "\n",
        "chain = chain_notypes.with_types(input_type=InputType)"
      ],
      "metadata": {
        "id": "-w1JKiqEwqT9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Best long term stocks to buy\"\n",
        "result = chain.invoke({\"question\": question})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voBQwzgayf_n",
        "outputId": "7e294944-686f-4c6e-a7f8-b2e988987069"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Best Long-Term Stocks to Buy**\n",
            "\n",
            "**Introduction:**\n",
            "\n",
            "Investing in long-term stocks offers the potential for steady and sustainable returns over extended periods of time. While there is no guaranteed \"best\" stock to buy, certain companies and industries consistently demonstrate strong growth potential and financial stability.\n",
            "\n",
            "**Factors to Consider:**\n",
            "\n",
            "* **Industry Fundamentals:** Analyze the industry's growth prospects, competitive landscape, and regulatory environment.\n",
            "* **Financial Strength:** Assess the company's financial health, debt-to-equity ratio, and cash flow generation capabilities.\n",
            "* **Growth Potential:** Evaluate the company's future earnings growth rate and potential for dividend growth.\n",
            "* **Valuation:** Determine the intrinsic value of the stock by comparing its current price to its underlying earnings and assets.\n",
            "\n",
            "**Top Long-Term Stocks to Consider:**\n",
            "\n",
            "**1. Apple (AAPL)**\n",
            "\n",
            "* Industry: Technology\n",
            "* Fundamentals: Strong brand recognition, leading market share in key products, high cash flow generation.\n",
            "* Growth Potential: High, driven by technological advancements and expanding product offerings.\n",
            "* Valuation: Currently trading at a premium to peers, but still offering significant growth potential.\n",
            "\n",
            "**2. Microsoft (MSFT)**\n",
            "\n",
            "* Industry: Technology\n",
            "* Fundamentals: Global market leader in software and cloud services, diversified product portfolio.\n",
            "* Growth Potential: Moderate, benefiting from strong demand for cloud computing and ongoing software innovation.\n",
            "* Valuation: Trading at a premium to peers, but still offering significant growth potential.\n",
            "\n",
            "**3. Amazon (AMZN)**\n",
            "\n",
            "* Industry: E-commerce and Technology\n",
            "* Fundamentals: Dominant e-commerce platform, cloud computing services, and growing streaming service.\n",
            "* Growth Potential: High, benefiting from expanding market share and technological advancements.\n",
            "* Valuation: Currently trading at a premium to peers, but still offering significant growth potential.\n",
            "\n",
            "**4. Tesla (TSLA)**\n",
            "\n",
            "* Industry: Automotive and Clean Energy\n",
            "* Fundamentals: Leader in electric vehicle and battery technology, expanding into solar energy and autonomous driving.\n",
            "* Growth Potential: High, benefiting from growing demand for sustainable transportation solutions.\n",
            "* Valuation: Trading at a premium to peers, but still offering significant growth potential.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Investing in these top long-term stocks offers investors the potential for steady and sustainable returns while providing long-term stability. While market conditions and individual circumstances may vary, these companies have a strong track record of growth and financial strength, making them compelling options for long-term investors.\n"
          ]
        }
      ]
    }
  ]
}