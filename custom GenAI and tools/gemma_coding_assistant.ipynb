{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o32lou1Gdi5W"
      },
      "source": [
        "# Recursive code generation with LangChain, LangGraph, LangSmith, HuggingFace and Gemma\n",
        "\n",
        "References: [AlphaCodium](https://github.com/Codium-ai/AlphaCodium) , [LangChain examples](https://github.com/langchain-ai/langgraph/blob/main/examples/code_assistant/langgraph_code_assistant.ipynb)\n",
        "\n",
        "Gemma is a relatively small model and you may need to add some more tweaks to the code in this notebook to improve the performance. E.g. you might want to finetune gemma first for code generation and only after that use it as cod assistant. Here we directly use gemma, but [here](https://github.com/VladimerKhasia/ML-in-Notebooks/blob/main/custom%20GenAI%20and%20tools/gemma_qlora_rlhf.ipynb) is the notebook showing how to fine-tune gemma for the code generation.\n",
        "\n",
        "Do not forget to set your HugggingFace `HF_TOKEN` and LangChain `LANGCHAIN_API_KEY` secrets in google colab.\n",
        "\n",
        "</br>\n",
        "NOTE:\n",
        "\n",
        "- Gemma 2b strugles wenn long questions end with question mark. If you remove question marks present in the evaluation dataset it will boost the performance.\n",
        "\n",
        "- In this notebook there is few-shot prompting implemented but commented out. You can create good propts in the following way: 1. write a prompt and get the models output, 2. restructure models output and make its structure as similar to your need as possible which gives you new prompt, 3. You can iterate and compare which prompt will give you bes desired results. It is basically injection of your needs and structure into the \"language\" of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBAfoIfwFWv"
      },
      "outputs": [],
      "source": [
        "#%%capture --no-stderr          # this is basically to avoid warnings\n",
        "%pip install -U -q langchain_community tiktoken langchainhub chromadb langchain langgraph faiss-cpu bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGQuLKNN3Xiw"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CdX0fVhjjZi6"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-2b-it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZeGdxLVltCm_"
      },
      "outputs": [],
      "source": [
        "#@title  for GPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_use_double_quant=True,\n",
        "#         bnb_4bit_quant_type=\"nf4\",\n",
        "#         bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "69cb6c16f5bc4ba683c4e13fb3a8022b",
            "dbaa7427d5c446bc9b3a0d46b680cf00",
            "7338f19f45b54d78937d277d5a5a8654",
            "a415597a6b6847dc9d640f64f451bc84",
            "c145db07a3484af48df65a9e5348d0c2",
            "cbbad540781e4b368c24b67418dd9657",
            "fa489145197344488d95df5dfaf8d451",
            "03ecb74addb14d38807a31af2bd04803",
            "6d858868f9b7459f983b06faca87d02e",
            "dfc60633d13c42b3b6727d8c57e2f68b",
            "ee0f65a1c1f9484080b4494a6f75c188"
          ]
        },
        "id": "ZkXAAiYLu9F_",
        "outputId": "77587d03-cedc-4026-8857-80e23e870cda"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69cb6c16f5bc4ba683c4e13fb3a8022b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title for CPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xxByl6cZFP2"
      },
      "source": [
        "# gemma coding assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4GOp8E7OTk7B"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as Soup\n",
        "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "# LCEL docs\n",
        "url = \"https://python.langchain.com/docs/expression_language/\"\n",
        "loader = RecursiveUrlLoader(\n",
        "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "# Sort the list based on the URLs and get the text\n",
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cguqJZW0iFaE"
      },
      "outputs": [],
      "source": [
        "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    AsyncCallbackManagerForLLMRun,\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
        "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from langchain_core.runnables import run_in_executor\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class GemmaChatModel(BaseChatModel):\n",
        "    \"\"\"\n",
        "    A custom chat model powered by Gemma from Hugging Face, designed to be informative, comprehensive, and engaging.\n",
        "    See the custom model guide here: https://python.langchain.com/docs/modules/model_io/chat/custom_chat_model/\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = \"gemma_chat_model\"  # Replace with the actual Gemma model name\n",
        "    task: str = \"conversational\"  # Task for the pipeline (conversational or summarization)\n",
        "    n: int = 2048\n",
        "    model : Any = None\n",
        "    tokenizer : Any = None\n",
        "\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            messages: The list of prompt messages.\n",
        "            stop: Optional list of stop tokens.\n",
        "            run_manager: Optional callback manager.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            A ChatResult object containing the generated response.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = messages[-1].content #[: self.n]\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model.generate(**input_ids, max_new_tokens=self.n)\n",
        "        text = self.tokenizer.decode(outputs[0])\n",
        "        #text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "        start_index, end_index = text.find(\"<eos>\"), text.rfind(\"<eos>\")\n",
        "        response = text[start_index+len(\"<eos>\"):end_index].strip()\n",
        "\n",
        "        message = AIMessage(content=response, additional_kwargs={}, response_metadata={\"time_in_seconds\": 3})\n",
        "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns the type of language model used: \"gemma_chat_model\".\n",
        "        \"\"\"\n",
        "        return \"gemma_chat_model\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns a dictionary of identifying parameters for LangChain callbacks.\n",
        "        \"\"\"\n",
        "        return {\"model_name\": self.model_name, \"task\": self.task}\n",
        "\n",
        "llm = GemmaChatModel()\n",
        "llm.model = model\n",
        "llm.tokenizer = tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIhhAQ4qqcFN",
        "outputId": "db54d801-284c-4782-a9ad-e0bc81bc2f54"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hello! 👋\\n\\nIt's great to hear from you. What can I do for you today? 😊\", response_metadata={'time_in_seconds': 3}, id='run-1eebbc13-9bcf-4c19-9331-14ec1cd15dde-0')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"hello\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R-ZVq0HAnGW9"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "# # This part is for few shot prompting\n",
        "# #https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples_chat/\n",
        "\n",
        "# example_input_one = \"write python function for adding two numbers\"\n",
        "# example_output_one = \"\"\"Get the two numbers from the user. a = int(input(\"Enter the first number: \")) b = int(input(\"Enter the second number: \"))\n",
        "# Add the two numbers and print the result. sum = add_two_numbers(a, b) print(f\"The sum of {a} and {b} is {sum}\")\n",
        "# import python\n",
        "# def add_two_numbers(a, b):\n",
        "#       \\\"\\\"\\\"This function adds two numbers. Args: a (int): The first number. b (int): The second number. Returns: int: The sum of a and b.\\\"\\\"\\\"\n",
        "#       return a + b \"\"\"\n",
        "\n",
        "# examples = [\n",
        "#     {\"input\": example_input_one, \"output\": example_output_one},\n",
        "# ]\n",
        "# example_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"human\", \"{input}\"),\n",
        "#         (\"ai\", \"{output}\"),\n",
        "#     ]\n",
        "# )\n",
        "# few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "#     example_prompt=example_prompt,\n",
        "#     examples=examples,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dacZlsRQ_TV7"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "def dictifier(ai_message):\n",
        "\n",
        "    def code_only(x):\n",
        "      if x.startswith('python'):\n",
        "        return x[6:]\n",
        "\n",
        "    def add_non_nones(acc, x):\n",
        "        if x is not None:\n",
        "            return acc + x\n",
        "        else:\n",
        "            return acc\n",
        "\n",
        "    filtered = list( map( code_only, ai_message.content.split(\"```\") ) )\n",
        "    code_string = reduce(add_non_nones, filtered, '')\n",
        "\n",
        "    # Create a dictionary\n",
        "    result_dict = {\n",
        "        'prefix': ai_message.content,\n",
        "        #'imports': import_section,\n",
        "        'code': code_string,\n",
        "    }\n",
        "    return json.dumps(result_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ap5HEOEdtDAV"
      },
      "outputs": [],
      "source": [
        "## Parser: https://python.langchain.com/docs/modules/model_io/output_parsers/types/pydantic/   https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start/\n",
        "##         https://python.langchain.com/docs/modules/model_io/output_parsers/custom/  custom one\n",
        "\n",
        "from typing import List\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
        "\n",
        "class Code(BaseModel):\n",
        "    \"\"\"Code output\"\"\"\n",
        "\n",
        "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
        "    ## imports: str = Field(description=\"Code block import statements\")\n",
        "    code: str = Field(description=\"Code block not including import statements\")\n",
        "    description = \"Schema for code solutions to questions about LCEL.\"\n",
        "\n",
        "    # # Custom validation logic with Pydantic for prefix.\n",
        "    # @validator(\"prefix\")\n",
        "    # def description_isnot_python_comment(cls, content):\n",
        "    #     if not content: #TODO: implement logic here\n",
        "    #         raise ValueError(\"Desctiption is not in a comment format!\")\n",
        "    #     return content\n",
        "\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=Code)\n",
        "\n",
        "# Just inspect what it contains\n",
        "##parser.dict()['pydantic_object'].__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H196zdK4Dgbm",
        "outputId": "e6208780-fccf-47dc-fb7f-f49e99b38d18"
      },
      "outputs": [],
      "source": [
        "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\",\"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
        "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user\n",
        "    question based on the above provided documentation. Ensure any code you provide can be executed \\n\n",
        "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
        "    Then list the imports. And finally list the functioning code block.\\n\n",
        "    Use these format instructions for the structure of your answer: {format_instructions}.\"\"\",),\n",
        "    #few_shot_prompt,\n",
        "    (\"user\", \"{messages}\"),])\n",
        "\n",
        "coder = code_gen_prompt | llm | dictifier | parser\n",
        "\n",
        "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt\"\n",
        "result = coder.invoke({\"context\": concatenated_content, \"format_instructions\": parser.get_format_instructions(), \"messages\":[(\"user\", question)]})\n",
        "# ## parser.invoke(result)   #use this if you have not imcluded parser in the chain and want to test how parser works\n",
        "# ## result.content          #use this in case you are dealing with direct llm output - it's AIMessage\n",
        "\n",
        "print(result.prefix, \"\\n--------------------------------------\\n\", result.code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "H4-lqiL8ZQLP"
      },
      "outputs": [],
      "source": [
        "#@title graph part\n",
        "\n",
        "from operator import itemgetter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from typing import Dict, TypedDict, List\n",
        "from langgraph.graph import END, StateGraph\n",
        "\n",
        "coder = code_gen_prompt | llm | dictifier | parser\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        error : Binary flag for control flow to indicate whether test error was tripped\n",
        "        messages : With user question, error messages, reasoning\n",
        "        generation : Code solution\n",
        "        iterations : Number of tries\n",
        "    \"\"\"\n",
        "\n",
        "    error : str\n",
        "    messages : List\n",
        "    generation : str\n",
        "    iterations : int\n",
        "\n",
        "\n",
        "### Parameter\n",
        "\n",
        "# Max tries\n",
        "max_iterations = 3\n",
        "# Reflect\n",
        "# flag = 'reflect'\n",
        "flag = 'do not reflect'\n",
        "\n",
        "### Nodes\n",
        "\n",
        "def generate(state: GraphState):\n",
        "    \"\"\"\n",
        "    Generate a code solution\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    error = state[\"error\"]\n",
        "\n",
        "    # We have been routed back to generation with an error\n",
        "    if error == \"yes\":\n",
        "        messages += [(\"user\",\"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\")]\n",
        "\n",
        "    # Solution\n",
        "    code_solution = coder.invoke({\"context\": concatenated_content, \"format_instructions\": parser.get_format_instructions(), \"messages\" : messages})\n",
        "    ## messages += [(\"assistant\",f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\")]\n",
        "    messages += [(\"assistant\",f\"{code_solution.prefix} \\n \\n Code: {code_solution.code}\")]\n",
        "\n",
        "    # Increment\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "def code_check(state: GraphState):\n",
        "    \"\"\"\n",
        "    Check code\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, error\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECKING CODE---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    # Get solution components\n",
        "    prefix = code_solution.prefix\n",
        "    ##imports = code_solution.imports\n",
        "    code = code_solution.code\n",
        "\n",
        "    # # Check imports\n",
        "    # try:\n",
        "    #     exec(imports)\n",
        "    # except Exception as e:\n",
        "    #     print(\"---CODE IMPORT CHECK: FAILED---\")\n",
        "    #     error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
        "    #     messages += error_message\n",
        "    #     return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "    # # Check execution\n",
        "    # try:\n",
        "    #     exec(imports + \"\\n\" + code)\n",
        "    # except Exception as e:\n",
        "    #     print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "    #     error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "    #     messages += error_message\n",
        "    #     return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "    try:\n",
        "        exec(code)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"yes\"}\n",
        "\n",
        "\n",
        "    # No errors\n",
        "    print(\"---NO CODE TEST FAILURES---\")\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations, \"error\": \"no\"}\n",
        "\n",
        "def reflect(state: GraphState):\n",
        "    \"\"\"\n",
        "    Reflect on errors\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "\n",
        "    # Prompt reflection\n",
        "    reflection_message = [(\"user\", \"\"\"You tried to solve this problem and failed a unit test. Reflect on this failure\n",
        "                                    given the provided documentation. Write a few key suggestions based on the\n",
        "                                    documentation to avoid making this mistake again.\"\"\")]\n",
        "\n",
        "    # Add reflection\n",
        "    reflections = coder.invoke({\"context\"  : concatenated_content, \"format_instructions\": parser.get_format_instructions(), \"messages\" : messages})\n",
        "    messages += [(\"assistant\" , f\"Here are reflections on the error: {reflections}\")]\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "### Edges\n",
        "\n",
        "def decide_to_finish(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines whether to finish.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "    error = state[\"error\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    if error == \"no\" or iterations == max_iterations:\n",
        "        print(\"---DECISION: FINISH---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
        "        if flag == 'reflect':\n",
        "            return \"reflect\"\n",
        "        else:\n",
        "            return \"generate\"\n",
        "\n",
        "\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"generate\", generate)  # generation solution\n",
        "workflow.add_node(\"check_code\", code_check)  # check code\n",
        "workflow.add_node(\"reflect\", reflect)  # reflect\n",
        "\n",
        "# Build graph\n",
        "workflow.set_entry_point(\"generate\")\n",
        "workflow.add_edge(\"generate\", \"check_code\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_code\",\n",
        "    decide_to_finish,\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"reflect\": \"reflect\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"reflect\", \"generate\")\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJJxKH9FZ4eB",
        "outputId": "26957515-5d40-487c-87f3-343d82bb9bda"
      },
      "outputs": [],
      "source": [
        "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt\"\n",
        "app.invoke({\"messages\":[(\"user\", question)],\"iterations\":0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8PV5oy0g2uX"
      },
      "source": [
        "# Evaluation of gemma coding assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tmhPajTpkboM"
      },
      "outputs": [],
      "source": [
        "import langsmith\n",
        "from langsmith.schemas import Example, Run\n",
        "from langsmith.evaluation import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ptf_O7HPhViN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "client = langsmith.Client()\n",
        "# Clone the dataset to your tenant to use it\n",
        "public_dataset = (\"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\")\n",
        "client.clone_public_dataset(public_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "V2tHWCiCiNY5"
      },
      "outputs": [],
      "source": [
        "# def check_import(run: Run, example: Example) -> dict:\n",
        "#     imports = run.outputs.get(\"imports\")\n",
        "#     try:\n",
        "#         exec(imports)\n",
        "#         return {\"key\": \"import_check\" , \"score\": 1}\n",
        "#     except:\n",
        "#         return {\"key\": \"import_check\" , \"score\": 0}\n",
        "\n",
        "# def check_execution(run: Run, example: Example) -> dict:\n",
        "#     imports = run.outputs.get(\"imports\")\n",
        "#     code = run.outputs.get(\"code\")\n",
        "#     try:\n",
        "#         exec(imports + \"\\n\" + code)\n",
        "#         return {\"key\": \"code_execution_check\" , \"score\": 1}\n",
        "#     except:\n",
        "#         return {\"key\": \"code_execution_check\" , \"score\": 0}\n",
        "\n",
        "def check_execution(run: Run, example: Example) -> dict:\n",
        "    code = run.outputs.get(\"code\")\n",
        "    try:\n",
        "        exec(\"\\n\" + code)\n",
        "        return {\"key\": \"code_execution_check\" , \"score\": 1}\n",
        "    except:\n",
        "        return {\"key\": \"code_execution_check\" , \"score\": 0}\n",
        "\n",
        "def predict_base_case(example: dict):\n",
        "    \"\"\" Context stuffing \"\"\"\n",
        "    solution = coder.invoke({\"context\"  : concatenated_content, \"messages\" : [(\"user\",example[\"question\"])]})\n",
        "    # return {\"imports\": solution_structured.imports, \"code\": solution_structured.code}\n",
        "    return {\"code\": solution.code}\n",
        "\n",
        "def predict_langgraph(example: dict):\n",
        "    \"\"\" LangGraph \"\"\"\n",
        "    graph = app.invoke({\"messages\":[(\"user\",example[\"question\"])],\"iterations\":0})\n",
        "    solution = graph[\"generation\"]\n",
        "    # return {\"imports\": solution.imports, \"code\": solution.code}\n",
        "    return {\"code\": solution.code}\n",
        "\n",
        "\n",
        "# Evaluator\n",
        "##code_evalulator = [check_import,check_execution]\n",
        "code_evalulator = [check_execution]\n",
        "\n",
        "# Dataset\n",
        "dataset_name = \"lcel-teacher-eval\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBbtKSXWiOaO"
      },
      "outputs": [],
      "source": [
        "# Run base case\n",
        "experiment_results_ = evaluate(\n",
        "    predict_base_case,\n",
        "    data=dataset_name,\n",
        "    evaluators=code_evalulator,\n",
        "    experiment_prefix=f\"test-without-langgraph-{model_id}\",\n",
        "    max_concurrency=2,\n",
        "    metadata={\n",
        "      \"llm\": model_id,\n",
        "    },\n",
        ")\n",
        "# Run with langgraph\n",
        "experiment_results = evaluate(\n",
        "    predict_langgraph,\n",
        "    data=dataset_name,\n",
        "    evaluators=code_evalulator,\n",
        "    experiment_prefix=f\"test-with-langgraph-{model_id}-{flag}\",\n",
        "    max_concurrency=2,\n",
        "    metadata={\n",
        "      \"llm\": model_id,\n",
        "      \"feedback\": flag,\n",
        "    },\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "o32lou1Gdi5W",
        "_xxByl6cZFP2",
        "X8PV5oy0g2uX"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03ecb74addb14d38807a31af2bd04803": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69cb6c16f5bc4ba683c4e13fb3a8022b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbaa7427d5c446bc9b3a0d46b680cf00",
              "IPY_MODEL_7338f19f45b54d78937d277d5a5a8654",
              "IPY_MODEL_a415597a6b6847dc9d640f64f451bc84"
            ],
            "layout": "IPY_MODEL_c145db07a3484af48df65a9e5348d0c2"
          }
        },
        "6d858868f9b7459f983b06faca87d02e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7338f19f45b54d78937d277d5a5a8654": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ecb74addb14d38807a31af2bd04803",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d858868f9b7459f983b06faca87d02e",
            "value": 2
          }
        },
        "a415597a6b6847dc9d640f64f451bc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfc60633d13c42b3b6727d8c57e2f68b",
            "placeholder": "​",
            "style": "IPY_MODEL_ee0f65a1c1f9484080b4494a6f75c188",
            "value": " 2/2 [00:43&lt;00:00, 18.17s/it]"
          }
        },
        "c145db07a3484af48df65a9e5348d0c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbad540781e4b368c24b67418dd9657": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbaa7427d5c446bc9b3a0d46b680cf00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbbad540781e4b368c24b67418dd9657",
            "placeholder": "​",
            "style": "IPY_MODEL_fa489145197344488d95df5dfaf8d451",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "dfc60633d13c42b3b6727d8c57e2f68b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee0f65a1c1f9484080b4494a6f75c188": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa489145197344488d95df5dfaf8d451": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
