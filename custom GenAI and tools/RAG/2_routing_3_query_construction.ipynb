{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "IOE0EQIpf3SE",
      "metadata": {
        "id": "IOE0EQIpf3SE"
      },
      "source": [
        "# Gemma model and GTE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o32lou1Gdi5W",
      "metadata": {
        "id": "o32lou1Gdi5W"
      },
      "source": [
        "#### Gemma model setup for Langchain RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "MkgFAXWVW3wm",
      "metadata": {
        "id": "MkgFAXWVW3wm"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_community sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NGQuLKNN3Xiw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGQuLKNN3Xiw",
        "outputId": "247c7b2a-a222-41e7-de29-b3c007bcda88"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes accelerate   # accelerate is for GPU and additionally bitsandbytes is for quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "CdX0fVhjjZi6",
      "metadata": {
        "id": "CdX0fVhjjZi6"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-2b-it\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZeGdxLVltCm_",
      "metadata": {
        "cellView": "form",
        "id": "ZeGdxLVltCm_"
      },
      "outputs": [],
      "source": [
        "#@title  for GPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_use_double_quant=True,\n",
        "#         bnb_4bit_quant_type=\"nf4\",\n",
        "#         bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "\n",
        "### model.save_pretrained(\"./model.\")\n",
        "### model = AutoModelForCausalLM.from_pretrained(\"./model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZkXAAiYLu9F_",
      "metadata": {
        "cellView": "form",
        "id": "ZkXAAiYLu9F_"
      },
      "outputs": [],
      "source": [
        "#@title for CPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "lXuW_0ZC2hRc",
      "metadata": {
        "id": "lXuW_0ZC2hRc"
      },
      "outputs": [],
      "source": [
        "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    AsyncCallbackManagerForLLMRun,\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
        "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from langchain_core.runnables import run_in_executor\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class GemmaChatModel(BaseChatModel):\n",
        "    \"\"\"\n",
        "    A custom chat model powered by Gemma from Hugging Face, designed to be informative, comprehensive, and engaging.\n",
        "    See the custom model guide here: https://python.langchain.com/docs/modules/model_io/chat/custom_chat_model/\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = \"gemma_chat_model\"  # Replace with the actual Gemma model name\n",
        "    task: str = \"conversational\"  # Task for the pipeline (conversational or summarization)\n",
        "    #temperature = 0.0\n",
        "    n: int = 2048\n",
        "    model : Any = None\n",
        "    tokenizer : Any = None\n",
        "\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            messages: The list of prompt messages.\n",
        "            stop: Optional list of stop tokens.\n",
        "            run_manager: Optional callback manager.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            A ChatResult object containing the generated response.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = messages[-1].content #[: self.n]\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model.generate(**input_ids, max_new_tokens=self.n)       # , temperature=self.temperature\n",
        "        text = self.tokenizer.decode(outputs[0])\n",
        "        #text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "        start_index, end_index = text.find(\"<eos>\"), text.rfind(\"<eos>\")\n",
        "        response = text[start_index+len(\"<eos>\"):end_index].strip()\n",
        "\n",
        "        message = AIMessage(content=response, additional_kwargs={}, response_metadata={\"time_in_seconds\": 3})\n",
        "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns the type of language model used: \"gemma_chat_model\".\n",
        "        \"\"\"\n",
        "        return \"gemma_chat_model\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns a dictionary of identifying parameters for LangChain callbacks.\n",
        "        \"\"\"\n",
        "        return {\"model_name\": self.model_name, \"task\": self.task}\n",
        "\n",
        "llm = GemmaChatModel()\n",
        "llm.model = model               # This is simple but not production level way of doing things. It's just for avoiding colab run out of memory on CPU\n",
        "llm.tokenizer = tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PVXsfwvTvVxf",
      "metadata": {
        "id": "PVXsfwvTvVxf"
      },
      "source": [
        "#### Custom embedding model setup for lanchain RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iv6_UMDOvhYA",
      "metadata": {
        "id": "iv6_UMDOvhYA"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ffa253-e427-4663-bc0b-9b1fcce508e6",
      "metadata": {
        "id": "d4ffa253-e427-4663-bc0b-9b1fcce508e6"
      },
      "source": [
        "# Routing\n",
        "\n",
        "- Logical routing\n",
        "- Semantic routing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc509da-52b2-49fc-bc45-e5fd75ff5fed",
      "metadata": {
        "id": "9bc509da-52b2-49fc-bc45-e5fd75ff5fed"
      },
      "outputs": [],
      "source": [
        "! pip install tiktoken langchainhub chromadb youtube-transcript-api pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3bc82800-5498-40be-86db-e1f6df8c86da",
      "metadata": {
        "id": "3bc82800-5498-40be-86db-e1f6df8c86da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45fd9558-c53f-4a6c-80f0-c31b3bfd55de",
      "metadata": {
        "id": "45fd9558-c53f-4a6c-80f0-c31b3bfd55de"
      },
      "source": [
        "## Logical and Semantic routing\n",
        "\n",
        "Use function-calling for classification.\n",
        "\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/query_analysis/techniques/routing#routing-to-multiple-indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mnNFcDPmmsnn",
      "metadata": {
        "id": "mnNFcDPmmsnn"
      },
      "source": [
        "#### Logical Routing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "R22OrISBsMia",
      "metadata": {
        "id": "R22OrISBsMia"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "#from langchain_core.load.dump import dumps  #or#  import json\n",
        "\n",
        "# Function to extract the leading word from a code block\n",
        "def artificial_parser(code_block):\n",
        "    class DotDict(dict):\n",
        "      def __getattr__(self, attr):\n",
        "          if attr in self:\n",
        "              return self[attr]\n",
        "          else:\n",
        "              raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
        "    # Regular expression pattern to match the leading word of the code block\n",
        "    pattern = r\"```(\\w+)\"\n",
        "    # Find the leading word using regular expression\n",
        "    match = re.search(pattern, code_block)\n",
        "    # Extract the leading word if found\n",
        "    if match:\n",
        "        leading_word = match.group(1)\n",
        "        obj = DotDict({\"datasource\" : leading_word, \"response\" : code_block})\n",
        "        return obj\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "04c2cf60-d636-4992-a021-1236f7688999",
      "metadata": {
        "id": "04c2cf60-d636-4992-a021-1236f7688999"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Data model\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
        "    )\n",
        "\n",
        "# LLM with function call\n",
        "## we cannot do this: structured_llm = llm.with_structured_output(RouteQuery)\n",
        "# but you can somewhat mimic this. We discussed some of them here: https://github.com/VladimerKhasia/ML-in-Notebooks/blob/main/custom%20GenAI%20and%20tools/gemma_coding_assistant.ipynb\n",
        "\n",
        "# Set up a parser + inject instructions into the prompt template.\n",
        "parser = PydanticOutputParser(pydantic_object=RouteQuery)\n",
        "\n",
        "# Prompt\n",
        "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
        "\n",
        "Based on the programming language the question is referring to, route it to the relevant data source.\\n\n",
        "Use these format instructions for the structure of your answer: {format_instructions}.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        #few_shot_prompt,     # additionally you can inject here few shot prompts: https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples_chat/\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define router\n",
        "router = prompt | llm | StrOutputParser() | artificial_parser #| parser # to actually use the parser for validation you have more work to do, as the actual output is far from the required"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb5aaa3-7826-471c-9203-390009cbb4d6",
      "metadata": {
        "id": "1cb5aaa3-7826-471c-9203-390009cbb4d6"
      },
      "source": [
        "Note: we used function calling to produce structured output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "cfc6febc-93df-49b4-9920-c93589ba021e",
      "metadata": {
        "id": "cfc6febc-93df-49b4-9920-c93589ba021e"
      },
      "outputs": [],
      "source": [
        "question = \"\"\"Why doesn't the following code work:\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
        "prompt.invoke(\"french\")\n",
        "\"\"\"\n",
        "\n",
        "result = router.invoke({\"question\": question, \"format_instructions\": parser.get_format_instructions()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "277536df-0904-4d99-92bb-652621afbdec",
      "metadata": {
        "id": "277536df-0904-4d99-92bb-652621afbdec"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "636a43ae-50f3-43a1-a1b7-93266ea13bcd",
        "outputId": "5aa56bf6-12a9-49d1-d693-615a353fdc81"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'python'"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# instead of directly accessing: result.datasource\n",
        "# requested struture also did not work well you can use few-shot technics to improve it\n",
        "result.datasource"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba569c12-ba1d-4486-a5be-bc8c31a8448c",
      "metadata": {
        "id": "ba569c12-ba1d-4486-a5be-bc8c31a8448c"
      },
      "source": [
        "Once we have this, it is trivial to define a branch that uses `result.datasource`\n",
        "\n",
        "https://python.langchain.com/docs/expression_language/how_to/routing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "01f15722-35c6-4456-ad1b-06463233db25",
      "metadata": {
        "id": "01f15722-35c6-4456-ad1b-06463233db25"
      },
      "outputs": [],
      "source": [
        "def choose_route(result):\n",
        "    if \"python\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for python_docs\"\n",
        "    elif \"js\" in result.datasource.lower():\n",
        "        ### Logic here\n",
        "        return \"chain for js_docs\"\n",
        "    else:\n",
        "        ### Logic here\n",
        "        return \"golang_docs\"\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "full_chain = router | RunnableLambda(choose_route)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6af07b77-0537-4635-87ec-ad8f59d34e9b",
        "outputId": "373fda31-8413-4010-c707-b0d8932ccba7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chain for python_docs'"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "full_chain.invoke({\"question\": question, \"format_instructions\": parser.get_format_instructions()})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949",
      "metadata": {
        "id": "cb1db843-95f4-4abd-8d4c-4a41f0910949"
      },
      "source": [
        "### Semantic routing\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://python.langchain.com/docs/expression_language/cookbook/embedding_router"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53cbfa72-c35a-4d1d-aa6d-08a570ab2170",
        "outputId": "ec46f21b-4833-481e-8668-709cee6a98b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using PHYSICS\n",
            "I know that a black hole is a region of spacetime where gravity is so strong that nothing, not even light, can escape. But what about the center of a black hole? Is it also a region of spacetime where gravity is so strong that nothing, not even light, can escape?\n",
            "\n",
            "Can you explain this concept in a simple and easy to understand way?\n"
          ]
        }
      ],
      "source": [
        "from langchain.utils.math import cosine_similarity\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "\n",
        "# Two prompts\n",
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit that you don't know.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
        "You are so good because you are able to break down hard problems into their component parts, \\\n",
        "answer the component parts, and then put them together to answer the broader question.\n",
        "\n",
        "Here is a question:\n",
        "{query}\"\"\"\n",
        "\n",
        "# Embed prompts\n",
        "prompt_templates = [physics_template, math_template]\n",
        "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
        "\n",
        "# Route question to prompt\n",
        "def prompt_router(input):\n",
        "    # Embed question\n",
        "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
        "    # Compute similarity\n",
        "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
        "    most_similar = prompt_templates[similarity.argmax()]\n",
        "    # Chosen prompt\n",
        "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
        "    return PromptTemplate.from_template(most_similar)\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"query\": RunnablePassthrough()}\n",
        "    | RunnableLambda(prompt_router)\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(chain.invoke(\"What's a black hole\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e93d04a-513e-435b-8e09-8df5d4976690",
      "metadata": {
        "id": "9e93d04a-513e-435b-8e09-8df5d4976690"
      },
      "source": [
        "# Query Construction\n",
        "\n",
        "For graph and SQL, see helpful resources:\n",
        "\n",
        "https://blog.langchain.dev/query-construction/\n",
        "\n",
        "https://blog.langchain.dev/enhancing-rag-based-applications-accuracy-by-constructing-and-leveraging-knowledge-graphs/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93d3516-3f77-4548-b55f-5db4d7c9fbbb",
      "metadata": {
        "id": "e93d3516-3f77-4548-b55f-5db4d7c9fbbb"
      },
      "source": [
        "## Query structuring for metadata filters\n",
        "\n",
        "\n",
        "Many vectorstores contain metadata fields.\n",
        "\n",
        "This makes it possible to filter for specific chunks based on metadata.\n",
        "\n",
        "Let's look at some example metadata we might see in a database of YouTube transcripts.\n",
        "\n",
        "Docs:\n",
        "\n",
        "https://python.langchain.com/docs/use_cases/query_analysis/techniques/structuring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "b22eb666-a6c2-4b3f-81dd-93ece81f035d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b22eb666-a6c2-4b3f-81dd-93ece81f035d",
        "outputId": "4d68b7e6-05ef-455e-8c3a-e70e2c45eff9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'pbAd8O1Lvm4',\n",
              " 'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG',\n",
              " 'description': 'Unknown',\n",
              " 'view_count': 14063,\n",
              " 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hq720.jpg',\n",
              " 'publish_date': '2024-02-07 00:00:00',\n",
              " 'length': 1058,\n",
              " 'author': 'LangChain'}"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "docs = YoutubeLoader.from_youtube_url(\n",
        "    \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\", add_video_info=True\n",
        ").load()\n",
        "\n",
        "docs[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53767f87-f38d-4f16-9b3b-b83ce4657f03",
      "metadata": {
        "id": "53767f87-f38d-4f16-9b3b-b83ce4657f03"
      },
      "source": [
        "Let’s assume we’ve built an index that:\n",
        "\n",
        "1. Allows us to perform unstructured search over the `contents` and `title` of each document\n",
        "2. And to use range filtering on `view count`, `publication date`, and `length`.\n",
        "\n",
        "We want to convert natural langugae into structured search queries.\n",
        "\n",
        "We can define a schema for structured search queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "7731745b-accc-4cf1-8291-e12d1aa46361",
      "metadata": {
        "id": "7731745b-accc-4cf1-8291-e12d1aa46361"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from typing import Literal, Optional, Tuple\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "class TutorialSearch(BaseModel):\n",
        "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
        "\n",
        "    content_search: str = Field(\n",
        "        ...,\n",
        "        description=\"Similarity search query applied to video transcripts.\",\n",
        "    )\n",
        "    title_search: str = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Alternate version of the content search query to apply to video titles. \"\n",
        "            \"Should be succinct and only include key words that could be in a video \"\n",
        "            \"title.\"\n",
        "        ),\n",
        "    )\n",
        "    min_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_view_count: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    earliest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    latest_publish_date: Optional[datetime.date] = Field(\n",
        "        None,\n",
        "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    min_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "    max_length_sec: Optional[int] = Field(\n",
        "        None,\n",
        "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
        "    )\n",
        "\n",
        "    def pretty_print(self) -> None:\n",
        "        for field in self.__fields__:\n",
        "            if getattr(self, field) is not None and getattr(self, field) != getattr(\n",
        "                self.__fields__[field], \"default\", None\n",
        "            ):\n",
        "                print(f\"{field}: {getattr(self, field)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6054bc98-0ae1-45e6-8f06-22c65ec47180",
      "metadata": {
        "id": "6054bc98-0ae1-45e6-8f06-22c65ec47180"
      },
      "source": [
        "Now, we prompt the LLM to produce queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "f699d9e7-468e-4574-bdba-f4be4a5779de",
      "metadata": {
        "id": "f699d9e7-468e-4574-bdba-f4be4a5779de"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "system = \"\"\"You are an expert at converting user questions into database queries. \\\n",
        "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
        "Given a question, return a database query optimized to retrieve the most relevant results.\\n\n",
        "Use these format instructions for the structure of your answer: {format_instructions}.\n",
        "\n",
        "If there are acronyms or words you are not familiar with, do not try to rephrase them.\"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "## Same here as above: we cannot do this with our model. Let's mimic it. You can also try .with_structured_output from langchain for better results beyond what we do here.\n",
        "##structured_llm = llm.with_structured_output(TutorialSearch)\n",
        "\n",
        "# Set up a parser instead + inject instructions into the prompt template + use few-shot prompts etc.\n",
        "parser = PydanticOutputParser(pydantic_object=TutorialSearch)\n",
        "\n",
        "query_analyzer = prompt | llm #| StrOutputParser() | artificial_parser #| parser ## we use artificially created parser not the actual parser which is basically rather a validator\n",
        "                              # I am not implementing the artificial_parser again for this case, but you get the idea from previous artificial_parser right?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b776858-a589-4fe5-a8a3-19530706075d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b776858-a589-4fe5-a8a3-19530706075d",
        "outputId": "042796a3-2329-418c-e0ea-cd6247c4fcaf"
      },
      "outputs": [],
      "source": [
        "result = query_analyzer.invoke({\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\", \"format_instructions\": parser.get_format_instructions()})\n",
        "result.pretty_print()\n",
        "                              # Example structure of the result you should get in case you decide to play with implementation of artificial_parser:\n",
        "\n",
        "                              # content_search: chat langchain\n",
        "                              # title_search: chat langchain\n",
        "                              # earliest_publish_date: 2024-01-01\n",
        "\n",
        "                              # artificial_parser is not a part of langchain and I showed it to you \n",
        "                              # just to partially overcome the limitations of models with no function calling pre-training.\n",
        "                              # additional helper tool for that may be few-shot prompting and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90111ec1-784f-4b8c-bd91-6122edb7eb25",
      "metadata": {
        "id": "90111ec1-784f-4b8c-bd91-6122edb7eb25"
      },
      "source": [
        "To then connect this to various vectorstores, you can follow [here](https://python.langchain.com/docs/modules/data_connection/retrievers/self_query#constructing-from-scratch-with-lcel)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IOE0EQIpf3SE",
        "d4ffa253-e427-4663-bc0b-9b1fcce508e6",
        "45fd9558-c53f-4a6c-80f0-c31b3bfd55de",
        "mnNFcDPmmsnn",
        "cb1db843-95f4-4abd-8d4c-4a41f0910949",
        "9e93d04a-513e-435b-8e09-8df5d4976690",
        "e93d3516-3f77-4548-b55f-5db4d7c9fbbb"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
