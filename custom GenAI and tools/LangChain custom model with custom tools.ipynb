{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BJgoSGmbzL-"
      },
      "source": [
        "# How to create custom LangChain model with custom tools \n",
        "\n",
        "- Not everyone can pay money for LLM APIs or run even pretty small open models on their device and the most of excelent tutorials and resources are focused on those APIs, which makes learning LangChain or other similar tools more difficult than it needs to be. \n",
        "\n",
        "- Goal of this notebook is to give basic framework where you can use relatively small but performant model gemma_2b_it with LangChain and augment it with you own custom tools.\n",
        "\n",
        "- References from LangChain documentation:\n",
        "[How to create Custom Langchain LLM](https://python.langchain.com/docs/modules/model_io/llms/custom_llm/)\n",
        "How to create custom tools: [one](https://python.langchain.com/docs/use_cases/tool_use/prompting/),  [two](https://python.langchain.com/docs/modules/tools/custom_tools/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zBAfoIfwFWv"
      },
      "outputs": [],
      "source": [
        "#%%capture --no-stderr          # this is basically for silencing warnings\n",
        "%pip install -U langchain_community tiktoken langchainhub chromadb langchain langgraph faiss-cpu\n",
        "\n",
        "!pip install -q bitsandbytes accelerate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, List, Mapping, Optional\n",
        "\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain.tools.render import render_text_description\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZeGdxLVltCm_"
      },
      "outputs": [],
      "source": [
        "#@title  for GPU\n",
        "\n",
        "model_id = \"google/gemma-2b-it\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "\n",
        "### model.save_pretrained(\"./model.\")\n",
        "### model = AutoModelForCausalLM.from_pretrained(\"./model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C4clFYRw44W"
      },
      "outputs": [],
      "source": [
        "# print( len( list( model.parameters() ) ) , model.__dict__['_modules']['model'] )\n",
        "# for param in model.parameters():\n",
        "#      print(type(param), param.size())\n",
        "\n",
        "# list( model.parameters() )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXtq6sQj6l5i"
      },
      "outputs": [],
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# pipeline = pipeline(\n",
        "#         \"text-generation\",\n",
        "#         model=model,\n",
        "#         tokenizer=tokenizer\n",
        "#     )\n",
        "\n",
        "# def model_gen(user_input: str = '', pipeline=pipeline):\n",
        "\n",
        "#     messages = [{\"role\": \"user\", \"content\": user_input},]\n",
        "#     prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "#     outputs = pipeline(\n",
        "#         prompt,\n",
        "#         max_new_tokens=256,\n",
        "#         do_sample=True,\n",
        "#         temperature=0.7,\n",
        "#         top_k=50,\n",
        "#         top_p=0.95\n",
        "#     )\n",
        "\n",
        "\n",
        "#     text = outputs[0][\"generated_text\"][len(prompt):]\n",
        "#     # from IPython.display import Markdown, display\n",
        "#     # display(Markdown(text))\n",
        "#     return text   #, print(prompt)\n",
        "\n",
        "\n",
        "# user_input = \"write python function for adding two numbers\"\n",
        "# model_gen(user_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BxafKIU3dorX"
      },
      "outputs": [],
      "source": [
        "# Create custom LangChain model from gemma \n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "class Gemma(LLM):\n",
        "\n",
        "    model: Any = None\n",
        "    tokenizer: Any = None\n",
        "    n: int = None\n",
        "\n",
        "    def __init__(self, model, tokenizer, n):\n",
        "        super(Gemma, self).__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.n = n\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"Gemma\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:\n",
        "\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model.generate(**input_ids, max_new_tokens=self.n)  #, temperature=0.0     # default self.n for gemma huggingface is 20\n",
        "        text = self.tokenizer.decode(outputs[0])\n",
        "\n",
        "        start_index, end_index = text.find(\"<eos>\"), text.rfind(\"<eos>\")\n",
        "        answer_text = text[start_index+len(\"<eos>\"):end_index].strip()\n",
        "\n",
        "        pattern = r'\\{.*?\\}'\n",
        "        matches = re.findall(pattern, answer_text)\n",
        "        if matches:\n",
        "            return json.dumps(matches[0])\n",
        "\n",
        "        if stop is not None:\n",
        "            raise ValueError(\"stop kwargs are not permitted.\")\n",
        "\n",
        "        return answer_text\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        \"\"\"Get the identifying parameters.\"\"\"\n",
        "        return {\"n\": self.n}\n",
        "\n",
        "llm =  Gemma(model, tokenizer, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8ByY8PC1YeD",
        "outputId": "1aacdd4f-8c88-401c-8ee4-f46ddf4a0ca4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'name': 'exponentiate',\n",
              " 'arguments': {'base': 13, 'exponent': 4},\n",
              " 'output': 28561}"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create Custo Tools:\n",
        "# NOTE: when model is not explicitely trained on \"function calling\" we can still achieve that behavior. \n",
        "#       because the models are not actually calling functions, they just give us specifically structured\n",
        "#       answers (that struture is predefined structure). These specific structures are important only because\n",
        "#       they must match the structure of the input of our functions, so that we can use them for calling our functions.\n",
        "#       So, llm does not do actual function calling (that is done by us), but we use LLMs for getting structured responses\n",
        "#       that become inputs of our functions. \n",
        "\n",
        "@tool\n",
        "def multiply(first_int: int, second_int: int) -> int:\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "    return first_int * second_int\n",
        "\n",
        "@tool\n",
        "def add(first_int: int, second_int: int) -> int:\n",
        "    \"Add two integers.\"\n",
        "    return first_int + second_int\n",
        "\n",
        "@tool\n",
        "def exponentiate(base: int, exponent: int) -> int:\n",
        "    \"Exponentiate the base to the exponent power.\"\n",
        "    return base**exponent\n",
        "\n",
        "tools = [add, exponentiate, multiply]\n",
        "tool_map = {tool.name: tool for tool in tools}\n",
        "\n",
        "system_prompt = f\"\"\"You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "\n",
        "import ast\n",
        "def dictifier(string, tool_map=tool_map):\n",
        "\n",
        "  if not ( string.endswith(')}') or string.endswith('}}') ):\n",
        "      string = string + '}'\n",
        "  dictionary = ast.literal_eval(string)\n",
        "\n",
        "  chosen_tool = tool_map[dictionary[\"name\"]]\n",
        "\n",
        "  if isinstance(dictionary['arguments'], tuple):\n",
        "    dictionary['arguments'] = {key: value for key, value in zip(chosen_tool.args.keys(), dictionary[\"arguments\"])}\n",
        "\n",
        "  return dictionary\n",
        "\n",
        "def tool_chain(dictionary, tool_map=tool_map):\n",
        "  chosen_tool = tool_map[dictionary[\"name\"]]\n",
        "  return itemgetter(\"arguments\") | chosen_tool\n",
        "\n",
        "chain = prompt | llm | JsonOutputParser() | dictifier | RunnablePassthrough.assign(output=tool_chain) ## or instead just do: | tool_chain\n",
        "chain.invoke({\"input\": \"what's thirteen exponent 4\"})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
