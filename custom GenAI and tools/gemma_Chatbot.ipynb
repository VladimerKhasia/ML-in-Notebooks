{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IOE0EQIpf3SE",
        "o8ajmxyFJ_et",
        "_ZzQUjBJ6qme"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma model"
      ],
      "metadata": {
        "id": "IOE0EQIpf3SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gemma model setup for Langchain RAG"
      ],
      "metadata": {
        "id": "o32lou1Gdi5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkgFAXWVW3wm"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_community sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes accelerate   # accelerate is for GPU and additionally bitsandbytes is for quantization"
      ],
      "metadata": {
        "id": "NGQuLKNN3Xiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-2b-it\""
      ],
      "metadata": {
        "id": "CdX0fVhjjZi6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeGdxLVltCm_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title  for GPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#         load_in_4bit=True,\n",
        "#         bnb_4bit_use_double_quant=True,\n",
        "#         bnb_4bit_quant_type=\"nf4\",\n",
        "#         bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
        "\n",
        "### model.save_pretrained(\"./model.\")\n",
        "### model = AutoModelForCausalLM.from_pretrained(\"./model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title for CPU\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZkXAAiYLu9F_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
        "\n",
        "from langchain_core.callbacks import (\n",
        "    AsyncCallbackManagerForLLMRun,\n",
        "    CallbackManagerForLLMRun,\n",
        ")\n",
        "from langchain_core.messages.ai import AIMessage\n",
        "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
        "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
        "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
        "from langchain_core.runnables import run_in_executor\n",
        "from transformers import pipeline\n",
        "import re\n",
        "import json\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class GemmaChatModel(BaseChatModel):\n",
        "    \"\"\"\n",
        "    A custom chat model powered by Gemma from Hugging Face, designed to be informative, comprehensive, and engaging.\n",
        "    See the custom model guide here: https://python.langchain.com/docs/modules/model_io/chat/custom_chat_model/\n",
        "    \"\"\"\n",
        "\n",
        "    model_name: str = \"gemma_chat_model\"  # Replace with the actual Gemma model name\n",
        "    task: str = \"conversational\"  # Task for the pipeline (conversational or summarization)\n",
        "    #temperature = 0.0\n",
        "    n: int = 2048\n",
        "    model : Any = None\n",
        "    tokenizer : Any = None\n",
        "\n",
        "\n",
        "    def _generate(\n",
        "        self,\n",
        "        messages: List[BaseMessage],\n",
        "        stop: Optional[List[str]] = None,\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> ChatResult:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            messages: The list of prompt messages.\n",
        "            stop: Optional list of stop tokens.\n",
        "            run_manager: Optional callback manager.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "\n",
        "        Returns:\n",
        "            A ChatResult object containing the generated response.\n",
        "        \"\"\"\n",
        "\n",
        "        prompt = messages[-1].content #[: self.n]\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        outputs = self.model.generate(**input_ids, max_new_tokens=self.n)       # , temperature=self.temperature\n",
        "        text = self.tokenizer.decode(outputs[0])\n",
        "        #text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "        start_index, end_index = text.find(\"<eos>\"), text.rfind(\"<eos>\")\n",
        "        response = text[start_index+len(\"<eos>\"):end_index].strip()\n",
        "\n",
        "        message = AIMessage(content=response, additional_kwargs={}, response_metadata={\"time_in_seconds\": 3})\n",
        "        return ChatResult(generations=[ChatGeneration(message=message)])\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"\n",
        "        Returns the type of language model used: \"gemma_chat_model\".\n",
        "        \"\"\"\n",
        "        return \"gemma_chat_model\"\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Returns a dictionary of identifying parameters for LangChain callbacks.\n",
        "        \"\"\"\n",
        "        return {\"model_name\": self.model_name, \"task\": self.task}\n",
        "\n",
        "llm = GemmaChatModel()\n",
        "llm.model = model               # This is simple but not production level way of doing things. It's just for avoiding colab run out of memory on CPU\n",
        "llm.tokenizer = tokenizer"
      ],
      "metadata": {
        "id": "lXuW_0ZC2hRc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Chat Agent"
      ],
      "metadata": {
        "id": "o8ajmxyFJ_et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/modules/memory/adding_memory/\n",
        "# https://python.langchain.com/docs/modules/memory/adding_memory_chain_multiple_inputs/    ## RAG-augmented\n",
        "# https://python.langchain.com/docs/modules/memory/conversational_customization/\n",
        "# https://python.langchain.com/docs/modules/agents/quick_start/                            ## But you can't use this directly for gemma"
      ],
      "metadata": {
        "id": "9MtWvms2D9I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "kwbxOtdLQDco"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a chatbot having a conversation with a human.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "Chatbot:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,   #if you want intermediate steps to be printed\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "Pm4gL40oQVGR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"Hi there my friend\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "2Vt_saRSRGoh",
        "outputId": "aef99816-fc95-473e-9f39-2db0e585ea85"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
            "\n",
            "[HumanMessage(content='Hi, my name is Lado'), AIMessage(content=\"Hello! It's a pleasure to meet you, Lado. What can I do for you today?\"), HumanMessage(content='do you remember my name?'), AIMessage(content='I am a chatbot and do not have personal memories or the ability to remember names. I am designed to assist you with information and tasks based on the knowledge I have been trained on.')]\n",
            "Human: Hi there my friend\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! It's a pleasure to meet you, Lado. What can I do for you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.predict(human_input=\"Not too bad - how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "WuooLFseYqkH",
        "outputId": "e07f95c2-9403-434a-fe3d-c3843b7fd483"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
            "\n",
            "[HumanMessage(content='Hi, my name is Lado'), AIMessage(content=\"Hello! It's a pleasure to meet you, Lado. What can I do for you today?\"), HumanMessage(content='do you remember my name?'), AIMessage(content='I am a chatbot and do not have personal memories or the ability to remember names. I am designed to assist you with information and tasks based on the knowledge I have been trained on.'), HumanMessage(content='Hi there my friend'), AIMessage(content=\"Hello! It's a pleasure to meet you, Lado. What can I do for you today?\")]\n",
            "Human: Not too bad - how are you?\n",
            "Chatbot:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am doing well, thank you for asking. I am here to assist you with any questions or tasks you may have. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio UI"
      ],
      "metadata": {
        "id": "_ZzQUjBJ6qme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "3e6tPI2oreyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the chatbot function\n",
        "def respond(message, history):\n",
        "  response = llm_chain.predict(human_input=message)\n",
        "  return response\n",
        "\n",
        "# Launch the Gradio interface\n",
        "gr.ChatInterface(\n",
        "    respond,\n",
        "    title=\"My Chatbot\",\n",
        "    description=\"Enter your message and chat with the bot!\",\n",
        "\n",
        ").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "bzYLn-ZOqM5c",
        "outputId": "933aeca5-15ce-4183-da90-dd05db001e65"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://33f98a69aa34448a1c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://33f98a69aa34448a1c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}