{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Gjto3yCm-B9b"},"source":["# Chest X-Ray Medical Diagnosis with Deep Learning"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6PMDCWQRn5yA"},"source":["\n","Download used data here: https://drive.google.com/file/d/1AUWthYTUeVYMRV69O-U2zyOhscXci3SZ/view?usp=sharing \n","\n","-----------------------------------------------------------------------------------------------------------\n","\n","general dataset: [ChestX-ray8 dataset](https://arxiv.org/abs/1705.02315) which contains 108,948 frontal-view X-ray images of 32,717 unique patients. \n","- Each image in the data set contains multiple text-mined labels identifying 14 different pathological conditions. \n","- These in turn can be used by physicians to diagnose 8 different diseases. \n","- We will use this data to develop a single model that will provide binary classification predictions for each of the 14 labeled pathologies. \n","- In other words it will predict 'positive' or 'negative' for each of the pathologies.\n"," \n","download the entire dataset: [here](https://nihcc.app.box.com/v/ChestXray-NIHCC). \n","\n","\n","- For deeper data preprocessing and analysis of the raw version of the same dataset, refer to pytorch\n","implementation of similar task in the same repository here:\n","https://github.com/VladimerKhasia/ML-in-Notebooks/blob/main/Pytorch/xray_img_captioning.ipynb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":358,"status":"ok","timestamp":1673870105938,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Je3yV0Wnn5x8","outputId":"cff8d48d-ea5a-4ff8-fa85-59717613b5fe","scrolled":true},"outputs":[{"data":{"text/plain":["('2.9.2', '2.9.0')"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["import sys\n","import os\n","import random\n","import cv2\n","from PIL import Image\n","import glob\n","from sklearn.metrics import roc_auc_score, roc_curve\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import tensorflow \n","from tensorflow.compat.v1.logging import INFO, set_verbosity\n","tensorflow.compat.v1.logging.set_verbosity(tensorflow.compat.v1.logging.ERROR)\n","tensorflow.compat.v1.disable_eager_execution()\n","from tensorflow import keras\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.densenet import DenseNet121\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras import backend\n","from keras import backend as K\n","\n","from keras.models import load_model\n","\n","tensorflow.__version__, keras.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlX9Y2Gv-Z_L"},"outputs":[],"source":["#@title helpers\n","\n","random.seed(a=None, version=2)\n","\n","set_verbosity(INFO)\n","\n","\n","# def get_mean_std_per_batch(image_dir, df, H=320, W=320):\n","#     sample_data = []\n","#     for img in df.sample(100)[\"Image\"].values:\n","#         image_path = os.path.join(image_dir, img)\n","#         sample_data.append(\n","#             np.array(tensorflow.keras.utils.load_img(image_path, target_size=(H, W))))\n","\n","#     mean = np.mean(sample_data, axis=(0, 1, 2, 3))\n","#     std = np.std(sample_data, axis=(0, 1, 2, 3), ddof=1)\n","#     return mean, std\n","\n","def get_mean_std_per_batch(image_dir, df, H=320, W=320, imagetype='png'):\n","    sample_data = []\n","    size = H, W\n","\n","    for infile in glob.glob(f\"{image_dir}*.{imagetype}\"):\n","        file, ext = os.path.splitext(infile)\n","        with Image.open(infile) as im:\n","            im.thumbnail(size)\n","            #im.save(file + \".thumbnail\", f\"{imagetype}\")\n","            input_arr = tensorflow.keras.utils.img_to_array(im)\n","            input_arr = np.array([input_arr])\n","            sample_data.append(input_arr)\n","    mean = np.mean(sample_data, axis=(0, 1, 2, 3))\n","    std = np.std(sample_data, axis=(0, 1, 2, 3), ddof=1)\n","    return mean, std\n","\n","\n","def load_image(img, image_dir, df, preprocess=True, H=320, W=320, metrics=None):\n","    \"\"\"Load and preprocess image.\"\"\"\n","    if metrics is None: \n","      mean, std = get_mean_std_per_batch(image_dir, df, H=H, W=W)\n","    else: \n","      mean, std = metrics\n","     \n","    img_path = os.path.join(image_dir, img)\n","    x = tensorflow.keras.utils.load_img(img_path, target_size=(H, W))\n","    if preprocess:\n","        x -= mean\n","        x /= std\n","        x = np.expand_dims(x, axis=0)\n","    return x\n","\n","\n","def grad_cam(input_model, image, cls, layer_name, H=320, W=320):\n","    \"\"\"GradCAM method for visualizing input saliency.\"\"\"\n","    y_c = input_model.output[0, cls]\n","    conv_output = input_model.get_layer(layer_name).output\n","    grads = K.gradients(y_c, conv_output)[0]\n","\n","    gradient_function = K.function([input_model.input], [conv_output, grads])\n","\n","    output, grads_val = gradient_function([image])\n","    output, grads_val = output[0, :], grads_val[0, :, :, :]\n","\n","    weights = np.mean(grads_val, axis=(0, 1))\n","    cam = np.dot(output, weights)\n","\n","    # Process CAM\n","    cam = cv2.resize(cam, (W, H), cv2.INTER_LINEAR)\n","    cam = np.maximum(cam, 0)\n","    cam = cam / cam.max()\n","    return cam\n","    \n","\n","def compute_gradcam(model, img, image_dir, df, labels, selected_labels,\n","                    layer_name='bn', metrics=None):\n","    preprocessed_input = load_image(img, image_dir, df, metrics=metrics)\n","    predictions = model.predict(preprocessed_input)\n","\n","    print(\"Loading original image\")\n","    plt.figure(figsize=(15, 10))\n","    plt.subplot(151)\n","    plt.title(\"Original\")\n","    plt.axis('off')\n","    plt.imshow(load_image(img, image_dir, df, preprocess=False, metrics=metrics), cmap='gray')\n","\n","    j = 1\n","    for i in range(len(labels)):\n","        if labels[i] in selected_labels:\n","            print(f\"Generating gradcam for class {labels[i]}\")\n","            gradcam = grad_cam(model, preprocessed_input, i, layer_name)\n","            plt.subplot(151 + j)\n","            plt.title(f\"{labels[i]}: p={predictions[0][i]:.3f}\")\n","            plt.axis('off')\n","            plt.imshow(load_image(img, image_dir, df, preprocess=False, metrics=metrics),\n","                       cmap='gray')\n","            plt.imshow(gradcam, cmap='jet', alpha=min(0.5, predictions[0][i]))\n","            j += 1\n","\n","\n","def get_roc_curve(labels, predicted_vals, generator):\n","    auc_roc_vals = []\n","    for i in range(len(labels)):\n","        try:\n","            gt = generator.labels[:, i]\n","            pred = predicted_vals[:, i]\n","            auc_roc = roc_auc_score(gt, pred)\n","            auc_roc_vals.append(auc_roc)\n","            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n","            plt.figure(1, figsize=(10, 10))\n","            plt.plot([0, 1], [0, 1], 'k--')\n","            plt.plot(fpr_rf, tpr_rf,\n","                     label=labels[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n","            plt.xlabel('False positive rate')\n","            plt.ylabel('True positive rate')\n","            plt.title('ROC curve')\n","            plt.legend(loc='best')\n","        except:\n","            print(\n","                f\"Error in generating ROC curve for {labels[i]}. \"\n","                f\"Dataset lacks enough examples.\"\n","            )\n","    plt.show()\n","    return auc_roc_vals\n","\n","\n","# LOAD MODEL\n","def load_model(path):\n","    labels = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Atelectasis',\n","              'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema', 'Consolidation']\n","\n","    train_df = pd.read_csv(path + \"./train-small.csv\")\n","    valid_df = pd.read_csv(path + \"./valid-small.csv\")\n","    test_df = pd.read_csv(path + \"./test.csv\")\n","\n","    class_pos = train_df.loc[:, labels].sum(axis=0)\n","    class_neg = len(train_df) - class_pos\n","    class_total = class_pos + class_neg\n","\n","    pos_weights = class_pos / class_total\n","    neg_weights = class_neg / class_total\n","    print(\"Got loss weights\")\n","    # create the base pre-trained model\n","    ### DenseNet121(weights=path+'densenet.hdf5', include_top=False) --> if our own pretrained model, otherwise:\n","    base_model = DenseNet121(weights='imagenet', include_top=False) \n","    print(\"Loaded DenseNet\")\n","    # add a global spatial average pooling layer\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    # and a logistic layer\n","    predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n","    print(\"Added layers\")\n","\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","\n","    def get_weighted_loss(neg_weights, pos_weights, epsilon=1e-7):\n","        def weighted_loss(y_true, y_pred):\n","            # L(X, y) = −w * y log p(Y = 1|X) − w *  (1 − y) log p(Y = 0|X)\n","            # from https://arxiv.org/pdf/1711.05225.pdf\n","            loss = 0\n","            for i in range(len(neg_weights)):\n","                loss -= (neg_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) + \n","                         pos_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n","            \n","            loss = K.sum(loss)\n","            return loss\n","        return weighted_loss\n","    \n","    model.compile(optimizer='adam', loss=get_weighted_loss(neg_weights, pos_weights))\n","    print(\"Compiled Model\")\n","\n","    # model.load_weights(path + \"pretrained_model.h5\")  ## this means that model trained on imagenet then was finetuned on \n","    # print(\"Loaded Weights\")                           ## chest x-ray images. So, You can replace this with \n","                                                      ## finetuning training see:\"X-ray image classification and interpretation\"\n","    return model           "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Hc_hvFC-oNp"},"outputs":[],"source":["#@title tests\n","\n","\n","### test helper functions\n","def datatype_check(expected_output, target_output, error):\n","    success = 0\n","    if isinstance(target_output, dict):\n","        for key in target_output.keys():\n","            try:\n","                success += datatype_check(expected_output[key], \n","                                         target_output[key], error)\n","            except:\n","                print(\"Error: {} in variable {}. Got {} but expected type {}\".format(error,\n","                                                                          key, type(target_output[key]), type(expected_output[key])))\n","        if success == len(target_output.keys()):\n","            return 1\n","        else:\n","            return 0\n","    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n","        for i in range(len(target_output)):\n","            try: \n","                success += datatype_check(expected_output[i], \n","                                         target_output[i], error)\n","            except:\n","                print(\"Error: {} in variable {}, Got {}  but expected type {}\".format(error,\n","                                                                          i, type(target_output[i]), type(expected_output[i])))\n","        if success == len(target_output):\n","            return 1\n","        else:\n","            return 0\n","                \n","    else:\n","        assert isinstance(target_output, type(expected_output))\n","        return 1\n","            \n","def equation_output_check(expected_output, target_output, error):\n","    success = 0\n","    if isinstance(target_output, dict):\n","        for key in target_output.keys():\n","            try:\n","                success += equation_output_check(expected_output[key], \n","                                         target_output[key], error)\n","            except:\n","                print(expected_output[key], \n","                                         target_output[key])\n","                print(\"Error: {} for variable {}.\".format(error,\n","                                                                          key))\n","        if success == len(target_output.keys()):\n","            return 1\n","        else:\n","            return 0\n","    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n","        for i in range(len(target_output)):\n","            try: \n","                success += equation_output_check(expected_output[i], \n","                                         target_output[i], error)\n","            except:\n","                print(\"Error: {} for variable in position {}.\".format(error, i))\n","        if success == len(target_output):\n","            return 1\n","        else:\n","            return 0\n","                \n","    else:\n","        if hasattr(target_output, 'shape'):\n","            np.testing.assert_array_almost_equal(target_output, expected_output)\n","        else:\n","            assert target_output == expected_output\n","        return 1\n","    \n","def shape_check(expected_output, target_output, error):\n","    success = 0\n","    if isinstance(target_output, dict):\n","        for key in target_output.keys():\n","            try:\n","                success += shape_check(expected_output[key], \n","                                         target_output[key], error)\n","            except:\n","                print(\"Error: {} for variable {}.\".format(error, key))\n","        if success == len(target_output.keys()):\n","            return 1\n","        else:\n","            return 0\n","    elif isinstance(target_output, tuple) or isinstance(target_output, list):\n","        for i in range(len(target_output)):\n","            try: \n","                success += shape_check(expected_output[i], \n","                                         target_output[i], error)\n","            except:\n","                print(\"Error: {} for variable {}.\".format(error, i))\n","        if success == len(target_output):\n","            return 1\n","        else:\n","            return 0\n","                \n","    else:\n","        if hasattr(target_output, 'shape'):\n","            assert target_output.shape == expected_output.shape\n","        return 1\n","                \n","def multiple_test(test_cases, target):\n","    success = 0\n","    for test_case in test_cases:\n","        try:\n","            target_answer = target(*test_case['input'])                   \n","            if test_case['name'] == \"datatype_check\":\n","                success += datatype_check(test_case['expected'], target_answer, test_case['error'])\n","            if test_case['name'] == \"equation_output_check\":\n","                success += equation_output_check(test_case['expected'], target_answer, test_case['error'])\n","            if test_case['name'] == \"shape_check\":\n","                success += shape_check(test_case['expected'], target_answer, test_case['error'])\n","        except:\n","            print(\"Error: \" + test_case['error'])\n","            \n","    if success == len(test_cases):\n","        print(\"\\033[92m All tests passed.\")\n","    else:\n","        print('\\033[92m', success,\" Tests passed\")\n","        print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n","        raise AssertionError(\"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n","        \n","def multiple_test_weight_loss(test_cases, target, sess):\n","    success = 0\n","    for test_case in test_cases:\n","        try:\n","            target_answer = target(*test_case['input']).eval(session=sess)                   \n","            if test_case['name'] == \"datatype_check\":\n","                success += datatype_check(test_case['expected'], target_answer, test_case['error'])\n","            if test_case['name'] == \"equation_output_check\":\n","                success += equation_output_check(test_case['expected'], target_answer, test_case['error'])\n","            if test_case['name'] == \"shape_check\":\n","                success += shape_check(test_case['expected'], target_answer, test_case['error'])\n","        except:\n","            print(\"Error: \" + test_case['error'])\n","            \n","    if success == len(test_cases):\n","        print(\"\\033[92m All tests passed.\")\n","    else:\n","        print('\\033[92m', success,\" Tests passed\")\n","        print('\\033[91m', len(test_cases) - success, \" Tests failed\")\n","        raise AssertionError(\"Not all tests were passed for {}. Check your equations and avoid using global variables inside the function.\".format(target.__name__))\n","\n","\n","### ex1\n","def check_for_leakage_test(target):\n","    df1 = pd.DataFrame({'patient_id': [0, 1, 2]})\n","    df2 = pd.DataFrame({'patient_id': [2, 3, 4]})\n","    expected_output_1 = True\n","    \n","    print(\"Test Case 1\\n\")\n","    print(\"df1\")\n","    print(df1)\n","    print(\"df2\")\n","    print(df2)\n","    print(\"leakage output:\", target(df1, df2, 'patient_id'), \"\\n-------------------------------------\")\n","    \n","    df3 = pd.DataFrame({'patient_id': [0, 1, 2]})\n","    df4 = pd.DataFrame({'patient_id': [3, 4, 5]})\n","    expected_output_2 = False\n","    \n","    print(\"Test Case 2\\n\")\n","    print(\"df1\") ### same heading for df3\n","    print(df3)\n","    print(\"df2\") ### same heading for df4\n","    print(df4)\n","    print(\"leakage output:\", target(df3, df4, 'patient_id'), \"\\n\")\n","    \n","    test_cases = [\n","        {\n","            \"name\":\"datatype_check\",\n","            \"input\": [df1, df2, 'patient_id'],\n","            \"expected\": expected_output_1,\n","            \"error\":\"Data-type mismatch, make sure you are using pandas functions\"\n","        },\n","        {\n","            \"name\":\"datatype_check\",\n","            \"input\": [df3, df4, 'patient_id'],\n","            \"expected\": expected_output_2,\n","            \"error\":\"Datatype mismatch, make sure you are using pandas functions\"\n","        },\n","        {\n","            \"name\": \"shape_check\",\n","            \"input\": [df1, df2, 'patient_id'],\n","            \"expected\": expected_output_1,\n","            \"error\": \"Wrong shape, make sure you are using pandas functions\"\n","        },\n","        {\n","            \"name\": \"shape_check\",\n","            \"input\": [df3, df4, 'patient_id'],\n","            \"expected\": expected_output_2,\n","            \"error\": \"Wrong shape, make sure you are using pandas functions\"\n","        },\n","        {\n","            \"name\": \"equation_output_check\",\n","            \"input\": [df1, df2, 'patient_id'],\n","            \"expected\": expected_output_1,\n","            \"error\": \"Wrong output, make sure you are using pandas functions\"\n","        },\n","        {\n","            \"name\": \"equation_output_check\",\n","            \"input\": [df3, df4, 'patient_id'],\n","            \"expected\": expected_output_2,\n","            \"error\": \"Wrong output, make sure you are using pandas functions\"\n","        }\n","    ]\n","\n","    multiple_test(test_cases, target)\n","    \n","### ex2\n","def compute_class_freqs_test(target):\n","    labels_matrix = np.array(\n","        [[1, 0, 0],\n","         [0, 1, 1],\n","         [1, 0, 1],\n","         [1, 1, 1],\n","         [1, 0, 1]]\n","    )\n","\n","    print(\"Labels:\")\n","    print(labels_matrix)\n","    pos_freqs, neg_freqs = target(labels_matrix)\n","    print(\"\\nPos Freqs: \", pos_freqs)\n","    print(\"Neg Freqs: \", neg_freqs, \"\\n\")\n","    \n","    expected_freqs = (np.array([0.8, 0.4, 0.8]), np.array([0.2, 0.6, 0.2]))\n","    \n","    test_cases = [\n","        {\n","            \"name\":\"datatype_check\",\n","            \"input\": [labels_matrix],\n","            \"expected\": expected_freqs,\n","            \"error\": \"Data-type mismatch.\"\n","        },\n","        {\n","            \"name\": \"shape_check\",\n","            \"input\": [labels_matrix],\n","            \"expected\": expected_freqs,\n","            \"error\": \"Wrong shape.\"\n","        },\n","        {\n","            \"name\": \"equation_output_check\",\n","            \"input\": [labels_matrix],\n","            \"expected\": expected_freqs,\n","            \"error\": \"Wrong output.\"\n","        }\n","    ]\n","    \n","    multiple_test(test_cases, target)\n","    \n","### ex3\n","def get_weighted_loss_test_case(sess):\n","    with sess.as_default() as sess:\n","        y_true = K.constant(np.array(\n","            [[1, 1, 1],\n","             [1, 1, 0],\n","             [0, 1, 0],\n","             [1, 0, 1]]\n","        ))\n","        \n","        w_p = np.array([0.25, 0.25, 0.5])\n","        w_n = np.array([0.75, 0.75, 0.5])\n","        \n","        y_pred_1 = K.constant(0.7*np.ones(y_true.shape))\n","        y_pred_2 = K.constant(0.3*np.ones(y_true.shape))\n","    \n","    return y_true.eval(session=sess), w_p, w_n, y_pred_1.eval(session=sess), y_pred_2.eval(session=sess)\n","def get_weighted_loss_test(target, epsilon, sess):\n","    y_true, w_p, w_n, y_pred_1, y_pred_2 = get_weighted_loss_test_case(sess)\n","    \n","    print(\"y_true:\")\n","    print(y_true)\n","    print(\"\\nw_p:\")\n","    print(w_p)\n","    print(\"\\nw_n:\")\n","    print(w_n)\n","    print(\"\\ny_pred_1:\")\n","    print(y_pred_1)\n","    print(\"\\ny_pred_2:\")\n","    print(y_pred_2)\n","    \n","    L = target(w_p, w_n, epsilon)\n","    L1 = L(y_true, y_pred_1).eval(session=sess)\n","    L2 = L(y_true, y_pred_2).eval(session=sess)\n","    \n","    print(\"\\nIf you weighted them correctly, you'd expect the two losses to be the same.\")\n","    print(\"With epsilon = 1, your losses should be, L(y_pred_1) = -0.4956203 and L(y_pred_2) = -0.4956203\\n\")\n","    print(\"Your outputs:\\n\")\n","    print(\"L(y_pred_1) = \", L1)\n","    print(\"L(y_pred_2) = \", L2)\n","    print(\"Difference: L(y_pred_1) - L(y_pred_2) = \", L1-L2, \"\\n\")\n","    \n","    expected_output_1 = np.float32(-0.4956203)\n","    expected_output_2 = np.float32(-0.4956203)\n","    \n","    test_cases = [\n","        {\n","            \"name\":\"datatype_check\",\n","            \"input\": [y_true, y_pred_1],\n","            \"expected\": expected_output_1,\n","            \"error\": \"Data-type mismatch. Make sure it is a np.float32 value.\"\n","        },\n","        {\n","            \"name\":\"datatype_check\",\n","            \"input\": [y_true, y_pred_2],\n","            \"expected\": expected_output_2,\n","            \"error\": \"Data-type mismatch. Make sure it is a np.float32 value.\"\n","        },\n","        {\n","            \"name\": \"shape_check\",\n","            \"input\": [y_true, y_pred_1],\n","            \"expected\": expected_output_1,\n","            \"error\": \"Wrong shape.\"\n","        },\n","        {\n","            \"name\": \"shape_check\",\n","            \"input\": [y_true, y_pred_2],\n","            \"expected\": expected_output_2,\n","            \"error\": \"Wrong shape.\"\n","        },\n","        {\n","            \"name\": \"equation_output_check\",\n","            \"input\": [y_true, y_pred_1],\n","            \"expected\": expected_output_1,\n","            \"error\": \"Wrong output. One possible mistake, your epsilon is not equal to 1.\"\n","        },\n","        {\n","            \"name\": \"equation_output_check\",\n","            \"input\": [y_true, y_pred_2],\n","            \"expected\": expected_output_2,\n","            \"error\": \"Wrong output. One possible mistake, your epsilon is not equal to 1.\"\n","        }\n","    ]\n","    \n","    multiple_test_weight_loss(test_cases, L, sess)    "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":50,"status":"ok","timestamp":1673865945138,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"5JRSHB7i0t_6","outputId":"b0a52b48-3ad9-417f-ed8b-5282f467ecad"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-af4b6000-29d0-4e6a-97c0-6ea667c0b3c6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Image</th>\n","      <th>Atelectasis</th>\n","      <th>Cardiomegaly</th>\n","      <th>Consolidation</th>\n","      <th>Edema</th>\n","      <th>Effusion</th>\n","      <th>Emphysema</th>\n","      <th>Fibrosis</th>\n","      <th>Hernia</th>\n","      <th>Infiltration</th>\n","      <th>Mass</th>\n","      <th>Nodule</th>\n","      <th>PatientId</th>\n","      <th>Pleural_Thickening</th>\n","      <th>Pneumonia</th>\n","      <th>Pneumothorax</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>00008270_015.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8270</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>00029855_001.png</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>29855</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>00001297_000.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1297</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>00012359_002.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>12359</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>00017951_001.png</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>17951</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af4b6000-29d0-4e6a-97c0-6ea667c0b3c6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-af4b6000-29d0-4e6a-97c0-6ea667c0b3c6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-af4b6000-29d0-4e6a-97c0-6ea667c0b3c6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["              Image  Atelectasis  Cardiomegaly  Consolidation  Edema  \\\n","0  00008270_015.png            0             0              0      0   \n","1  00029855_001.png            1             0              0      0   \n","2  00001297_000.png            0             0              0      0   \n","3  00012359_002.png            0             0              0      0   \n","4  00017951_001.png            0             0              0      0   \n","\n","   Effusion  Emphysema  Fibrosis  Hernia  Infiltration  Mass  Nodule  \\\n","0         0          0         0       0             0     0       0   \n","1         1          0         0       0             1     0       0   \n","2         0          0         0       0             0     0       0   \n","3         0          0         0       0             0     0       0   \n","4         0          0         0       0             1     0       0   \n","\n","   PatientId  Pleural_Thickening  Pneumonia  Pneumothorax  \n","0       8270                   0          0             0  \n","1      29855                   0          0             0  \n","2       1297                   1          0             0  \n","3      12359                   0          0             0  \n","4      17951                   0          0             0  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.read_csv(\"./train-small.csv\")\n","valid_df = pd.read_csv(\"./valid-small.csv\")\n","\n","test_df = pd.read_csv(\"./test.csv\")\n","\n","train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrDoMlsun5yE"},"outputs":[],"source":["labels = ['Cardiomegaly', \n","          'Emphysema', \n","          'Effusion', \n","          'Hernia', \n","          'Infiltration', \n","          'Mass', \n","          'Nodule', \n","          'Atelectasis',\n","          'Pneumothorax',\n","          'Pleural_Thickening', \n","          'Pneumonia', \n","          'Fibrosis', \n","          'Edema', \n","          'Consolidation']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jz6dwTSrUcKc"},"outputs":[],"source":["def check_for_leakage(df1, df2, patient_col):\n","    \"\"\"\n","    Return True if there any patients are in both df1 and df2.\n","\n","    Args:\n","        df1 (dataframe): dataframe describing first dataset\n","        df2 (dataframe): dataframe describing second dataset\n","        patient_col (str): string name of column with patient IDs\n","    \n","    Returns:\n","        leakage (bool): True if there is leakage, otherwise False\n","    \"\"\"\n","\n","    # leakage contains true if there is patient overlap, otherwise false.\n","    # boolean (true if there is at least 1 patient in both groups)\n","    df1_patients_unique = set(df1[patient_col].values)\n","    df2_patients_unique = set(df2[patient_col].values)\n","    patients_in_both_groups = list(df1_patients_unique.intersection(df2_patients_unique))\n","    leakage = False if len(patients_in_both_groups) == 0 else True  \n","    \n","    return leakage"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1673865945140,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Rh2p1krrV1g5","outputId":"7d7e705c-7c42-4e8a-801a-0941415fda78"},"outputs":[],"source":["check_for_leakage_test(check_for_leakage)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673865945141,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"AMF3Wd3yW-RS","outputId":"09a6c8d3-b82d-409b-8dcc-3564051b52ba"},"outputs":[],"source":["print(\"leakage between train and valid: {}\".format(check_for_leakage(train_df, valid_df, 'PatientId')))\n","print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, test_df, 'PatientId')))\n","print(\"leakage between valid and test: {}\".format(check_for_leakage(valid_df, test_df, 'PatientId')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAgVGOAju8pX"},"outputs":[],"source":["def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 320, target_h = 320):\n","    \"\"\"\n","    Return generator for training set, normalizing using batch\n","    statistics.\n","\n","    Args:\n","      train_df (dataframe): dataframe specifying training data.\n","      image_dir (str): directory where image files are held.\n","      x_col (str): name of column in df that holds filenames.\n","      y_cols (list): list of strings that hold y labels for images.\n","      batch_size (int): images per batch to be fed into model during training.\n","      seed (int): random seed.\n","      target_w (int): final width of input images.\n","      target_h (int): final height of input images.\n","    \n","    Returns:\n","        train_generator (DataFrameIterator): iterator over training set\n","    \"\"\"        \n","    print(\"getting train generator...\") \n","    # normalize images\n","    image_generator = ImageDataGenerator(\n","        samplewise_center=True,\n","        samplewise_std_normalization= True)\n","    \n","    # flow from directory with specified batch size\n","    # and target image size\n","    generator = image_generator.flow_from_dataframe(\n","            dataframe=df,\n","            directory=image_dir,\n","            x_col=x_col,\n","            y_col=y_cols,\n","            class_mode=\"raw\",\n","            batch_size=batch_size,\n","            shuffle=shuffle,\n","            seed=seed,\n","            target_size=(target_w,target_h))\n","    \n","    return generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtWEAfAnrhMq"},"outputs":[],"source":["def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=8, seed=1, target_w = 320, target_h = 320):\n","    \"\"\"\n","    Return generator for validation set and test set using \n","    normalization statistics from training set.\n","\n","    Args:\n","      valid_df (dataframe): dataframe specifying validation data.\n","      test_df (dataframe): dataframe specifying test data.\n","      train_df (dataframe): dataframe specifying training data.\n","      image_dir (str): directory where image files are held.\n","      x_col (str): name of column in df that holds filenames.\n","      y_cols (list): list of strings that hold y labels for images.\n","      sample_size (int): size of sample to use for normalization statistics.\n","      batch_size (int): images per batch to be fed into model during training.\n","      seed (int): random seed.\n","      target_w (int): final width of input images.\n","      target_h (int): final height of input images.\n","    \n","    Returns:\n","        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n","    \"\"\"\n","    print(\"getting train and valid generators...\")\n","    # get generator to sample dataset\n","    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n","        dataframe=train_df, \n","        directory=IMAGE_DIR, \n","        x_col=\"Image\", \n","        y_col=labels, \n","        class_mode=\"raw\", \n","        batch_size=sample_size, \n","        shuffle=True, \n","        target_size=(target_w, target_h))\n","    \n","    # get data sample\n","    batch = raw_train_generator.next()\n","    data_sample = batch[0]\n","\n","    # use sample to fit mean and std for test set generator\n","    image_generator = ImageDataGenerator(\n","        featurewise_center=True,\n","        featurewise_std_normalization= True)\n","    \n","    # fit generator to sample from training data\n","    image_generator.fit(data_sample)\n","\n","    # get test generator\n","    valid_generator = image_generator.flow_from_dataframe(\n","            dataframe=valid_df,\n","            directory=image_dir,\n","            x_col=x_col,\n","            y_col=y_cols,\n","            class_mode=\"raw\",\n","            batch_size=batch_size,\n","            shuffle=False,\n","            seed=seed,\n","            target_size=(target_w,target_h))\n","\n","    test_generator = image_generator.flow_from_dataframe(\n","            dataframe=test_df,\n","            directory=image_dir,\n","            x_col=x_col,\n","            y_col=y_cols,\n","            class_mode=\"raw\",\n","            batch_size=batch_size,\n","            shuffle=False,\n","            seed=seed,\n","            target_size=(target_w,target_h))\n","    return valid_generator, test_generator"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2280,"status":"ok","timestamp":1673865947397,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"rNE3HWRbn5yL","outputId":"5f55b997-8fb4-466a-d495-dbfa5df4a3fa","scrolled":true},"outputs":[],"source":["IMAGE_DIR = \"./images/\"\n","train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\", labels)\n","valid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"Image\", labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"elapsed":676,"status":"ok","timestamp":1673865948056,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"Jh77vpN-n5yO","outputId":"b4dc7ab9-0b1b-4ace-976d-88fcd058ecb2"},"outputs":[],"source":["x, y = train_generator.__getitem__(0)\n","plt.imshow(x[0]);"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1673865948057,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"-OvyPe5en5yR","outputId":"16ae2267-0033-4af6-ea7b-c6400a705547"},"outputs":[],"source":["plt.xticks(rotation=90)\n","plt.bar(x=labels, height=np.mean(train_generator.labels, axis=0))\n","plt.title(\"Frequency of Each Class\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TpDGeY2cChYD"},"outputs":[],"source":["def compute_class_freqs(labels):\n","    \"\"\"\n","    Compute positive and negative frequences for each class.\n","\n","    Args:\n","        labels (np.array): matrix of labels, size (num_examples, num_classes)\n","    Returns:\n","        positive_frequencies (np.array): array of positive frequences for each\n","                                         class, size (num_classes)\n","        negative_frequencies (np.array): array of negative frequences for each\n","                                         class, size (num_classes)\n","    \"\"\"\n","    N = labels.shape[0]\n","    positive_frequencies = np.sum(labels,axis=0) / N\n","    negative_frequencies = 1 - np.sum(labels,axis=0) / N\n","    \n","    return positive_frequencies, negative_frequencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1673865948062,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"BqidQvCaD_xi","outputId":"0528431b-7672-4afd-abc7-9514dd8d4e63"},"outputs":[],"source":["### do not edit this code cell       \n","compute_class_freqs_test(compute_class_freqs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1673865948063,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"LoxM5jQ0E30D","outputId":"870155d7-b232-4fc9-cb92-b7e847fa6a0f"},"outputs":[],"source":["freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\n","freq_pos"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":1122,"status":"ok","timestamp":1673865949167,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"IqnNCu4In5yW","outputId":"50e050d6-94aa-4f67-8671-8a88154f48e7","scrolled":true},"outputs":[],"source":["data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\n","data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\n","plt.xticks(rotation=90)\n","f = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zs3_Rgwwn5yZ"},"outputs":[],"source":["pos_weights = freq_neg\n","neg_weights = freq_pos\n","pos_contribution = freq_pos * pos_weights \n","neg_contribution = freq_neg * neg_weights"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1673865949168,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"LPfSFrxjn5yb","outputId":"1ce0b37c-4e6b-4f28-dea4-c58c001b9a51","scrolled":true},"outputs":[],"source":["data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\n","data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n","                        for l,v in enumerate(neg_contribution)], ignore_index=True)\n","plt.xticks(rotation=90)\n","sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPIBVAasn5yd"},"outputs":[],"source":["def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n","    \"\"\"\n","    Return weighted loss function given negative weights and positive weights.\n","\n","    Args:\n","      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n","      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n","    \n","    Returns:\n","      weighted_loss (function): weighted loss function\n","    \"\"\"\n","    def weighted_loss(y_true, y_pred):\n","        \"\"\"\n","        Return weighted loss value. \n","\n","        Args:\n","            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n","            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n","        Returns:\n","            loss (float): overall scalar loss summed across all classes\n","        \"\"\"\n","        # initialize loss to zero\n","        loss = 0.0\n","        for i in range(len(pos_weights)):\n","            # # for each class, add average weighted loss for that class \n","            # typ = pos_weights[i].dtype\n","            # true = tensorflow.cast(y_true[:,i], typ)\n","            # pred = tensorflow.cast(y_pred[:,i], typ) \n","            # loss+= -(K.mean((pos_weights[i] * true * K.log(pred + epsilon)) + (neg_weights[i] * (1-true) * K.log(1-pred + epsilon)),axis = 0))\n","        \n","            loss += K.mean(-(pos_weights[i] * y_true[:,i] * K.log(y_pred[:,i] + epsilon) \n","                             + neg_weights[i]* (1 - y_true[:,i]) * K.log( 1 - y_pred[:,i] + epsilon)))        \n","        return loss \n","\n","    return weighted_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1112,"status":"ok","timestamp":1673866347469,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"CFjYda3Wulbm","outputId":"a9f06632-21e3-4bd7-88b1-d14752d54dc4"},"outputs":[],"source":["# test with a large epsilon in order to catch errors. \n","# set epsilon = 1\n","epsilon = 1\n","\n","### do not edit anything below\n","sess = K.get_session()\n","get_weighted_loss_test(get_weighted_loss, epsilon, sess)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16040,"status":"ok","timestamp":1673866450770,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"gZlxoCTgn5yi","outputId":"0cf7c900-3373-47e7-9509-71fc380f4a47","scrolled":true},"outputs":[],"source":["# create the base pre-trained model\n","## in case we already have our own pretrained weights we use it like this:\n","## base_model = DenseNet121(weights='./models/densenet.hdf5', include_top=False)\n","## in case we use some specific pretrained weights e.g. from imagenet:\n","base_model = DenseNet121(weights='imagenet', include_top=False)\n","\n","x = base_model.output\n","\n","# add a global spatial average pooling layer\n","x = GlobalAveragePooling2D()(x)\n","\n","# and a logistic layer\n","predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n","\n","model = Model(inputs=base_model.input, outputs=predictions)\n","model.compile(optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":60525,"status":"ok","timestamp":1673866517217,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"887bSajLn5yq","outputId":"a0877ce3-a132-420b-ed2f-57d7382f0840"},"outputs":[],"source":["## if we would already have finetuned model on our medical dataset we would load the model like this:\n","## model.load_weights(\"./models/pretrained_model.h5\") \n","\n","history = model.fit(train_generator, \n","                    validation_data=valid_generator,\n","                    steps_per_epoch=100, \n","                    validation_steps=25, \n","                    epochs = 3\n","                    )\n","\n","plt.plot(history.history['loss'])\n","plt.ylabel(\"loss\")\n","plt.xlabel(\"epoch\")\n","plt.title(\"Training Loss Curve\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72039,"status":"ok","timestamp":1673866603007,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"QzNrhtf1w2bI","outputId":"3a13377c-d84c-4d7c-c104-26639ec0ff4d","scrolled":true},"outputs":[],"source":["predicted_vals = model.predict(test_generator, steps = len(test_generator))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":621},"executionInfo":{"elapsed":1079,"status":"ok","timestamp":1673866691863,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"6SLI8FHun5yw","outputId":"e51fcd2e-47c0-4a6a-be79-345adb6541e5","scrolled":true},"outputs":[],"source":["auc_rocs = get_roc_curve(labels, predicted_vals, test_generator)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14175,"status":"ok","timestamp":1673878508744,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"6kahoZbJn5yz","outputId":"6b8a6161-f172-479b-e0a2-002f104b467f","scrolled":true},"outputs":[],"source":["df = pd.read_csv(\"./train-small.csv\")\n","IMAGE_DIR = \"./images/\"\n","\n","# calculate general reusable metrics for mean and std\n","metrics = get_mean_std_per_batch(IMAGE_DIR, df)\n","mean, std = metrics\n","\n","# only show the labels with top 4 AUC\n","labels_to_show = np.take(labels, np.argsort(auc_rocs)[::-1])[:4]\n","labels_to_show, mean[0], std[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":278},"executionInfo":{"elapsed":90699,"status":"ok","timestamp":1673870279738,"user":{"displayName":"David Learn","userId":"08846131768927784273"},"user_tz":-240},"id":"yB6WN-Qn-B-W","outputId":"621f2017-dd33-4402-8211-3ec5e470a405","scrolled":true},"outputs":[],"source":["compute_gradcam(model, '00008270_015.png', IMAGE_DIR, df, labels, labels_to_show, metrics=metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzCRNWW2earF"},"outputs":[],"source":["### again select the image from training set but randomly\n","random_img = df.sample(1)[\"Image\"].values[0]\n","compute_gradcam(model, random_img, IMAGE_DIR, df, labels, labels_to_show, metrics=metrics)"]},{"cell_type":"markdown","metadata":{"id":"CVTYQ6pAHvxq"},"source":["# GradCam interpretation in more detail"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXa1BOKdH3sy"},"outputs":[],"source":["def grad_cam(input_model, image, category_index, layer_name):\n","    \"\"\"\n","    GradCAM method for visualizing input saliency.\n","    \n","    Args:\n","        input_model (Keras.model): model to compute cam for\n","        image (tensor): input to model, shape (1, H, W, 3), where H (int) is height W (int) is width\n","        category_index (int): class to compute cam with respect to\n","        layer_name (str): relevant layer in model\n","    Return:\n","        cam ()\n","    \"\"\"\n","    cam = None\n","    \n","    # 1. Get placeholders for class output and last layer\n","    # Get the model's output\n","    output_with_batch_dim = input_model.output\n","    \n","    # Remove the batch dimension\n","    output_all_categories = output_with_batch_dim[0]\n","    \n","    # Retrieve only the disease category at the given category index\n","    y_c = output_all_categories[category_index]\n","    \n","    # Get the input model's layer specified by layer_name, and retrive the layer's output tensor\n","    spatial_map_layer = input_model.get_layer(layer_name).output\n","\n","    # 2. Get gradients of last layer with respect to output\n","\n","    # get the gradients of y_c with respect to the spatial map layer (it's a list of length 1)\n","    grads_l = K.gradients(y_c, spatial_map_layer)\n","    \n","    # Get the gradient at index 0 of the list\n","    grads = grads_l[0]\n","        \n","    # 3. Get hook for the selected layer and its gradient, based on given model's input\n","    # Hint: Use the variables produced by the previous two lines of code\n","    spatial_map_and_gradient_function = K.function([input_model.input], [spatial_map_layer, grads])\n","    \n","    # Put in the image to calculate the values of the spatial_maps (selected layer) and values of the gradients\n","    spatial_map_all_dims, grads_val_all_dims = spatial_map_and_gradient_function([image])\n","\n","    # Reshape activations and gradient to remove the batch dimension\n","    # Shape goes from (B, H, W, C) to (H, W, C)\n","    # B: Batch. H: Height. W: Width. C: Channel    \n","    # Reshape spatial map output to remove the batch dimension\n","    spatial_map_val = spatial_map_all_dims[0]\n","    \n","    # Reshape gradients to remove the batch dimension\n","    grads_val = grads_val_all_dims[0]\n","    \n","    # 4. Compute weights using global average pooling on gradient \n","    # grads_val has shape (Height, Width, Channels) (H,W,C)\n","    # Take the mean across the height and also width, for each channel\n","    # Make sure weights have shape (C)\n","    weights = np.mean(grads_val,axis=(0,1))\n","    \n","    # 5. Compute dot product of spatial map values with the weights\n","    cam = np.dot(spatial_map_val,weights)\n","    \n","    # We'll take care of the postprocessing.\n","    H, W = image.shape[1], image.shape[2]\n","    cam = np.maximum(cam, 0) # ReLU so we only get positive importance\n","    cam = cv2.resize(cam, (W, H), cv2.INTER_NEAREST)\n","    cam = cam / cam.max()\n","\n","    return cam\n","\n","\n","def compute_gradcam(model, img, mean, std, data_dir, df, \n","                    labels, selected_labels, layer_name='conv5_block16_concat'):\n","    \"\"\"\n","    Compute GradCAM for many specified labels for an image. \n","    This method will use the `grad_cam` function.\n","    \n","    Args:\n","        model (Keras.model): Model to compute GradCAM for\n","        img (string): Image name we want to compute GradCAM for.\n","        mean (float): Mean to normalize to image.\n","        std (float): Standard deviation to normalize the image.\n","        data_dir (str): Path of the directory to load the images from.\n","        df(pd.Dataframe): Dataframe with the image features.\n","        labels ([str]): All output labels for the model.\n","        selected_labels ([str]): All output labels we want to compute the GradCAM for.\n","        layer_name: Intermediate layer from the model we want to compute the GradCAM for.\n","    \"\"\"\n","    img_path = data_dir + img\n","    preprocessed_input = load_image_normalize(img_path, mean, std)\n","    predictions = model.predict(preprocessed_input)\n","    print(\"Ground Truth: \", \", \".join(np.take(labels, np.nonzero(df[df[\"Image\"] == img][labels].values[0]))[0]))\n","\n","    plt.figure(figsize=(15, 10))\n","    plt.subplot(151)\n","    plt.title(\"Original\")\n","    plt.axis('off')\n","    plt.imshow(load_image(img_path, df, preprocess=False), cmap='gray')\n","    \n","    j = 1\n","    \n","    # Loop through all labels\n","    for i in range(len(labels)): # complete this line\n","        # Compute CAM and show plots for each selected label.\n","        \n","        # Check if the label is one of the selected labels\n","        if labels[i] in selected_labels: # complete this line\n","            \n","            # Use the grad_cam function to calculate gradcam\n","            gradcam = grad_cam(model, preprocessed_input, i, layer_name)\n","            \n","            print(\"Generating gradcam for class %s (p=%2.2f)\" % (labels[i], round(predictions[0][i], 3)))\n","            plt.subplot(151 + j)\n","            plt.title(labels[i] + \": \" + str(round(predictions[0][i], 3)))\n","            plt.axis('off')\n","            plt.imshow(load_image(img_path, df, preprocess=False), cmap='gray')\n","            plt.imshow(gradcam, cmap='magma', alpha=min(0.5, predictions[0][i]))\n","            j +=1\n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["Gjto3yCm-B9b","CVTYQ6pAHvxq"],"provenance":[]},"coursera":{"schema_names":["AI4MC1-1"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":0}
