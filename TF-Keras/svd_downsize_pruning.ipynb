{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc-enDOhazoe"
      },
      "source": [
        "## Overview of Singular Value Decomposition\n",
        "\n",
        "In this takehome, you will be working on a problem involving singular value decomposition. Singular Value Decomposition (SVD) exists for every rectangular matrix. The nice thing about SVD is that the original matrix can be expressed as the sum of outer products of left and singular vectors scaled by the corresponding singular values. Formally:\n",
        "\n",
        "> Let ùõ¢ be a rectangular matrix of dimensions ùëöùòπùëõ, then the SVD of the matrix A is given by $ A = Uùõ¥V^T$ where $U$ is an orthogonal matrix of shape mxm containing the left singular vectors, $V$ is an orthogonal matrix of shape nxn containing the right singular vectors and $ùõ¥$ is a diagonal matrix containing the singular values of $A$. This formulation of SVD can be re-expressed as \\begin{align} A = \\sum_{i=1}^{r} s_i. u_i v_i^T \\end{align} where $r = \\text{min}(m,n)$ represents the rank of the matrix, $s_i$ is the $i$th singular value and $u_i v_i^T$ is the outer product of the $i$th left and right singular vectors. \n",
        "\n",
        "<!-- \\begin{align}\n",
        "A = \\sum_{i=1}^{\\text{min}(m,n)} s_i. u_i v_i^T\n",
        "\\end{align}\n",
        "\\begin{align} -->\n",
        "\n",
        "> The singular values $ùõ¥$ are decreasing in order. So, each outer product is scaled by a smaller value as we compute each term in the sum above. This gives us an opportunity to approximate $A$ using only the sum of the first $k$ outer products where $k < \\text{min}(m,n)$ $-$ this effectively means that we are zero-ing out some of the singular values by assuming that the contribution to the sum is negligible. This is called low-rank approximation.\n",
        "\n",
        "If you aren't familiar with singular value decomposition, or the above feels rusty, don't worry. Take a moment to brush up your knowledge using any of the following resources:\n",
        "* [stanford lecture notes on low rank approximations](https://web.stanford.edu/class/cs168/l/l9.pdf)\n",
        "* [youtube series of short and beginner friendly lectures](https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcL7u9wJjPF3"
      },
      "source": [
        "## Check for understanding (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-iHTH3nasL"
      },
      "source": [
        "#### Q1: What are some real world applications of low rank approximations?\n",
        "\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "1. It can be used for **compression** (meaning, we use only few components that contain the most of the information e.g. compressing an image to use less storage). \n",
        "2. Low rank approximation can be use for **de-noising** as the very understanding of noise is that it should not be the definitive part of the information and so it should live in low importance components we chop off (e.g. removing licence plates to prepare images for some DL model). \n",
        "3. A third application of low rank approximations would be **matrix completion** when our row-wise and column-wise correlated matrix is sparse and we want to fill gaps for some similarity based applications (e.g. large matrices for recomender systems seem to be most beloved example of this).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHGqpn3tkFRL"
      },
      "source": [
        "#### Q2: What are the benefits of compressing a deep neural network? How would you measure the benefits of compression?\n",
        "\n",
        "\n",
        "#### Answer: \n",
        "####A. Benefits of compresing DL model may be following: \n",
        "\n",
        "1. we may want to satisfy some resource constraint like less memory usage/ power consumption/ network communication bandwidth. \n",
        "2. we may reduce latency as well\n",
        "\n",
        "main goal is optimal choice between accuracy, latency and resourse requirements tailored for a given case. \n",
        "\n",
        "####B. Measurement of the benefits of compression (written in general form, can be rewritten in layer number dependent way):\n",
        "\n",
        "1. **Speed up:** from the $min(O(n^2m), O(m^2n))$  to the $O(nmk)$ because k is significantly smaller than given n or m\n",
        "2. **Storage needed:**  from the $O(nm)$ to the $O(k(n+m))$\n",
        "\n",
        "of course $(A ‚àí A_k)_F ‚â§ (A ‚àí B)_F$ where $B$ is rank $k$ and $k>=1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9r4v5vng9ma"
      },
      "source": [
        "#### Q3: In this takehome, we will consider how singular value decomposition can be used to compress a deep neural network. Compared to other compression methods used for deep neural networks such as pruning, quantization, or efficient architectures, what are the relative merits/demerits of low rank approximations? Choose one or two alternative compression methods and compare with singular value decomposition.\n",
        "\n",
        "#### Answer: \n",
        "\n",
        "This question allows to go very fahr/deep in the literature and in choosing the comparison criteria. Here I try to focus only on just a general notion of needed space (without specifying for GPU, CPU, TPU etc.), on general notion of latency (inference mainly) on methods generaly (not on their diverse types) on example of ConvNets.\n",
        "\n",
        "1. Among **pruning, quantization, low rank approximation**, the latest is most prone to **accuracy drop with increasing compression rate**, while there is no clear winner between pruning, quantization. In some architectures prunning winns (e.g. vgg) in others quantization performes better (e.g. ResNet). [Here](https://faculty.ucmerced.edu/mcarreira-perpinan/papers/ijcnn21c.pdf) and [here](https://arxiv.org/pdf/2101.09671.pdf) are actual comparisons under comparable settings for nn performance of different compressed architectures. \n",
        "2. On the other hand **prunning** requires much longer training time (needs several iterations between prunning and retraining/or tunning) and \n",
        "3. **quantization** can be extremely trickty to actually implement (as an optimal implementation can be extremely hardware dependent and also quantization of just pretrained weights is not all we want - we want quantization of operators as well and unseen inputs and potential bit overflows may be a problem). \n",
        "P.S. this [blog](https://medium.com/gsi-technology/an-overview-of-model-compression-techniques-for-deep-learning-in-space-3fd8d4ce84e5) helped a lot in the beginning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O81HH6D3Lugd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoWy4-iQIYJ0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBFa6wXqAKgK",
        "outputId": "6c83fd76-6442-4b2a-b3f8-713d0ef4a144"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350 kB 18.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145 kB 46.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85 kB 2.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q dm-haiku optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzdlf_milUbR"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, Mapping, Tuple\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from absl import app\n",
        "import haiku as hk\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "import math\n",
        "\n",
        "Batch = Tuple[np.ndarray, np.ndarray]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsYJmUqz-uFT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTGxEfqnSmd"
      },
      "source": [
        "## Coding Challenge Part I : Debugging Challenge (10 Points)\n",
        "\n",
        "\n",
        "We are now going to explore using SVD to compute low rank approximations of the parameters of a small deep neural network. You are using a very simple toy model as a first baseline. Section 3 will give you the chance to improve baseline accuracy beyond this very simple model -- this is just a toy setting to first explore low rank approximations.\n",
        "\n",
        "The first part of this challenge is primarily a debugging challenge. It will require removing bugs in order to train a very simple network. We have introduced several bugs -- some are subtle and will not break your code but will degrade final performance. These subtle bugs are introduced to understand your grasp of fundamental machine learning principles. There are also more obvious bugs designed to break your code. \n",
        "\n",
        "* [**4 points**] Your goal is to get the code working. There are 4 bugs in the code, these are subtle bugs which are designed to impair test accuracy but not break the code. You will get partial points for each of the 4 bugs you find. After finding all bugs, your test performance should be around 66-67% test accuracy.\n",
        "\n",
        "* [**2 points**] We will give extra points for also adding improved documentation to each of the functions we introduce in this section, and for describing the fixes to the bugs. \n",
        "\n",
        "* [**4 points**] There are also two functions you will need to code up in this section -- we indicate where these code changes need to happen with TODO comments. \n",
        "\n",
        "* Do not alter the model architecture or the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqpgWQJs-evw"
      },
      "outputs": [],
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
        "\n",
        "\n",
        "def net_fn(batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\" MLP network based on haiku sequential list of layers \n",
        "      for 10 class based image classification \n",
        "      Input: batch of type batch/tuple with 2 numpy.ndarray elements,\n",
        "             Tuple[np.ndarray, np.ndarray] :\n",
        "             batch[0] is (64, 32, 32, 3) numpy.ndarray \n",
        "                          where 64 is batch size and can be any size\n",
        "                          32,32,3 corresponds to image dimensions\n",
        "             batch[1] is (64,) numpy.ndarray:\n",
        "                          is array of labels 0 to 9, corresponding to 10 classes\n",
        "      Output:\n",
        "             10 prediction values for each class \"\"\"\n",
        "  x = normalize(batch[0])  \n",
        "  \n",
        "  # Do NOT alter the architecture definition below.\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)\n",
        "\n",
        "def load_dataset(\n",
        "    split: str,\n",
        "    *,\n",
        "    is_training: bool,\n",
        "    batch_size: int,\n",
        ") -> Iterator[tuple]:\n",
        "  \"\"\"Loads the dataset as a generator of batches.\n",
        "     Input: 1. split: part of dataset that we want to load should be used like python string slicing\n",
        "               Optional[Tree[splits_lib.SplitArg]] = None,\n",
        "            2. * : placeholder for args\n",
        "            3. is_training: type of bool, if True it shuffles data randomly each time batch is called\n",
        "            4. batch_size: type of integer, defines how many `picture` label pairs are processed in a \n",
        "               single forward pass. treated as tfds.typing.Dim \n",
        "      Output: Iterable over two element tuples Tuple[np.ndarray, np.ndarray]\n",
        "              consisting of:\n",
        "              batch[0] is (64, 32, 32, 3) numpy.ndarray \n",
        "                  where 64 is batch size and can be any size\n",
        "                  32,32,3 corresponds to image dimensions\n",
        "             batch[1] is (64,) numpy.ndarray:\n",
        "                  is array of labels 0 to 9, corresponding to 10 classes\"\"\"\n",
        "  ds = tfds.load('cifar10', split=split, as_supervised=True).cache().repeat()\n",
        "  if is_training:\n",
        "    # changed this: 10 * batch_size, to whole dataset size not to add bias for different sizes of tested batches, just to be on safe side. 10 * batch_size,\n",
        "    # and jes, I am not spliting training data into train/val\n",
        "    ds = ds.shuffle(50000, seed=0) \n",
        "  ds = ds.batch(batch_size)\n",
        "  return iter(tfds.as_numpy(ds))\n",
        "\n",
        "@jax.jit\n",
        "def compute_loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Computes the loss of the network with L2 norm penalty.\n",
        "     Input: 1. params: parameters of the MLP having type of haiku.Params\n",
        "            2. batch: having type of batch/two element tuple\n",
        "                      Tuple[np.ndarray, np.ndarray]:\n",
        "                      batch[0] is (64, 32, 32, 3) numpy.ndarray \n",
        "                          where 64 is batch size and can be any size\n",
        "                          32,32,3 corresponds to image dimensions\n",
        "                      batch[1] is (64,) numpy.ndarray:\n",
        "                          is array of labels 0 to 9, corresponding to 10 classes\n",
        "      Output: cross-entropy loss with regularization penalty, type of float\"\"\" \n",
        "  x, y = batch\n",
        "  logits = net.apply(params, batch)\n",
        "  labels = jax.nn.one_hot(y, 10)\n",
        "\n",
        "  # TODO: add code below to compute the l2_loss variable\n",
        "  \n",
        "  ### here jax.tree_util.tree_leaves(params) gives list of weight and bias parameters from the model 'params'\n",
        "  ### every second starting from 0 is a bias and every second starting from 1 is a weight. we take those weights,\n",
        "  ### we square them, we sum up results as a L2_loss\n",
        "  ### at first I thought task was actual l2 not its specific understanding as Frobenius for regularization: l2_loss = jnp.sum((labels - logits)**2) where it appeares optax.l2_loss() can be also used                 \n",
        "  ### I am still not shure if the L2 regularization of Ws is meant here - everything looks like it should mean it.\n",
        "  ### l2_loss = sum([jnp.sum(w**2) for i, w in enumerate(jax.tree_util.tree_leaves(params)) if i/2!=0]) \n",
        "  ### as biases are scallars I ommit them though we can include them in l2_loss = sum([jnp.sum(w**2) for w in jax.tree_util.tree_leaves(params)]) \n",
        "  ### jnp.sum(jnp.vdot(w, w) or jnp.sum(w**2.0) # leaves, _ = jax.tree_util.tree_flatten(params) or jax.tree_util.tree_leaves(params)\n",
        "  l2_loss = sum([jnp.sum( jnp.vdot(w, w)) for i, w in enumerate(jax.tree_util.tree_leaves(params)) if i/2!=0])    \n",
        "  \n",
        "  weighted_l2_loss =  0.5 * l2_loss                        \n",
        "\n",
        "  ## not a subtle bug, it appeares: jax.nn.log_softmax(a)==jax.nn.log_softmax(a, axis=1) # still added axis=1 to be on safe side\n",
        "  ## I changed jax.nn.log_softmax to optax.sigmoid_binary_cross_entropy juat to use (0s) false minimization as well with true maximization for the learning\n",
        "  ## softmax_xent = jnp.sum( optax.sigmoid_binary_cross_entropy(logits, labels)) it worked better! but in case I'm not allowed to do so I go back to original one\n",
        "  \n",
        "  ### here we use softmax with sum reduction for calculating crossentropy, but jnp.mean() instead of jnp.sum() gave more stable results and +2% accuracy\n",
        "  ### So, I mimiced mean reduction by applying: softmax_xent /= y.shape[0] but with some learning rates just sum reduction works better\n",
        "  ### besides binary cross entropy we could use multinomial logistic regression for multiple classes \n",
        "  \n",
        "  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits, axis=1))\n",
        "  ### softmax_xent /= y.shape[0]\n",
        "\n",
        "  ### I deliberately did not use averaging softmax_xent /= y.shape[0]\n",
        "  ### I focused more on increase of test accuracy little bit on expance of train accuracy as well, that's one reason for not using: softmax_xent /= y.shape[0]\n",
        "  \n",
        "      ### ### two subtle bugs in -->  softmax_xent - (1e-4 * l2_loss) : ### ###\n",
        "\n",
        "  ### subtle bug 1:\n",
        "  ### 1. here should be addition, not softmax_xent - (1e-4 * l2_loss) becausse we want to accumulate losses to forse nn make both losses smaller\n",
        "  ### starting accuracy was 10%. \n",
        "  ### Fixing this bug increased test and validation accuracy at least by 20% \n",
        "  \n",
        "  ### subtle bug 2:\n",
        "  ### 2. there was l2_loss standing in place of weighted_l2_loss. So I replaced it.  \n",
        "  ### (Than I made mistake as according to the math formula weighted_l2_loss should be devided over 2m and m in this case is batch size: \n",
        "  ### batch[0].shape[0] if I would use optax.sigmoid_binary_cross_entropy then it would be batch[0].shape[0]*10 )\n",
        "  ### I added division over 2m ((1 / (2*batch[0].shape[0]*10) ) * weighted_l2_loss) \n",
        "  ### but than I realized that division by m is done afterwards in with backward process and division by 2 we have already done by 0.5*L2_loss. So, removed it.\n",
        "  ### second bug fix caused increase in accuracy (test-with average params, valid-with params) by around 3-4%\n",
        "\n",
        "  ### First I thought lambda=1e-4 was subtle bug 3 --> but it is not! \n",
        "  ### I am leaving though what I went through as suggested at the introductory introductory meeting:\n",
        "\n",
        "      ### got Train / Validation / Test accuracy: 0.992 / 0.543 / 0.531 with lambda=1e-4 -->obviously diversity gap is large and regularization \n",
        "      ### Should help as it is exactly designed for addressing overfitting problem. I already moved as much data as possible to train set. \n",
        "      ### as this is bug fixing homework I do not apply other possible methods like augmentation, dropout, weight clipping etc or bach normalization,  \n",
        "      ### and I will only try to tune Lambda between zero and 10 on the log scale (not to be biased) or later will further cross validate it.\n",
        "      ### I am looking at: \n",
        "        # 1.how fast training accuracy diverges from valid accuracy and how fahr - as ideally if train acc becomes as big as our wanted 67% val_acc/test_acc will be adjusted \n",
        "        # 2. bigger batch size to some point allows us to be more aggressive and that point is untill training accuracy does not get to bad(e.g.<67%) or to good(>80)\n",
        "        # 3. I am stopping early when I see any type of big fluctuations or too mutch inconsistent pick up of values between train and validation accuracies (test mirrors validation later)\n",
        "        # 4. how fast is transition of accuracy from 10s to 20s to 30s  etc. \n",
        "      ### Lambda= (13.6 * 1e-2) or (11.6 * 1e-2 ) instead Lambda= 1e-4 boosted test accuracy to 61.1 % (by 8%) \n",
        "      ### todo: if time left go through literature to find better methods for adjusting lambda (there are some, even kernel dependents) \n",
        "      ### here is still a lot of room for improvement\n",
        "      ### If somewhere else is the bug: \n",
        "      ### If overfitting can not be solved by our single tool - regularization, so the bug may be somewhere in data retreival (dataloader?) or in access of data:\n",
        "      ### (cache? reshaffling? randomseed?). And Yes, this direction for bug search was right. Several Bugs are in normalize function.\n",
        "\n",
        "  #### we could just:\n",
        "  ####  logits = net.apply(params, batch)\n",
        "  ####  labels = jax.nn.one_hot(batch[1], 10)\n",
        "  ####  l2_loss = 0.5 * sum(jnp.sum(jnp.square(parameter)) for parameter in jax.tree_util.tree_leaves(params))\n",
        "  ####  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "  ####  softmax_xent /= labels.shape[0]\n",
        "  \n",
        "  return softmax_xent + (11.6 * 1e-2 * weighted_l2_loss)  \n",
        "\n",
        "@jax.jit\n",
        "def compute_accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Computes the accuracy of MLP predictions as a fraction of correct guesses over all predictions.\n",
        "     Input: 1. params: parameters of the MLP having type of haiku.Params.\n",
        "            2. batch: having type of batch/two element tuple\n",
        "                      Tuple[np.ndarray, np.ndarray]:\n",
        "                      batch[0] is (64, 32, 32, 3) numpy.ndarray \n",
        "                          where 64 is batch size and can be any size\n",
        "                          32,32,3 corresponds to image dimensions\n",
        "                      batch[1] is (64,) numpy.ndarray:\n",
        "                          is array of labels 0 to 9, corresponding to 10 classes\n",
        "      Output: type of float accuracy describing fraction of correct guesses over all predictions\"\"\" \n",
        "\n",
        "  predictions = net.apply(params, batch)\n",
        "\n",
        "  # TODO: add code below to compute the accuracy over the batch.\n",
        "\n",
        "  ### line 1: we take labels from each memeber of the tuple batch (`img`, `label`), where labels are integers 0-9\n",
        "  ### line 2: in predictions each row is dedicated to one single image prediction with 10 different scores, where place of score\n",
        "  ### describes the class 0-9th position and score describes how much confident is the model, so we need place of highest score across the row\n",
        "  ### which we retrieve with jnp.argmax(). So we have models prediction in form of class 0-9 and we check if this one corresponds with the given label ('==') \n",
        "  ### to get right guesses as Trues and incorrect guesses as Falses\n",
        "  ### line 3: As right guesses are True and incorrect ones are False, they correspond to 1s and 0s. \n",
        "  ### So, we count nonzeros and devide them by length of all labels to get the portion of right guesses among all guesses.\n",
        "  _, lables = batch\n",
        "  right_guesses = jnp.argmax(predictions, axis=1)==lables \n",
        "  accuracy = jnp.count_nonzero(right_guesses) / len(lables)\n",
        "  return accuracy                                          ## or just jnp.mean(jnp.argmax(predictions, axis=-1) == lables)\n",
        "\n",
        "@jax.jit\n",
        "def update(\n",
        "    params: hk.Params,\n",
        "    opt_state: optax.OptState,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, optax.OptState]:\n",
        "  \"\"\"Defines the learning rule (the stochastic gradient descent).\n",
        "     Input: 1. params: parameters of the MLP having type of haiku.Params.\n",
        "            2. opt_state: having type of optax.OptState contains an optimizer state \n",
        "                          that is used to calculate parameter updates from their gradients.\n",
        "            3. batch: having type of batch/ two element tuple\n",
        "                      Tuple[np.ndarray, np.ndarray]:\n",
        "                      batch[0] is (64, 32, 32, 3) numpy.ndarray \n",
        "                          where 64 is batch size and can be any size\n",
        "                          32,32,3 corresponds to image dimensions\n",
        "                      batch[1] is (64,) numpy.ndarray:\n",
        "                          is array of labels 0 to 9, corresponding to 10 classes\n",
        "      Output: updated parameters and current optimizer state \"\"\" \n",
        "  grads = jax.grad(compute_loss)(params, batch)\n",
        "  updates, opt_state = opt.update(grads, opt_state)\n",
        "  new_params = optax.apply_updates(params, updates)\n",
        "  return new_params, opt_state\n",
        "\n",
        "@jax.jit\n",
        "def ema_update(params, avg_params):\n",
        "  \"\"\"Polyak exponential moving average calculation \n",
        "     based on previous exp moving average and new parameter state\n",
        "     used only for evaluation, not for learning.\n",
        "     Input: \n",
        "          1. params: new updated parameters of the MLP, having type of haiku.Params.\n",
        "          2. avg_params: exponential moving average of all previous states of MLP parameters,\n",
        "                         having type of haiku.Params.\n",
        "      Output: new exponential moving average considering current parameter state\"\"\"\n",
        "  return optax.incremental_update(params, avg_params, step_size=0.001) #polyak\n",
        "\n",
        "#### subtle bugs 3 and 4 (probably 5 as well and bug 6 comes in the next cell):\n",
        "#### subtle bug 3:\n",
        "#### conversion of whole batch to jnp.int8 even before applying mean&std alters images, as cyfar10 images, according to tensorflow docs are datatype tf.uint8\n",
        "#### so, images have type on which the mean and std is calculated and by using additional unnecessary conversion even before applying \n",
        "#### the normalization or standartization we are very much altering the data. I found this bug only after the bug 4 (+5) that comes next. \n",
        "#### I changed order just because of (+5) bug is associated with bug 4 and I did not want to have \"unreal bug 5\" between real bugs (real bug 6 comes in next cell)\n",
        "#### I got accuracy boost to 70% after fixing this bug [Step 10000] Train / Validation / Test accuracy: 0.844 / 0.734 / 0.703.\n",
        "\n",
        "#### subptle bug 4 (and 5):\n",
        "#### here we had x /- mean instead of x-=mean, which was incorrect, as we want to substract mean from x and so we have to do it as follows: x-=mean\n",
        "#### this is because we do per-dataset / feature-wise standartization, \n",
        "#### but when we do it with entire datasets mean&std on the whole batch vs single image results are different! (I do Not mean per-image/sample-wise)\n",
        "#### So, something else is wrong here as well\n",
        "#### Yes, it appeares application of  normalization and standartization (even int8 type) applied to a single image gives very different result \n",
        "#### from when applied on batch. Fixing this 4 (+5) bug caused very minor increase in accuracy, around 4%\n",
        "\n",
        "def normalize(images):\n",
        "  \"\"\" Function applies pixel Normalization and \n",
        "      pixel standartization with pixel Centering to the input.\n",
        "      Input:  np.ndarray corresponding to the batch of `images`, \n",
        "              having tyoe of Tuple[np.ndarray, np.ndarray]:\n",
        "              batch[0] is (64, 32, 32, 3) numpy.ndarray \n",
        "              where 64 is batch size and can be any size\n",
        "              32,32,3 corresponds to image dimensions\n",
        "      Output: normalized and standartized ndarray of same shape (64, 32, 32, 3)\"\"\" \n",
        "\n",
        "  mean = np.asarray(CIFAR10_MEAN)\n",
        "  std = np.asarray(CIFAR10_STD)\n",
        "  # ## 'Pixel' Normalization\n",
        "  # x = images.astype(jnp.int8) / 255.\n",
        "  # ## 'Pixel' standartization that includes 'Pixel' Centering    \n",
        "  # x -= mean\n",
        "  # x /= std \n",
        "  # x = images.astype(jnp.int8)  \n",
        "  # ## this part with a for iterator does not work as jax is tracking x and conflicts with np.asarray(x, dtype=np.float64) converting it to \n",
        "  # ## jax array causes another problem\n",
        "  # x = [((x.astype(jnp.int8)/225.)-mean)/std for x in images]  \n",
        "  # x = np.asarray(x, dtype=np.float64)  #or np.array(x).astype(np.float64)  \n",
        "  # ## with jnp.int8 we get rough contures of the shape, the information about the object is altered \n",
        "  # convert = lambda z: ((z.astype(jnp.int8)/225.)-mean)/std               \n",
        "  # x = convert(images)\n",
        "  convert = lambda z: ((z/225.)-mean)/std               \n",
        "  x = convert(images)\n",
        "\n",
        "  return x\n",
        "\n",
        "## test:\n",
        "# validation = load_dataset(\"test[0%:50%]\", is_training=False, batch_size=64) \n",
        "# batch=next(validation)\n",
        "# mean = np.asarray(CIFAR10_MEAN)\n",
        "# std = np.asarray(CIFAR10_STD)\n",
        "# plt.imshow(((batch[0][11].astype(jnp.int8)/225.)-mean)/std)\n",
        "#plt.imshow(normalize(batch[0][11]))\n",
        "#plt.imshow(batch[0][11])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5iI930cIjzM"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4-HuMSH_Cbw",
        "outputId": "61e456af-219a-42c2-d81f-406bdb2ec68a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 0] Train / Validation / Test accuracy: 0.094 / 0.094 / 0.103.\n",
            "[Step 100] Train / Validation / Test accuracy: 0.156 / 0.297 / 0.171.\n",
            "[Step 200] Train / Validation / Test accuracy: 0.234 / 0.172 / 0.171.\n",
            "[Step 300] Train / Validation / Test accuracy: 0.250 / 0.312 / 0.191.\n",
            "[Step 400] Train / Validation / Test accuracy: 0.266 / 0.328 / 0.209.\n",
            "[Step 500] Train / Validation / Test accuracy: 0.250 / 0.344 / 0.230.\n",
            "[Step 600] Train / Validation / Test accuracy: 0.516 / 0.438 / 0.248.\n",
            "[Step 700] Train / Validation / Test accuracy: 0.312 / 0.484 / 0.270.\n",
            "[Step 800] Train / Validation / Test accuracy: 0.578 / 0.469 / 0.286.\n",
            "[Step 900] Train / Validation / Test accuracy: 0.406 / 0.438 / 0.300.\n",
            "[Step 1000] Train / Validation / Test accuracy: 0.516 / 0.516 / 0.319.\n",
            "[Step 1100] Train / Validation / Test accuracy: 0.531 / 0.453 / 0.337.\n",
            "[Step 1200] Train / Validation / Test accuracy: 0.531 / 0.516 / 0.359.\n",
            "[Step 1300] Train / Validation / Test accuracy: 0.516 / 0.500 / 0.378.\n",
            "[Step 1400] Train / Validation / Test accuracy: 0.516 / 0.438 / 0.396.\n",
            "[Step 1500] Train / Validation / Test accuracy: 0.625 / 0.562 / 0.417.\n",
            "[Step 1600] Train / Validation / Test accuracy: 0.641 / 0.578 / 0.436.\n",
            "[Step 1700] Train / Validation / Test accuracy: 0.641 / 0.578 / 0.456.\n",
            "[Step 1800] Train / Validation / Test accuracy: 0.609 / 0.547 / 0.477.\n",
            "[Step 1900] Train / Validation / Test accuracy: 0.688 / 0.484 / 0.498.\n",
            "[Step 2000] Train / Validation / Test accuracy: 0.766 / 0.672 / 0.513.\n",
            "[Step 2100] Train / Validation / Test accuracy: 0.609 / 0.609 / 0.531.\n",
            "[Step 2200] Train / Validation / Test accuracy: 0.609 / 0.562 / 0.545.\n",
            "[Step 2300] Train / Validation / Test accuracy: 0.562 / 0.578 / 0.560.\n",
            "[Step 2400] Train / Validation / Test accuracy: 0.703 / 0.641 / 0.570.\n",
            "[Step 2500] Train / Validation / Test accuracy: 0.609 / 0.609 / 0.581.\n",
            "[Step 2600] Train / Validation / Test accuracy: 0.672 / 0.500 / 0.592.\n",
            "[Step 2700] Train / Validation / Test accuracy: 0.516 / 0.547 / 0.599.\n",
            "[Step 2800] Train / Validation / Test accuracy: 0.703 / 0.500 / 0.608.\n",
            "[Step 2900] Train / Validation / Test accuracy: 0.656 / 0.531 / 0.617.\n",
            "[Step 3000] Train / Validation / Test accuracy: 0.641 / 0.672 / 0.623.\n",
            "[Step 3100] Train / Validation / Test accuracy: 0.734 / 0.812 / 0.628.\n",
            "[Step 3200] Train / Validation / Test accuracy: 0.578 / 0.672 / 0.633.\n",
            "[Step 3300] Train / Validation / Test accuracy: 0.719 / 0.531 / 0.637.\n",
            "[Step 3400] Train / Validation / Test accuracy: 0.609 / 0.594 / 0.640.\n",
            "[Step 3500] Train / Validation / Test accuracy: 0.672 / 0.750 / 0.643.\n",
            "[Step 3600] Train / Validation / Test accuracy: 0.516 / 0.594 / 0.646.\n",
            "[Step 3700] Train / Validation / Test accuracy: 0.609 / 0.656 / 0.651.\n",
            "[Step 3800] Train / Validation / Test accuracy: 0.641 / 0.688 / 0.654.\n",
            "[Step 3900] Train / Validation / Test accuracy: 0.672 / 0.734 / 0.658.\n",
            "[Step 4000] Train / Validation / Test accuracy: 0.719 / 0.609 / 0.660.\n",
            "[Step 4100] Train / Validation / Test accuracy: 0.703 / 0.656 / 0.662.\n",
            "[Step 4200] Train / Validation / Test accuracy: 0.703 / 0.672 / 0.665.\n",
            "[Step 4300] Train / Validation / Test accuracy: 0.828 / 0.625 / 0.667.\n",
            "[Step 4400] Train / Validation / Test accuracy: 0.750 / 0.719 / 0.667.\n",
            "[Step 4500] Train / Validation / Test accuracy: 0.625 / 0.594 / 0.669.\n",
            "[Step 4600] Train / Validation / Test accuracy: 0.703 / 0.594 / 0.672.\n",
            "[Step 4700] Train / Validation / Test accuracy: 0.797 / 0.609 / 0.673.\n",
            "[Step 4800] Train / Validation / Test accuracy: 0.844 / 0.656 / 0.675.\n",
            "[Step 4900] Train / Validation / Test accuracy: 0.734 / 0.672 / 0.675.\n",
            "[Step 5000] Train / Validation / Test accuracy: 0.797 / 0.609 / 0.677.\n",
            "[Step 5100] Train / Validation / Test accuracy: 0.578 / 0.609 / 0.679.\n",
            "[Step 5200] Train / Validation / Test accuracy: 0.625 / 0.500 / 0.683.\n",
            "[Step 5300] Train / Validation / Test accuracy: 0.688 / 0.547 / 0.685.\n",
            "[Step 5400] Train / Validation / Test accuracy: 0.750 / 0.688 / 0.685.\n",
            "[Step 5500] Train / Validation / Test accuracy: 0.781 / 0.625 / 0.687.\n",
            "[Step 5600] Train / Validation / Test accuracy: 0.781 / 0.719 / 0.688.\n",
            "[Step 5700] Train / Validation / Test accuracy: 0.875 / 0.703 / 0.690.\n",
            "[Step 5800] Train / Validation / Test accuracy: 0.719 / 0.719 / 0.693.\n",
            "[Step 5900] Train / Validation / Test accuracy: 0.750 / 0.609 / 0.695.\n",
            "[Step 6000] Train / Validation / Test accuracy: 0.781 / 0.750 / 0.694.\n",
            "[Step 6100] Train / Validation / Test accuracy: 0.641 / 0.625 / 0.697.\n",
            "[Step 6200] Train / Validation / Test accuracy: 0.672 / 0.562 / 0.700.\n",
            "[Step 6300] Train / Validation / Test accuracy: 0.812 / 0.547 / 0.702.\n",
            "[Step 6400] Train / Validation / Test accuracy: 0.734 / 0.625 / 0.702.\n",
            "[Step 6500] Train / Validation / Test accuracy: 0.781 / 0.625 / 0.700.\n",
            "[Step 6600] Train / Validation / Test accuracy: 0.812 / 0.578 / 0.699.\n",
            "[Step 6700] Train / Validation / Test accuracy: 0.734 / 0.594 / 0.701.\n",
            "[Step 6800] Train / Validation / Test accuracy: 0.734 / 0.688 / 0.700.\n",
            "[Step 6900] Train / Validation / Test accuracy: 0.719 / 0.625 / 0.702.\n",
            "[Step 7000] Train / Validation / Test accuracy: 0.625 / 0.578 / 0.702.\n",
            "[Step 7100] Train / Validation / Test accuracy: 0.766 / 0.578 / 0.703.\n",
            "[Step 7200] Train / Validation / Test accuracy: 0.734 / 0.672 / 0.703.\n",
            "[Step 7300] Train / Validation / Test accuracy: 0.844 / 0.750 / 0.705.\n",
            "[Step 7400] Train / Validation / Test accuracy: 0.703 / 0.594 / 0.704.\n",
            "[Step 7500] Train / Validation / Test accuracy: 0.797 / 0.703 / 0.702.\n",
            "[Step 7600] Train / Validation / Test accuracy: 0.844 / 0.609 / 0.703.\n",
            "[Step 7700] Train / Validation / Test accuracy: 0.734 / 0.766 / 0.703.\n",
            "[Step 7800] Train / Validation / Test accuracy: 0.719 / 0.641 / 0.705.\n",
            "[Step 7900] Train / Validation / Test accuracy: 0.750 / 0.688 / 0.704.\n",
            "[Step 8000] Train / Validation / Test accuracy: 0.766 / 0.609 / 0.703.\n",
            "[Step 8100] Train / Validation / Test accuracy: 0.672 / 0.688 / 0.703.\n",
            "[Step 8200] Train / Validation / Test accuracy: 0.797 / 0.641 / 0.704.\n",
            "[Step 8300] Train / Validation / Test accuracy: 0.828 / 0.734 / 0.705.\n",
            "[Step 8400] Train / Validation / Test accuracy: 0.828 / 0.719 / 0.703.\n",
            "[Step 8500] Train / Validation / Test accuracy: 0.766 / 0.719 / 0.703.\n",
            "[Step 8600] Train / Validation / Test accuracy: 0.781 / 0.703 / 0.704.\n",
            "[Step 8700] Train / Validation / Test accuracy: 0.719 / 0.703 / 0.703.\n",
            "[Step 8800] Train / Validation / Test accuracy: 0.766 / 0.656 / 0.703.\n",
            "[Step 8900] Train / Validation / Test accuracy: 0.781 / 0.766 / 0.702.\n",
            "[Step 9000] Train / Validation / Test accuracy: 0.797 / 0.609 / 0.703.\n",
            "[Step 9100] Train / Validation / Test accuracy: 0.844 / 0.625 / 0.704.\n",
            "[Step 9200] Train / Validation / Test accuracy: 0.797 / 0.641 / 0.704.\n",
            "[Step 9300] Train / Validation / Test accuracy: 0.812 / 0.625 / 0.702.\n",
            "[Step 9400] Train / Validation / Test accuracy: 0.781 / 0.625 / 0.702.\n",
            "[Step 9500] Train / Validation / Test accuracy: 0.828 / 0.641 / 0.701.\n",
            "[Step 9600] Train / Validation / Test accuracy: 0.891 / 0.703 / 0.702.\n",
            "[Step 9700] Train / Validation / Test accuracy: 0.828 / 0.656 / 0.702.\n",
            "[Step 9800] Train / Validation / Test accuracy: 0.781 / 0.688 / 0.702.\n",
            "[Step 9900] Train / Validation / Test accuracy: 0.812 / 0.703 / 0.701.\n",
            "[Step 10000] Train / Validation / Test accuracy: 0.844 / 0.734 / 0.703.\n"
          ]
        }
      ],
      "source": [
        "net = hk.without_apply_rng(hk.transform(net_fn)) \n",
        "\n",
        "# Do not change learning rate\n",
        "opt = optax.adam(1e-3)\n",
        "\n",
        "### subtle bug 5 or 6:\n",
        "### trn = train[80%:] , val= train[0%:80%] splits were not focusing on having as much training samples as possible \n",
        "### because validset would get 80% and trainset would get 20% of the splited data. Here I put all 50000 images in training set and split testset into val&test\n",
        "### boosted accuracy by around 16%\n",
        "\n",
        "### for small data, espetially when we have so many training steps big batch may cause little overfitting + I had to make batch sizes smaller for fast testing \n",
        "### & just made batch sizes more in correspondence to info retrival exponents of 2 (RAM or 256 bit M4000 GPU) \n",
        "### boosted accuracy by around 4% (assuming more granular orthogonalization is not needed)\n",
        "\n",
        "train = load_dataset(\"train\", is_training=True, batch_size=64)                  # smalest batch size with least fluctuation was 128     \n",
        "validation = load_dataset(\"test[0%:50%]\", is_training=False, batch_size=64) \n",
        "test = load_dataset(\"test[50%:]\", is_training=False, batch_size=5000)         \n",
        "\n",
        "\n",
        "params = avg_params = net.init(jax.random.PRNGKey(42), next(train))       \n",
        "opt_state = opt.init(params) \n",
        "\n",
        "for step in range(10001): \n",
        "  if step % 100 == 0:   #1000 changed with 100 just to see more often what is happening\n",
        "    #### added this just to keep track on difference between test and validation to see what is the bias\n",
        "    train_accuracy = compute_accuracy(params, next(train))\n",
        "    #### changed here avg_params to params to make it comparable with train_accuracy for diversity check\n",
        "    val_accuracy = compute_accuracy(params, next(validation))\n",
        "    test_accuracy = compute_accuracy(avg_params, next(test))  #### data mismatch is not expected, if there will be any gaps probably because of exponential moving averages step being not in compliance with used steps and curve was shifted to the right. So, first check that.\n",
        "    train_accuracy, val_accuracy, test_accuracy = jax.device_get(\n",
        "        (train_accuracy, val_accuracy, test_accuracy))\n",
        "    print(f\"[Step {step}] Train / Validation / Test accuracy: \"\n",
        "          f\"{train_accuracy:.3f} / {val_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "  params, opt_state = update(params, opt_state, next(train))\n",
        "  avg_params = ema_update(params, avg_params)\n",
        "\n",
        "### [Step 10000] Train / Validation / Test accuracy: 0.844 / 0.734 / 0.703. is achieved result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQmOspeD0jCh"
      },
      "source": [
        "## Coding Challenge Part 2: Compression through Low Rank Approximation (8 points)\n",
        "\n",
        "In this section, you will add code to compute the low rank approximation and to compute evaluation metrics. We will evaluate whether the low rank approximation allows for speed up in inference time. We define inference time as the average time to compute the prediction for all examples in the test set.\n",
        "\n",
        "* [**4 points**] You will need to add code to define both the compute_eval_metrics and rank_approximated weight function. \n",
        "* [**4 points**] Q4 and Q5 are worth 2 points each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdZqO7W_KSX8"
      },
      "outputs": [],
      "source": [
        "def compute_eval_metrics(params, batch, n_samples):\n",
        "# TODO: add code to compute the time for inference.\n",
        "  duration_list = []\n",
        "  accuracy_list = []\n",
        "\n",
        "  #took some guidance from pytorch, but not everything applies: https://deci.ai/blog/measure-inference-time-deep-neural-networks/\n",
        "  # I assume that  \"warm_up\" like concept in some form applies to jax as well as JIT compilation should affect inference time\n",
        "  # So, to be on safe side I adopt this concept from pytorch example\n",
        "  # I still have to look if there are torch like concepts about gradient tracing (like e.g. torch.no_grad()) and automatic diff\n",
        "  for _ in range(10):\n",
        "    warm_up = compute_accuracy(params, batch).block_until_ready()\n",
        "\n",
        "  for _ in range(n_samples):\n",
        "    # generally timeit() seems to be better choice if our result should be purified from garmadge collecction processes as well\n",
        "    # https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit and https://stackoverflow.com/questions/70161804/python-time-difference-jax-library\n",
        "    # I use time.time() or time.process_time() as we have time imported above\n",
        "    start = time.time()  \n",
        "\n",
        "    # TODO: add code to correctly compute the accuracy on a given batch.\n",
        "\n",
        "    # used block_until_ready() for correct measurements with asinchronous execution https://jax.readthedocs.io/en/latest/async_dispatch.html\n",
        "    acc = compute_accuracy(params, batch).block_until_ready() \n",
        "      #### I should not do this here, because it appeares tehre are other excercises in following sections dedicated to this:\n",
        "      #### divided on batch[0].shape[0] because it is batch size and we get average time for individual predictions, but this can be ommited\n",
        "      #### duration = (time.time() - start) / batch[0].shape[0]  \n",
        "      #### but it would be easier to do this here and would need less code        \n",
        "    duration = (time.time() - start)    # or time.process_time() - start\n",
        "    duration_list.append(duration)\n",
        "    accuracy_list.append(acc)\n",
        "\n",
        "  return accuracy_list, duration_list\n",
        "\n",
        "## test:\n",
        "# accuracy_list, duration_list = compute_eval_metrics(params, next(test), 5) \n",
        "# accuracy_list, duration_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8U8Nlp9IS4q"
      },
      "outputs": [],
      "source": [
        "def rank_approximated_weight(weight: jnp.ndarray, rank_fraction: float):\n",
        "  # TODO: replace the code below with code to compute the SVD of the matrix to return the rank approximated weights u and v for a given matrix.\n",
        "  # u = jax.random.normal(jax.random.PRNGKey(42), shape=weight.shape)\n",
        "  # size = weight.shape[1]\n",
        "  # v = jax.random.normal(jax.random.PRNGKey(42), shape=(size,size))\n",
        "\n",
        "  #full_matrices=True is default but we want economy svd\n",
        "  u, s, v = jnp.linalg.svd(weight, full_matrices=False)   #np.linalg.svd(weight, full_matrices=False) \n",
        "  rank_fraction = int(rank_fraction // 1)   #here I just take integer because function indicates that for some reason it gets float, maybe I see later why\n",
        "  u, v = u[:, :rank_fraction], s[:rank_fraction].reshape(-1, 1) * v[:rank_fraction, :]\n",
        "\n",
        "  return jnp.array(u, dtype=weight.dtype), np.array(v, dtype=weight.dtype) #np.array(u, dtype=weight.dtype), np.array(v, dtype=weight.dtype)\n",
        "\n",
        "# weight = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n",
        "# u, s, v = np.linalg.svd(weight, full_matrices=True)       \n",
        "# u.shape, s.shape, v.shape\n",
        "# s_diagonal = np.diag(s)\n",
        "# s_diagonal\n",
        "# sigma = np.zeros((s_diagonal.shape[0], v.shape[0]))\n",
        "# sigma\n",
        "# difference = v.shape[0] - s_diagonal.shape[0]\n",
        "# difference\n",
        "# if difference>0: \n",
        "#   sigma[:, :-difference] = s_diagonal\n",
        "# else: sigma = s_diagonal\n",
        "# sigma\n",
        "# np.array((np.dot(np.dot(u, sigma), v)), dtype=weight.dtype) == weight\n",
        "# u[:, :2], v[:2, :]\n",
        "\n",
        "# ## test:\n",
        "# U, V = rank_approximated_weight(weight, 1.5)\n",
        "# ## np.array(U, dtype=weight.dtype) == np.array(u[:, :1], dtype=weight.dtype), np.array(V, dtype=weight.dtype) == np.array(v[:1, :], dtype=weight.dtype)\n",
        "# U == np.array(u[:, :1], dtype=weight.dtype), V == np.array(v[:1, :], dtype=weight.dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvm9InR4JG4F"
      },
      "source": [
        "### Evaluations at different ranks\n",
        "\n",
        "The code below first replaces the weights with the low rank factorizations at different rank fractions. For each modified net, we compute the new eval accuracy. Firstly, add code for the rank_approximated_weight and add code to correctly compute the time for inference (the duration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEjLxaDPEGEY",
        "outputId": "58f97397-85cf-46a5-9a1e-41b7020f70fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model at 1.00\n",
            "Rank Fraction / Test accuracy: 1.00 / 0.669.\n",
            "Rank Fraction / Duration: 1.00 / 0.000024320766.\n",
            "Evaluating the model at 0.90\n",
            "Rank Fraction / Test accuracy: 0.90 / 0.668.\n",
            "Rank Fraction / Duration: 0.90 / 0.000024385331.\n",
            "Evaluating the model at 0.80\n",
            "Rank Fraction / Test accuracy: 0.80 / 0.662.\n",
            "Rank Fraction / Duration: 0.80 / 0.000024256930.\n",
            "Evaluating the model at 0.70\n",
            "Rank Fraction / Test accuracy: 0.70 / 0.666.\n",
            "Rank Fraction / Duration: 0.70 / 0.000024371918.\n",
            "Evaluating the model at 0.60\n",
            "Rank Fraction / Test accuracy: 0.60 / 0.662.\n",
            "Rank Fraction / Duration: 0.60 / 0.000024382053.\n",
            "Evaluating the model at 0.50\n",
            "Rank Fraction / Test accuracy: 0.50 / 0.664.\n",
            "Rank Fraction / Duration: 0.50 / 0.000024374065.\n",
            "Evaluating the model at 0.40\n",
            "Rank Fraction / Test accuracy: 0.40 / 0.614.\n",
            "Rank Fraction / Duration: 0.40 / 0.000024386460.\n",
            "Evaluating the model at 0.30\n",
            "Rank Fraction / Test accuracy: 0.30 / 0.550.\n",
            "Rank Fraction / Duration: 0.30 / 0.000024360138.\n",
            "Evaluating the model at 0.20\n",
            "Rank Fraction / Test accuracy: 0.20 / 0.386.\n",
            "Rank Fraction / Duration: 0.20 / 0.000024333899.\n",
            "Evaluating the model at 0.10\n",
            "Rank Fraction / Test accuracy: 0.10 / 0.196.\n",
            "Rank Fraction / Duration: 0.10 / 0.000024324183.\n"
          ]
        }
      ],
      "source": [
        "rank_truncated_params = deepcopy(params)\n",
        "ranks_and_accuracies = []\n",
        "ranks_and_times = []\n",
        "for rank_fraction in np.arange(1.0, 0.0, -0.1):\n",
        "\n",
        "  print(f\"Evaluating the model at {rank_fraction:.2f}\")\n",
        "  for layer in params.keys():\n",
        "    if 'conv' in layer:\n",
        "      continue\n",
        "    weight = params[layer]['w']\n",
        "    # TODO: complete coding the rank_approximated_weight function to compute \n",
        "    # the SVD of the matrix to return the rank approximated weights u and v for a given matrix.\n",
        "    u, v = rank_approximated_weight(weight, rank_fraction*weight.shape[1])\n",
        "    rank_truncated_params[layer]['w'] = u@v\n",
        "\n",
        "  test_batch = next(test)\n",
        "  # we compute metrics over 50 samples to reduce noise in the measurement.\n",
        "  n_samples = 50\n",
        "\n",
        "  # TODO: complete coding the compute_eval_metrics function to compute the time taken for inference.\n",
        "  \n",
        "  ### If I have understood it correctly model inference time is defined as: \n",
        "  ### \"the average time to compute the prediction for all examples in the test set\" \n",
        "  ### which probably means all_time/(batch_size*number_of_batches)\n",
        "  ### here this would mean all_time/(test_batch*n_samples)  but we do later np.mean(latency) which ommits x/y*n_samples and we have only x/y\n",
        "  ### So, at this stage we have to devide every member of latencies over batch size\n",
        "  # note for myself! if you want that batch reshaffles each time put here directly next(test) not test_batch, where you have to mind possibility of small last batches\n",
        "  test_accuracy, latency = compute_eval_metrics(rank_truncated_params, next(test), n_samples)\n",
        "  latency = [(i/test_batch[0].shape[0]) for i in latency]\n",
        "\n",
        "  print(f\"Rank Fraction / Test accuracy: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(test_accuracy):.3f}.\")\n",
        "  ranks_and_accuracies.append((rank_fraction, np.mean(test_accuracy)))\n",
        "  print(f\"Rank Fraction / Duration: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(latency):.12f}.\")\n",
        "  ranks_and_times.append((rank_fraction, np.mean(latency)))\n",
        "\n",
        "## compare:\n",
        "# accur, lat = compute_eval_metrics(params, test_batch, n_samples)\n",
        "# print(np.mean(accur), np.mean(lat)) \n",
        "# print(compute_accuracy(params, next(test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzyvqIr38eWw"
      },
      "source": [
        "### Q4: What do you observe as the relationship between rank fraction and test accuracy?\n",
        "\n",
        "Plot this relationship showing accuracy (y-axis) vs rank percentage of the matrix (x-axis). You should use the ranks_and_accuracies list computed above.\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "TCdYJ6lSEKM9",
        "outputId": "85c7aed1-fa86-4cbc-bc71-aa41ae7214af"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## TODO: add your code below to plot the relationship between time and test set accuracy\n",
        "######## !!! please note, this TODO does not correspond to Q4 task, I am following the Q4 task description as it seems to be more useful\n",
        "times = [t for _, t in ranks_and_times]\n",
        "accuracies = [acc for _, acc in ranks_and_accuracies]\n",
        "rank_percentage = np.arange(1.0, 0.0, -0.1)*100\n",
        "\n",
        "plt.title('rank vs accuracy')\n",
        "plt.xlabel(\"rank percentage\")\n",
        "plt.ylabel(\"test set accuracy\")\n",
        "\n",
        "plt.plot(rank_percentage, accuracies, 'mo-');\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRkI8rAO5UYe"
      },
      "source": [
        "### Q5: Does replacing the weight matrix with the low factor matrix result in latency speed ups?\n",
        "\n",
        "Plot the relationship of time (y-axis) vs rank percentage (x-axis). To do so add code to compute the ranks_and_times list.\n",
        "\n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "f7jlMYxhi7E-",
        "outputId": "4fe185ee-28d8-480f-da50-922ba3fa94fd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## TODO: add your code below to plot the relationship between time and rank percentage\n",
        "plt.title('time vs rank')\n",
        "plt.xlabel(\"rank percentage\" )\n",
        "plt.ylabel(\"time\")\n",
        "\n",
        "plt.plot(rank_percentage, times, 'co-');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR3JTDWb2QII"
      },
      "source": [
        "## Coding Challenge Part 3: Perform evaluations on the dataset in factorized space. (4 points)\n",
        "\n",
        "In this section, you will perform evaluations on the dataset in factorized space.\n",
        "\n",
        "* [**4 points**] 2 pts for question 6 and question 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OctdUxerSZut"
      },
      "outputs": [],
      "source": [
        "def low_rank_net_fn(batch: Batch, rank: float) -> jnp.ndarray:\n",
        "  \n",
        "  x = normalize(batch[0])\n",
        "  total_input_dim = np.prod(x.shape[1:])\n",
        "\n",
        "  # Do not alter the architecture code.\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(int(rank * min(total_input_dim, 3000)), with_bias=False),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 2000), with_bias=False), \n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 2000), with_bias=False), \n",
        "      hk.Linear(2000), jax.nn.relu,      \n",
        "      hk.Linear(int(rank * 1000), with_bias=False), \n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 10), with_bias=False),\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi2TN_WV-jgZ",
        "outputId": "eda5c2a6-c9f0-46b8-93b2-4d66d4dfdd8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model at 1.00\n",
            "Rank Fraction / Test accuracy: 1.00 / 0.669.\n",
            "Rank Fraction / Duration: 1.00 / 0.000036570114.\n",
            "Evaluating the model at 0.90\n",
            "Rank Fraction / Test accuracy: 0.90 / 0.668.\n",
            "Rank Fraction / Duration: 0.90 / 0.000034454433.\n",
            "Evaluating the model at 0.80\n",
            "Rank Fraction / Test accuracy: 0.80 / 0.662.\n",
            "Rank Fraction / Duration: 0.80 / 0.000029003370.\n",
            "Evaluating the model at 0.70\n",
            "Rank Fraction / Test accuracy: 0.70 / 0.666.\n",
            "Rank Fraction / Duration: 0.70 / 0.000027413589.\n",
            "Evaluating the model at 0.60\n",
            "Rank Fraction / Test accuracy: 0.60 / 0.662.\n",
            "Rank Fraction / Duration: 0.60 / 0.000024387344.\n",
            "Evaluating the model at 0.50\n",
            "Rank Fraction / Test accuracy: 0.50 / 0.664.\n",
            "Rank Fraction / Duration: 0.50 / 0.000020984558.\n",
            "Evaluating the model at 0.40\n",
            "Rank Fraction / Test accuracy: 0.40 / 0.614.\n",
            "Rank Fraction / Duration: 0.40 / 0.000018668693.\n",
            "Evaluating the model at 0.30\n",
            "Rank Fraction / Test accuracy: 0.30 / 0.550.\n",
            "Rank Fraction / Duration: 0.30 / 0.000015497186.\n",
            "Evaluating the model at 0.20\n",
            "Rank Fraction / Test accuracy: 0.20 / 0.386.\n",
            "Rank Fraction / Duration: 0.20 / 0.000012840132.\n",
            "Evaluating the model at 0.10\n",
            "Rank Fraction / Test accuracy: 0.10 / 0.196.\n",
            "Rank Fraction / Duration: 0.10 / 0.000010087856.\n"
          ]
        }
      ],
      "source": [
        "vanilla_to_low_rank_map = {\n",
        "    'conv2_d': 'conv2_d',\n",
        "    'conv2_d_1': 'conv2_d_1',\n",
        "    'linear': ['linear', 'linear_1'],\n",
        "    'linear_1': ['linear_2', 'linear_3'],\n",
        "    'linear_2': ['linear_4', 'linear_5'],\n",
        "    'linear_3': ['linear_6', 'linear_7'],\n",
        "    'linear_4': ['linear_8', 'linear_9']\n",
        "}\n",
        "\n",
        "\n",
        "ranks_and_accuracies = []\n",
        "ranks_and_times = []\n",
        "for rank_fraction in np.arange(1.0, 0.0, -0.1):\n",
        "  low_rank_net_fn_partial = partial(low_rank_net_fn, rank=rank_fraction)\n",
        "  net = hk.without_apply_rng(hk.transform(low_rank_net_fn_partial)) \n",
        "  low_rank_params = net.init(jax.random.PRNGKey(42), next(train))\n",
        "\n",
        "  print(f\"Evaluating the model at \" f\"{rank_fraction:.2f}\")\n",
        "\n",
        "  for layer in vanilla_to_low_rank_map.keys():\n",
        "    if 'conv' in layer:\n",
        "      low_rank_params[layer] = params[layer]\n",
        "      continue\n",
        "    weight = params[layer]['w']\n",
        "    # TODO: complete coding the rank_approximated_weight function to compute the SVD of the matrix to return the rank approximated weights u and v for a given matrix.\n",
        "    u, v = rank_approximated_weight(weight, rank_fraction*weight.shape[1])\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][0]]['w'] = u\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][1]]['w'] = v\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][1]]['b'] = params[layer]['b']\n",
        "  \n",
        "  # TODO: modify the compute_eval_metrics function below to compute the time taken for inference.\n",
        "  batch = next(test)\n",
        "  test_accuracy, duration = compute_eval_metrics(low_rank_params, next(test), 50)\n",
        "  duration = [(i/batch[0].shape[0]) for i in duration]\n",
        "\n",
        "  ranks_and_times.append((rank_fraction, np.mean(duration)))\n",
        "  ranks_and_accuracies.append((rank_fraction, np.mean(test_accuracy)))\n",
        "  print(f\"Rank Fraction / Test accuracy: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(test_accuracy):.3f}.\")\n",
        "  print(f\"Rank Fraction / Duration: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(duration):.12f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObBn-Pf_996r"
      },
      "source": [
        "### Q6: Plot a curve showing time vs rank percentage of the matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "c0bJJFL4LM7q",
        "outputId": "5cc55ef1-f7b1-45ea-9092-813bd90a698e"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3debxVZdn/8c9XxAEnKogcgJOmkqI5oE9mgxqp+eTwK1MUxSlRMmetlB5ME800zSEHnCvAsQwtURTUygSZFMckBQQJUBREnOBcvz/udfJwPAcOeNZZZ+/1fb9e+3X2XmvttS+3m33te1jXrYjAzMzKa7WiAzAzs2I5EZiZlZwTgZlZyTkRmJmVnBOBmVnJORGYmZWcE4FVDUndJC2S1K7oWIogaTdJM4uOwyqPE4FVLEnTJPWuexwRMyJi3YhYWmRcZpXGicCsjZC0etExWDk5EVhFkvR7oBtwb9Yd9GNJNZKi7gtV0iOSzpf0eHbMvZI+I2mopIWSnpRUU++cPSSNkjRf0ouSDmritQ+WNL7BtlMljcju7yPpOUlvS5ol6YwmznOkpH9IukzSG8DPJW0mabSkNyS9nsXasd5zpkk6Q9LTkhZIul3SWk2c/6Qsjk1W6s210nEisIoUEYcDM4B9s+6gXzVxaB/gcGBjYDPgn8DNwKeB54FzACStA4wChgGfzZ53taStGjnnvcCWkjavt+3Q7LkANwLHRcR6QE9g9HL+U/4HeBnoAgwGBFwIbAR8EegK/LzBcw4C9gY+D2wLHNnwpJIGZdu/EREeN7DlqshEIOkmSXMlPdNC51sqaXJ2G9ES57Q24+aI+HdELADuB/4dEQ9FxBLgTmD77LjvANMi4uaIWBIRk4C7ge83PGFELAb+DBwCkCWEHkDdZ+dDYCtJ60fEmxExcTnxvRYRV2av+W5ETI2IURHxfkTMAy4FvtHgOVdExGsRMZ+UlLart0+SLgX2BHbPzmG2XBWZCIBbSL+IWsq7EbFddtuvBc9rxZtT7/67jTxeN7vfHfgfSW/V3YC+wOeaOO8wskRAag3ckyUIgO8B+wDTJT0qaZflxPdq/QeSuki6LetSWgj8AejU4Dn/qXd/cb3/BoCOQH/gwiz5ma1QRSaCiHgMmF9/W9a3OlLSBEl/k9SjoPCs9bRk6dxXgUcjomO927oRMaCJ40cBnSVtR0oIdd1CRMSTEbE/qYvpHuCO5bxuw/+GC7Jt20TE+sBhpO6i5nqT1Lq5WdKuK/E8K7GKTARNGAKcGBE7AmcAV6/Ec9eSNF7SE5IOyCc8y8EcYNMWOtd9wBaSDpfUPrvtJOmLjR0cER+SupYuJo03jAKQtIakvpI2yI5ZCNSuRBzrAYuABZI2Bs5c2f+QiHiE1Jr5o6SdV/b5Vj5VkQgkrQt8BbhT0mTgOmDDbN93JT3TyO2BeqfoHhG9SE3830jarNX/I2xVXAj8LOvKaXRmTnNFxNukfvU+wGuk7peLgDWX87RhQG/gzmzMoc7hwLSsa+d40pdyc50L7AAsAP4C/HElnvtfETEKOJo0q2qHVTmHlYcqdWGabNrffRHRU9L6wIsRsWELnPeW7Lx3fdJzmZlVgqpoEUTEQuAVSd+HNG1C0pea81xJn5K0Zna/E7Ar8FxuwZqZtTEVmQgkDSfNB99S0kxJx5Ca38dIegp4Fti/maf7IjA+e94Y4JcR4URgZqVRsV1DZmbWMiqyRWBmZi2n4opcderUKWpqaooOw8ysokyYMOH1iOjc2L6KSwQ1NTWMHz9+xQeamdl/SZre1D53DZmZlZwTgZlZyTkRmJmVnBOBmVnJORGYmZWcE4GZWRs3dCjU1MBqq6W/Q4e27PkrbvqomVmZDB0K/fvD4mzZo+nT02OAvitT13Y53CIwM2vDBg78KAnUWbw4bW8pTgRmZm3UnDmpBdCYGTNa7nWcCMzM2ph//xsGDIDu3Zs+plu3lns9JwIzszZi0iTo0we22AJuugn69YNLLoEOHZY9rkMHGDy45V7Xg8VmZgWKgEcegV/+Eh58ENZbD844A045BTbM1lz83OfSmMCMGaklMHhwyw0UgxOBmVkhamvhnntSAnjySejSBS68EI4/Hjp2XPbYvn1b9ou/IScCM7NW9P778Ic/wMUXw4svwmabwbXXwhFHwFprFROTE4GZWStYuBCGDIHLLoPXXoPtt4fbb4fvfQ/atSs2NicCM7MczZkDV1wBV18Nb70Fe+wBt9wCvXuDVHR0iROBmVkOXn45zfi5+ebUHfTd78JPfgI77VR0ZB+X2/RRSWtJGifpKUnPSjq3kWOOlDRP0uTs9oO84jEzaw2TJ8Mhh8Dmm8MNN8Bhh8Hzz8Ndd7XNJAD5tgjeB/aIiEWS2gN/l3R/RDzR4LjbI+JHOcZhZparCHj00TQD6IEHYN114fTT0xTQjTYqOroVyy0RREQAi7KH7bNb5PV6ZmatrbYW/vznlADGjYPPfjbN8R8wAD71qaKja75cryyW1E7SZGAuMCoixjZy2PckPS3pLkldmzhPf0njJY2fN29eniGbma3QBx+kK3+32ir1/b/+OlxzDUybBmefXVlJAHJOBBGxNCK2AzYBdpbUs8Eh9wI1EbEtMAq4tYnzDImIXhHRq3PnznmGbGb2Xw3XAbjxRvj1r2HTTeGYY2DttWH48HQ9wPHHp8eVqFVmDUXEW5LGAHsDz9Tb/ka9w24AftUa8ZiZrUhj6wD8IJvOsvvuKSnsuWfbmQL6SeSWCCR1Bj7MksDawLeAixocs2FEzM4e7gc8n1c8ZmYro7F1ACDV/Rk9uvXjyVOeLYINgVsltSN1Qd0REfdJOg8YHxEjgJMk7QcsAeYDR+YYj5lZszVV73/OnNaNozXkOWvoaWD7RrYPqnf/LOCsvGIwM1tVnTvD3Lkf396S6wC0FV6PwMysgalTYdGij/f/t/Q6AG2FE4GZWT0LFsB++6VKoL/+dVolTEp/hwzJtxx0UVxryMwss2RJWiHspZdg1CjYbTc49dSio8qfE4GZWebMM2HkSLjuupQEysJdQ2ZmpAJxv/kNnHRSun6gTJwIzKz0HnsMfvhD2GuvNC5QNk4EZlZqL7+c6gVtuincdhusXsIOcycCMyuthQth331TFdF77/34ovFlUcLcZ2YGS5fCoYemgnEPPpgWkikrJwIzK6Wf/hT+8pe0lvAeexQdTbHcNWRmpXPLLWk94RNOSIvIlJ0TgZmVyt//nqaHfvObcNllRUfTNjgRmFlpTJuWZgjV1MCdd0L79kVH1DY4EZhZKbz9dqoh9OGHaYZQpS0nmScPFptZ1authcMOg+eeg/vvhy23LDqitsWJwMyq3tlnw4gRcOWV8K1vFR1N2+OuITOrar//PVx0ERx3XJolZB/nRGBmVeuf/0wLzu++e2oNVMNC83lwIjCzqjRjBhxwAHTt6hlCK+IxAjOrOosWpRlC770HjzwCn/lM0RG1bU4EZlZVamuhXz+YMiWVkPjiF4uOqO1zIjCzqjJoEPzpT+mq4b33LjqayuAxAjOrGsOGweDBaYD45JOLjqZyOBGYWVUYOxaOPhq+8Q347W89Q2hlOBGYWcWbOTPNENpoI7jrLlhjjaIjqiweIzCzivbOO7D//unvQw9Bp05FR1R5nAjMrGLV1sKRR8KkSamQ3NZbFx1RZXIiMLOKde65qSvokkvgf/+36GgqV25jBJLWkjRO0lOSnpV0biPHrCnpdklTJY2VVJNXPGZWXW6/Hc47D446Ck47rehoKlueg8XvA3tExJeA7YC9JX25wTHHAG9GxBeAy4CLcozHzKrE+PGpS+irX4VrrvEMoU8qt0QQyaLsYfvsFg0O2x+4Nbt/F/BNyf9Lzaxps2alweEuXeDuu2HNNYuOqPLlOn1UUjtJk4G5wKiIGNvgkI2BVwEiYgmwAPhYVRBJ/SWNlzR+3rx5eYZsZm3Y4sVpmujChWlw+LOfLTqi6pBrIoiIpRGxHbAJsLOknqt4niER0SsienXu3LllgzSzihCRLhibMCFdQbzNNkVHVD1a5YKyiHgLGAM0rPwxC+gKIGl1YAPgjdaIycwqy/nnpwHiX/4S9t236GiqS56zhjpL6pjdXxv4FvBCg8NGAEdk9w8ERkdEw3EEMyu5u+9OxeT69YMzzyw6muqT53UEGwK3SmpHSjh3RMR9ks4DxkfECOBG4PeSpgLzgT45xmNmFWjiRDj8cNhlF7juOs8QykNuiSAinga2b2T7oHr33wO+n1cMZlaZhg6FgQPTKmOrrQYbbJBKS6+1VtGRVScXnTOzNmXoUOjfH6ZPTwPES5em2UIPPVR0ZNXLicDM2pQf/zh98df33nuphWD5cCIws8K99lpaUWynndL9xsyY0boxlYmLzplZId58M80GGjYsLTAfATvuCJ/6VNrXULdurR5iabhFYGatZvHidC3AAQekEhHHHpsWlRk0CF54IdUQuvJK6NBh2ed16JCWoLR8uEVgZrn68MM00DtsGNxzDyxalFYSO/FEOOSQ1AqoPyW0b9/0t27WULduKQnUbbeW50RgZi2uthYefxyGD4c77oDXX4eOHaFPHzj0UPj616Fdu6af37evv/hbkxOBmbWICJgyJf3yHz48/Zpfe23Yb7/05b/XXq4U2lY5EZjZJ/LKK+mLf9gwePbZ9Et/r73gggtSElhvvaIjtBVxIjCzlTZnTuryGTYMnngibfvqV+Hqq+HAA8FFgiuLE4GZNcuCBanMw/DhafC3tha+9CW46CI4+GDo3r3oCG1VORGY2X/Vr/HTrRv8/Oew/vrpl/9998H778PnPw9nnZVm/Gy9ddERW0twIjAz4KMaP3XlHaZPTwvDQ5rzf9xxadB3551dAbTaOBGYGZBaAg1r/EBaDnLmTFjd3xZVy1cWmxm1takF0Jh585wEqp0TgVnJvfwy7LFH0/td46f6ORGYlVRtLVx1VVoEftKkVPfHNX7KyYnArITqWgEnnpjKPTz7LAwZkm7du6fB4O7d02OXeqh+7vkzK5HaWrjmGvjJT9IVwDfemGYG1c0Cco2fcnIiMCuJV16Bo49Otf/32guuvx66di06KmsL3DVkVuVqa+G3v01jARMnwg03wP33OwnYR9wiMKtir7wCxxwDY8bAnnumJOAEYA25RWBWherGArbZJq36df31MHKkk4A1zi0CsyozbVoaCxgzBr71rdQK8LUAtjxuEZhVibpWQM+eqRUwZAg88ICTgK2YWwRmVWDatDQWMHo09O6dpoU6AVhzuUVgVsEi4Npr01jAuHFw3XXw4INOArZy3CIwq1DTp6dWwMMPp1bADTd4cRhbNbm1CCR1lTRG0nOSnpV0ciPH7CZpgaTJ2W1QXvGYVYuI9Mu/Z08YOza1CB580EnAVl2eLYIlwOkRMVHSesAESaMi4rkGx/0tIr6TYxxmVWP6dPjBD9JSkd/8ZmoF1NQUHZVVutxaBBExOyImZvffBp4HNs7r9cyqWUSaBbTNNmmx+GuugVGjnASsZbTKYLGkGmB7YGwju3eR9JSk+yV5BVSzBmbMSLWBjjsOdtoJpkyB44/3cpHWcnJPBJLWBe4GTomIhQ12TwS6R8SXgCuBe5o4R39J4yWNnzdvXr4Bm7UREemK4J494fHH4eqr3QqwfOSaCCS1JyWBoRHxx4b7I2JhRCzK7v8VaC+pUyPHDYmIXhHRq3PnznmGbFaYoUPTl/xqq8Emm8C226bF5Hv1Sq2AAQPSPrOWlttgsSQBNwLPR8SlTRzzOWBORISknUmJ6Y28YjJrq4YOTV/6dYvHz5qVbkccATfd5ARg+cpz1tCuwOHAFEmTs21nA90AIuJa4EBggKQlwLtAn4iIHGMya5MGDvwoCdT3yCNOApa/FSYCSVsA1wBdIqKnpG2B/SLi/OU9LyL+Dix3OCsirgKuWol4zarSjBkrt92sJTXnt8b1wFnAhwAR8TTQJ8+gzMpi6VI455w0MNwYl4qw1tCcRNAhIsY12LYkj2DMymT27FQm+rzz4Gtfg7XXXnZ/hw4weHAxsVm5NCcRvC5pMyAAJB0IzM41KrMq9/DDsP326eKwm2+Gxx5LU0W7d0/XB3Tvni4g80Ly1hqaM1h8AjAE6CFpFvAKcFiuUZlVqaVL4Re/SK2AHj1SQtg6u4yyb19/8VsxVpgIIuJloLekdYDVsnIRZraS/vOf9EU/ejQcfni6QGzddYuOyqx5s4Y6Av2AGmB1Zde1R8RJuUZmVkVGj4ZDD4WFC9N1AUce6RIR1nY0p2vor8ATwBSgNt9wzKrL0qVw/vlw7rmw5ZapamjPnkVHZbas5iSCtSLitNwjMasyc+akrqCHH4bDDksVQ90VZG1RcxLB7yUdC9wHvF+3MSLm5xaVWYUbMyZ1Bb31Vlo/+Kij3BVkbVdzpo9+AFwM/BOYkN3G5xmUWaWqmxXUuzd07JjWET76aCcBa9ua0yI4HfhCRLyedzBmlWzOnNQF9NBDqUvo2mvdFWSVoTmJYCrQSDksM6vzyCNwyCGpK+iGG9wKsMrSnETwDjBZ0hiWHSPw9FErvaVL4cILU72gL3wBHnggrSNgVkmakwjuoYmVw8zKbO7c1BU0alQaGL72WlhvvaKjMlt5zbmy+NbWCMSskjz6aOoKevPNVCPomGPcFWSVq8lEIOmOiDhI0hSygnP1RLbOsFmp1NamrqBBg1JX0MiR7gqyyre8FsHJ2d/ngTPrbRfwq9wiMmuj5s5NNYIefDC1Bq67zl1BVh2aTAQRUVdq+gsRMb3+Pkk9co3KrI157LH05f/GGykBHHusu4KsejR5QZmkAVm30JaSnq53ewV4uvVCNCtObS1ccAHsvjussw6MHZsWmXcSsGqyvK6hYcD9wIXAT+ttf9vlJawM5s1LXUEPPAB9+qSFYtwVZNVoeV1DC4AFwCGtF45Z2/C3v6Uvf3cFWRk0p9aQWWnUzQqq6wp64gl3BVn1cyKw0hs6FGpqYLXV0pf/2WfDgQfC+PGw3XZFR2eWv+ZcWWxWtYYOTb/4F2fVtN57D9ZYA/bdF9Zfv9jYzFqLWwRWamee+VESqPPBBzBwYDHxmBXBicBKaerUNCNo9uzG98+Y0brxmBXJicBKZdq0VBeoRw+4++6mu3+6dWvVsMwK5URgpTBzJgwYAFtskcYFfvQjePlluPpq6NBh2WM7dIDBg4uJ06wIHiy2qjZ7dpoOet11EAE/+EGaFbTJJml/377p78CBqTuoW7eUBOq2m5VBbolAUlfgd0AXUvXSIRFxeYNjBFwO7ENaBe3IiJiYV0xWHvPmwUUXpV/8H3yQFo//2c+ge/ePH9u3r7/4rdzybBEsAU6PiImS1gMmSBoVEc/VO+bbwObZ7X+Aa7K/Zqtk/ny45BK44gp49920cMygQbDZZkVHZtZ25ZYIsuqls7P7b0t6HtgYqJ8I9gd+FxEBPCGpo6QN61U+NWuWBQvgssvg0kth0SI4+OC0fGQP18k1W6FWGSOQVANsD4xtsGtj4NV6j2dm25ZJBJL6A/0Bunk6h9Xz9tvp1/8ll6SF47/7XTj3XOjZs+jIzCpH7rOGJK0L3A2cEhELV+UcETEkInpFRK/OnTu3bIBWkRYvhosvhk03TX3/X/saTJyYpoQ6CZitnFwTgaT2pCQwNCL+2Mghs4Cu9R5vkm0za9R778Hll6cE8OMfw447pjUCRoyA7bcvOjqzypRbIshmBN0IPB8RlzZx2Aign5IvAws8PmCNef/9NANos83glFNgq61SqeiRI2HnnYuOzqyy5TlGsCtwODBF0uRs29lAN4CIuBb4K2nq6FTS9NGjcozHKtCHH8Ktt8IvfpHm+e+6K/zhD6lMtJm1jDxnDf2dtND98o4J4IS8YrDKtXRpugL4vPPg3/+GnXZKK4TtuafXBjBraS4xYW1KbS3cdhtsvTUccURaGnLEiDQOsNdeTgJmeXAisMLUXxCme3c4+WTYdls45BBo3z7NAJowIa0N4ARglh/XGrJCNFwQZsaMdD3A5z4Hw4fDQQelBGFm+XMisEIMHPjxBWEgrQ7Wp0/rx2NWZv7NZa0uAqZPb3zfq682vt3M8uNEYK1q7lw44ICm97uCiFnrcyKwVnPffbDNNvDAA6nssxeEMWsbnAgsd++8A8cdl2b/dOkCTz6ZLgobMiTNFpLS3yFDvC6AWRE8WGy5GjcurQkwdSqccQacfz6suWba5wVhzNoGtwgsF0uWpHLQX/lKKhQ3enSqFlqXBMys7XCLwFrcSy/B4Yenq4EPPRR++1vo2LHoqMysKW4RWIuJgOuvh+22gxdfTBeGDR3qJGDW1rlFYC1i7lw49thUF2iPPeCWW6Br1xU+zczaALcI7BOrPy300kth1CgnAbNK4kRgq+ydd+D445edFnrqqa4RZFZp/E/WVsm4cWlpyCFD0rTQJ59MrQIzqzxOBLZSPC3UrPp4sNiazdNCzaqTWwS2Qp4Walbd3CKw5fK0ULPq5xaBNcnTQs3KwYnAPsbTQs3Kxf+0bRmeFmpWPk4EBnhaqFmZebDYmDo1rRngaaFm5eQWQQkNHQo1NanP/zOfga239rRQszJzi6Bkhg6F/v1h8eL0eP78lBDOPRf69Ck2NjMrhlsEJXP22R8lgTq1tWl6qJmVU26JQNJNkuZKeqaJ/btJWiBpcnYblFcslowZAzNmNL6vqe1mVv3ybBHcAuy9gmP+FhHbZbfzcoyl1GbMgIMOSlcGt2vX+DHdurVuTGbWduSWCCLiMWB+Xue3FXv3XTjvPOjRA+69N40DXH89dOiw7HEdOsDgwcXEaGbFK3qweBdJTwGvAWdExLONHSSpP9AfoJt/uq5QBPzpT3DaaTB9Onz/++magO7d0/411oCBA1NLoVu3lAT69i02ZjMrjiIiv5NLNcB9EdGzkX3rA7URsUjSPsDlEbH5is7Zq1evGD9+fIvHWi2eew5OPhkeegh69oQrroDddy86KjMrmqQJEdGrsX2FzRqKiIURsSi7/1egvaRORcVT6d56K9UD2nZbGD8errwSJk1yEjCzFSusa0jS54A5ERGSdiYlpTeKiqdS1dbCzTfDWWfB66+nktHnnw+dOxcdmZlVitwSgaThwG5AJ0kzgXOA9gARcS1wIDBA0hLgXaBP5NlPVYWeeAJOPDG1AL7yFRg5EnbYoeiozKzS5JYIIuKQFey/Crgqr9evZrNnpxbArbfChhvCH/6QagRJRUdmZpXIVxZXkA8+gEsugS23hGHD4Kc/TTWC+vZ1EjCzVVf09FFrppEj02ygf/0LvvOdVBJi8xXOsTIzWzG3CNq4qVNhv/3g299O1wf85S/p4jAnATNrKU4EbdSiRemir623TjWCLroIpkyBffYpOjIzqzbuGmpjIuC22+DMM2HWLDj8cPjlL2GjjYqOzMyqlVsEbcjkyfD1r6cZQF26wD/+Ab/7nZOAmeXLiaANeOMNGDAAdtwRXnghLRw/bly6NsDMLG9OBK2o/hKRNTXw+9+n9YE33zxVBf3Rj9KsoGOPbbpctJlZS/MYQStpuETk9OlwxBFpTGCPPeDyy1ORODOz1uZE0EoGDvz4EpER0KlTqhTqC8LMrCjuGmolTS0F+cYbTgJmViwnglbw6KOwehNtL6+zY2ZFcyLI0Zw50K8f7LYbrL8+rLnmsvu9RKSZtQVOBDlYujTNBtpyy3RxWN2ykDfemJaLlNLfIUO8RKSZFc+DxS1s7Fj44Q9h4kTo3RuuuiolBEhf+v7iN7O2xi2CFjJ/Phx3HOyyS1ov4Lbb4MEHP0oCZmZtlRPBJ1RbCzfdlL7wb7wRTjklXR188MGeDWRmlcFdQ5/AU0+lbqDHH4ddd4Wrr06Lx5uZVRK3CFbBwoVw6qmpNtC//pUWj3/sMScBM6tMbhGshAi4/XY47TT4z39SyYgLLoBPf7royMzMVp0TQTO98AKccAKMHp1aAn/+M+y0U9FRmZl9cu4aWoHFi9N1ANtuCxMmpOsDxo51EjCz6uEWwXKMGAEnnZQqhfbrB7/6VVowxsysmrhF0IhXXoF994X994d11021gm691UnAzKqTE0E9778P558PW22VFoy/+GKYNCktH2lmVq3cNZQZNSoNBr/0Ehx4IFx2GWyySdFRmZnlr/Qtglmz0lXAe+6ZpoeOHAl33ukkYGblUdpE8OGHcOml0KNHGhQ+7zyYMgX22qvoyMzMWlduiUDSTZLmSnqmif2SdIWkqZKelrRDXrE0XDR+0CDYYQc4/fTU///ss/B//wdrrZVXBGZmbVeeLYJbgL2Xs//bwObZrT9wTR5B1C0aP3166vqZPh1+8Qt47TX405/gvvtg003zeGUzs8qQWyKIiMeA+cs5ZH/gd5E8AXSUtGFLx9HYovEA66wDBxzgCqFmZkWOEWwMvFrv8cxsW4tqatH4mTNb+pXMzCpTRQwWS+ovabyk8fPmzVup5za1OLwXjTczS4pMBLOArvUeb5Jt+5iIGBIRvSKiV+fOnVfqRQYPTovE1+dF483MPlJkIhgB9MtmD30ZWBARs1v6Rfr2TYvEe9F4M7PG5XZlsaThwG5AJ0kzgXOA9gARcS3wV2AfYCqwGDgqr1i8aLyZWdNySwQRccgK9gdwQl6vb2ZmzVMRg8VmZpYfJwIzs5JzIjAzKzknAjOzklMas60ckuYB04uO4xPqBLxedBBtiN+PZfn9+Ijfi2V9kveje0Q0eiFWxSWCaiBpfET0KjqOtsLvx7L8fnzE78Wy8no/3DVkZlZyTgRmZiXnRFCMIUUH0Mb4/ViW34+P+L1YVi7vh8cIzMxKzi0CM7OScyIwMys5J4KcSeoqaYyk5yQ9K+nkbPunJY2S9FL291NFx9paJLWTNEnSfdnjz0saK2mqpNslrVF0jK1FUkdJd0l6QdLzknYp+Wfj1OzfyTOShktaqyyfD0k3SZor6Zl62xr9LGTl+6/I3pOnJe3wSV7biSB/S4DTI2Ir4MvACZK2An4KPBwRmwMPZ4/L4mTg+XqPLwIui4gvAG8CxxQSVTEuB0ZGRA/gS6T3pZSfDUkbAycBvSKiJ9AO6EN5Ph+3AHs32NbUZ+HbwObZrT9wzSd5YSeCnEXE7MmhQi0AAAXySURBVIiYmN1/m/QPfWNgf+DW7LBbgQOKibB1SdoE+F/ghuyxgD2Au7JDyvRebAB8HbgRICI+iIi3KOlnI7M6sLak1YEOwGxK8vmIiMeA+Q02N/VZ2B/4XSRPAB0lbbiqr+1E0Iok1QDbA2OBLvVWZPsP0KWgsFrbb4AfA7XZ488Ab0XEkuzxTFKiLIPPA/OAm7OushskrUNJPxsRMQu4BJhBSgALgAmU9/MBTX8WNgZerXfcJ3pfnAhaiaR1gbuBUyJiYf192SI9VT+PV9J3gLkRMaHoWNqI1YEdgGsiYnvgHRp0A5XlswGQ9X/vT0qQGwHr8PGuktLK87PgRNAKJLUnJYGhEfHHbPOcuqZc9nduUfG1ol2B/SRNA24jNfkvJzVr61bL2wSYVUx4rW4mMDMixmaP7yIlhjJ+NgB6A69ExLyI+BD4I+kzU9bPBzT9WZgFdK133Cd6X5wIcpb1gd8IPB8Rl9bbNQI4Irt/BPDn1o6ttUXEWRGxSUTUkAYBR0dEX2AMcGB2WCneC4CI+A/wqqQts03fBJ6jhJ+NzAzgy5I6ZP9u6t6PUn4+Mk19FkYA/bLZQ18GFtTrQlppvrI4Z5K+CvwNmMJH/eJnk8YJ7gC6kcpqHxQRDQeKqpak3YAzIuI7kjYltRA+DUwCDouI94uMr7VI2o40cL4G8DJwFOkHWik/G5LOBQ4mzbabBPyA1Pdd9Z8PScOB3UilpucA5wD30MhnIUuUV5G6zhYDR0XE+FV+bScCM7Nyc9eQmVnJORGYmZWcE4GZWck5EZiZlZwTgZlZyTkRWGlI+rmkM4qOozkkbSdpn6LjsHJwIrCKk11E0+Y+u/Wufm0J2wFOBNYq2tw/JrPGSKqR9KKk3wHPAF0lXSNpfFa//tx6x06TdK6kiZKmSOrRyPmOlXS/pLUbbL9F0rXZef+V1UeqW0PhYklPZvXfj8u27ybpb5JGAM9lx12S1dN/WtKJ2XE7SnpU0gRJD9QrG/CIpIskjcte72tZvf3zgIMlTZZ0sKSdJf0zK073eN3VyNlVuHcorXfxJ6W6/b2yfXtmz5ko6c6s3pXZx7TkLxizvG0OHJGV3UXSwOwqy3bAw5K2jYins2Nfj4gdJP0QOIN0hSrZ834EfAs4oIkrVGuAnYHNgDGSvgD0I13Gv5OkNYF/SHowO34HoGdEvCJpQPb87SJiidLCIu2BK4H9I2KepIOBwcDR2fNXj4ids66gcyKit6RBpLr8P8piXh/4WnbO3sAFwPeAHwJvRsRWknoCk7PjOwE/A3pHxDuSfgKcRkowZstwIrBKMr0uCWQOktSf9DneENgKqEsEdcX9JgDfrfecfqTyvQdkhc0ac0dE1AIvSXoZ6AHsCWwrqa7mzQakxPQBMC4iXsm29waurSubnCWqnkBPYFSqDEA7UpnlOvVjrWkipg2AWyVtTqpA2T7b/lVS4T4i4hlJdf/9X87ej39kr7kG8M8mzm0l50RgleSdujuSPk/6pb9TRLwp6RZgrXrH1v3SX8qyn/MppP73TYBXaFzDuisBCDgxIh6ovyOrmfQOyyfg2YjYpYn9TcVa3y+AMRHx/5TWtXikGa85KiIOWcFxZh4jsIq1PukLeIGkLqSl+5pjEnAcMELSRk0c831Jq0naDNgUeBF4ABiQdfMgaQulRWQaGgUcVzdwLOnT2fM7S9ol29Ze0tYriPNtYL16jzfgozLDR9bb/g/goOy8WwHbZNufAHbNurWQtI6kLVbwmlZSTgRWkSLiKdKX+gvAMNIXYnOf+3dSa+IvWV96QzOAccD9wPER8R6pQuhzwESlxcWvo/Ff7zdkz39a0lPAoRHxAamM8kXZtsnAV1YQ5hhgq7rBYuBXwIWSJjV43atJSeY54HzgWdJYxjxSwhiedRf9k9TFZfYxrj5qVk/WxXRfRNy1omPbgmygvH1EvJe1YB4CtsySj1mzeIzArLJ1IM1sak8aF/ihk4CtLLcIzMxKzmMEZmYl50RgZlZyTgRmZiXnRGBmVnJOBGZmJff/AVLMFWAKaV/4AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: add code to plot the relationship between time vs percentage rank of the matrix.\n",
        "\n",
        "times = [t for _, t in ranks_and_times]\n",
        "rank_percentage = np.arange(1.0, 0.0, -0.1)*100\n",
        "\n",
        "plt.title('time vs rank')\n",
        "plt.xlabel(\"rank percentage\" )\n",
        "plt.ylabel(\"time\")\n",
        "\n",
        "plt.plot(rank_percentage, times, 'bo-');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313ALwDu93k1"
      },
      "source": [
        "### Q7: What do you observe between time and the percentage rank of the matrix.\n",
        "\n",
        "### Put your answer here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcBKkogM6uV"
      },
      "source": [
        "\n",
        "<font color=#f5072f>\n",
        "In the figure, the increasing inference time and the increasing rank percentage show a very strong positive and almost symmetric linear relationship. In other words, the higher rank approximation requires more inference time and vice versa. Inference time spans scale from 0 to little bit above 6e-9 with step size 1e-9 and rank percentage spans scale from 0 to 100 with step size 20. The relationship between two variables is established by 10 points.\n",
        "</ font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEAu9Vu-0rWX"
      },
      "source": [
        "## Coding Challenge Part 4: Take this Further (10 bonus points)\n",
        "\n",
        "This part of the challenge is designed to be open ended. If you wanted to show some more skills, here is your chance to shine. We include two options below -- **only do one of the options**:\n",
        "\n",
        "**Option 1:** Implement a change that isn't SVD but minimizes inference latency while preserving accuracy. Can you outperform SVD? \n",
        "\n",
        "\n",
        "\n",
        "**Option 2:** Improve the quality of code for this takehome. Pretend you are reviewing a peer and add comments to cells with suggestions of how to improve the code quality. Try and make your comments action orientated and precise. \n",
        "\n",
        "\n",
        "**For Option 1, DO NOT alter the previous code sections, instead add any new code below. You should not need to add new code for Option 2, instead just add comments to cells.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eix1pZBgzkn6"
      },
      "source": [
        "A. here starts the layer removal implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoN-IhY9hNQE",
        "outputId": "5ab377aa-612e-4a00-f893-27fd5d3e9e33"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable: ['linear']\n",
            "non_trainable: ['conv2_d', 'conv2_d_1']\n"
          ]
        }
      ],
      "source": [
        "# TODO: add code for option 1 here\n",
        "\n",
        "### please note that this code uses data loaded in in previous home works, some reusable functions and results of svd for comparison\n",
        "\n",
        "### before going in advanced compression techniques, probably the simplest way of reducing models size would be\n",
        "### chopping of the obviously unnecessary huge stack of 3 or 4 unnecessary  layers: linear 0, 1, 2 and 3.\n",
        "### stacking of many linear layers with relu is unnecessary because is very simple dataset and conv layers do most important job for image understanding.\n",
        "### additionally even leaving only one of the two conv layers could still give a not very bad performance\n",
        "### So, first I will try compression just by removing 4 or 3 linear layers. I will also try to implement second type of compression later. \n",
        "\n",
        "### So, here will be two versions of compression: \n",
        "### A. removal of model layers \n",
        "### B. if time's left, I'll also try to implement pruning based on deepmind example\n",
        "### of course we also could chain, first apply A. than B. but I just want to compare them, so I won't chain them\n",
        "\n",
        "\n",
        "### A. compression by removal of whole layers \n",
        "       ## there are two choices: \n",
        "       ## 1. define that smaller model and just copy-paste code of training from homework 1 to train model from scratch\n",
        "       ## 2. use already trained model, chop of last layers like while finetuning and retrain with frozen then unfroze parameters\n",
        "       ## first is very obvious how to do, but in real life second may be more useful let's do it. have no idea how to implement in jax though\n",
        "\n",
        "### A.\n",
        "# define a smaller model without many linear layers and take weights and biases only for remaining layers from params of already trained model \n",
        "\n",
        "def smallnet_fn(batch: Batch) -> jnp.ndarray:\n",
        "  x = normalize(batch[0])  \n",
        "\n",
        "  smnet = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      ## hk.Linear(3000), jax.nn.relu,\n",
        "      ## hk.Linear(2000), jax.nn.relu,\n",
        "      ## hk.Linear(2000), jax.nn.relu,\n",
        "      ## hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return smnet(x)\n",
        "\n",
        "smallnet = hk.without_apply_rng(hk.transform(smallnet_fn)) \n",
        "smallnet_params = smallnet_avg_params = smallnet.init(jax.random.PRNGKey(42), next(train))\n",
        "\n",
        "for layer in params.keys():\n",
        "  if 'conv' in layer:\n",
        "    smallnet_params[layer]['w'] = params[layer]['w']\n",
        "    smallnet_params[layer]['b'] = params[layer]['b']\n",
        "  else: continue\n",
        "\n",
        "### test with this:\n",
        "# def parameter_shapes(params):\n",
        "#   return jax.tree_util.tree_map(lambda p: p.shape, params)\n",
        "# parameter_shapes(smallnet_params), smallnet_params.keys() #,smallnet_params #jax.tree_util.tree_structure(smallnet_params)\n",
        "\n",
        "\n",
        "# got help from the haiku documentation https://dm-haiku.readthedocs.io/en/latest/notebooks/non_trainable.html\n",
        "non_trainable_params, trainable_params = hk.data_structures.partition(\n",
        "    lambda m, n, p: m != \"linear\", smallnet_params)    # generally it would be like \"jax_module/mlp/~/linear\"\n",
        "\n",
        "print(\"trainable:\", list(trainable_params))\n",
        "print(\"non_trainable:\", list(non_trainable_params))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Ey74noy6q8",
        "outputId": "e2b300a0-1453-4167-eee1-b12c122938c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 0] Train / Validation / Test accuracy: 0.062 / 0.172 / 0.099.\n",
            "[Step 100] Train / Validation / Test accuracy: 0.547 / 0.438 / 0.244.\n",
            "[Step 200] Train / Validation / Test accuracy: 0.641 / 0.594 / 0.445.\n",
            "[Step 300] Train / Validation / Test accuracy: 0.562 / 0.531 / 0.529.\n",
            "[Step 400] Train / Validation / Test accuracy: 0.578 / 0.641 / 0.566.\n",
            "[Step 500] Train / Validation / Test accuracy: 0.641 / 0.547 / 0.589.\n",
            "[Step 600] Train / Validation / Test accuracy: 0.578 / 0.625 / 0.606.\n",
            "[Step 700] Train / Validation / Test accuracy: 0.719 / 0.641 / 0.614.\n",
            "[Step 800] Train / Validation / Test accuracy: 0.641 / 0.625 / 0.621.\n",
            "[Step 900] Train / Validation / Test accuracy: 0.609 / 0.609 / 0.626.\n",
            "[Step 1000] Train / Validation / Test accuracy: 0.609 / 0.609 / 0.631.\n",
            "[Step 1100] Train / Validation / Test accuracy: 0.625 / 0.672 / 0.636.\n",
            "[Step 1200] Train / Validation / Test accuracy: 0.641 / 0.719 / 0.640.\n",
            "[Step 1300] Train / Validation / Test accuracy: 0.656 / 0.656 / 0.642.\n",
            "[Step 1400] Train / Validation / Test accuracy: 0.641 / 0.719 / 0.646.\n",
            "[Step 1500] Train / Validation / Test accuracy: 0.719 / 0.750 / 0.648.\n",
            "[Step 1600] Train / Validation / Test accuracy: 0.703 / 0.672 / 0.649.\n",
            "[Step 1700] Train / Validation / Test accuracy: 0.766 / 0.641 / 0.649.\n",
            "[Step 1800] Train / Validation / Test accuracy: 0.656 / 0.703 / 0.651.\n",
            "[Step 1900] Train / Validation / Test accuracy: 0.688 / 0.703 / 0.653.\n",
            "[Step 2000] Train / Validation / Test accuracy: 0.672 / 0.641 / 0.656.\n",
            "[Step 2100] Train / Validation / Test accuracy: 0.688 / 0.766 / 0.658.\n",
            "[Step 2200] Train / Validation / Test accuracy: 0.719 / 0.703 / 0.660.\n",
            "[Step 2300] Train / Validation / Test accuracy: 0.656 / 0.734 / 0.662.\n",
            "[Step 2400] Train / Validation / Test accuracy: 0.656 / 0.625 / 0.664.\n",
            "[Step 2500] Train / Validation / Test accuracy: 0.734 / 0.719 / 0.664.\n",
            "[Step 2600] Train / Validation / Test accuracy: 0.594 / 0.719 / 0.667.\n",
            "[Step 2700] Train / Validation / Test accuracy: 0.656 / 0.672 / 0.668.\n",
            "[Step 2800] Train / Validation / Test accuracy: 0.672 / 0.641 / 0.669.\n",
            "[Step 2900] Train / Validation / Test accuracy: 0.719 / 0.547 / 0.669.\n",
            "[Step 3000] Train / Validation / Test accuracy: 0.797 / 0.609 / 0.671.\n",
            "[Step 3100] Train / Validation / Test accuracy: 0.781 / 0.734 / 0.672.\n",
            "[Step 3200] Train / Validation / Test accuracy: 0.734 / 0.703 / 0.672.\n",
            "[Step 3300] Train / Validation / Test accuracy: 0.688 / 0.750 / 0.672.\n",
            "[Step 3400] Train / Validation / Test accuracy: 0.641 / 0.641 / 0.672.\n",
            "[Step 3500] Train / Validation / Test accuracy: 0.734 / 0.703 / 0.671.\n",
            "[Step 3600] Train / Validation / Test accuracy: 0.797 / 0.656 / 0.671.\n",
            "[Step 3700] Train / Validation / Test accuracy: 0.750 / 0.719 / 0.672.\n",
            "[Step 3800] Train / Validation / Test accuracy: 0.812 / 0.625 / 0.673.\n",
            "[Step 3900] Train / Validation / Test accuracy: 0.672 / 0.641 / 0.674.\n",
            "[Step 4000] Train / Validation / Test accuracy: 0.781 / 0.688 / 0.675.\n",
            "[Step 4100] Train / Validation / Test accuracy: 0.703 / 0.672 / 0.676.\n",
            "[Step 4200] Train / Validation / Test accuracy: 0.812 / 0.656 / 0.675.\n",
            "[Step 4300] Train / Validation / Test accuracy: 0.656 / 0.609 / 0.675.\n",
            "[Step 4400] Train / Validation / Test accuracy: 0.625 / 0.578 / 0.676.\n",
            "[Step 4500] Train / Validation / Test accuracy: 0.750 / 0.719 / 0.677.\n",
            "[Step 4600] Train / Validation / Test accuracy: 0.656 / 0.703 / 0.676.\n",
            "[Step 4700] Train / Validation / Test accuracy: 0.750 / 0.703 / 0.676.\n",
            "[Step 4800] Train / Validation / Test accuracy: 0.719 / 0.578 / 0.676.\n",
            "[Step 4900] Train / Validation / Test accuracy: 0.688 / 0.703 / 0.676.\n",
            "[Step 5000] Train / Validation / Test accuracy: 0.734 / 0.688 / 0.677.\n",
            "[Step 5100] Train / Validation / Test accuracy: 0.688 / 0.734 / 0.677.\n",
            "[Step 5200] Train / Validation / Test accuracy: 0.734 / 0.750 / 0.677.\n",
            "[Step 5300] Train / Validation / Test accuracy: 0.656 / 0.656 / 0.676.\n",
            "[Step 5400] Train / Validation / Test accuracy: 0.750 / 0.734 / 0.676.\n",
            "[Step 5500] Train / Validation / Test accuracy: 0.781 / 0.656 / 0.677.\n",
            "[Step 5600] Train / Validation / Test accuracy: 0.719 / 0.688 / 0.677.\n",
            "[Step 5700] Train / Validation / Test accuracy: 0.812 / 0.734 / 0.676.\n",
            "[Step 5800] Train / Validation / Test accuracy: 0.750 / 0.703 / 0.677.\n",
            "[Step 5900] Train / Validation / Test accuracy: 0.703 / 0.750 / 0.678.\n",
            "[Step 6000] Train / Validation / Test accuracy: 0.781 / 0.781 / 0.678.\n",
            "[Step 6100] Train / Validation / Test accuracy: 0.797 / 0.703 / 0.678.\n",
            "[Step 6200] Train / Validation / Test accuracy: 0.828 / 0.703 / 0.679.\n",
            "[Step 6300] Train / Validation / Test accuracy: 0.719 / 0.719 / 0.680.\n",
            "[Step 6400] Train / Validation / Test accuracy: 0.797 / 0.797 / 0.680.\n",
            "[Step 6500] Train / Validation / Test accuracy: 0.672 / 0.719 / 0.680.\n",
            "[Step 6600] Train / Validation / Test accuracy: 0.844 / 0.672 / 0.680.\n",
            "[Step 6700] Train / Validation / Test accuracy: 0.703 / 0.625 / 0.681.\n",
            "[Step 6800] Train / Validation / Test accuracy: 0.609 / 0.672 / 0.681.\n",
            "[Step 6900] Train / Validation / Test accuracy: 0.672 / 0.672 / 0.681.\n",
            "[Step 7000] Train / Validation / Test accuracy: 0.703 / 0.656 / 0.680.\n",
            "[Step 7100] Train / Validation / Test accuracy: 0.719 / 0.656 / 0.681.\n",
            "[Step 7200] Train / Validation / Test accuracy: 0.688 / 0.719 / 0.681.\n",
            "[Step 7300] Train / Validation / Test accuracy: 0.781 / 0.594 / 0.681.\n",
            "[Step 7400] Train / Validation / Test accuracy: 0.688 / 0.531 / 0.682.\n",
            "[Step 7500] Train / Validation / Test accuracy: 0.734 / 0.812 / 0.682.\n",
            "[Step 7600] Train / Validation / Test accuracy: 0.672 / 0.656 / 0.681.\n",
            "[Step 7700] Train / Validation / Test accuracy: 0.781 / 0.703 / 0.681.\n",
            "[Step 7800] Train / Validation / Test accuracy: 0.703 / 0.609 / 0.681.\n",
            "[Step 7900] Train / Validation / Test accuracy: 0.703 / 0.703 / 0.682.\n",
            "[Step 8000] Train / Validation / Test accuracy: 0.734 / 0.672 / 0.681.\n",
            "[Step 8100] Train / Validation / Test accuracy: 0.781 / 0.656 / 0.681.\n",
            "[Step 8200] Train / Validation / Test accuracy: 0.797 / 0.750 / 0.681.\n",
            "[Step 8300] Train / Validation / Test accuracy: 0.688 / 0.672 / 0.680.\n",
            "[Step 8400] Train / Validation / Test accuracy: 0.703 / 0.641 / 0.679.\n",
            "[Step 8500] Train / Validation / Test accuracy: 0.734 / 0.672 / 0.680.\n",
            "[Step 8600] Train / Validation / Test accuracy: 0.766 / 0.719 / 0.679.\n",
            "[Step 8700] Train / Validation / Test accuracy: 0.781 / 0.672 / 0.680.\n",
            "[Step 8800] Train / Validation / Test accuracy: 0.734 / 0.688 / 0.681.\n",
            "[Step 8900] Train / Validation / Test accuracy: 0.828 / 0.750 / 0.681.\n",
            "[Step 9000] Train / Validation / Test accuracy: 0.781 / 0.672 / 0.681.\n",
            "[Step 9100] Train / Validation / Test accuracy: 0.781 / 0.719 / 0.681.\n",
            "[Step 9200] Train / Validation / Test accuracy: 0.750 / 0.750 / 0.681.\n",
            "[Step 9300] Train / Validation / Test accuracy: 0.672 / 0.844 / 0.681.\n",
            "[Step 9400] Train / Validation / Test accuracy: 0.703 / 0.734 / 0.681.\n",
            "[Step 9500] Train / Validation / Test accuracy: 0.719 / 0.609 / 0.682.\n",
            "[Step 9600] Train / Validation / Test accuracy: 0.797 / 0.719 / 0.682.\n",
            "[Step 9700] Train / Validation / Test accuracy: 0.641 / 0.750 / 0.682.\n",
            "[Step 9800] Train / Validation / Test accuracy: 0.750 / 0.625 / 0.681.\n",
            "[Step 9900] Train / Validation / Test accuracy: 0.703 / 0.703 / 0.681.\n",
            "[Step 10000] Train / Validation / Test accuracy: 0.781 / 0.719 / 0.682.\n"
          ]
        }
      ],
      "source": [
        "### A.\n",
        "\n",
        "# here i implement a slightly modeified training with freezed layers except the last one (we also could train the whole model)\n",
        "# got help from the haiku documentation https://dm-haiku.readthedocs.io/en/latest/notebooks/non_trainable.html\n",
        "\n",
        "\n",
        "# we need different approaches for untrained and trained parameters though for loss calculation we need to join them so we have to reimplement loss and forward\n",
        "# loss from scratch, because our update function directly calls loss calculator and we cannot decouple them here\n",
        "\n",
        "def loss_fn(trainable_params, non_trainable_params, batch):\n",
        "  # we need to combine trainable and non trainable before calling apply.\n",
        "  merged_params = hk.data_structures.merge(non_trainable_params, trainable_params)\n",
        "  # same as before\n",
        "  logits = smallnet.apply(merged_params, batch)   #None in between is not needed because we applied without rng key\n",
        "  images, labels = batch\n",
        "  labels = jax.nn.one_hot(labels, logits.shape[-1])\n",
        "  softmax_xent =  -jnp.sum(labels * jax.nn.log_softmax(logits)) / labels.shape[0]\n",
        "  l2_loss = sum([jnp.sum( jnp.vdot(w, w)) for i, w in enumerate(jax.tree_util.tree_leaves(trainable_params)) if i/2!=0])    \n",
        "  weighted_l2_loss =  0.5 * l2_loss                    \n",
        "  return softmax_xent + (1e-4 * weighted_l2_loss) \n",
        "\n",
        "\n",
        "def sgd_step(parameters, grads, *, lr):\n",
        "  return jax.tree_util.tree_map(lambda p, g: p - g * lr, parameters, grads)  \n",
        "\n",
        "@jax.jit\n",
        "def smallnet_update(\n",
        "    trainable_params: hk.Params,\n",
        "    non_trainable_params: hk.Params,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, optax.OptState]:\n",
        "\n",
        "  trainable_params_grads = jax.grad(loss_fn)(trainable_params, non_trainable_params, batch)\n",
        "  trainable_params = sgd_step(trainable_params, trainable_params_grads, lr=0.1)\n",
        "  return trainable_params\n",
        "\n",
        "@jax.jit\n",
        "def smallnet_ema_update(smallnet_params, smallnet_avg_params):\n",
        "  return optax.incremental_update(smallnet_params, smallnet_avg_params, step_size=0.001) \n",
        "\n",
        "@jax.jit\n",
        "def smallnet_accuracy(smallnet_params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  predictions = smallnet.apply(smallnet_params, batch)\n",
        "  _, lables = batch\n",
        "  right_guesses = jnp.argmax(predictions, axis=1)==lables \n",
        "  accuracy = jnp.count_nonzero(right_guesses) / len(lables)\n",
        "  return accuracy  \n",
        "\n",
        "\n",
        "for step in range(10001): \n",
        "  if step % 100 == 0:   \n",
        "    train_accuracy = smallnet_accuracy(smallnet_params, next(train))\n",
        "    val_accuracy = smallnet_accuracy(smallnet_params, next(validation))\n",
        "    test_accuracy = smallnet_accuracy(smallnet_avg_params, next(test))  \n",
        "   \n",
        "    train_accuracy, val_accuracy, test_accuracy = jax.device_get(\n",
        "        (train_accuracy, val_accuracy, test_accuracy))\n",
        "    print(f\"[Step {step}] Train / Validation / Test accuracy: \"\n",
        "          f\"{train_accuracy:.3f} / {val_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "\n",
        "  trainable_params = smallnet_update(trainable_params, non_trainable_params, next(train))\n",
        "  smallnet_params = hk.data_structures.merge(non_trainable_params, trainable_params)\n",
        "  smallnet_avg_params = smallnet_ema_update(smallnet_params, smallnet_avg_params)\n",
        "\n",
        "\n",
        "## one last time combine trainable and untrainable params\n",
        "smallnet_params = hk.data_structures.merge(non_trainable_params, trainable_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "W8Pbq71LkOrB",
        "outputId": "f8f52f47-974b-4bb3-c85e-0e53ec7350b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.68\n",
            "Duration:  0.000006761654.\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "### A.\n",
        "\n",
        "### from the training results we see, that there is almost no change in validation or test accuracy\n",
        "### still, let's check accuracy and inference time using same methods we used on svd based compression\n",
        "\n",
        "batch = next(test)\n",
        "\n",
        "def evaluate(smallnet_accuracy, smallnet_params, batch, n_samples):\n",
        "  duration_list = []\n",
        "  accuracy_list = []\n",
        "  for _ in range(10):\n",
        "    warm_up = smallnet_accuracy(smallnet_params, batch).block_until_ready()\n",
        "  for _ in range(n_samples):\n",
        "    start = time.time()  \n",
        "    acc = smallnet_accuracy(smallnet_params, batch).block_until_ready()   \n",
        "    ## !!! I am already deviding it on batch size here\n",
        "    duration = (time.time() - start) / batch[0].shape[0]    \n",
        "    duration_list.append(duration)\n",
        "    accuracy_list.append(acc)\n",
        "    # print(f\"---------------\\n\" \n",
        "    #       f\"Accuracy: {acc:.2f}\\n\" \n",
        "    #       f\"Duration:  {duration:.12f}.\")\n",
        "  return accuracy_list, duration_list\n",
        "\n",
        "\n",
        "accuracy_list, duration_list = evaluate(smallnet_accuracy=smallnet_accuracy, \n",
        "                                        smallnet_params=smallnet_params, \n",
        "                                        batch=batch, \n",
        "                                        n_samples=50)\n",
        "\n",
        "### let's compare what can be compared with the results of svd compresion method\n",
        "### comparison showes, that using layer removal we achieved as high accuracy as via highest rank svd compression,  \n",
        "### but much smaller latency than lowest rank svd compresion\n",
        "\n",
        "plt.title('inference time and accuracies')\n",
        "plt.xlabel(\"accuracy\")\n",
        "plt.ylabel(\"time\")\n",
        "\n",
        "plt.plot(np.mean(accuracy_list), np.mean(duration_list), 'ro')\n",
        "plt.plot(np.repeat(np.mean(accuracy_list), len(accuracies)), times, 'b--');\n",
        "plt.plot(accuracies, np.repeat(np.mean(duration_list), len(times)), 'c--');\n",
        "plt.plot(accuracies, times, 'mo-');\n",
        "plt.legend(['time & acc of rm_layer', 'acc of rm_layer', 'time of rm_layer', 'svd']);\n",
        "\n",
        "print(f\"Accuracy: {np.mean(accuracy_list):.2f}\\n\" f\"Duration:  {np.mean(duration_list):.12f}.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyglB6KtzYCb"
      },
      "source": [
        "B. here starts the pruning implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VySTO-wEI9G-"
      },
      "outputs": [],
      "source": [
        "### B. compression by prunning\n",
        "\n",
        "#### I learned how to do pruning from deepmind example and the refered paper in that example. I also followed the example code there. \n",
        "#### R E F E R E N C E:\n",
        "#### haiku example for pruning here: https://github.com/deepmind/dm-haiku/blob/main/examples/mnist_pruning.py\n",
        "\n",
        "#### if in section A. we removed whole layers, used conv layers parameters and updated weight only for last newly added layer\n",
        "#### in section B. we apply masks for whole network parameter making during forward and backward pass and update masked parameters\n",
        "#### we gradually decrease used parameters (make mask more sparse) according to algorithm for automated gradual pruning schedule\n",
        "#### performance at different levels of sparsity like we did it with different svd ranks.\n",
        "\n",
        "from typing import Callable, Sequence\n",
        "\n",
        "Predicate = Callable[[str, str, jnp.ndarray], bool]\n",
        "PredicateMap = Mapping[Predicate, jnp.ndarray]\n",
        "ModuleSparsity = Sequence[Tuple[Predicate, jnp.ndarray]]\n",
        "\n",
        "\n",
        "def topk_mask(value: jnp.ndarray, density_fraction: float) -> jnp.ndarray:\n",
        " \n",
        "  def topk_mask_internal(value):\n",
        "    assert value.ndim == 1\n",
        "    indices = jnp.argsort(value)\n",
        "    k = jnp.round(density_fraction * jnp.size(value)).astype(jnp.int32)\n",
        "    mask = jnp.greater_equal(np.arange(value.size), value.size - k)\n",
        "    #because jnp creates imutable array, even if we would not do this, still would get an array just with new memory loc\n",
        "    mask = jnp.zeros_like(mask).at[indices].set(mask) \n",
        "    #generally float,int in jax is max 32, so 64 anyway will get converted down to 32\n",
        "    return mask.astype(np.int32)    \n",
        "\n",
        "  # shuffle value so that identical values aren't always pruned\n",
        "  # with a bias to lower indices\n",
        "  orig_shape = value.shape\n",
        "  value = jnp.reshape(value, -1)\n",
        "  \n",
        "  ##### jax.random.shuffle is deprecated, one solution for the future use here: https://github.com/google/jax/discussions/8429\n",
        "  shuffled_indices = jax.random.shuffle(\n",
        "      jax.random.PRNGKey(42), jnp.arange(0, jnp.size(value), dtype=jnp.int32))\n",
        "\n",
        "\n",
        "  shuffled_mask = topk_mask_internal(value[shuffled_indices])\n",
        "  mask = jnp.zeros_like(shuffled_mask).at[shuffled_indices].set(shuffled_mask)\n",
        "  mask = jnp.reshape(mask, orig_shape)\n",
        "  return mask\n",
        "\n",
        "## from the same paper: https://ar5iv.labs.arxiv.org/html/1710.01878 or pdf: https://arxiv.org/pdf/1710.01878.pdf\n",
        "## important take away 1: paper introduces algorithm for automated gradual pruning schedule\n",
        "## important take away 2: pruning schedule must be close to lr schedule\n",
        "## as we take initial sparsity s_i=0 and final sparsity s_f=1 we have the following implementation:\n",
        "def zhugupta_func(progress: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"1. decides 'To Prune or Not To Prune' \n",
        "     2. applies automated pruning schedule \n",
        "        by determining pruning values for each new pruning step\n",
        "        :cite:`zhu2017prune`.\"\"\"\n",
        "  return 1. - (1. - progress)**3\n",
        "\n",
        "\n",
        "def _create_partitions(\n",
        "    module_sparsity: ModuleSparsity, params: hk.Params\n",
        ") -> Tuple[Sequence[hk.Params], Sequence[jnp.ndarray], hk.Params]:\n",
        "\n",
        "  list_of_trees = []\n",
        "  sparsity_list = []\n",
        "\n",
        "  tail = params\n",
        "  # Greedily match so that no parameter can be matched more than once\n",
        "  for predicate, sparsity in module_sparsity:\n",
        "    head, tail = hk.data_structures.partition(predicate, tail)\n",
        "    list_of_trees.append(head)\n",
        "    sparsity_list.append(sparsity)\n",
        "\n",
        "  return list_of_trees, sparsity_list, tail ## list to be pruned, list of pruning levels, list that will not be pruned\n",
        "\n",
        "\n",
        "def sparsity_ignore(m: str, n: str, v: jnp.ndarray) -> bool:\n",
        "  \"\"\" conditions when pruning should not happen\"\"\"\n",
        "  # n == 'b' when param is a bias\n",
        "  return n == \"b\" or v.ndim == 1 or \"batchnorm\" in m or \"batch_norm\" in m\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=2)\n",
        "def apply_mask(params: hk.Params, masks: Sequence[hk.Params],\n",
        "               module_sparsity: ModuleSparsity) -> hk.Params:   \n",
        "  # module_sparsity tuples predicates and corresponding sparsity levels \n",
        "  params_to_prune, _, params_no_prune = _create_partitions(\n",
        "      module_sparsity, params) \n",
        "  pruned_params = []\n",
        "  for value, mask in zip(params_to_prune, masks):\n",
        "    pruned_params.append(\n",
        "        jax.tree_util.tree_map(lambda x, y: x * y, value, mask))\n",
        "  params = hk.data_structures.merge(*pruned_params, params_no_prune)\n",
        "  return params\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=2) \n",
        "def update_mask(params: hk.Params, sparsity_fraction: float,\n",
        "                module_sparsity: ModuleSparsity) -> Sequence[hk.Params]:\n",
        "  \"\"\"Generate masks based on module_sparsity and sparsity_fraction.\"\"\"\n",
        "  params_to_prune, sparsities, _ = _create_partitions(module_sparsity, params)\n",
        "  masks = []\n",
        "\n",
        "  def map_fn(x: jnp.ndarray, sparsity: float) -> jnp.ndarray:\n",
        "    return topk_mask(jnp.abs(x), 1. - sparsity * sparsity_fraction)\n",
        "\n",
        "  for tree, sparsity in zip(params_to_prune, sparsities):\n",
        "    #we imported partial from functools: functools.partial\n",
        "    map_fn_sparsity = partial(map_fn, sparsity=sparsity) \n",
        "    mask = jax.tree_util.tree_map(map_fn_sparsity, tree)\n",
        "    masks.append(mask)\n",
        "  return masks\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def get_sparsity(params: hk.Params):\n",
        "  \"\"\"Calculate the number of all params, the total sparsity and tensor-wise sparsity of params.\"\"\"\n",
        "  total_params = sum(jnp.size(x) for x in jax.tree_util.tree_leaves(params))\n",
        "  total_nnz = sum(jnp.sum(x != 0.) for x in jax.tree_util.tree_leaves(params))\n",
        "  leaf_sparsity = jax.tree_util.tree_map(\n",
        "      lambda x: jnp.sum(x == 0) / jnp.size(x), params)\n",
        "  return total_params, total_nnz, leaf_sparsity\n",
        "\n",
        "\n",
        "# Define layerwise sparsities\n",
        "def module_matching(s):\n",
        "\n",
        "  def match_func(m, n, k):\n",
        "    return m.endswith(s) and not sparsity_ignore(m, n, k)\n",
        "\n",
        "  return match_func\n",
        "\n",
        "\n",
        "## initial sparsity is mostly no sparsity (0) which corresponds to 1 here. \n",
        "## see update_mask function: e.g. (1. - 0.98 * sparsity_fraction) for linear_1\n",
        "## so, the smaller values here mean more resistance to pruning\n",
        "module_sparsity = ((module_matching(\"conv2_d\"), 0.98),\n",
        "                   (module_matching(\"conv2_d_1\"), 0.98),\n",
        "                   (module_matching(\"linear\"), 0.98),   \n",
        "                   (module_matching(\"linear_1\"), 0.98),\n",
        "                   (module_matching(\"linear_2\"), 0.98),\n",
        "                   (module_matching(\"linear_3\"), 0.98),\n",
        "                   (module_matching(\"linear_4\"), 0.98))\n",
        "\n",
        "### of course module_sparsity should correspond to net_fn for quick lookup use this, or scroll up to net_fn\n",
        "# def parameter_shapes(params):\n",
        "#   return jax.tree_util.tree_map(lambda p: p.shape, params)\n",
        "#parameter_shapes(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msIJ2aV-FUrA",
        "outputId": "274182e3-d402-44ee-929d-90cddb8facae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:399: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Step 0] Train / Test accuracy: 0.781 / 0.669.\n",
            "Non-zero params / Total: 30416239 / 30462226; Total Sparsity: 0.002\n",
            "[Step 1000] Train / Test accuracy: 0.859 / 0.698.\n",
            "Non-zero params / Total: 30460849 / 30462226; Total Sparsity: 0.000\n",
            "[Step 2000] Train / Test accuracy: 0.891 / 0.700.\n",
            "Non-zero params / Total: 22374212 / 30462226; Total Sparsity: 0.266\n",
            "[Step 3000] Train / Test accuracy: 0.875 / 0.694.\n",
            "Non-zero params / Total: 14509572 / 30462226; Total Sparsity: 0.524\n",
            "[Step 4000] Train / Test accuracy: 0.828 / 0.646.\n",
            "Non-zero params / Total: 8813360 / 30462226; Total Sparsity: 0.711\n",
            "[Step 5000] Train / Test accuracy: 0.828 / 0.605.\n",
            "Non-zero params / Total: 4935833 / 30462226; Total Sparsity: 0.838\n",
            "[Step 6000] Train / Test accuracy: 0.656 / 0.525.\n",
            "Non-zero params / Total: 2527243 / 30462226; Total Sparsity: 0.917\n",
            "[Step 7000] Train / Test accuracy: 0.609 / 0.514.\n",
            "Non-zero params / Total: 1237845 / 30462226; Total Sparsity: 0.959\n",
            "[Step 8000] Train / Test accuracy: 0.547 / 0.471.\n",
            "Non-zero params / Total: 717886 / 30462226; Total Sparsity: 0.976\n",
            "[Step 9000] Train / Test accuracy: 0.703 / 0.609.\n",
            "Non-zero params / Total: 617625 / 30462226; Total Sparsity: 0.980\n",
            "[Step 10000] Train / Test accuracy: 0.766 / 0.631.\n",
            "Non-zero params / Total: 617159 / 30462226; Total Sparsity: 0.980\n",
            "{'conv2_d': {'b': array(0., dtype=float32), 'w': array(0.98, dtype=float32)}, 'conv2_d_1': {'b': array(0., dtype=float32), 'w': array(0.98, dtype=float32)}, 'linear': {'b': array(0., dtype=float32), 'w': array(0.98, dtype=float32)}, 'linear_1': {'b': array(0., dtype=float32), 'w': array(0.98, dtype=float32)}, 'linear_2': {'b': array(0., dtype=float32), 'w': array(0.98, dtype=float32)}, 'linear_3': {'b': array(0., dtype=float32), 'w': array(0.98, dtype=float32)}, 'linear_4': {'b': array(0., dtype=float32), 'w': array(0.97999996, dtype=float32)}}\n"
          ]
        }
      ],
      "source": [
        "### B.\n",
        "\n",
        "#### jax.numpy.save('./params.npy', params) #jax.numpy.savez('./params.npz', params) \n",
        "#### par = jax.numpy.load('./params.npy', allow_pickle=True) #generally avoid allow_pickle=True it has vulnerabilities\n",
        "#### check documentation on that https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.load.html\n",
        "\n",
        "### these two lines are here to prevent jax from using already jited version 'lasely' that breakes the code\n",
        "net = hk.without_apply_rng(hk.transform(net_fn)) \n",
        "model_params = prune_avg_params = net.init(jax.random.PRNGKey(42), next(train))\n",
        "\n",
        "params_pruned = model_params = prune_avg_params = deepcopy(params)\n",
        "\n",
        "masks = update_mask(model_params, 0., module_sparsity)\n",
        "prune_opt = optax.adam(1e-3)\n",
        "prune_opt_state = prune_opt.init(model_params)\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def get_updates(\n",
        "    params: hk.Params,\n",
        "    opt_state: optax.OptState,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, optax.OptState]:\n",
        "  \"\"\"Learning rule (stochastic gradient descent).\"\"\"\n",
        "  grads = jax.grad(compute_loss)(params, batch)          # I reuse loss and accuracy functions from first coding task \n",
        "  updates, opt_state = opt.update(grads, opt_state)\n",
        "  ## we do not apply updates here\n",
        "  ## so, we ommit new_params = optax.apply_updates(someparams, someupdates) as we do this in a training loop separately\n",
        "  return updates, opt_state\n",
        "\n",
        "pruning_states = []\n",
        "prune_lvl = 0\n",
        "\n",
        "for step in range(10001):\n",
        "    # evaluate classification accuracy on train & test sets by using current masked parameters (at given stage of prunning)   \n",
        "    if step % 1000 == 0:\n",
        "      \n",
        "      prune_avg_params = apply_mask(prune_avg_params, masks, module_sparsity)\n",
        "      \n",
        "      train_accuracy = compute_accuracy(prune_avg_params, next(train))\n",
        "      test_accuracy = compute_accuracy(prune_avg_params, next(test))\n",
        "\n",
        "      total_params, total_nnz, per_layer_sparsities = get_sparsity(prune_avg_params)\n",
        "      total_sparsity = 1. - total_nnz / total_params\n",
        "      \n",
        "      train_accuracy, test_accuracy, total_nnz, per_layer_sparsities = (\n",
        "          jax.device_get(\n",
        "              (train_accuracy, test_accuracy, total_nnz, per_layer_sparsities)))\n",
        "      print(f\"[Step {step}] Train / Test accuracy: \"\n",
        "            f\"{train_accuracy:.3f} / {test_accuracy:.3f}.\")\n",
        "      print(f\"Non-zero params / Total: {total_nnz} / {total_params}; \"\n",
        "            f\"Total Sparsity: {total_sparsity:.3f}\")\n",
        "      \n",
        "      \n",
        "    if prune_lvl<(total_sparsity*10) or (total_sparsity==0.98): \n",
        "      ## 0.98 because pruning was set not to reach 1 but around 0.98\n",
        "      pruning_states.append((model_params, total_sparsity*100))\n",
        "      prune_lvl+=1\n",
        "\n",
        "    # Do SGD on a batch of training examples.\n",
        "    params_pruned = apply_mask(model_params, masks, module_sparsity)\n",
        "    updates, prune_opt_state = get_updates(params_pruned, prune_opt_state, next(train))\n",
        "    # before applying updates on parameters we use mask to expose only those parameters we want to apply updates on\n",
        "    updates = apply_mask(updates, masks, module_sparsity)\n",
        "    model_params = optax.apply_updates(model_params, updates)\n",
        "    \n",
        "    # start pruning at iteration t_zero = 1000 and end at iteration 8000\n",
        "    # as the params are already trained we also could start pruning earlier at t_zero = 0\n",
        "    # this part is only filling the main automated gradual pruning algorithm that is implemented in zhugupta_func above\n",
        "    progress = min(max((step - 1000.) / 8000., 0.), 1.)  \n",
        "    #so, 0=<progress=<1, n span of pruning steps: (8000-1000) and pruning frequency delta_t: (1/200) \n",
        "    if step % 200 == 0:\n",
        "      sparsity_fraction = zhugupta_func(progress)\n",
        "      masks = update_mask(model_params, sparsity_fraction, module_sparsity)\n",
        "    \n",
        "    prune_avg_params = ema_update(model_params, prune_avg_params)\n",
        "print(per_layer_sparsities)\n",
        "\n",
        "### let's calculate accuracies and inference times as we did for svd\n",
        "batch = next(test)\n",
        "prune_accuracies, prune_durations, prune_levels = [], [], []\n",
        "\n",
        "for sparse_model_params, sparsity_level in pruning_states:\n",
        "  prune_accuracy_list, prune_duration_list = evaluate(compute_accuracy, sparse_model_params, batch, 50)\n",
        "  ### evaluate function internally devides times on batch size\n",
        "  \n",
        "  prune_accuracies.append(np.mean(prune_accuracy_list))\n",
        "  prune_durations.append(np.mean(prune_duration_list))\n",
        "  prune_levels.append(sparsity_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "XxtKVsS0Wo0n",
        "outputId": "79779cb8-8259-470c-e33e-df2d22d99310"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1008x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "#### B.\n",
        "\n",
        "### let's compare svd, layer removal, and pruning results\n",
        "\n",
        "figure, axis = plt.subplots(1, 2)\n",
        "figure.set_size_inches(14, 4)\n",
        "\n",
        "axis[0].set_title(\"compression level and accuracy\")\n",
        "axis[0].set_xlabel(\"accuracy\")\n",
        "axis[0].set_ylabel(\"compression levels\")\n",
        "axis[0].plot(accuracies, rank_percentage, 'm-')\n",
        "axis[0].plot(prune_accuracies[::-1], prune_levels, 'k-')\n",
        "axis[0].legend(['svd', 'pruned'])\n",
        "\n",
        "axis[1].set_title('inference time and accuracy')\n",
        "axis[1].set_xlabel(\"accuracy\")\n",
        "axis[1].set_ylabel(\"time\")\n",
        "axis[1].plot(np.mean(accuracy_list), np.mean(duration_list), 'ro')\n",
        "axis[1].plot(np.repeat(np.mean(accuracy_list), len(accuracies)), times, 'b--')\n",
        "axis[1].plot(accuracies, np.repeat(np.mean(duration_list), len(times)), 'c--')\n",
        "axis[1].plot(accuracies, times, 'mo-')\n",
        "axis[1].plot(prune_accuracies[::-1], prune_durations[::-1], 'ko-')\n",
        "axis[1].legend(['time & acc of rm_layer', 'acc of rm_layer', 'time of rm_layer', 'svd', 'pruned'])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ogol7gXNATl"
      },
      "source": [
        "What results may indicate for our toy example:\n",
        "\n",
        "Pruning\n",
        "1. **inference time:** pruning results are not much sensitive to inference time, so pruning level (here also refered as compression level / pruning percentage) doesn't give much different inference times for different compression levels (see right figure).\n",
        "2. **accuracy:** both figures show, that pruning appeares still quite robust to accuracy drop especially when we reach very high levels of sparcity, which may be the gain for \"compression\".\n",
        "\n",
        "SVD\n",
        "\n",
        "schoves positive dependece in both cases compression level vs accuracy, and inference time vs accuracy (see both figures). One nice characteristic of svd here is, that to some point reasonably high compression does not affect the accuracy too mutch (left figure) and at the same time each level of those compressions greatly decreases inference time (right figure). \n",
        "\n",
        "Layer removal\n",
        "\n",
        "almost fully retained accuracy of original model (function) and at the same time greately decreased inference time. In both metrics it outperformed best variants fo pruning and svd.\n",
        "\n",
        "Lastly we can also chain all these model compressions in different ways."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
