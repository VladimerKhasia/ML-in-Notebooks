{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w9c0Bi3elbGb"
      },
      "source": [
        "References:\n",
        "\n",
        "\n",
        "   https://github.com/https-deeplearning-ai\n",
        "\n",
        "   https://github.com/https-deeplearning-ai/tensorflow-2-public\n",
        "\n",
        "   https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/serving/rest_simple.ipynb#scrollTo=ygwa9AgRloYy\n",
        "   \n",
        "   https://www.tensorflow.org/tfx/serving/setup\n",
        "   \n",
        "   https://www.tensorflow.org/tfx/tutorials/serving/rest_simple \n",
        "\n",
        "\n",
        "  https://github.com/tensorflow/federated/tree/v0.48.0/docs/tutorials\n",
        "\n",
        "  https://colab.research.google.com/github/tensorflow/federated/blob/master/docs/tutorials/custom_federated_algorithms_1.ipynb\n",
        "\n",
        "  https://github.com/tensorflow/federated/blob/v0.48.0/docs/tutorials/custom_federated_algorithms_2.ipynb\n",
        "\n",
        "  https://ai.googleblog.com/2017/04/federated-learning-collaborative.html\n",
        "\n",
        "  https://eprint.iacr.org/2017/281.pdf\n",
        "\n",
        "  https://research.google/pubs/pub47976/\n",
        "\n",
        "  https://www.tensorflow.org/federated\n",
        "\n",
        "  https://www.youtube.com/watch?v=89BGjQYA0uE\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIMkYxzRu1QV"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests\n",
        "##!pip install -Uq grpcio==1.26.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRCo1PynpjwN",
        "outputId": "aebc4886-c7ef-4ec0-cac5-9bdcc719c9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaaZ1KsTpZPF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "from datetime import datetime\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import sklearn.metrics\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import json\n",
        "import tempfile\n",
        "import requests\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "print(\"\\u2022 Using TensorFlow Version:\", tf.__version__)\n",
        "print(\"\\u2022 Using TensorFlow Hub Version: \", hub.__version__)\n",
        "print('\\u2022 GPU Device Found.' if tf.test.is_gpu_available() else '\\u2022 GPU Device Not Found. Running on CPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "# Simple detection model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## Select the Hub/TF2 Module to Use\n",
        "\n",
        "Hub modules for TF 1.x won't work here, please use one of the selections provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlsEcKVeuCnf",
        "outputId": "f41cd843-3dfb-4e53-bda5-0c575a27e84f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4 with input size (224, 224) and output dimension 1280\n"
          ]
        }
      ],
      "source": [
        "module_selection = (\"mobilenet_v2\", 224, 1280) #@param [\"(\\\"mobilenet_v2\\\", 224, 1280)\", \"(\\\"inception_v3\\\", 299, 2048)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels, FV_SIZE = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/tf2-preview/{}/feature_vector/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Using {} with input size {} and output dimension {}\".format(MODULE_HANDLE, IMAGE_SIZE, FV_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYUsgwCBv87A"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nqVX3KYwGPh"
      },
      "source": [
        "Use [TensorFlow Datasets](http://tensorflow.org/datasets) to load the `rock_paper_scissors` dataset.\n",
        "\n",
        "This `tfds` package is the easiest way to load pre-defined data. If you have your own data, and are interested in importing using it with TensorFlow see [loading image data](../load_data/images.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkF4Boe5wN7N"
      },
      "source": [
        "The `tfds.load` method downloads and caches the data, and returns a `tf.data.Dataset` object. These objects provide powerful, efficient methods for manipulating data and piping it into your model.\n",
        "\n",
        "Dividing the `train` split of this dataset into (train, validation, test) with 80%, 10%, 10% of the data respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ9xK9F2wGD8",
        "outputId": "11d5b2ec-b302-4dea-b23e-f30737d7c8af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset 219.53 MiB (download: 219.53 MiB, generated: Unknown size, total: 219.53 MiB) to /root/tensorflow_datasets/rock_paper_scissors/3.0.0...\n",
            "Dataset rock_paper_scissors downloaded and prepared to /root/tensorflow_datasets/rock_paper_scissors/3.0.0. Subsequent calls will reuse this data.\n"
          ]
        }
      ],
      "source": [
        "### this data goes in the root/  directory\n",
        "\n",
        "(train_examples, validation_examples, test_examples), info = tfds.load('rock_paper_scissors',\n",
        "                                                                       with_info=True, \n",
        "                                                                       as_supervised=True, \n",
        "                                                                       split=['train[:80%]',\n",
        "                                                                              'train[80%:90%]',\n",
        "                                                                              'train[90%:]'])\n",
        "\n",
        "num_examples = info.splits['train'].num_examples\n",
        "num_classes = info.features['label'].num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXQYXNWwf19"
      },
      "source": [
        "### Format the Data\n",
        "\n",
        "Use the `tf.image` module to format the images for the task.\n",
        "\n",
        "Resize the images to a fixes input size, and rescale the input channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7UyXblSwkUS"
      },
      "outputs": [],
      "source": [
        "def format_image(image, label):\n",
        "    image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
        "    return  image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nrDR8CnwrVk"
      },
      "source": [
        "Now shuffle and batch the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAEUG7vawxLm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHEC9mbswxvM"
      },
      "outputs": [],
      "source": [
        "train_batches = train_examples.shuffle(num_examples // 4).batch(BATCH_SIZE).map(format_image).prefetch(1)\n",
        "validation_batches = validation_examples.batch(BATCH_SIZE).map(format_image).prefetch(1)\n",
        "test_batches = test_examples.batch(1).map(format_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghQhZjgEw1cK"
      },
      "source": [
        "Inspect a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz0xsMCjwx54",
        "outputId": "2b16260d-1c24-49ce-fcbe-b287cce01ccd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([32, 224, 224, 3])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for image_batch, label_batch in train_batches.take(1):\n",
        "    pass\n",
        "\n",
        "image_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS_gVStowW3G"
      },
      "source": [
        "## Defining the Model\n",
        "\n",
        "All it takes is to put a linear classifier on top of the `feature_extractor_layer` with the Hub module.\n",
        "\n",
        "For speed, we start out with a non-trainable `feature_extractor_layer`, but you can also enable fine-tuning for greater accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaJW3XrPyFiF"
      },
      "outputs": [],
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS3b2GZOBCkP"
      },
      "outputs": [],
      "source": [
        "feature_extractor = hub.KerasLayer(MODULE_HANDLE,\n",
        "                                   input_shape=IMAGE_SIZE + (3,), \n",
        "                                   output_shape=[FV_SIZE],\n",
        "                                   trainable=do_fine_tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T_Km85XBCkP",
        "outputId": "2382ab70-fc3c-44e4-ec9b-fe869bd308cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building model with https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1280)              2257984   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,261,827\n",
            "Trainable params: 3,843\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"Building model with\", MODULE_HANDLE)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "        feature_extractor,\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhTJQcjzBCkP"
      },
      "outputs": [],
      "source": [
        "#@title (Optional) Unfreeze some layers\n",
        "NUM_LAYERS = 10 #@param {type:\"slider\", min:1, max:50, step:1}\n",
        "      \n",
        "if do_fine_tuning:\n",
        "    feature_extractor.trainable = True\n",
        "    \n",
        "    for layer in model.layers[-NUM_LAYERS:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "else:\n",
        "    feature_extractor.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VLwff0Xqnqa"
      },
      "outputs": [],
      "source": [
        "if do_fine_tuning:\n",
        "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.002, momentum=0.9),\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "else:\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBEFjNs3oOEl"
      },
      "source": [
        "## Tensorboard tools and functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mef3XAZcoP-J"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i648dHsoonok"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, class_names):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "    \n",
        "    Args:\n",
        "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "       class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "    \n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "    \n",
        "    # Normalize the confusion matrix.\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "    \n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.\n",
        "    \n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    return figure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h31uyWLvo_eF"
      },
      "outputs": [],
      "source": [
        "## we use https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
        "\n",
        "# Clear logs prior to logging data.\n",
        "!rm -rf logs/image\n",
        "\n",
        "# Create log directory\n",
        "logdir = \"logs/image/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# EXERCISE: Define a TensorBoard callback. Use the log_dir parameter\n",
        "# to specify the path to the directory where you want to save the\n",
        "# log files to be parsed by TensorBoard.\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "\n",
        "file_writer_cm = tf.summary.create_file_writer(logdir + '/cm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSA5s1hNpSH8"
      },
      "outputs": [],
      "source": [
        "def plot_to_image(figure):\n",
        "  \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n",
        "  returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n",
        "  # Save the plot to a PNG in memory.\n",
        "  buf = io.BytesIO()\n",
        "  plt.savefig(buf, format='png')\n",
        "  # Closing the figure prevents it from being displayed directly inside\n",
        "  # the notebook.\n",
        "  plt.close(figure)\n",
        "  buf.seek(0)\n",
        "  # Convert PNG buffer to TF image\n",
        "  image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "  # Add the batch dimension\n",
        "  image = tf.expand_dims(image, 0)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLxsJcJRpYT0"
      },
      "outputs": [],
      "source": [
        "class_names = info.name.split('_')\n",
        "\n",
        "def log_confusion_matrix(epoch, logs):\n",
        "\n",
        "    # From batch of images and labels to just images\n",
        "    random_batch = random.choice(range(len(validation_batches)))\n",
        "\n",
        "    for image_batch, label_batch in validation_batches.take(random_batch):\n",
        "        pass\n",
        "    \n",
        "    # Use the model to predict the values from the test_images.\n",
        "    val_pred_raw = model.predict(image_batch)\n",
        "    \n",
        "    val_pred = np.argmax(val_pred_raw, axis=1)\n",
        "    \n",
        "    # Calculate the confusion matrix using sklearn.metrics\n",
        "    cm = sklearn.metrics.confusion_matrix(label_batch, val_pred)\n",
        "    figure = plot_confusion_matrix(cm, class_names=class_names)\n",
        "    cm_image = plot_to_image(figure)\n",
        "    \n",
        "    # Log the confusion matrix as an image summary.\n",
        "    with file_writer_cm.as_default():\n",
        "        tf.summary.image(\"Confusion Matrix\", cm_image, step=epoch)\n",
        "\n",
        "# Define the per-epoch callback.\n",
        "cm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBHRugUfp22b"
      },
      "outputs": [],
      "source": [
        "# # Start TensorBoard.\n",
        "# %tensorboard --logdir logs/image ## or if does not start use ## %tensorboard --logdir logs/image --host localhost \n",
        "\n",
        "# # Train the classifier.\n",
        "# model.fit(train_images,\n",
        "#           train_labels,\n",
        "#           epochs=5,\n",
        "#           verbose=0, # Suppress chatty output\n",
        "#           callbacks=[cm_callback, tensorboard_callback],  ### [tensorboard_callback, cm_callback]\n",
        "#           validation_data=(test_images, test_labels),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_YKX2Qnfg6x"
      },
      "outputs": [],
      "source": [
        "## Start TensorBoard. At the begginning it might show you only Graphs section. In that case make sure to inspect time series and other sections from \n",
        "## the UP Right rolldown handle\n",
        "%tensorboard --logdir logs/image ## or if it will not start then use ## %tensorboard --logdir logs/image --host localhost\n",
        "\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "hist = model.fit(train_batches, \n",
        "                 epochs=EPOCHS,\n",
        "                 verbose=0, # Suppress chatty output\n",
        "                 callbacks=[cm_callback, tensorboard_callback],  ### [tensorboard_callback, cm_callback]  ## only added line for tensorboard                \n",
        "                 validation_data=validation_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6zom1Imibjt"
      },
      "source": [
        "# Tensorflow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWmmKlijmMv4"
      },
      "source": [
        "## Save the Model\n",
        "\n",
        "To load the trained model into TensorFlow Serving we first need to save it in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format.  This will create a protobuf file in a well-defined directory hierarchy, and will include a version number.  [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/serving_config) allows us to select which version of a model, or \"servable\" we want to use when we make inference requests.  Each version will be exported to a different sub-directory under the given path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onE0MUVPmrzA"
      },
      "outputs": [],
      "source": [
        "# MODEL_DIR = tempfile.gettempdir()\n",
        "\n",
        "# version = 1\n",
        "\n",
        "# export_path = os.path.join(MODEL_DIR, str(version))\n",
        "\n",
        "# if os.path.isdir(export_path):\n",
        "#     print('\\nAlready saved a model, cleaning up\\n')\n",
        "#     !rm -r {export_path}\n",
        "\n",
        "# model.save(export_path, save_format=\"tf\")\n",
        "\n",
        "# print('\\nexport_path = {}'.format(export_path))  ## find model in /tmp/1 do not search it in /content/ file in colab\n",
        "# !ls -l {export_path}\n",
        "\n",
        "### Fetch the Keras session and save the model\n",
        "### The signature definition is defined by the input and output tensors,\n",
        "### and stored with the default serving key\n",
        "\n",
        "\n",
        "MODEL_DIR = tempfile.gettempdir()\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "\n",
        "### deprecated:\n",
        "# tf.compat.v1.saved_model.simple_save(\n",
        "#     keras.backend.get_session(), \n",
        "#     export_path, \n",
        "#     inputs = {model.input}, \n",
        "#     outputs = {t.name:t for t in model.outputs}, \n",
        "#     legacy_init_op=None\n",
        "# )\n",
        "\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgJNMvpGoF7u"
      },
      "source": [
        "## Examine Your Saved Model\n",
        "\n",
        "We'll use the command line utility `saved_model_cli` to look at the `MetaGraphDefs` and `SignatureDefs` in our SavedModel. The signature definition is defined by the input and output tensors, and stored with the default serving key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO5jXpgBoHDG"
      },
      "outputs": [],
      "source": [
        "### NOTE: when it comes to shapes, input, output - ignore the minus one\n",
        "\n",
        "!saved_model_cli show --dir {export_path} --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4cEIi0Wl27N"
      },
      "source": [
        "## Add TensorFlow Serving Distribution URI as a Package Source\n",
        "\n",
        "**Warning: If you are running this NOT on a Google Colab,** following cells\n",
        "will install packages on the system with root access. If you want to run it in\n",
        "a local Jupyter notebook, please proceed with caution.\n",
        "\n",
        "### Add TensorFlow Serving distribution URI as a package source:\n",
        "\n",
        "We're preparing to install TensorFlow Serving using [Aptitude](https://wiki.debian.org/Aptitude) since this Colab runs in a Debian environment.  We'll add the `tensorflow-model-server` package to the list of packages that Aptitude knows about.  Note that we're running as root.\n",
        "\n",
        "Note: This example is running TensorFlow Serving natively, but [you can also run it in a Docker container](https://www.tensorflow.org/tfx/serving/docker), which is one of the easiest ways to get started using TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4hLCdduOoQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# We need sudo prefix if not on a Google Colab.\n",
        "if 'google.colab' not in sys.modules:\n",
        "  SUDO_IF_NEEDED = 'sudo'\n",
        "else:\n",
        "  SUDO_IF_NEEDED = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKh-YARbig27"
      },
      "outputs": [],
      "source": [
        "# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n",
        "# You would instead do:\n",
        "# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n",
        "curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add -\n",
        "!{SUDO_IF_NEEDED} apt update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QeMkGJHmAkQ"
      },
      "source": [
        "## Install TensorFlow Serving\n",
        "\n",
        "Now that the Aptitude packages have been updated, we can use the `apt-get` command to install the TensorFlow model server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwyF7gPtmGkA"
      },
      "outputs": [],
      "source": [
        "### !apt-get install tensorflow-model-server\n",
        "\n",
        "# TODO: Use the latest model server version when colab supports it.\n",
        "#!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server\n",
        "# We need to install Tensorflow Model server 2.8 instead of latest version\n",
        "# Tensorflow Serving >2.9.0 required `GLIBC_2.29` and `GLIBCXX_3.4.26`. Currently colab environment doesn't support latest version of`GLIBC`,so workaround is to use specific version of Tensorflow Serving `2.8.0` to mitigate issue.\n",
        "\n",
        "# !echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | tee /etc/apt/sources.list.d/tensorflow-serving.list && curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | apt-key add -\n",
        "# !apt update\n",
        "\n",
        "!wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb'\n",
        "!dpkg -i tensorflow-model-server_2.8.0_all.deb\n",
        "!pip3 install tensorflow-serving-api==2.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPMjLY3_ovYd"
      },
      "source": [
        "## Run the TensorFlow Model Server\n",
        "\n",
        "We will now launch the TensorFlow model server with a bash script. We will use the argument `--bg` to run the script in the background.\n",
        "\n",
        "Our script will start running TensorFlow Serving and will load our model. Here are the parameters we will use:\n",
        "\n",
        "* `rest_api_port`: The port that you'll use for requests.\n",
        "\n",
        "\n",
        "* `model_name`: You'll use this in the URL of your requests.  It can be anything.\n",
        "\n",
        "\n",
        "* `model_base_path`: This is the path to the directory where you've saved your model.\n",
        "\n",
        "Also, because the variable that points to the directory containing the model is in Python, we need a way to tell the bash script where to find the model. To do this, we will write the value of the Python variable to an environment variable using the `os.environ` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx8wnNoKpLjF"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MODEL_DIR\"] = MODEL_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRG-HB4dozAF"
      },
      "outputs": [],
      "source": [
        "%%bash --bg \n",
        "nohup tensorflow_model_server \\\n",
        "  --rest_api_port=8501 \\\n",
        "  --model_name=detect \\\n",
        "  --model_base_path=\"${MODEL_DIR}\" >server.log 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjGp2IJZp24U"
      },
      "outputs": [],
      "source": [
        "## take look at the server log\n",
        "## NOTE: if connection will not work for the repeated/second time just delete tensorflow-model-server from /content/\n",
        "   ## and reinstall the serving distribution URI and tfserving again\n",
        "!tail server.log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrDslj3zwMUG"
      },
      "source": [
        "## Make a request to your model in TensorFlow Serving\n",
        "\n",
        "First, let's take a look at a random example from our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcgZaDiB4Qxm"
      },
      "outputs": [],
      "source": [
        "def sample_tester(which_example=1, show=False, test_examples=test_examples, format_image=format_image):\n",
        "\n",
        "  for data in test_examples.take(which_example):\n",
        "\n",
        "      ### model.input.shape ### model.output.shape\n",
        "      image, label = data\n",
        "      image, label = format_image(image, label)\n",
        "\n",
        "  if show:  \n",
        "    print(\"Label: {}\".format(label))\n",
        "    plt.imshow(image.numpy().squeeze(), cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "\n",
        "  label = label.numpy()\n",
        "  image = np.expand_dims(image.numpy().squeeze(), axis=0)\n",
        "\n",
        "  return image, label   \n",
        "\n",
        "\n",
        "import random\n",
        "random_example = 1 if 0 else random.choice(range(len(test_examples)-1))\n",
        "\n",
        "image, label = sample_tester(random_example, show=True)  \n",
        "print(f'Input Shape: {image.shape}, Label: {label}, Example N: {random_example}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrPlambb6NMy"
      },
      "source": [
        "Now create the JSON object for a batch of three inference requests, and see how well our model recognizes things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGT52DVT6Ldy",
        "outputId": "387ddebf-b3a1-4207-c7f4-2c8d073fc80e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data: {\"signature_name\": \"serving_default\", \"instances\": ... 6827393, 0.9771358966827393, 0.9771358966827393]]]]}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": sample_tester(random_example)[0].tolist()})\n",
        "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FDF2f6FEFBA"
      },
      "source": [
        "#### Make REST requests\n",
        "\n",
        "Newest version of the servable\n",
        "\n",
        "We'll send a predict request as a POST to our server's REST endpoint, and pass it three examples.  We'll ask our server to give us the latest version of our servable by not specifying a particular version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdAARhMEEVFA",
        "outputId": "2e80978a-6b3b-444c-9807-dc444b06b0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model thought this was a rock (class 0), and it was actually a rock (class 0)\n"
          ]
        }
      ],
      "source": [
        "headers = {\"content-type\": \"application/json\"}\n",
        "                                                            ## detect is what we called the model and \n",
        "                                                            ## :predict tag is for prediction\n",
        "json_response = requests.post('http://localhost:8501/v1/models/detect:predict', data=data, headers=headers)\n",
        "predictions = json.loads(json_response.text)[\"predictions\"]\n",
        "\n",
        "class_names = info.name.split('_')\n",
        "\n",
        "print('The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[label], label))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ashOgAmYG7_F",
        "outputId": "7d5fcb60-8d13-4337-82b3-cd8c67fff677"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\\n    \"predictions\": [[0.991667449, 0.00468994165, 0.00364255207]\\n    ]\\n}'"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "json_response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOglj2XvYUKK"
      },
      "source": [
        "#### A particular version of the servable\n",
        "\n",
        "Now let's specify a particular version of our servable.  Since we only have one, let's select version 1.  We'll also look at all three results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzcZIHJrYTZj",
        "outputId": "398f1e1d-7caa-4c97-9c09-65805aca80b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model thought this was a rock (class 0), and it was actually a rock (class 0)\n"
          ]
        }
      ],
      "source": [
        "# docs_infra: no_execute\n",
        "headers = {\"content-type\": \"application/json\"}\n",
        "json_response = requests.post('http://localhost:8501/v1/models/detect/versions/1:predict', data=data, headers=headers)\n",
        "predictions = json.loads(json_response.text)['predictions']\n",
        "\n",
        "\n",
        "print('The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
        "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[label], label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGWcPfqZLVlL"
      },
      "source": [
        "# Prepare Tensorflow Js files inc. json to use the model in browser and web applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofk5zg2RLvNJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdz0p3dBMUXB"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "saved_model_path = \"./{}.h5\".format(int(time.time()))\n",
        "model.save(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhCkOLrtMYhx",
        "outputId": "03aa2b33-0c4e-4456-89cf-86b172d2b2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-02-08 18:18:15.840155: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-08 18:18:15.840424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-08 18:18:15.840460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "## all converted files should be coppied to corresponding directory of web application, \n",
        "## In same directory where the model is hosted, html or whatever file it is.\n",
        "\n",
        "!tensorflowjs_converter --input_format=keras {saved_model_path} ./web/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNcPVBk4JEc8"
      },
      "source": [
        "# Prepare TF Lite flatbuffer for deploiment mobile and test it's performance in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_psFoTeLpHU"
      },
      "source": [
        "## Export the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaSb5nVzHcVv"
      },
      "outputs": [],
      "source": [
        "RPS_SAVED_MODEL = \"rps_saved_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZqRAg1uz1Nu"
      },
      "source": [
        "Export the SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJMue5YgnwtN"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(model, RPS_SAVED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOQF4cOan0SY",
        "outputId": "2fe8fb71-9607-47c0-e82f-2501d63fbfec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['keras_layer_input'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 224, 224, 3)\n",
            "      name: serving_default_keras_layer_input:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['dense'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 3)\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-08 18:19:03.945605: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-08 18:19:03.945846: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2023-02-08 18:19:03.945890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "%%bash -s $RPS_SAVED_MODEL\n",
        "saved_model_cli show --dir $1 --tag_set serve --signature_def serving_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY7QGBgBytwX"
      },
      "outputs": [],
      "source": [
        "loaded = tf.saved_model.load(RPS_SAVED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIhPyMISz952",
        "outputId": "712da729-fcca-4f3b-969f-9268c9fb96b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['serving_default']\n",
            "((), {'keras_layer_input': TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_layer_input')})\n",
            "{'dense': TensorSpec(shape=(None, 3), dtype=tf.float32, name='dense')}\n"
          ]
        }
      ],
      "source": [
        "print(list(loaded.signatures.keys()))\n",
        "infer = loaded.signatures[\"serving_default\"]\n",
        "print(infer.structured_input_signature)\n",
        "print(infer.structured_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSOiPhfsIPgo"
      },
      "source": [
        "## Convert Using TFLite's Converter - **important how to's**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aUYvCpfWmrQ"
      },
      "source": [
        "Load the TFLiteConverter with the SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqJRyIg8Wl1n"
      },
      "outputs": [],
      "source": [
        "##converter = tf.lite.TFLiteConverter.from_saved_model(RPS_SAVED_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AudcNjT0UtfF"
      },
      "source": [
        "### Post-Training Quantization\n",
        "The simplest form of post-training quantization quantizes weights from floating point to 8-bits of precision. This technique is enabled as an option in the TensorFlow Lite converter. At inference, weights are converted from 8-bits of precision to floating point and computed using floating-point kernels. This conversion is done once and cached to reduce latency.\n",
        "\n",
        "To further improve latency, hybrid operators dynamically quantize activations to 8-bits and perform computations with 8-bit weights and activations. This optimization provides latencies close to fully fixed-point inference. However, the outputs are still stored using floating point, so that the speedup with hybrid ops is less than a full fixed-point computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZFIcLGGIPhG"
      },
      "outputs": [],
      "source": [
        "##converter.optimizations = [tf.lite.Optimize.DEFAULT]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpCijI08UxP0"
      },
      "source": [
        "### Post-Training Integer Quantization\n",
        "We can get further latency improvements, reductions in peak memory usage, and access to integer only hardware accelerators by making sure all model math is quantized. To do this, we need to measure the dynamic range of activations and inputs with a representative data set. You can simply create an input data generator and provide it to our converter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clM_dTIkWdIa"
      },
      "outputs": [],
      "source": [
        "# def representative_data_gen():\n",
        "#     for input_value, _ in test_batches.take(100):\n",
        "#         yield [input_value]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oPkAxDvUias"
      },
      "outputs": [],
      "source": [
        "##converter.representative_dataset = representative_data_gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGUAVTqXVfnu"
      },
      "source": [
        "The resulting model will be fully quantized but still take float input and output for convenience.\n",
        "\n",
        "Ops that do not have quantized implementations will automatically be left in floating point. This allows conversion to occur smoothly but may restrict deployment to accelerators that support float. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPVdjaEJVkHy"
      },
      "source": [
        "### Full Integer Quantization\n",
        "\n",
        "To require the converter to only output integer operations, one can specify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQi1aO2cVhoL"
      },
      "outputs": [],
      "source": [
        "##converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snwssESbVtFw"
      },
      "source": [
        "### Finally convert the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUEgr46WVsqd"
      },
      "outputs": [],
      "source": [
        "# tflite_model = converter.convert()\n",
        "# tflite_model_file = 'converted_model.tflite'\n",
        "\n",
        "# with open(tflite_model_file, \"wb\") as f:\n",
        "#     f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxLiLC8n0H16"
      },
      "source": [
        "## Convert Using TFLite's Converter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmSr2-yZoUhz",
        "outputId": "ed63b5a7-5d13-4a44-e7dc-180094a0ea3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(RPS_SAVED_MODEL)\n",
        "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j0yvfIUBCkX"
      },
      "outputs": [],
      "source": [
        "tflite_model_file = 'converted_model.tflite'\n",
        "\n",
        "with open(tflite_model_file, \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbTF6nd1KG2o"
      },
      "source": [
        "## Test the TFLite Model Using the Python Interpreter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg2NkVTmLUdJ"
      },
      "outputs": [],
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "with open(tflite_model_file, 'rb') as fid:\n",
        "    tflite_model = fid.read()\n",
        "    \n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snJQVs9JNglv",
        "outputId": "1b6eeb30-a5ab-4603-85c2-c07b1bfda384"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 17.99it/s]\n"
          ]
        }
      ],
      "source": [
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "\n",
        "test_labels, test_imgs = [], []\n",
        "for img, label in tqdm(test_batches.take(10)):\n",
        "    interpreter.set_tensor(input_index, img)\n",
        "    interpreter.invoke()\n",
        "    predictions.append(interpreter.get_tensor(output_index))\n",
        "    \n",
        "    test_labels.append(label.numpy()[0])\n",
        "    test_imgs.append(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpcZoVyuGdQi"
      },
      "source": [
        "NOTE: Colab runs on server CPUs. At the time of writing this, TensorFlow Lite doesn't have super optimized server CPU kernels. For this reason post-training full-integer quantized models  may be slower here than the other kinds of optimized models. But for mobile CPUs, considerable speedup can be observed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YMTWNqPpNiAI"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions for plotting\n",
        "# Utilities for plotting\n",
        "\n",
        "class_names = ['rock', 'paper', 'scissors']\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "    predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "    plt.grid(False)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    \n",
        "    img = np.squeeze(img)\n",
        "    \n",
        "    plt.imshow(img, cmap=plt.cm.binary)\n",
        "    \n",
        "    predicted_label = np.argmax(predictions_array)\n",
        "    \n",
        "    print(type(predicted_label), type(true_label))\n",
        "    \n",
        "    if predicted_label == true_label:\n",
        "        color = 'green'\n",
        "    else:\n",
        "        color = 'red'\n",
        "        \n",
        "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                         100*np.max(predictions_array),\n",
        "                                         class_names[true_label]), color=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-lbnicPNkZs"
      },
      "outputs": [],
      "source": [
        "#@title Visualize the outputs { run: \"auto\" }\n",
        "index = 0 #@param {type:\"slider\", min:0, max:9, step:1}\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(index, predictions, test_labels, test_imgs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHLKgM8eBCkb"
      },
      "source": [
        "Create a file to save the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQtnbPnbBCkb"
      },
      "outputs": [],
      "source": [
        "with open('labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmZRieHmKLY5"
      },
      "source": [
        "If you are running this notebook in a Colab, you can run the cell below to download the model and labels to your local disk.\n",
        "\n",
        "**Note**: If the files do not download when you run the cell, try running the cell a second time. Your browser might prompt you to allow multiple files to be downloaded. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0jJAxrQB2VFw",
        "outputId": "a8d8189c-1754-4e53-9d35-ccff2c2e7406"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_533ec914-ff6d-4fe3-905e-72c59e80fcc5\", \"converted_model.tflite\", 2513488)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_5273e069-e754-4a82-bb59-8e7692dea91b\", \"labels.txt\", 19)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import files\n",
        "    files.download('converted_model.tflite')\n",
        "    files.download('labels.txt')\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDlmpjC6VnFZ"
      },
      "source": [
        "## Prepare the Test Images for Download (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1ja_WA0WZOH"
      },
      "source": [
        "This part involves downloading additional test images for the Mobile Apps only in case you need to try out more samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzLKEBrfTREA"
      },
      "outputs": [],
      "source": [
        "!mkdir -p test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn7ukNQCSewb"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "for index, (image, label) in enumerate(test_batches.take(50)):\n",
        "    image = tf.cast(image * 255.0, tf.uint8)\n",
        "    image = tf.squeeze(image).numpy()\n",
        "    pil_image = Image.fromarray(image)\n",
        "    pil_image.save('test_images/{}_{}.jpg'.format(class_names[label[0]], index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVKKWUG8UMO5",
        "outputId": "40645617-65a4-42a1-f62f-c79d85525a1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "paper_0.jpg   paper_27.jpg  rock_15.jpg  rock_38.jpg\t  scissors_17.jpg\n",
            "paper_12.jpg  paper_2.jpg   rock_1.jpg\t rock_39.jpg\t  scissors_29.jpg\n",
            "paper_14.jpg  paper_31.jpg  rock_20.jpg  rock_41.jpg\t  scissors_33.jpg\n",
            "paper_18.jpg  paper_3.jpg   rock_21.jpg  rock_42.jpg\t  scissors_37.jpg\n",
            "paper_19.jpg  paper_40.jpg  rock_28.jpg  rock_43.jpg\t  scissors_46.jpg\n",
            "paper_22.jpg  paper_45.jpg  rock_30.jpg  rock_44.jpg\t  scissors_4.jpg\n",
            "paper_23.jpg  paper_48.jpg  rock_32.jpg  rock_47.jpg\t  scissors_5.jpg\n",
            "paper_24.jpg  paper_49.jpg  rock_34.jpg  rock_9.jpg\t  scissors_6.jpg\n",
            "paper_25.jpg  rock_11.jpg   rock_35.jpg  scissors_10.jpg  scissors_7.jpg\n",
            "paper_26.jpg  rock_13.jpg   rock_36.jpg  scissors_16.jpg  scissors_8.jpg\n"
          ]
        }
      ],
      "source": [
        "!ls test_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_w_-UdlS9Vi"
      },
      "outputs": [],
      "source": [
        "!zip -qq rps_test_images.zip -r test_images/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DctmWA30BCkd"
      },
      "source": [
        "If you are running this notebook in a Colab, you can run the cell below to download the Zip file with the images to your local disk. \n",
        "\n",
        "**Note**: If the Zip file does not download when you run the cell, try running the cell a second time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Giva6EHwWm6Y",
        "outputId": "b07a652b-ed45-4744-8d39-49720cc60baa"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_9a41bb98-8afc-49ba-b48e-335a7bc41444\", \"rps_test_images.zip\", 189964)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "try:\n",
        "    files.download('rps_test_images.zip')\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKjAjx5vLRu6"
      },
      "source": [
        "For further learning consider end to end projects on raspberry pi: \n",
        "\n",
        "https://github.com/https-deeplearning-ai/tensorflow-2-public/tree/main/C2_Device-based-TF-lite/W4/ungraded_labs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32xflLc4NTx-"
      },
      "source": [
        "# Basic usage of tensorflow federated \n",
        "\n",
        "All guides from tensorflow: https://github.com/tensorflow/federated/tree/v0.48.0/docs/tutorials\n",
        "\n",
        "this one follows the one of tensorflow guides from above: https://github.com/tensorflow/federated/blob/v0.48.0/docs/tutorials/custom_federated_algorithms_2.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPv1X8c5OJlV",
        "outputId": "53f6adf8-fb1d-4fac-91be-67d22dff0887"
      },
      "outputs": [],
      "source": [
        "## inspect system version of python\n",
        "import sys\n",
        "print(sys.version)\n",
        "\n",
        "## python version\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB1ovcX1mBxQ"
      },
      "outputs": [],
      "source": [
        "## because of compatibility issue for colab: https://github.com/tensorflow/federated/issues/3428\n",
        "## I do not upgrade tensorflow-federated\n",
        "## tried solutions here, but they cause issues and do not reccomend that much: https://stackoverflow.com/questions/68657341/how-can-i-update-google-colabs-python-version\n",
        "\n",
        "# !pip install --quiet --upgrade tensorflow-federated\n",
        "# !pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "!pip install --quiet tensorflow-federated==0.20.0  ## if this does not work in the future allways look at python version and tensorflow federated compatibility\n",
        "!pip install --quiet --upgrade nest-asyncio\n",
        "\n",
        "import nest_asyncio    ## Did it crush ? have restarted runntime as instructed in the previous cell ?\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-skNC6aovM46"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Must use the Python context because it\n",
        "# supports tff.sequence_* intrinsics.\n",
        "executor_factory = tff.framework.local_executor_factory(\n",
        "    support_sequence_ops=True)\n",
        "execution_context = tff.framework.SyncExecutionContext(\n",
        "    executor_fn=executor_factory)\n",
        "tff.framework.set_default_context(execution_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzXwGnZamIMM"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation\n",
        "def hello_world():\n",
        "  return 'Hello, World!'\n",
        "\n",
        "hello_world()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uThZM4Ds-KDQ"
      },
      "outputs": [],
      "source": [
        "mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkJc5rHA2no_"
      },
      "outputs": [],
      "source": [
        "[(x.dtype, x.shape) for x in mnist_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTaTLiq5GNqy"
      },
      "outputs": [],
      "source": [
        "NUM_EXAMPLES_PER_USER = 1000\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "def get_data_for_digit(source, digit):\n",
        "  output_sequence = []\n",
        "  all_samples = [i for i, d in enumerate(source[1]) if d == digit]\n",
        "  for i in range(0, min(len(all_samples), NUM_EXAMPLES_PER_USER), BATCH_SIZE):\n",
        "    batch_samples = all_samples[i:i + BATCH_SIZE]\n",
        "    output_sequence.append({\n",
        "        'x':\n",
        "            np.array([source[0][i].flatten() / 255.0 for i in batch_samples],\n",
        "                     dtype=np.float32),\n",
        "        'y':\n",
        "            np.array([source[1][i] for i in batch_samples], dtype=np.int32)\n",
        "    })\n",
        "  return output_sequence\n",
        "\n",
        "\n",
        "federated_train_data = [get_data_for_digit(mnist_train, d) for d in range(10)]\n",
        "\n",
        "federated_test_data = [get_data_for_digit(mnist_test, d) for d in range(10)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTNuL1W4bcuc"
      },
      "outputs": [],
      "source": [
        "federated_train_data[5][-1]['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI4aat1za525"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.imshow(federated_train_data[5][-1]['x'][-1].reshape(28, 28), cmap='gray')\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "653xv5NXd4fy"
      },
      "outputs": [],
      "source": [
        "BATCH_SPEC = collections.OrderedDict(\n",
        "    x=tf.TensorSpec(shape=[None, 784], dtype=tf.float32),\n",
        "    y=tf.TensorSpec(shape=[None], dtype=tf.int32))\n",
        "BATCH_TYPE = tff.to_type(BATCH_SPEC)\n",
        "\n",
        "str(BATCH_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og7VViafh-30"
      },
      "outputs": [],
      "source": [
        "MODEL_SPEC = collections.OrderedDict(\n",
        "    weights=tf.TensorSpec(shape=[784, 10], dtype=tf.float32),\n",
        "    bias=tf.TensorSpec(shape=[10], dtype=tf.float32))\n",
        "MODEL_TYPE = tff.to_type(MODEL_SPEC)\n",
        "print(MODEL_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EObiz_Ke0uK"
      },
      "outputs": [],
      "source": [
        "# NOTE: `forward_pass` is defined separately from `batch_loss` so that it can \n",
        "# be later called from within another tf.function. Necessary because a\n",
        "# @tf.function  decorated method cannot invoke a @tff.tf_computation.\n",
        "\n",
        "@tf.function\n",
        "def forward_pass(model, batch):\n",
        "  predicted_y = tf.nn.softmax(\n",
        "      tf.matmul(batch['x'], model['weights']) + model['bias'])\n",
        "  return -tf.reduce_mean(\n",
        "      tf.reduce_sum(\n",
        "          tf.one_hot(batch['y'], 10) * tf.math.log(predicted_y), axis=[1]))\n",
        "\n",
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE)\n",
        "def batch_loss(model, batch):\n",
        "  return forward_pass(model, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WXEAY8Nr89V"
      },
      "outputs": [],
      "source": [
        "str(batch_loss.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8Ne8igan3os"
      },
      "outputs": [],
      "source": [
        "initial_model = collections.OrderedDict(\n",
        "    weights=np.zeros([784, 10], dtype=np.float32),\n",
        "    bias=np.zeros([10], dtype=np.float32))\n",
        "\n",
        "sample_batch = federated_train_data[5][-1]\n",
        "\n",
        "batch_loss(initial_model, sample_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4uaVxw3AyYS"
      },
      "outputs": [],
      "source": [
        "@tff.tf_computation(MODEL_TYPE, BATCH_TYPE, tf.float32)\n",
        "def batch_train(initial_model, batch, learning_rate):\n",
        "  # Define a group of model variables and set them to `initial_model`. Must\n",
        "  # be defined outside the @tf.function.\n",
        "  model_vars = collections.OrderedDict([\n",
        "      (name, tf.Variable(name=name, initial_value=value))\n",
        "      for name, value in initial_model.items()\n",
        "  ])\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "\n",
        "  @tf.function\n",
        "  def _train_on_batch(model_vars, batch):\n",
        "    # Perform one step of gradient descent using loss from `batch_loss`.\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss = forward_pass(model_vars, batch)\n",
        "    grads = tape.gradient(loss, model_vars)\n",
        "    optimizer.apply_gradients(\n",
        "        zip(tf.nest.flatten(grads), tf.nest.flatten(model_vars)))\n",
        "    return model_vars\n",
        "\n",
        "  return _train_on_batch(model_vars, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y84gQsaohC38"
      },
      "outputs": [],
      "source": [
        "str(batch_train.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8edcJTlXUULm"
      },
      "outputs": [],
      "source": [
        "model = initial_model\n",
        "losses = []\n",
        "for _ in range(5):\n",
        "  model = batch_train(model, sample_batch, 0.1)\n",
        "  losses.append(batch_loss(model, sample_batch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3n1onojT1zHv"
      },
      "outputs": [],
      "source": [
        "losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfPD5a6QVNXM"
      },
      "outputs": [],
      "source": [
        "LOCAL_DATA_TYPE = tff.SequenceType(BATCH_TYPE)\n",
        "\n",
        "@tff.federated_computation(MODEL_TYPE, tf.float32, LOCAL_DATA_TYPE)\n",
        "def local_train(initial_model, learning_rate, all_batches):\n",
        "\n",
        "  @tff.tf_computation(LOCAL_DATA_TYPE, tf.float32)\n",
        "  def _insert_learning_rate_to_sequence(dataset, learning_rate):\n",
        "    return dataset.map(lambda x: (x, learning_rate))\n",
        "\n",
        "  batches_with_learning_rate = _insert_learning_rate_to_sequence(all_batches, learning_rate)\n",
        "\n",
        "  # Mapping function to apply to each batch.\n",
        "  @tff.federated_computation(MODEL_TYPE, batches_with_learning_rate.type_signature.element)\n",
        "  def batch_fn(model, batch_with_lr):\n",
        "    batch, lr = batch_with_lr\n",
        "    return batch_train(model, batch, lr)\n",
        "\n",
        "  return tff.sequence_reduce(batches_with_learning_rate, initial_model, batch_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAhkS5yKUgjC"
      },
      "outputs": [],
      "source": [
        "str(local_train.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnWFLoZGcSby"
      },
      "outputs": [],
      "source": [
        "locally_trained_model = local_train(initial_model, 0.1, federated_train_data[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RiODuc6z7Ln"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
        "def local_eval(model, all_batches):\n",
        "\n",
        "  @tff.tf_computation(MODEL_TYPE, LOCAL_DATA_TYPE)\n",
        "  def _insert_model_to_sequence(model, dataset):\n",
        "    return dataset.map(lambda x: (model, x))\n",
        "\n",
        "  model_plus_data = _insert_model_to_sequence(model, all_batches)\n",
        "\n",
        "  @tff.tf_computation(tf.float32, batch_loss.type_signature.result)\n",
        "  def tff_add(accumulator, arg):\n",
        "    return accumulator + arg\n",
        "\n",
        "  return tff.sequence_reduce(\n",
        "      tff.sequence_map(\n",
        "          batch_loss,\n",
        "          model_plus_data), 0., tff_add)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH2XPEAKa4Dg"
      },
      "outputs": [],
      "source": [
        "str(local_eval.type_signature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPw6JSVf5q_x"
      },
      "outputs": [],
      "source": [
        "print('initial_model loss =', local_eval(initial_model,\n",
        "                                         federated_train_data[5]))\n",
        "print('locally_trained_model loss =',\n",
        "      local_eval(locally_trained_model, federated_train_data[5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjF0NYAj5wls"
      },
      "outputs": [],
      "source": [
        "print('initial_model loss =', local_eval(initial_model,\n",
        "                                         federated_train_data[0]))\n",
        "print('locally_trained_model loss =',\n",
        "      local_eval(locally_trained_model, federated_train_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjGGhpoEBh_6"
      },
      "outputs": [],
      "source": [
        "SERVER_MODEL_TYPE = tff.type_at_server(MODEL_TYPE)\n",
        "CLIENT_DATA_TYPE = tff.type_at_clients(LOCAL_DATA_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zChEPzEBx4T"
      },
      "outputs": [],
      "source": [
        "@tff.federated_computation(SERVER_MODEL_TYPE, CLIENT_DATA_TYPE)\n",
        "def federated_eval(model, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_eval, [tff.federated_broadcast(model),  data]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbmtJItcn94j"
      },
      "outputs": [],
      "source": [
        "print('initial_model loss =', federated_eval(initial_model,\n",
        "                                             federated_train_data))\n",
        "print('locally_trained_model loss =',\n",
        "      federated_eval(locally_trained_model, federated_train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBOC4uoG6dd-"
      },
      "outputs": [],
      "source": [
        "SERVER_FLOAT_TYPE = tff.type_at_server(tf.float32)\n",
        "\n",
        "\n",
        "@tff.federated_computation(SERVER_MODEL_TYPE, SERVER_FLOAT_TYPE,\n",
        "                           CLIENT_DATA_TYPE)\n",
        "def federated_train(model, learning_rate, data):\n",
        "  return tff.federated_mean(\n",
        "      tff.federated_map(local_train, [\n",
        "          tff.federated_broadcast(model),\n",
        "          tff.federated_broadcast(learning_rate), data\n",
        "      ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLx-3rLs9jGY"
      },
      "outputs": [],
      "source": [
        "model = initial_model\n",
        "learning_rate = 0.1\n",
        "for round_num in range(5):\n",
        "  model = federated_train(model, learning_rate, federated_train_data)\n",
        "  learning_rate = learning_rate * 0.9\n",
        "  loss = federated_eval(model, federated_train_data)\n",
        "  print('round {}, loss={}'.format(round_num, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaZT45yFMOaM"
      },
      "outputs": [],
      "source": [
        "print('initial_model test loss =',\n",
        "      federated_eval(initial_model, federated_test_data))\n",
        "print('trained_model test loss =', federated_eval(model, federated_test_data))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oYM61xrTsP5d",
        "LBEFjNs3oOEl",
        "u2e5WupIw2N2",
        "d6zom1Imibjt",
        "aWmmKlijmMv4",
        "CgJNMvpGoF7u",
        "f4cEIi0Wl27N",
        "0QeMkGJHmAkQ",
        "SGWcPfqZLVlL",
        "BNcPVBk4JEc8",
        "u_psFoTeLpHU",
        "fSOiPhfsIPgo",
        "XxLiLC8n0H16",
        "BbTF6nd1KG2o",
        "32xflLc4NTx-"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
