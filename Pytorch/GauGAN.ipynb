{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "# GauGAN\n",
        "\n",
        "see architecture: https://www.researchgate.net/figure/Detail-of-GauGAN-architecture_fig5_348871964 \n",
        "\n",
        "Park et al. 2019, Semantic Image Synthesis with Spatially-Adaptive Normalization https://arxiv.org/abs/1903.07291 \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCxv5rhWNhr"
      },
      "source": [
        "import getpass, os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAjgcHDS64_o"
      },
      "source": [
        "# Store necessary cookies\n",
        "username = input(\"What is your Cityscapes username? (http://cityscapes-dataset.com) \")\n",
        "password = getpass.getpass(\"What is your Cityscapes password? \")\n",
        "os.mkdir(\"data\")\n",
        "os.system(f\"wget --keep-session-cookies --save-cookies=data/cookies.txt --post-data 'username={username}&password={password}&submit=Login' https://www.cityscapes-dataset.com/login/\")\n",
        "# Download data\n",
        "!cd data; wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=1\n",
        "!cd data; wget --load-cookies cookies.txt --content-disposition https://www.cityscapes-dataset.com/file-handling/?packageID=3\n",
        "# Unzip data\n",
        "!cd data; unzip leftImg8bit_trainvaltest\n",
        "!cd data; unzip gtFine_trainvaltest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mII5L2cZLlpO"
      },
      "source": [
        "class CityscapesDataset(torch.utils.data.Dataset):\n",
        "    '''\n",
        "    CityscapesDataset Class\n",
        "    Values:\n",
        "        paths: (a list of) paths to construct dataset from, a list or string\n",
        "        img_size: tuple containing the (height, width) for resizing, a tuple\n",
        "        n_classes: the number of object classes, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, paths, img_size=(256, 512), n_classes=35):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Collect list of examples\n",
        "        self.examples = {}\n",
        "        if type(paths) == str:\n",
        "            self.load_examples_from_dir(paths)\n",
        "        elif type(paths) == list:\n",
        "            for path in paths:\n",
        "                self.load_examples_from_dir(path)\n",
        "        else:\n",
        "            raise ValueError('`paths` should be a single path or list of paths')\n",
        "\n",
        "        self.examples = list(self.examples.values())\n",
        "        assert all(len(example) == 2 for example in self.examples)\n",
        "\n",
        "        # Initialize transforms for the real color image\n",
        "        self.img_transforms = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "\n",
        "        # Initialize transforms for semantic label maps\n",
        "        self.map_transforms = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.Lambda(lambda img: np.array(img)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def load_examples_from_dir(self, abs_path):\n",
        "        '''\n",
        "        Given a folder of examples, this function returns a list of paired examples.\n",
        "        '''\n",
        "        assert os.path.isdir(abs_path)\n",
        "\n",
        "        img_suffix = '_leftImg8bit.png'\n",
        "        label_suffix = '_gtFine_labelIds.png'\n",
        "\n",
        "        for root, _, files in os.walk(abs_path):\n",
        "            for f in files:\n",
        "                if f.endswith(img_suffix):\n",
        "                    prefix = f[:-len(img_suffix)]\n",
        "                    attr = 'orig_img'\n",
        "                elif f.endswith(label_suffix):\n",
        "                    prefix = f[:-len(label_suffix)]\n",
        "                    attr = 'label_map'\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "                if prefix not in self.examples.keys():\n",
        "                    self.examples[prefix] = {}\n",
        "                self.examples[prefix][attr] = root + '/' + f\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "\n",
        "        # Load image and maps\n",
        "        img = Image.open(example['orig_img']).convert('RGB') # color image: (3, h, w)\n",
        "        label = Image.open(example['label_map'])             # semantic label map: (1, h, w)\n",
        "\n",
        "        # Apply corresponding transforms\n",
        "        img = self.img_transforms(img)\n",
        "        label = self.map_transforms(label).long() * 255\n",
        "\n",
        "        # Convert labels to one-hot vectors\n",
        "        label = F.one_hot(label, num_classes=self.n_classes)\n",
        "        label = label.squeeze(0).permute(2, 0, 1).to(img.dtype)\n",
        "\n",
        "        return (img, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        imgs, labels = [], []\n",
        "        for (x, l) in batch:\n",
        "            imgs.append(x)\n",
        "            labels.append(l)\n",
        "        return torch.stack(imgs, dim=0), torch.stack(labels, dim=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPADE(nn.Module):\n",
        "    '''\n",
        "    SPADE Class\n",
        "    Values:\n",
        "        channels: the number of channels in the input, a scalar\n",
        "        cond_channels: the number of channels in conditional input (one-hot semantic labels), a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, cond_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.batchnorm = nn.BatchNorm2d(channels)\n",
        "        self.spade = nn.Sequential(\n",
        "            nn.Conv2d(cond_channels, channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels, 2 * channels, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        # Apply normalization\n",
        "        x = self.batchnorm(x)\n",
        "\n",
        "        # Compute denormalization\n",
        "        seg = F.interpolate(seg, size=x.shape[-2:], mode='nearest')\n",
        "        gamma, beta = torch.chunk(self.spade(seg), 2, dim=1)\n",
        "\n",
        "        # Apply denormalization\n",
        "        x = x * (1 + gamma) + beta\n",
        "        return x"
      ],
      "metadata": {
        "id": "yMAqm_P0He_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHD_wif07f4b"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    ResidualBlock Class\n",
        "    Values:\n",
        "        in_channels: the number of input channels, a scalar\n",
        "        out_channels: the number of output channels, a scalar\n",
        "        cond_channels: the number of channels in conditional input in spade layer, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, cond_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        hid_channels = min(in_channels, out_channels)\n",
        "\n",
        "        self.proj = in_channels != out_channels\n",
        "        if self.proj:\n",
        "            self.norm0 = SPADE(in_channels, cond_channels)\n",
        "            self.conv0 = nn.utils.spectral_norm(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "            )\n",
        "\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.norm1 = SPADE(in_channels, cond_channels)\n",
        "        self.norm2 = SPADE(hid_channels, cond_channels)\n",
        "        self.conv1 = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(in_channels, hid_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.conv2 = nn.utils.spectral_norm(\n",
        "            nn.Conv2d(hid_channels, out_channels, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        dx = self.norm1(x, seg)           # seg is segmentation map, generally we call its channels the conditional channels\n",
        "        dx = self.activation(dx)\n",
        "        dx = self.conv1(dx)\n",
        "        dx = self.norm2(dx, seg)\n",
        "        dx = self.activation(dx)\n",
        "        dx = self.conv2(dx)\n",
        "\n",
        "        # Learn skip connection if in_channels != out_channels\n",
        "        if self.proj:\n",
        "            x = self.norm0(x, seg)\n",
        "            x = self.conv0(x)\n",
        "\n",
        "        return x + dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfLUrDS440vm"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder Class\n",
        "    Values:\n",
        "        spatial_size: tuple specifying (height, width) of full size image, a tuple\n",
        "        z_dim: number of dimensions of latent noise vector (z), a scalar\n",
        "        n_downsample: number of downsampling blocks in the encoder, a scalar\n",
        "        base_channels: number of channels in the last hidden layer, a scalar\n",
        "    '''\n",
        "\n",
        "    max_channels = 512\n",
        "\n",
        "    def __init__(self, spatial_size, z_dim=256, n_downsample=6, base_channels=64):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        channels = base_channels\n",
        "        for i in range(n_downsample):\n",
        "            in_channels = 3 if i == 0 else channels\n",
        "            out_channels = 2 * z_dim if i < n_downsample else max(self.max_channels, channels * 2)\n",
        "            layers += [\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(in_channels, out_channels, stride=2, kernel_size=3, padding=1)\n",
        "                ),\n",
        "                nn.InstanceNorm2d(out_channels),\n",
        "                nn.LeakyReLU(0.2),\n",
        "            ]\n",
        "            channels = out_channels\n",
        "\n",
        "        h, w = spatial_size[0] // 2 ** n_downsample, spatial_size[1] // 2 ** n_downsample\n",
        "        layers += [\n",
        "            nn.Flatten(1),\n",
        "            nn.Linear(channels * h * w, 2 * z_dim),\n",
        "        ]\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.chunk(self.layers(x), 2, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqgMm3Gs0B0a"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        n_classes: the number of object classes in the dataset, a scalar\n",
        "        spatial_size: the downsampled spatial size of the image, a scalar\n",
        "        z_dim: the number of dimensions the z noise vector has, a scalar\n",
        "        base_channels: the number of channels in last hidden layer, a scalar\n",
        "        n_upsample: the number of upsampling operations to apply, a scalar\n",
        "    '''\n",
        "\n",
        "    max_channels = 1024\n",
        "\n",
        "    def __init__(self, n_classes, spatial_size, z_dim=256, base_channels=64, n_upsample=6):\n",
        "        super().__init__()\n",
        "\n",
        "        h, w = spatial_size[0] // 2 ** n_upsample, spatial_size[1] // 2 ** n_upsample\n",
        "        self.proj_z = nn.Linear(z_dim, self.max_channels * h * w)\n",
        "        self.reshape = lambda x: torch.reshape(x, (-1, self.max_channels, h, w))\n",
        "\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        for i in reversed(range(n_upsample)):\n",
        "            in_channels = min(self.max_channels, base_channels * 2 ** (i+1))\n",
        "            out_channels = min(self.max_channels, base_channels * 2 ** i)\n",
        "            self.res_blocks.append(ResidualBlock(in_channels, out_channels, n_classes))\n",
        "\n",
        "        self.proj_o = nn.Sequential(\n",
        "            nn.Conv2d(base_channels, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z, seg):\n",
        "        h = self.proj_z(z)\n",
        "        h = self.reshape(h)\n",
        "        for res_block in self.res_blocks:\n",
        "            h = res_block(h, seg)\n",
        "            h = self.upsample(h)\n",
        "        h = self.proj_o(h)\n",
        "        return h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYgX2B_hDxkA"
      },
      "source": [
        "class PatchGANDiscriminator(nn.Module):\n",
        "    '''\n",
        "    PatchGANDiscriminator Class\n",
        "    Implements the discriminator class for a subdiscriminator, \n",
        "    which can be used for all the different scales, just with different argument values.\n",
        "    Values:\n",
        "        in_channels: the number of channels in input, a scalar\n",
        "        base_channels: the number of channels in first convolutional layer, a scalar\n",
        "        n_layers: the number of convolutional layers, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, base_channels=64, n_layers=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Use nn.ModuleList so we can output intermediate values for loss.\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # Initial convolutional layer\n",
        "        self.layers.append(\n",
        "            nn.Sequential(\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(in_channels, base_channels, kernel_size=4, stride=2, padding=2)\n",
        "                ),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Downsampling convolutional layers\n",
        "        channels = base_channels\n",
        "        for _ in range(1, n_layers):\n",
        "            prev_channels = channels\n",
        "            channels = min(2 * channels, 512)\n",
        "            self.layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.utils.spectral_norm(\n",
        "                        nn.Conv2d(prev_channels, channels, kernel_size=4, stride=2, padding=2)\n",
        "                    ),\n",
        "                    nn.InstanceNorm2d(channels, affine=False),\n",
        "                    nn.LeakyReLU(0.2, inplace=True),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Output convolutional layer\n",
        "        prev_channels = channels\n",
        "        channels = min(2 * channels, 512)\n",
        "        self.layers.append(\n",
        "            nn.Sequential(\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(prev_channels, channels, kernel_size=4, stride=1, padding=2))\n",
        "                ,\n",
        "                nn.InstanceNorm2d(channels, affine=False),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 1, kernel_size=4, stride=1, padding=2)\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [] # for feature matching loss\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            outputs.append(x)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_kqcsh4Jwjz"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Discriminator Class\n",
        "    Values:\n",
        "        in_channels: number of input channels to each discriminator, a scalar\n",
        "        base_channels: number of channels in last hidden layer, a scalar\n",
        "        n_layers: number of downsampling layers in each discriminator, a scalar\n",
        "        n_discriminators: number of discriminators at different scales, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, base_channels=64, n_layers=3, n_discriminators=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize all discriminators\n",
        "        self.discriminators = nn.ModuleList()\n",
        "        for _ in range(n_discriminators):\n",
        "            self.discriminators.append(\n",
        "                PatchGANDiscriminator(in_channels, base_channels=base_channels, n_layers=n_layers)\n",
        "            )\n",
        "\n",
        "        # Downsampling layer to pass inputs between discriminators at different scales\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "\n",
        "        for i, discriminator in enumerate(self.discriminators):\n",
        "            # Downsample input for subsequent discriminators\n",
        "            if i != 0:\n",
        "                x = self.downsample(x)\n",
        "\n",
        "            outputs.append(discriminator(x))\n",
        "\n",
        "        # Return list of multiscale discriminator outputs\n",
        "        return outputs\n",
        "\n",
        "    @property\n",
        "    def n_discriminators(self):\n",
        "        return len(self.discriminators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slAXJc_wzxVT"
      },
      "source": [
        "class GauGAN(nn.Module):\n",
        "    '''\n",
        "    GauGAN Class\n",
        "    Values:\n",
        "        n_classes: number of object classes in dataset, a scalar\n",
        "        spatial_size: tuple containing (height, width) of full-size image, a tuple\n",
        "        base_channels: number of channels in last generator & first discriminator layers, a scalar\n",
        "        z_dim: number of dimensions in noise vector (z), a scalar\n",
        "        n_upsample: number of downsampling (encoder) and upsampling (generator) operations, a scalar\n",
        "        n_disc_layer:: number of discriminator layers, a scalar\n",
        "        n_disc: number of discriminators (at different scales), a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_classes,\n",
        "        spatial_size,\n",
        "        base_channels=64,\n",
        "        z_dim=256,\n",
        "        n_upsample=6,\n",
        "        n_disc_layers=3,\n",
        "        n_disc=3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(\n",
        "            spatial_size, z_dim=z_dim, n_downsample=n_upsample, base_channels=base_channels,\n",
        "        )\n",
        "        self.generator = Generator(\n",
        "            n_classes, spatial_size, z_dim=z_dim, base_channels=base_channels, n_upsample=n_upsample,\n",
        "        )\n",
        "        self.discriminator = Discriminator(\n",
        "            n_classes + 3, base_channels=base_channels, n_layers=n_disc_layers, n_discriminators=n_disc,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        ''' Performs a full forward pass for training. '''\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.sample_z(mu, logvar)\n",
        "        x_fake = self.generate(z, seg)\n",
        "        pred = self.discriminate(x_fake, seg)\n",
        "        return x_fake, pred\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def generate(self, z, seg):\n",
        "        ''' Generates fake image from noise vector and segmentation. '''\n",
        "        return self.generator(z, seg)\n",
        "\n",
        "    def discriminate(self, x, seg):\n",
        "        ''' Predicts whether input image is real. '''\n",
        "        return self.discriminator(torch.cat((x, seg), dim=1))\n",
        "\n",
        "    @staticmethod\n",
        "    def sample_z(mu, logvar):\n",
        "        ''' Samples noise vector with reparameterization trick. '''\n",
        "        eps = torch.randn(mu.size(), device=mu.device).to(mu.dtype)        # follows the way of torch.dstributions.normal.Normal.rsample() but not exactly\n",
        "        return (logvar / 2).exp() * eps + mu\n",
        "\n",
        "    @property\n",
        "    def n_disc(self):\n",
        "        return self.discriminator.n_discriminators "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCXhrrsudbB2"
      },
      "source": [
        "class VGG19(nn.Module):\n",
        "    '''\n",
        "    VGG19 Class\n",
        "    Wrapper for pretrained torchvision.models.vgg19 to output intermediate feature maps\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg_features = models.vgg19(pretrained=True).features\n",
        "\n",
        "        self.f1 = nn.Sequential(*[vgg_features[x] for x in range(2)])\n",
        "        self.f2 = nn.Sequential(*[vgg_features[x] for x in range(2, 7)])\n",
        "        self.f3 = nn.Sequential(*[vgg_features[x] for x in range(7, 12)])\n",
        "        self.f4 = nn.Sequential(*[vgg_features[x] for x in range(12, 21)])\n",
        "        self.f5 = nn.Sequential(*[vgg_features[x] for x in range(21, 30)])\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = self.f1(x)\n",
        "        h2 = self.f2(h1)\n",
        "        h3 = self.f3(h2)\n",
        "        h4 = self.f4(h3)\n",
        "        h5 = self.f5(h4)\n",
        "        return [h1, h2, h3, h4, h5]\n",
        "\n",
        "class Loss(nn.Module):\n",
        "    '''\n",
        "    Loss Class\n",
        "    Implements composite loss for GauGAN\n",
        "    Values:\n",
        "        lambda1: weight for feature matching loss, a float\n",
        "        lambda2: weight for vgg perceptual loss, a float\n",
        "        lambda3: weight for KLD loss, a float\n",
        "        device: 'cuda' or 'cpu' for hardware to use\n",
        "        norm_weight_to_one: whether to normalize weights to (0, 1], a bool\n",
        "    '''\n",
        "\n",
        "    def __init__(self, lambda1=10., lambda2=10., lambda3=0.05, device='cuda', norm_weight_to_one=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vgg = VGG19().to(device)\n",
        "        self.vgg_weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
        "\n",
        "        lambda0 = 1.0\n",
        "        # Keep ratio of composite loss, but scale down max to 1.0\n",
        "        scale = max(lambda0, lambda1, lambda2, lambda3) if norm_weight_to_one else 1.0\n",
        "\n",
        "        self.lambda0 = lambda0 / scale\n",
        "        self.lambda1 = lambda1 / scale\n",
        "        self.lambda2 = lambda2 / scale\n",
        "        self.lambda3 = lambda3 / scale\n",
        "\n",
        "    def kld_loss(self, mu, logvar):\n",
        "        return -0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp())           # give here a link where you used it from. It's a cool simple idea\n",
        "\n",
        "    def g_adv_loss(self, discriminator_preds):\n",
        "        adv_loss = 0.0\n",
        "        for preds in discriminator_preds:\n",
        "            pred = preds[-1]\n",
        "            adv_loss += -pred.mean()\n",
        "        return adv_loss\n",
        "\n",
        "    def d_adv_loss(self, discriminator_preds, is_real):\n",
        "        adv_loss = 0.0\n",
        "        for preds in discriminator_preds:\n",
        "            pred = preds[-1]\n",
        "            target = -1 + pred if is_real else -1 - pred\n",
        "            mask = target < 0\n",
        "            adv_loss += (mask * target).mean()\n",
        "        return adv_loss\n",
        "\n",
        "    def fm_loss(self, real_preds, fake_preds):\n",
        "        fm_loss = 0.0\n",
        "        for real_features, fake_features in zip(real_preds, fake_preds):\n",
        "            for real_feature, fake_feature in zip(real_features, fake_features):\n",
        "                fm_loss += F.l1_loss(real_feature.detach(), fake_feature)\n",
        "        return fm_loss\n",
        "\n",
        "    def vgg_loss(self, x_real, x_fake):\n",
        "        vgg_real = self.vgg(x_real)\n",
        "        vgg_fake = self.vgg(x_fake)\n",
        "\n",
        "        vgg_loss = 0.0\n",
        "        for real, fake, weight in zip(vgg_real, vgg_fake, self.vgg_weights):\n",
        "            vgg_loss += weight * F.l1_loss(real.detach(), fake)\n",
        "        return vgg_loss\n",
        "\n",
        "    def forward(self, x_real, label_map, gaugan):\n",
        "        '''\n",
        "        Function that computes the forward pass and total loss for GauGAN.\n",
        "        '''\n",
        "        mu, logvar = gaugan.encode(x_real)\n",
        "        z = gaugan.sample_z(mu, logvar)\n",
        "        x_fake = gaugan.generate(z, label_map)\n",
        "\n",
        "        # Get necessary outputs for loss/backprop for both generator and discriminator\n",
        "        fake_preds_for_g = gaugan.discriminate(x_fake, label_map)\n",
        "        fake_preds_for_d = gaugan.discriminate(x_fake.detach(), label_map)\n",
        "        real_preds_for_d = gaugan.discriminate(x_real.detach(), label_map)\n",
        "\n",
        "        g_loss = (\n",
        "            self.lambda0 * self.g_adv_loss(fake_preds_for_g) + \\\n",
        "            self.lambda1 * self.fm_loss(real_preds_for_d, fake_preds_for_g) / gaugan.n_disc + \\\n",
        "            self.lambda2 * self.vgg_loss(x_fake, x_real) + \\\n",
        "            self.lambda3 * self.kld_loss(mu, logvar)\n",
        "        )\n",
        "        d_loss = 0.5 * (\n",
        "            self.d_adv_loss(real_preds_for_d, True) + \\\n",
        "            self.d_adv_loss(fake_preds_for_d, False)\n",
        "        )\n",
        "\n",
        "        return g_loss, d_loss, x_fake.detach()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG1ZIAI8DgJX"
      },
      "source": [
        "def lr_lambda(epoch):\n",
        "    ''' Function for scheduling learning rate '''\n",
        "    return 1. if epoch < decay_after else 1 - float(epoch - decay_after) / (epochs - decay_after)\n",
        "\n",
        "def weights_init(m):\n",
        "    ''' Function for initializing all model weights '''\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weights_init)\n",
        "\n",
        "# Initialize model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gaugan_config = {\n",
        "    'n_classes': 35,\n",
        "    'spatial_size': (128, 256), # Default (256, 512): halve size for memory\n",
        "    'base_channels': 32,        # Default 64: halve channels for memory\n",
        "    'z_dim': 256,\n",
        "    'n_upsample': 5,            # Default 6: decrease layers for memory\n",
        "    'n_disc_layers': 2,\n",
        "    'n_disc': 3,\n",
        "}\n",
        "gaugan = GauGAN(**gaugan_config).to(device)\n",
        "loss = Loss(device=device)\n",
        "\n",
        "# Initialize dataloader\n",
        "train_dir = ['data']\n",
        "batch_size = 16                 # Default 32: decrease for memory\n",
        "dataset = CityscapesDataset(\n",
        "    train_dir, img_size=gaugan_config['spatial_size'], n_classes=gaugan_config['n_classes'],\n",
        ")\n",
        "dataloader = DataLoader(\n",
        "    dataset, collate_fn=CityscapesDataset.collate_fn,\n",
        "    batch_size=batch_size, shuffle=True,\n",
        "    drop_last=False, pin_memory=True,\n",
        ")\n",
        "\n",
        "# Initialize optimizers + schedulers\n",
        "epochs = 200                    # total number of train epochs\n",
        "decay_after = 100               # number of epochs with constant lr\n",
        "betas = (0.0, 0.999)\n",
        "\n",
        "g_params = list(gaugan.generator.parameters()) + list(gaugan.encoder.parameters())\n",
        "d_params = list(gaugan.discriminator.parameters())\n",
        "\n",
        "g_optimizer = torch.optim.Adam(g_params, lr=1e-4, betas=betas)\n",
        "d_optimizer = torch.optim.Adam(d_params, lr=4e-4, betas=betas)\n",
        "g_scheduler = torch.optim.lr_scheduler.LambdaLR(g_optimizer, lr_lambda)\n",
        "d_scheduler = torch.optim.lr_scheduler.LambdaLR(d_optimizer, lr_lambda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-2lU4TnK5ik"
      },
      "source": [
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parse torch version for autocast\n",
        "version = torch.__version__\n",
        "version = tuple(int(n) for n in version.split('.')[:-1])\n",
        "has_autocast = version >= (1, 6)\n",
        "#\n",
        "\n",
        "def show_tensor_images(image_tensor):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:1], nrow=1)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def train(dataloader, gaugan, optimizers, schedulers, device):\n",
        "    g_optimizer, d_optimizer = optimizers\n",
        "    g_scheduler, d_scheduler = schedulers\n",
        "\n",
        "    cur_step = 0\n",
        "    display_step = 100\n",
        "\n",
        "    mean_g_loss = 0.0\n",
        "    mean_d_loss = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training epoch\n",
        "        for (x_real, labels) in tqdm(dataloader, position=0):\n",
        "            x_real = x_real.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    g_loss, d_loss, x_fake = loss(x_real, labels, gaugan)\n",
        "            else:\n",
        "                g_loss, d_loss, x_fake = loss(x_real, labels, gaugan)\n",
        "\n",
        "            g_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            d_optimizer.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            mean_g_loss += g_loss.item() / display_step\n",
        "            mean_d_loss += d_loss.item() / display_step\n",
        "\n",
        "            if cur_step % display_step == 0 and cur_step > 0:\n",
        "                print('Step {}: Generator loss: {:.5f}, Discriminator loss: {:.5f}'\n",
        "                      .format(cur_step, mean_g_loss, mean_d_loss))\n",
        "                show_tensor_images(x_fake.to(x_real.dtype))\n",
        "                show_tensor_images(x_real)\n",
        "                mean_g_loss = 0.0\n",
        "                mean_d_loss = 0.0\n",
        "            cur_step += 1\n",
        "\n",
        "        g_scheduler.step()\n",
        "        d_scheduler.step()\n",
        "\n",
        "train(\n",
        "    dataloader, gaugan,\n",
        "    [g_optimizer, d_optimizer],\n",
        "    [g_scheduler, d_scheduler],\n",
        "    device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}