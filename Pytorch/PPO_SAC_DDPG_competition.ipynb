{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# References: \n",
        "\n",
        "https://arxiv.org/pdf/1707.06347.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1509.02971.pdf\n",
        "\n",
        "https://arxiv.org/pdf/1812.05905.pdf\n",
        "\n",
        "https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py\n",
        "\n",
        "https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch\n",
        "\n",
        "\n",
        "https://spinningup.openai.com/en/latest/algorithms/ppo.html ppo, sac, ddpg\n",
        "\n",
        "`helper references`\n",
        "\n",
        "https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0\n",
        "\n",
        "https://keras.io/examples/rl/\n"
      ],
      "metadata": {
        "id": "4ZqQ30Qjz5ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "import random\n",
        "from random import randint\n",
        "from collections import namedtuple, deque\n",
        "from contextlib import closing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from IPython.display import clear_output\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim import optimizer\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Normal, Categorical\n",
        "\n",
        "from torch.multiprocessing import Pool, Process, set_start_method\n",
        "## workaround for cuda multiprocessing problem + if loading datase use num_workers=0 and then move to GPU\n",
        "## same is true for everything --> first initialize normally on CPU only for usage move to GPU\n",
        "## https://github.com/pytorch/pytorch/issues/40403 \n",
        "## But it extremely slows down the training process on GPU or stucks rather if name==main: does not help --> TODO: find ways to optimize for GPU\n",
        "# torch.multiprocessing.set_start_method('spawn')     \n",
        "\n",
        "import gym\n",
        "from gym import wrappers"
      ],
      "metadata": {
        "id": "Ytg4WL7i-s8u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title environment\n",
        "\n",
        "### TODO: follow the documetation but implement the environment by yourself from scratch\n",
        "\n",
        "### take closer look to the environment in the documentation: https://github.com/openai/gym/blob/master/gym/envs/classic_control/continuous_mountain_car.py\n",
        "# http://incompleteideas.net/sutton/MountainCar/MountainCar1.cp\n",
        "# permalink: https://perma.cc/6Z2N-PFWC\n",
        "\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "!pip install gym[classic_control]\n",
        "environment = gym.make('MountainCarContinuous-v0', render_mode='human').env  \n",
        "environment.reset() \n",
        "environment.render()\n",
        "\n",
        "print(f\"state space: {environment.observation_space}\")\n",
        "print(f\"action space: {environment.action_space}\")"
      ],
      "metadata": {
        "id": "-ij015qPQnN9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent Neural Network  \n",
        "\n",
        "\n",
        "\n",
        "class AgentNN(nn.Module):\n",
        "    \"\"\"Creates a PyTorch neural network\n",
        "    Args:\n",
        "        - input_dim: Integer to indicate the dimension of the input into the network\n",
        "        - layers_info: List of integers to indicate the width and number of linear layers you want in your network,\n",
        "                      e.g. [5, 8, 1] would produce a network with 3 linear layers of width 5, 8 and then 1\n",
        "        - hidden_activations: String or list of string to indicate the activations you want used on the output of hidden layers\n",
        "                              (not including the output layer). Default is ReLU.\n",
        "        - output_activation: String to indicate the activation function you want the output to go through. Provide a list of\n",
        "                             strings if you want multiple output heads\n",
        "        - dropout: Float to indicate what dropout probability you want applied after each hidden layer\n",
        "        - initialiser: String to indicate which initialiser you want used to initialise all the parameters. All PyTorch\n",
        "                       initialisers are supported. PyTorch's default initialisation is the default.\n",
        "        - batch_norm: Boolean to indicate whether you want batch norm applied to the output of every hidden layer. Default is False\n",
        "        - columns_of_data_to_be_embedded: List to indicate the columns numbers of the data that you want to be put through an embedding layer\n",
        "                                          before being fed through the other layers of the network. Default option is no embeddings\n",
        "        - embedding_dimensions: If you have categorical variables you want embedded before flowing through the network then\n",
        "                                you specify the embedding dimensions here with a list like so: [ [embedding_input_dim_1, embedding_output_dim_1],\n",
        "                                [embedding_input_dim_2, embedding_output_dim_2] ...]. Default is no embeddings\n",
        "        - y_range: Tuple of float or integers of the form (y_lower, y_upper) indicating the range you want to restrict the\n",
        "                   output values to in regression tasks. Default is no range restriction\n",
        "        - random_seed: Integer to indicate the random seed you want to use\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, layers_info, output_activation=None,\n",
        "                 hidden_activations=nn.ReLU(), dropout=0.0, initialiser=\"default\", batch_norm=False,\n",
        "                 columns_of_data_to_be_embedded=[], embedding_dimensions=[], y_range= (), random_seed=0):\n",
        "        nn.Module.__init__(self)\n",
        "\n",
        "        self.embedding_dimensions = embedding_dimensions\n",
        "        self.set_all_random_seeds(random_seed)\n",
        "        self.input_dim = input_dim\n",
        "        self.layers_info = layers_info\n",
        "        self.hidden_activations = hidden_activations\n",
        "        self.output_activation = output_activation\n",
        "        self.dropout = dropout\n",
        "        self.initialiser = initialiser\n",
        "        self.batch_norm = batch_norm\n",
        "        self.y_range = y_range\n",
        "        self.hidden_layers = self.create_hidden_layers()\n",
        "        self.output_layers = self.create_output_layers()\n",
        "        self.dropout_layer = self.create_dropout_layer()\n",
        "        if self.batch_norm: self.batch_norm_layers = self.create_batch_norm_layers()\n",
        "        self.checked_forward_input_data_once = False\n",
        "\n",
        "        self.embedding_to_occur = len(columns_of_data_to_be_embedded) > 0\n",
        "        self.columns_of_data_to_be_embedded = columns_of_data_to_be_embedded\n",
        "        self.embedding_layers = self.create_embedding_layers()\n",
        "        self.initialise_all_parameters()\n",
        "\n",
        "    def set_all_random_seeds(self, random_seed):\n",
        "        \"\"\"Sets all possible random seeds so results can be reproduced\"\"\"\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.manual_seed(random_seed)\n",
        "        random.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    def create_dropout_layer(self):\n",
        "        \"\"\"Creates a dropout layer\"\"\"\n",
        "        return nn.Dropout(p=self.dropout)\n",
        "\n",
        "    def create_embedding_layers(self):\n",
        "        \"\"\"Creates the embedding layers in the network\"\"\"\n",
        "        embedding_layers = nn.ModuleList([])\n",
        "        for embedding_dimension in self.embedding_dimensions:\n",
        "            input_dim, output_dim = embedding_dimension\n",
        "            embedding_layers.extend([nn.Embedding(input_dim, output_dim)])\n",
        "        return embedding_layers\n",
        "\n",
        "    def initialise_parameters(self, parameters_list):\n",
        "        \"\"\"Initialises the list of parameters given\"\"\"\n",
        "        if self.initialiser != \"default\":\n",
        "            for parameters in parameters_list:\n",
        "                if type(parameters) == nn.Linear or type(parameters) == nn.Conv2d:\n",
        "                    self.initialiser(parameters.weight)\n",
        "                elif type(parameters) in [nn.LSTM, nn.RNN, nn.GRU]:\n",
        "                    self.initialiser(parameters.weight_hh_l0)\n",
        "                    self.initialiser(parameters.weight_ih_l0)\n",
        "\n",
        "    def flatten_tensor(self, tensor):\n",
        "        \"\"\"Flattens a tensor of shape (a, b, c, d, ...) into shape (a, b * c * d * .. )\"\"\"\n",
        "        return tensor.reshape(tensor.shape[0], -1)\n",
        "\n",
        "    def print_model_summary(self):\n",
        "        print(self)   \n",
        "    \n",
        "    \n",
        "    def check_all_user_inputs_valid(self):\n",
        "        \"\"\"Checks that all the user inputs were valid\"\"\"\n",
        "        self.check_NN_input_dim_valid()\n",
        "        self.check_NN_layers_valid()\n",
        "        self.check_activations_valid()\n",
        "        self.check_embedding_dimensions_valid()\n",
        "        self.check_initialiser_valid()\n",
        "        self.check_y_range_values_valid()\n",
        "\n",
        "    def create_hidden_layers(self):\n",
        "        \"\"\"Creates the linear layers in the network\"\"\"\n",
        "        linear_layers = nn.ModuleList([])\n",
        "        input_dim = int(self.input_dim - len(self.embedding_dimensions) + np.sum([output_dims[1] for output_dims in self.embedding_dimensions]))\n",
        "        for hidden_unit in self.layers_info[:-1]:\n",
        "            linear_layers.extend([nn.Linear(input_dim, hidden_unit)])\n",
        "            input_dim = hidden_unit\n",
        "        return linear_layers\n",
        "\n",
        "    def create_output_layers(self):\n",
        "        \"\"\"Creates the output layers in the network\"\"\"\n",
        "        output_layers = nn.ModuleList([])\n",
        "        if len(self.layers_info) >= 2: input_dim = self.layers_info[-2]\n",
        "        else: input_dim = self.input_dim\n",
        "        if not isinstance(self.layers_info[-1], list): output_layer = [self.layers_info[-1]]\n",
        "        else: output_layer = self.layers_info[-1]\n",
        "        for output_dim in output_layer:\n",
        "            output_layers.extend([nn.Linear(input_dim, output_dim)])\n",
        "        return output_layers\n",
        "\n",
        "    def create_batch_norm_layers(self):\n",
        "        \"\"\"Creates the batch norm layers in the network\"\"\"\n",
        "        batch_norm_layers = nn.ModuleList([nn.BatchNorm1d(num_features=hidden_unit) for hidden_unit in self.layers_info[:-1]])\n",
        "        return batch_norm_layers\n",
        "\n",
        "    def initialise_all_parameters(self):\n",
        "        \"\"\"Initialises the parameters in the linear and embedding layers\"\"\"\n",
        "        self.initialise_parameters(self.hidden_layers)\n",
        "        self.initialise_parameters(self.output_layers)\n",
        "        self.initialise_parameters(self.embedding_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass for the network\"\"\"\n",
        "        if not self.checked_forward_input_data_once: self.check_input_data_into_forward_once(x)\n",
        "        if self.embedding_to_occur: x = self.incorporate_embeddings(x)\n",
        "        x = self.process_hidden_layers(x)\n",
        "        out = self.process_output_layers(x)\n",
        "        if self.y_range: out = self.y_range[0] + (self.y_range[1] - self.y_range[0])*nn.Sigmoid()(out)\n",
        "        return out\n",
        "\n",
        "    def check_input_data_into_forward_once(self, x):\n",
        "        \"\"\"Checks the input data into forward is of the right format. Then sets a flag indicating that this has happened once\n",
        "        so that we don't keep checking as this would slow down the model too much\"\"\"\n",
        "        for embedding_dim in self.columns_of_data_to_be_embedded:\n",
        "            data = x[:, embedding_dim]\n",
        "            data_long = data.long()\n",
        "            assert all(data_long >= 0), \"All data to be embedded must be integers 0 and above -- {}\".format(data_long)\n",
        "            assert torch.sum(abs(data.float() - data_long.float())) < 0.0001, \"\"\"Data columns to be embedded should be integer \n",
        "                                                                                values 0 and above to represent the different \n",
        "                                                                                classes\"\"\"\n",
        "        if self.input_dim > len(self.columns_of_data_to_be_embedded):\n",
        "          assert isinstance(x, torch.FloatTensor) or isinstance(x, torch.cuda.FloatTensor), \"Input data must be a float tensor\"\n",
        "        assert len(x.shape) == 2, \"X should be a 2-dimensional tensor: {}\".format(x.shape)\n",
        "        self.checked_forward_input_data_once = True #So that it doesn't check again\n",
        "\n",
        "    def incorporate_embeddings(self, x):\n",
        "        \"\"\"Puts relevant data through embedding layers and then concatenates the result with the rest of the data ready\n",
        "        to then be put through the linear layers\"\"\"\n",
        "        all_embedded_data = []\n",
        "        for embedding_layer_ix, embedding_var in enumerate(self.columns_of_data_to_be_embedded):\n",
        "            data = x[:, embedding_var].long()\n",
        "            embedded_data = self.embedding_layers[embedding_layer_ix](data)\n",
        "            all_embedded_data.append(embedded_data)\n",
        "        all_embedded_data = torch.cat(tuple(all_embedded_data), dim=1)\n",
        "        x = torch.cat((x[:, [col for col in range(x.shape[1]) if col not in self.columns_of_data_to_be_embedded]].float(), all_embedded_data), dim=1)\n",
        "        return x\n",
        "\n",
        "    def get_activation(self, activations, ix=None):\n",
        "        \"\"\"Gets the activation function\"\"\"\n",
        "        if isinstance(activations, list):\n",
        "            return activations[ix]\n",
        "        return activations\n",
        "\n",
        "\n",
        "    def process_hidden_layers(self, x):\n",
        "        \"\"\"Puts the data x through all the hidden layers\"\"\"\n",
        "        for layer_ix, linear_layer in enumerate(self.hidden_layers):\n",
        "            x = self.get_activation(self.hidden_activations, layer_ix)(linear_layer(x))\n",
        "            if self.batch_norm: x = self.batch_norm_layers[layer_ix](x)\n",
        "            if self.dropout != 0.0: x = self.dropout_layer(x)\n",
        "        return x\n",
        "\n",
        "    def process_output_layers(self, x):\n",
        "        \"\"\"Puts the data x through all the output layers\"\"\"\n",
        "        out = None\n",
        "        for output_layer_ix, output_layer in enumerate(self.output_layers):\n",
        "            activation = self.get_activation(self.output_activation, output_layer_ix)\n",
        "            temp_output = output_layer(x)\n",
        "            if activation is not None: temp_output = activation(temp_output)\n",
        "            if out is None: out = temp_output\n",
        "            else: out = torch.cat((out, temp_output), dim=1)\n",
        "        return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nTAytmPsPeMd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XcNYOU1hPscJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Agent base class and essential parts for agents including reply buffer class\n",
        "\n",
        "\n",
        "class Config(object):\n",
        "    \"\"\"Object to hold the config requirements for an agent/game\"\"\"\n",
        "    def __init__(self):\n",
        "        self.seed = None\n",
        "        self.environment = None\n",
        "        self.requirements_to_solve_game = None\n",
        "        self.num_episodes_to_run = None\n",
        "        self.file_to_save_data_results = None\n",
        "        self.file_to_save_results_graph = None\n",
        "        self.runs_per_agent = None\n",
        "        self.visualise_overall_results = None\n",
        "        self.visualise_individual_results = None\n",
        "        self.hyperparameters = None\n",
        "        self.use_GPU = None\n",
        "        self.overwrite_existing_results_file = None\n",
        "        self.save_model = False\n",
        "        self.standard_deviation_results = 1.0\n",
        "        self.randomise_random_seed = True\n",
        "        self.show_solution_score = False\n",
        "        self.debug_mode = False\n",
        "\n",
        "\n",
        "class Base_Agent(object):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        # self.logger = self.setup_logger()\n",
        "        self.debug_mode = config.debug_mode\n",
        "        # if self.debug_mode: self.tensorboard = SummaryWriter()\n",
        "        self.config = config\n",
        "        self.set_random_seeds(config.seed)\n",
        "        self.environment = config.environment\n",
        "        self.environment_title = self.get_environment_title()\n",
        "        self.action_types = \"DISCRETE\" if self.environment.action_space.dtype == np.int64 else \"CONTINUOUS\"\n",
        "        self.action_size = int(self.get_action_size())\n",
        "        self.config.action_size = self.action_size\n",
        "\n",
        "        self.lowest_possible_episode_score = self.get_lowest_possible_episode_score()\n",
        "\n",
        "        self.state_size =  int(self.get_state_size())\n",
        "        self.hyperparameters = config.hyperparameters\n",
        "        self.average_score_required_to_win = self.get_score_required_to_win()\n",
        "        # self.rolling_score_window = self.get_trials()\n",
        "        # self.max_steps_per_episode = self.environment.spec.max_episode_steps\n",
        "        self.total_episode_score_so_far = 0\n",
        "        self.game_full_episode_scores = []\n",
        "        self.rolling_results = []\n",
        "        self.max_rolling_score_seen = float(\"-inf\")\n",
        "        self.max_episode_score_seen = float(\"-inf\")\n",
        "        self.episode_number = 0\n",
        "        self.device = \"cuda:0\" if config.use_GPU else \"cpu\"\n",
        "        self.visualise_results_boolean = config.visualise_individual_results\n",
        "        self.global_step_number = 0\n",
        "        self.turn_off_exploration = False\n",
        "        gym.logger.set_level(40)  # stops it from printing an unnecessary warning\n",
        "        self.log_game_info()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Takes a step in the game. This method must be overriden by any agent\"\"\"\n",
        "        raise ValueError(\"Step needs to be implemented by the agent\")\n",
        "\n",
        "    def get_environment_title(self):\n",
        "        \"\"\"Extracts name of environment from it\"\"\"\n",
        "        try:\n",
        "            name = self.environment.unwrapped.id\n",
        "        except AttributeError:\n",
        "            try:\n",
        "                name = self.environment.spec.id.split(\"-\")[0]\n",
        "            except:\n",
        "                name = str(self.environment.env)\n",
        "                if name[0:10] == \"TimeLimit<\": name = name[10:]\n",
        "                name = name.split(\" \")[0]\n",
        "                if name[0] == \"<\": name = name[1:]\n",
        "                if name[-3:] == \"Env\": name = name[:-3]\n",
        "\n",
        "        return name\n",
        "\n",
        "    def get_lowest_possible_episode_score(self):\n",
        "        \"\"\"Returns the lowest possible episode score you can get in an environment\"\"\"\n",
        "        return None\n",
        "\n",
        "    def get_action_size(self):\n",
        "        \"\"\"Gets the action_size for the gym env into the correct shape for a neural network\"\"\"\n",
        "        if \"overwrite_action_size\" in self.config.__dict__: return self.config.overwrite_action_size\n",
        "        if \"action_size\" in self.environment.__dict__: return self.environment.action_size\n",
        "        if self.action_types == \"DISCRETE\": return self.environment.action_space.n\n",
        "        else: return self.environment.action_space.shape[0]\n",
        "\n",
        "    def get_state_size(self):\n",
        "        \"\"\"Gets the state_size for the gym env into the correct shape for a neural network\"\"\"\n",
        "        random_state = self.environment.reset() ## fix for deprecation error obs, info = self.env.reset(seed=seed, return_info=True, options=options) \n",
        "        if isinstance(random_state, dict):\n",
        "            state_size = random_state[\"observation\"].shape[0] + random_state[\"desired_goal\"].shape[0]\n",
        "            return state_size\n",
        "        else:\n",
        "            return random_state.size\n",
        "\n",
        "    def get_score_required_to_win(self):\n",
        "        \"\"\"Gets average score required to win game\"\"\"\n",
        "        print(\"TITLE \", self.environment_title)\n",
        "        try: return self.environment.unwrapped.reward_threshold\n",
        "        except AttributeError:\n",
        "            try:\n",
        "                return self.environment.spec.reward_threshold\n",
        "            except AttributeError:\n",
        "                return self.environment.unwrapped.spec.reward_threshold\n",
        "\n",
        "    # def get_trials(self):\n",
        "    #     \"\"\"Gets the number of trials to average a score over\"\"\"\n",
        "    #     if self.environment_title in [\"AntMaze\", \"FetchReach\", \"Hopper\", \"Walker2d\", \"CartPole\"]: return 100\n",
        "    #     try: return self.environment.unwrapped.trials\n",
        "    #     except AttributeError: return self.environment.spec.trials\n",
        "\n",
        "\n",
        "    def log_game_info(self):\n",
        "        \"\"\"Logs info relating to the game\"\"\"\n",
        "        for ix, param in enumerate([self.environment_title, self.action_types, self.action_size, self.lowest_possible_episode_score,\n",
        "                      self.state_size, self.hyperparameters, self.average_score_required_to_win, ##self.rolling_score_window,\n",
        "                      self.device]):\n",
        "            # self.logger.info(\"{} -- {}\".format(ix, param))\n",
        "            print(\"{} -- {}\".format(ix, param))\n",
        "\n",
        "    def set_random_seeds(self, random_seed):\n",
        "        \"\"\"Sets all possible random seeds so results can be reproduced\"\"\"\n",
        "        os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.manual_seed(random_seed)\n",
        "        # tf.set_random_seed(random_seed)\n",
        "        random.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.manual_seed_all(random_seed)\n",
        "            torch.cuda.manual_seed(random_seed)\n",
        "        if hasattr(gym.spaces, 'prng'):\n",
        "            gym.spaces.prng.seed(random_seed)\n",
        "\n",
        "    def reset_game(self):\n",
        "        \"\"\"Resets the game information so we are ready to play a new episode\"\"\"\n",
        "        self.environment.seed(self.config.seed)\n",
        "        self.state = self.environment.reset()  ## fix for deprecation error obs, info = self.env.reset(seed=seed, return_info=True, options=options)\n",
        "        self.next_state = None\n",
        "        self.action = None\n",
        "        self.reward = None\n",
        "        self.done = False\n",
        "        self.total_episode_score_so_far = 0\n",
        "        self.episode_states = []\n",
        "        self.episode_rewards = []\n",
        "        self.episode_actions = []\n",
        "        self.episode_next_states = []\n",
        "        self.episode_dones = []\n",
        "        self.episode_desired_goals = []\n",
        "        self.episode_achieved_goals = []\n",
        "        self.episode_observations = []\n",
        "        if \"exploration_strategy\" in self.__dict__.keys(): self.exploration_strategy.reset()\n",
        "        ##self.logger.info(\"Reseting game -- New start state {}\".format(self.state))\n",
        "        print(\"Reseting game -- New start state {}\".format(self.state))\n",
        "\n",
        "    def track_episodes_data(self):\n",
        "        \"\"\"Saves the data from the recent episodes\"\"\"\n",
        "        self.episode_states.append(self.state)\n",
        "        self.episode_actions.append(self.action)\n",
        "        self.episode_rewards.append(self.reward)\n",
        "        self.episode_next_states.append(self.next_state)\n",
        "        self.episode_dones.append(self.done)\n",
        "\n",
        "    def run_n_episodes(self, num_episodes=None, show_whether_achieved_goal=True, save_and_print_results=True):\n",
        "        \"\"\"Runs game to completion n times and then summarises results and saves model (if asked to)\"\"\"\n",
        "        if num_episodes is None: num_episodes = self.config.num_episodes_to_run\n",
        "        start = time.time()\n",
        "        while self.episode_number < num_episodes:\n",
        "            self.reset_game()\n",
        "            self.step()\n",
        "            if save_and_print_results: self.save_and_print_result()\n",
        "        time_taken = time.time() - start\n",
        "        if show_whether_achieved_goal: self.show_whether_achieved_goal()\n",
        "        if self.config.save_model: self.locally_save_policy()\n",
        "\n",
        "        return self.game_full_episode_scores, self.rolling_results, time_taken\n",
        "\n",
        "\n",
        "    def conduct_action(self, action):\n",
        "        \"\"\"Conducts an action in the environment\"\"\"\n",
        "        self.next_state, self.reward, self.done, _ = self.environment.step(action)\n",
        "        self.total_episode_score_so_far += self.reward\n",
        "        if self.hyperparameters[\"clip_rewards\"]: self.reward =  max(min(self.reward, 1.0), -1.0)\n",
        "\n",
        "\n",
        "    def save_and_print_result(self):\n",
        "        \"\"\"Saves and prints results of the game\"\"\"\n",
        "        self.save_result()\n",
        "        self.print_rolling_result()\n",
        "\n",
        "    def save_result(self):\n",
        "        \"\"\"Saves the result of an episode of the game\"\"\"\n",
        "        self.game_full_episode_scores.append(self.total_episode_score_so_far)\n",
        "        ##self.rolling_results.append(np.mean(self.game_full_episode_scores[-1 * self.rolling_score_window:]))\n",
        "        self.rolling_results.append(np.mean(self.game_full_episode_scores[-1:]))\n",
        "        self.save_max_result_seen()\n",
        "\n",
        "    def save_max_result_seen(self):\n",
        "        \"\"\"Updates the best episode result seen so far\"\"\"\n",
        "        if self.game_full_episode_scores[-1] > self.max_episode_score_seen:\n",
        "            self.max_episode_score_seen = self.game_full_episode_scores[-1]\n",
        "\n",
        "        # if self.rolling_results[-1] > self.max_rolling_score_seen:\n",
        "        #     if len(self.rolling_results) > self.rolling_score_window:\n",
        "        #         self.max_rolling_score_seen = self.rolling_results[-1]\n",
        "\n",
        "    def print_rolling_result(self):\n",
        "        \"\"\"Prints out the latest episode results\"\"\"\n",
        "        text = \"\"\"\"\\r Episode {0}, Score: {3: .2f}, Max score seen: {4: .2f}, Rolling score: {1: .2f}, Max rolling score seen: {2: .2f}\"\"\"\n",
        "        sys.stdout.write(text.format(len(self.game_full_episode_scores), self.rolling_results[-1], self.max_rolling_score_seen,\n",
        "                                     self.game_full_episode_scores[-1], self.max_episode_score_seen))       \n",
        "        sys.stdout.flush()\n",
        "        \n",
        "\n",
        "    def show_whether_achieved_goal(self):\n",
        "        \"\"\"Prints out whether the agent achieved the environment target goal\"\"\"\n",
        "        index_achieved_goal = self.achieved_required_score_at_index()\n",
        "        print(\" \")\n",
        "        if index_achieved_goal == -1: #this means agent never achieved goal\n",
        "            print(\"\\033[91m\" + \"\\033[1m\" +\n",
        "                  \"{} did not achieve required score \\n\".format(self.agent_name) +\n",
        "                  \"\\033[0m\" + \"\\033[0m\")\n",
        "        else:\n",
        "            print(\"\\033[92m\" + \"\\033[1m\" +\n",
        "                  \"{} achieved required score at episode {} \\n\".format(self.agent_name, index_achieved_goal) +\n",
        "                  \"\\033[0m\" + \"\\033[0m\")\n",
        "\n",
        "    def achieved_required_score_at_index(self):\n",
        "        \"\"\"Returns the episode at which agent achieved goal or -1 if it never achieved it\"\"\"\n",
        "        for ix, score in enumerate(self.rolling_results):\n",
        "            if score > self.average_score_required_to_win:\n",
        "                return ix\n",
        "        return -1\n",
        "\n",
        "    def update_learning_rate(self, starting_lr,  optimizer):\n",
        "        \"\"\"Lowers the learning rate according to how close we are to the solution\"\"\"\n",
        "        if len(self.rolling_results) > 0:\n",
        "            last_rolling_score = self.rolling_results[-1]\n",
        "            if last_rolling_score > 0.75 * self.average_score_required_to_win:\n",
        "                new_lr = starting_lr / 100.0\n",
        "            elif last_rolling_score > 0.6 * self.average_score_required_to_win:\n",
        "                new_lr = starting_lr / 20.0\n",
        "            elif last_rolling_score > 0.5 * self.average_score_required_to_win:\n",
        "                new_lr = starting_lr / 10.0\n",
        "            elif last_rolling_score > 0.25 * self.average_score_required_to_win:\n",
        "                new_lr = starting_lr / 2.0\n",
        "            else:\n",
        "                new_lr = starting_lr\n",
        "            for g in optimizer.param_groups:\n",
        "                g['lr'] = new_lr\n",
        "        if random.random() < 0.001: self.logger.info(\"Learning rate {}\".format(new_lr))\n",
        "\n",
        "\n",
        "    def enough_experiences_to_learn_from(self):\n",
        "        \"\"\"Boolean indicated whether there are enough experiences in the memory buffer to learn from\"\"\"\n",
        "        return len(self.memory) > self.hyperparameters[\"batch_size\"]\n",
        "\n",
        "    def save_experience(self, memory=None, experience=None):\n",
        "        \"\"\"Saves the recent experience to the memory buffer\"\"\"\n",
        "        if memory is None: memory = self.memory\n",
        "        if experience is None: experience = self.state, self.action, self.reward, self.next_state, self.done\n",
        "        memory.add_experience(*experience)\n",
        "\n",
        "    def take_optimisation_step(self, optimizer, network, loss, clipping_norm=None, retain_graph=False):\n",
        "        \"\"\"Takes an optimisation step by calculating gradients given the loss and then updating the parameters\"\"\"\n",
        "        if not isinstance(network, list): network = [network]\n",
        "        optimizer.zero_grad() #reset gradients to 0\n",
        "        loss.backward(retain_graph=retain_graph) #this calculates the gradients\n",
        "        ##self.logger.info(\"Loss -- {}\".format(loss.item()))\n",
        "        print(\"Loss -- {}\".format(loss.item()))\n",
        "        if self.debug_mode: self.log_gradient_and_weight_information(network, optimizer)\n",
        "        if clipping_norm is not None:\n",
        "            for net in network:\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(), clipping_norm) #clip gradients to help stabilise training\n",
        "        optimizer.step() #this applies the gradients\n",
        "\n",
        "    def log_gradient_and_weight_information(self, network, optimizer):\n",
        "\n",
        "        # log weight information\n",
        "        total_norm = 0\n",
        "        for name, param in network.named_parameters():\n",
        "            param_norm = param.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "        total_norm = total_norm ** (1. / 2)\n",
        "        self.logger.info(\"Gradient Norm {}\".format(total_norm))\n",
        "\n",
        "        for g in optimizer.param_groups:\n",
        "            learning_rate = g['lr']\n",
        "            break\n",
        "        #self.logger.info(\"Learning Rate {}\".format(learning_rate))\n",
        "        print(\"Learning Rate {}\".format(learning_rate))\n",
        "\n",
        "\n",
        "    def soft_update_of_target_network(self, local_model, target_model, tau):\n",
        "        \"\"\"Updates the target network in the direction of the local network but by taking a step size\n",
        "        less than one so the target network's parameter values trail the local networks. This helps stabilise training\"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
        "\n",
        "    def create_NN(self, input_dim, output_dim, key_to_use=None, override_seed=None, hyperparameters=None):\n",
        "        \"\"\"Creates a neural network for the agents to use\"\"\"\n",
        "        if hyperparameters is None: hyperparameters = self.hyperparameters\n",
        "        if key_to_use: hyperparameters = hyperparameters[key_to_use]\n",
        "        if override_seed: seed = override_seed\n",
        "        else: seed = self.config.seed\n",
        "\n",
        "        default_hyperparameter_choices = {\"output_activation\": None, \"hidden_activations\": nn.ReLU(), \"dropout\": 0.0,\n",
        "                                          \"initialiser\": \"default\", \"batch_norm\": False,\n",
        "                                          \"columns_of_data_to_be_embedded\": [],\n",
        "                                          \"embedding_dimensions\": [], \"y_range\": ()}\n",
        "\n",
        "        for key in default_hyperparameter_choices:\n",
        "            if key not in hyperparameters.keys():\n",
        "                hyperparameters[key] = default_hyperparameter_choices[key]\n",
        "\n",
        "        return AgentNN(input_dim=input_dim, layers_info=hyperparameters[\"linear_hidden_units\"] + [output_dim],\n",
        "                  output_activation=hyperparameters[\"final_layer_activation\"],\n",
        "                  batch_norm=hyperparameters[\"batch_norm\"], dropout=hyperparameters[\"dropout\"],\n",
        "                  hidden_activations=hyperparameters[\"hidden_activations\"], initialiser=hyperparameters[\"initialiser\"],\n",
        "                  columns_of_data_to_be_embedded=hyperparameters[\"columns_of_data_to_be_embedded\"],\n",
        "                  embedding_dimensions=hyperparameters[\"embedding_dimensions\"], y_range=hyperparameters[\"y_range\"],\n",
        "                  random_seed=seed).to(self.device)\n",
        "\n",
        "    def turn_on_any_epsilon_greedy_exploration(self):\n",
        "        \"\"\"Turns off all exploration with respect to the epsilon greedy exploration strategy\"\"\"\n",
        "        print(\"Turning on epsilon greedy exploration\")\n",
        "        self.turn_off_exploration = False\n",
        "\n",
        "    def turn_off_any_epsilon_greedy_exploration(self):\n",
        "        \"\"\"Turns off all exploration with respect to the epsilon greedy exploration strategy\"\"\"\n",
        "        print(\"Turning off epsilon greedy exploration\")\n",
        "        self.turn_off_exploration = True\n",
        "\n",
        "    def freeze_all_but_output_layers(self, network):\n",
        "        \"\"\"Freezes all layers except the output layer of a network\"\"\"\n",
        "        print(\"Freezing hidden layers\")\n",
        "        for param in network.named_parameters():\n",
        "            param_name = param[0]\n",
        "            assert \"hidden\" in param_name or \"output\" in param_name or \"embedding\" in param_name, \"Name {} of network layers not understood\".format(param_name)\n",
        "            if \"output\" not in param_name:\n",
        "                param[1].requires_grad = False\n",
        "\n",
        "    def unfreeze_all_layers(self, network):\n",
        "        \"\"\"Unfreezes all layers of a network\"\"\"\n",
        "        print(\"Unfreezing all layers\")\n",
        "        for param in network.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    @staticmethod\n",
        "    def move_gradients_one_model_to_another(from_model, to_model, set_from_gradients_to_zero=False):\n",
        "        \"\"\"Copies gradients from from_model to to_model\"\"\"\n",
        "        for from_model, to_model in zip(from_model.parameters(), to_model.parameters()):\n",
        "            to_model._grad = from_model.grad.clone()\n",
        "            if set_from_gradients_to_zero: from_model._grad = None\n",
        "\n",
        "    @staticmethod\n",
        "    def copy_model_over(from_model, to_model):\n",
        "        \"\"\"Copies model parameters from from_model to to_model\"\"\"\n",
        "        for to_model, from_model in zip(to_model.parameters(), from_model.parameters()):\n",
        "            to_model.data.copy_(from_model.data.clone())\n",
        "\n",
        "\n",
        "\n",
        "### ----------------------------------- Replay Buffer\n",
        "\n",
        "class Replay_Buffer(object):\n",
        "    \"\"\"Replay buffer to store past experiences that the agent can then use for training data\"\"\"\n",
        "    \n",
        "    def __init__(self, buffer_size, batch_size, seed, device=None):\n",
        "\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "        if device:\n",
        "            self.device = torch.device(device)\n",
        "        else:\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def add_experience(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"Adds experience(s) into the replay buffer\"\"\"\n",
        "        if type(dones) == list:\n",
        "            assert type(dones[0]) != list, \"A done shouldn't be a list\"\n",
        "            experiences = [self.experience(state, action, reward, next_state, done)\n",
        "                           for state, action, reward, next_state, done in\n",
        "                           zip(states, actions, rewards, next_states, dones)]\n",
        "            self.memory.extend(experiences)\n",
        "        else:\n",
        "            experience = self.experience(states, actions, rewards, next_states, dones)\n",
        "            self.memory.append(experience)\n",
        "   \n",
        "    def sample(self, num_experiences=None, separate_out_data_types=True):\n",
        "        \"\"\"Draws a random sample of experience from the replay buffer\"\"\"\n",
        "        experiences = self.pick_experiences(num_experiences)\n",
        "        if separate_out_data_types:\n",
        "            states, actions, rewards, next_states, dones = self.separate_out_data_types(experiences)\n",
        "            return states, actions, rewards, next_states, dones\n",
        "        else:\n",
        "            return experiences\n",
        "            \n",
        "    def separate_out_data_types(self, experiences):\n",
        "        \"\"\"Puts the sampled experience into the correct format for a PyTorch neural network\"\"\"\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([int(e.done) for e in experiences if e is not None])).float().to(self.device)\n",
        "        \n",
        "        return states, actions, rewards, next_states, dones\n",
        "    \n",
        "    def pick_experiences(self, num_experiences=None):\n",
        "        if num_experiences is not None: batch_size = num_experiences\n",
        "        else: batch_size = self.batch_size\n",
        "        return random.sample(self.memory, k=batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "### ------------------------------- Ornstein-Uhlenbeck Noise\n",
        "\n",
        "class OU_Noise(object):\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
        "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.seed = random.seed(seed)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        dx = self.theta * (self.mu - self.state) + self.sigma * np.array([np.random.normal() for _ in range(len(self.state))])\n",
        "        self.state += dx\n",
        "        return self.state\n",
        "\n",
        "### ----------------------------- Backbone of Exploration Strategies\n",
        "\n",
        "class Base_Exploration_Strategy(object):\n",
        "    \"\"\"Base abstract class for agent exploration strategies. Every exploration strategy must inherit from this class\n",
        "    and implement the methods perturb_action_for_exploration_purposes and add_exploration_rewards\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def perturb_action_for_exploration_purposes(self, action_info):\n",
        "        \"\"\"Perturbs the action of the agent to encourage exploration\"\"\"\n",
        "        raise ValueError(\"Must be implemented\")\n",
        "\n",
        "    def add_exploration_rewards(self, reward_info):\n",
        "        \"\"\"Actions intrinsic rewards to encourage exploration\"\"\"\n",
        "        raise ValueError(\"Must be implemented\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the noise process\"\"\"\n",
        "        raise ValueError(\"Must be implemented\")\n",
        "\n",
        "\n",
        "### ------------------------------ Ornstein-Uhlenbeck Noise Exploration\n",
        "\n",
        "class OU_Noise_Exploration(Base_Exploration_Strategy):\n",
        "    \"\"\"Ornstein-Uhlenbeck noise process exploration strategy\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.noise = OU_Noise(self.config.action_size, self.config.seed, self.config.hyperparameters[\"mu\"],\n",
        "                              self.config.hyperparameters[\"theta\"], self.config.hyperparameters[\"sigma\"])\n",
        "\n",
        "    def perturb_action_for_exploration_purposes(self, action_info):\n",
        "        \"\"\"Perturbs the action of the agent to encourage exploration\"\"\"\n",
        "        action = action_info[\"action\"]\n",
        "        action += self.noise.sample()\n",
        "        return action\n",
        "\n",
        "    def add_exploration_rewards(self, reward_info):\n",
        "        \"\"\"Actions intrinsic rewards to encourage exploration\"\"\"\n",
        "        raise ValueError(\"Must be implemented\")\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the noise process\"\"\"\n",
        "        self.noise.reset()\n",
        "\n",
        "\n",
        "### ------------------------------- Epsylon Greedy Exploration\n",
        "\n",
        "class Epsilon_Greedy_Exploration(Base_Exploration_Strategy):\n",
        "    \"\"\"Implements an epsilon greedy exploration strategy\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.notified_that_exploration_turned_off = False\n",
        "        if \"exploration_cycle_episodes_length\" in self.config.hyperparameters.keys():\n",
        "            print(\"Using a cyclical exploration strategy\")\n",
        "            self.exploration_cycle_episodes_length = self.config.hyperparameters[\"exploration_cycle_episodes_length\"]\n",
        "        else:\n",
        "            self.exploration_cycle_episodes_length = None\n",
        "\n",
        "        if \"random_episodes_to_run\" in self.config.hyperparameters.keys():\n",
        "            self.random_episodes_to_run = self.config.hyperparameters[\"random_episodes_to_run\"]\n",
        "            print(\"Running {} random episodes\".format(self.random_episodes_to_run))\n",
        "        else:\n",
        "            self.random_episodes_to_run = 0\n",
        "\n",
        "    def perturb_action_for_exploration_purposes(self, action_info):\n",
        "        \"\"\"Perturbs the action of the agent to encourage exploration\"\"\"\n",
        "        action_values = action_info[\"action_values\"]\n",
        "        turn_off_exploration = action_info[\"turn_off_exploration\"]\n",
        "        episode_number = action_info[\"episode_number\"]\n",
        "        if turn_off_exploration and not self.notified_that_exploration_turned_off:\n",
        "            print(\" \")\n",
        "            print(\"Exploration has been turned OFF\")\n",
        "            print(\" \")\n",
        "            self.notified_that_exploration_turned_off = True\n",
        "        epsilon = self.get_updated_epsilon_exploration(action_info)\n",
        "\n",
        "\n",
        "        if (random.random() > epsilon or turn_off_exploration) and (episode_number >= self.random_episodes_to_run):\n",
        "            return torch.argmax(action_values).item()\n",
        "        return  np.random.randint(0, action_values.shape[1])\n",
        "\n",
        "    def get_updated_epsilon_exploration(self, action_info, epsilon=1.0):\n",
        "        \"\"\"Gets the probability that we just pick a random action. This probability decays the more episodes we have seen\"\"\"\n",
        "        episode_number = action_info[\"episode_number\"]\n",
        "        epsilon_decay_denominator = self.config.hyperparameters[\"epsilon_decay_rate_denominator\"]\n",
        "\n",
        "        if self.exploration_cycle_episodes_length is None:\n",
        "            epsilon = epsilon / (1.0 + (episode_number / epsilon_decay_denominator))\n",
        "        else:\n",
        "            epsilon = self.calculate_epsilon_with_cyclical_strategy(episode_number)\n",
        "        return epsilon\n",
        "\n",
        "    def calculate_epsilon_with_cyclical_strategy(self, episode_number):\n",
        "        \"\"\"Calculates epsilon according to a cyclical strategy\"\"\"\n",
        "        max_epsilon = 0.5\n",
        "        min_epsilon = 0.001\n",
        "        increment = (max_epsilon - min_epsilon) / float(self.exploration_cycle_episodes_length / 2)\n",
        "        cycle = [ix for ix in range(int(self.exploration_cycle_episodes_length / 2))] + [ix for ix in range(\n",
        "            int(self.exploration_cycle_episodes_length / 2), 0, -1)]\n",
        "        cycle_ix = episode_number % self.exploration_cycle_episodes_length\n",
        "        epsilon = max_epsilon - cycle[cycle_ix] * increment\n",
        "        return epsilon\n",
        "\n",
        "    def add_exploration_rewards(self, reward_info):\n",
        "        \"\"\"Actions intrinsic rewards to encourage exploration\"\"\"\n",
        "        return reward_info[\"reward\"]\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the noise process\"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PPO agent\n",
        "\n",
        "### does not use reply buffer here and uses epsilon greedy exploration\n",
        "\n",
        "\n",
        "def normalise_rewards(rewards):\n",
        "    \"\"\"Normalises rewards to mean 0 and standard deviation 1\"\"\"\n",
        "    mean_reward = np.mean(rewards)\n",
        "    std_reward = np.std(rewards)\n",
        "    return (rewards - mean_reward) / (std_reward + 1e-8) #1e-8 added for stability\n",
        "\n",
        "def create_actor_distribution(action_types, actor_output, action_size):\n",
        "    \"\"\"Creates a distribution that the actor can then use to randomly draw actions\"\"\"\n",
        "    if action_types == \"DISCRETE\":\n",
        "        assert actor_output.size()[1] == action_size, \"Actor output the wrong size\"\n",
        "        action_distribution = Categorical(actor_output)  # this creates a distribution to sample from from torch.distributions\n",
        "    else:\n",
        "        assert actor_output.size()[1] == action_size * 2, \"Actor output the wrong size\"\n",
        "        means = actor_output[:, :action_size].squeeze(0)\n",
        "        stds = actor_output[:,  action_size:].squeeze(0)\n",
        "        if len(means.shape) == 2: means = means.squeeze(-1)\n",
        "        if len(stds.shape) == 2: stds = stds.squeeze(-1)\n",
        "        if len(stds.shape) > 1 or len(means.shape) > 1:\n",
        "            raise ValueError(\"Wrong mean and std shapes - {} -- {}\".format(stds.shape, means.shape))\n",
        "\n",
        "        action_distribution = Normal(means.squeeze(0), torch.abs(stds))                       ## from torch.distributions import normal\n",
        "    return action_distribution\n",
        "\n",
        "\n",
        "class Parallel_Experience_Generator(object):\n",
        "    \"\"\" Plays n episode in parallel using a fixed agent. Only works for PPO or DDPG type agents at the moment, not Q-learning agents\"\"\"\n",
        "    def __init__(self, environment, policy, seed, hyperparameters, action_size, use_GPU=False, action_choice_output_columns=None):\n",
        "        self.use_GPU = use_GPU\n",
        "        self.environment =  environment\n",
        "        self.action_types = \"DISCRETE\" if self.environment.action_space.dtype in [int, 'int64'] else \"CONTINUOUS\"\n",
        "        self.action_size = action_size\n",
        "        self.policy = policy\n",
        "        self.action_choice_output_columns = action_choice_output_columns\n",
        "        self.hyperparameters = hyperparameters\n",
        "        if self.action_types == \"CONTINUOUS\": self.noise = OU_Noise(self.action_size, seed, self.hyperparameters[\"mu\"],\n",
        "                            self.hyperparameters[\"theta\"], self.hyperparameters[\"sigma\"])\n",
        "\n",
        "\n",
        "    def play_n_episodes(self, n, exploration_epsilon=None):\n",
        "        \"\"\"Plays n episodes in parallel using the fixed policy and returns the data\"\"\"\n",
        "        self.exploration_epsilon = exploration_epsilon\n",
        "        with closing(Pool(processes=n)) as pool:\n",
        "            results = pool.map(self, range(n))\n",
        "            pool.terminate()\n",
        "        states_for_all_episodes = [episode[0] for episode in results]\n",
        "        actions_for_all_episodes = [episode[1] for episode in results]\n",
        "        rewards_for_all_episodes = [episode[2] for episode in results]\n",
        "        return states_for_all_episodes, actions_for_all_episodes, rewards_for_all_episodes\n",
        " \n",
        "\n",
        "    def __call__(self, n):\n",
        "        exploration = max(0.0, random.uniform(self.exploration_epsilon / 3.0, self.exploration_epsilon * 3.0))\n",
        "        return self.play_1_episode(exploration)\n",
        "\n",
        "    def play_1_episode(self, epsilon_exploration):\n",
        "        \"\"\"Plays 1 episode using the fixed policy and returns the data\"\"\"\n",
        "        state = self.reset_game()\n",
        "        done = False\n",
        "        episode_states = []\n",
        "        episode_actions = []\n",
        "        episode_rewards = []\n",
        "        while not done:\n",
        "            action = self.pick_action(self.policy, state, epsilon_exploration)\n",
        "            next_state, reward, done, _ = self.environment.step(action)\n",
        "            if self.hyperparameters[\"clip_rewards\"]: reward = max(min(reward, 1.0), -1.0)\n",
        "            episode_states.append(state)\n",
        "            episode_actions.append(action)\n",
        "            episode_rewards.append(reward)\n",
        "            state = next_state\n",
        "        return episode_states, episode_actions, episode_rewards\n",
        "\n",
        "    def reset_game(self):\n",
        "        \"\"\"Resets the game environment so it is ready to play a new episode\"\"\"\n",
        "        seed = randint(0, sys.maxsize)\n",
        "        torch.manual_seed(seed) # Need to do this otherwise each worker generates same experience\n",
        "        state = self.environment.reset()\n",
        "        if self.action_types == \"CONTINUOUS\": self.noise.reset()\n",
        "        return state\n",
        "\n",
        "    def pick_action(self, policy, state, epsilon_exploration=None):\n",
        "        \"\"\"Picks an action using the policy\"\"\"\n",
        "        if self.action_types == \"DISCRETE\":\n",
        "            if random.random() <= epsilon_exploration:\n",
        "                action = random.randint(0, self.action_size - 1)\n",
        "                return action\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        actor_output = policy.forward(state)\n",
        "        if self.action_choice_output_columns is not None:\n",
        "            actor_output = actor_output[:, self.action_choice_output_columns]\n",
        "        action_distribution = create_actor_distribution(self.action_types, actor_output, self.action_size)\n",
        "        action = action_distribution.sample().cpu()\n",
        "\n",
        "        if self.action_types == \"CONTINUOUS\": action += torch.Tensor(self.noise.sample())\n",
        "        else: action = action.item()\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "class PPO(Base_Agent):\n",
        "    \"\"\"Proximal Policy Optimization agent\"\"\"\n",
        "    agent_name = \"PPO\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        Base_Agent.__init__(self, config)\n",
        "        self.policy_output_size = self.calculate_policy_output_size()\n",
        "        self.policy_new = self.create_NN(input_dim=self.state_size, output_dim=self.policy_output_size)\n",
        "        self.policy_old = self.create_NN(input_dim=self.state_size, output_dim=self.policy_output_size)\n",
        "        self.policy_old.load_state_dict(copy.deepcopy(self.policy_new.state_dict()))\n",
        "        self.policy_new_optimizer = optim.Adam(self.policy_new.parameters(), lr=self.hyperparameters[\"learning_rate\"], eps=1e-4)\n",
        "        self.episode_number = 0\n",
        "        self.many_episode_states = []\n",
        "        self.many_episode_actions = []\n",
        "        self.many_episode_rewards = []\n",
        "        self.experience_generator = Parallel_Experience_Generator(self.environment, self.policy_new, self.config.seed,\n",
        "                                                                  self.hyperparameters, self.action_size)\n",
        "        self.exploration_strategy = Epsilon_Greedy_Exploration(self.config)\n",
        "\n",
        "    def calculate_policy_output_size(self):\n",
        "        \"\"\"Initialises the policies\"\"\"\n",
        "        if self.action_types == \"DISCRETE\":\n",
        "            return self.action_size\n",
        "        elif self.action_types == \"CONTINUOUS\":\n",
        "            return self.action_size * 2 #Because we need 1 parameter for mean and 1 for std of distribution\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Runs a step for the PPO agent\"\"\"\n",
        "        exploration_epsilon =  self.exploration_strategy.get_updated_epsilon_exploration({\"episode_number\": self.episode_number})\n",
        "        self.many_episode_states, self.many_episode_actions, self.many_episode_rewards = self.experience_generator.play_n_episodes(\n",
        "            self.hyperparameters[\"episodes_per_learning_round\"], exploration_epsilon)\n",
        "        self.episode_number += self.hyperparameters[\"episodes_per_learning_round\"]\n",
        "        self.policy_learn()\n",
        "        self.update_learning_rate(self.hyperparameters[\"learning_rate\"], self.policy_new_optimizer)\n",
        "        self.equalise_policies()\n",
        "\n",
        "    def policy_learn(self):\n",
        "        \"\"\"A learning iteration for the policy\"\"\"\n",
        "        all_discounted_returns = self.calculate_all_discounted_returns()\n",
        "        if self.hyperparameters[\"normalise_rewards\"]:\n",
        "            all_discounted_returns = normalise_rewards(all_discounted_returns)\n",
        "        for _ in range(self.hyperparameters[\"learning_iterations_per_round\"]):\n",
        "            all_ratio_of_policy_probabilities = self.calculate_all_ratio_of_policy_probabilities()\n",
        "            loss = self.calculate_loss([all_ratio_of_policy_probabilities], all_discounted_returns)\n",
        "            self.take_policy_new_optimisation_step(loss)\n",
        "\n",
        "    def calculate_all_discounted_returns(self):\n",
        "        \"\"\"Calculates the cumulative discounted return for each episode which we will then use in a learning iteration\"\"\"\n",
        "        all_discounted_returns = []\n",
        "        for episode in range(len(self.many_episode_states)):\n",
        "            discounted_returns = [0]\n",
        "            for ix in range(len(self.many_episode_states[episode])):\n",
        "                return_value = self.many_episode_rewards[episode][-(ix + 1)] + self.hyperparameters[\"discount_rate\"]*discounted_returns[-1]\n",
        "                discounted_returns.append(return_value)\n",
        "            discounted_returns = discounted_returns[1:]\n",
        "            all_discounted_returns.extend(discounted_returns[::-1])\n",
        "        return all_discounted_returns\n",
        "\n",
        "    def calculate_all_ratio_of_policy_probabilities(self):\n",
        "        \"\"\"For each action calculates the ratio of the probability that the new policy would have picked the action vs.\n",
        "         the probability the old policy would have picked it. This will then be used to inform the loss\"\"\"\n",
        "        all_states = [state for states in self.many_episode_states for state in states]\n",
        "        all_actions = [[action] if self.action_types == \"DISCRETE\" else action for actions in self.many_episode_actions for action in actions ]\n",
        "        all_states = torch.stack([torch.Tensor(states).float().to(self.device) for states in all_states])\n",
        "\n",
        "        all_actions = torch.stack([torch.Tensor(actions).float().to(self.device) for actions in all_actions])\n",
        "        all_actions = all_actions.view(-1, len(all_states))\n",
        "\n",
        "        new_policy_distribution_log_prob = self.calculate_log_probability_of_actions(self.policy_new, all_states, all_actions)\n",
        "        old_policy_distribution_log_prob = self.calculate_log_probability_of_actions(self.policy_old, all_states, all_actions)\n",
        "        ratio_of_policy_probabilities = torch.exp(new_policy_distribution_log_prob) / (torch.exp(old_policy_distribution_log_prob) + 1e-8)\n",
        "        return ratio_of_policy_probabilities\n",
        "\n",
        "    def calculate_log_probability_of_actions(self, policy, states, actions):\n",
        "        \"\"\"Calculates the log probability of an action occuring given a policy and starting state\"\"\"\n",
        "        policy_output = policy.forward(states).to(self.device)\n",
        "        policy_distribution = create_actor_distribution(self.action_types, policy_output, self.action_size)\n",
        "        policy_distribution_log_prob = policy_distribution.log_prob(actions)\n",
        "        return policy_distribution_log_prob\n",
        "\n",
        "    def calculate_loss(self, all_ratio_of_policy_probabilities, all_discounted_returns):\n",
        "        \"\"\"Calculates the PPO loss\"\"\"\n",
        "        all_ratio_of_policy_probabilities = torch.squeeze(torch.stack(all_ratio_of_policy_probabilities))\n",
        "        all_ratio_of_policy_probabilities = torch.clamp(input=all_ratio_of_policy_probabilities,\n",
        "                                                        min = -sys.maxsize,\n",
        "                                                        max = sys.maxsize)\n",
        "        all_discounted_returns = torch.tensor(all_discounted_returns).to(all_ratio_of_policy_probabilities)\n",
        "        potential_loss_value_1 = all_discounted_returns * all_ratio_of_policy_probabilities\n",
        "        potential_loss_value_2 = all_discounted_returns * self.clamp_probability_ratio(all_ratio_of_policy_probabilities)\n",
        "        loss = torch.min(potential_loss_value_1, potential_loss_value_2)\n",
        "        loss = -torch.mean(loss)\n",
        "        return loss\n",
        "\n",
        "    def clamp_probability_ratio(self, value):\n",
        "        \"\"\"Clamps a value between a certain range determined by hyperparameter clip epsilon\"\"\"\n",
        "        return torch.clamp(input=value, min=1.0 - self.hyperparameters[\"clip_epsilon\"],\n",
        "                                  max=1.0 + self.hyperparameters[\"clip_epsilon\"])\n",
        "\n",
        "    def take_policy_new_optimisation_step(self, loss):\n",
        "        \"\"\"Takes an optimisation step for the new policy\"\"\"\n",
        "        self.policy_new_optimizer.zero_grad()  # reset gradients to 0\n",
        "        loss.backward()  # this calculates the gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_new.parameters(), self.hyperparameters[\n",
        "            \"gradient_clipping_norm\"])  # clip gradients to help stabilise training\n",
        "        self.policy_new_optimizer.step()  # this applies the gradients\n",
        "\n",
        "    def equalise_policies(self):\n",
        "        \"\"\"Sets the old policy's parameters equal to the new policy's parameters\"\"\"\n",
        "        for old_param, new_param in zip(self.policy_old.parameters(), self.policy_new.parameters()):\n",
        "            old_param.data.copy_(new_param.data)\n",
        "\n",
        "    def save_result(self):\n",
        "        \"\"\"Save the results seen by the agent in the most recent experiences\"\"\"\n",
        "        for ep in range(len(self.many_episode_rewards)):\n",
        "            total_reward = np.sum(self.many_episode_rewards[ep])\n",
        "            self.game_full_episode_scores.append(total_reward)\n",
        "            ##self.rolling_results.append(np.mean(self.game_full_episode_scores[-1 * self.rolling_score_window:]))\n",
        "            self.rolling_results.append(np.mean(self.game_full_episode_scores[-1:]))\n",
        "        self.save_max_result_seen()\n"
      ],
      "metadata": {
        "id": "aOjmBY0EQQ9U",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title SAC agent\n",
        "\n",
        "### uses reply buffer and Ornstein-Uhlenbeck noise\n",
        "\n",
        "\n",
        "LOG_SIG_MAX = 2\n",
        "LOG_SIG_MIN = -20\n",
        "TRAINING_EPISODES_PER_EVAL_EPISODE = 10\n",
        "EPSILON = 1e-6\n",
        "\n",
        "\n",
        "class SAC(Base_Agent):\n",
        "    \"\"\"Soft Actor-Critic model based on the 2018 paper https://arxiv.org/abs/1812.05905 and on this github implementation\n",
        "      https://github.com/pranz24/pytorch-soft-actor-critic. It is an actor-critic algorithm where the agent is also trained\n",
        "      to maximise the entropy of their actions as well as their cumulative reward\"\"\"\n",
        "    agent_name = \"SAC\"\n",
        "    def __init__(self, config):\n",
        "        Base_Agent.__init__(self, config)\n",
        "        assert self.action_types == \"CONTINUOUS\", \"Action types must be continuous. Use SAC Discrete instead for discrete actions\"\n",
        "        assert self.config.hyperparameters[\"Actor\"][\"final_layer_activation\"] != \"Softmax\", \"Final actor layer must not be softmax\"\n",
        "        self.hyperparameters = config.hyperparameters\n",
        "        self.critic_local = self.create_NN(input_dim=self.state_size + self.action_size, output_dim=1, key_to_use=\"Critic\")\n",
        "        self.critic_local_2 = self.create_NN(input_dim=self.state_size + self.action_size, output_dim=1,\n",
        "                                           key_to_use=\"Critic\", override_seed=self.config.seed + 1)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(),\n",
        "                                                 lr=self.hyperparameters[\"Critic\"][\"learning_rate\"], eps=1e-4)\n",
        "        self.critic_optimizer_2 = torch.optim.Adam(self.critic_local_2.parameters(),\n",
        "                                                   lr=self.hyperparameters[\"Critic\"][\"learning_rate\"], eps=1e-4)\n",
        "        self.critic_target = self.create_NN(input_dim=self.state_size + self.action_size, output_dim=1,\n",
        "                                           key_to_use=\"Critic\")\n",
        "        self.critic_target_2 = self.create_NN(input_dim=self.state_size + self.action_size, output_dim=1,\n",
        "                                            key_to_use=\"Critic\")\n",
        "        Base_Agent.copy_model_over(self.critic_local, self.critic_target)\n",
        "        Base_Agent.copy_model_over(self.critic_local_2, self.critic_target_2)\n",
        "        self.memory = Replay_Buffer(self.hyperparameters[\"Critic\"][\"buffer_size\"], self.hyperparameters[\"batch_size\"],\n",
        "                                    self.config.seed)\n",
        "        self.actor_local = self.create_NN(input_dim=self.state_size, output_dim=self.action_size * 2, key_to_use=\"Actor\")\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(),\n",
        "                                          lr=self.hyperparameters[\"Actor\"][\"learning_rate\"], eps=1e-4)\n",
        "        self.automatic_entropy_tuning = self.hyperparameters[\"automatically_tune_entropy_hyperparameter\"]\n",
        "        if self.automatic_entropy_tuning:\n",
        "            self.target_entropy = -torch.prod(torch.Tensor(self.environment.action_space.shape).to(self.device)).item() # heuristic value from the paper\n",
        "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "            self.alpha_optim = Adam([self.log_alpha], lr=self.hyperparameters[\"Actor\"][\"learning_rate\"], eps=1e-4)\n",
        "        else:\n",
        "            self.alpha = self.hyperparameters[\"entropy_term_weight\"]\n",
        "\n",
        "        self.add_extra_noise = self.hyperparameters[\"add_extra_noise\"]\n",
        "        if self.add_extra_noise:\n",
        "            self.noise = OU_Noise(self.action_size, self.config.seed, self.hyperparameters[\"mu\"],\n",
        "                                  self.hyperparameters[\"theta\"], self.hyperparameters[\"sigma\"])\n",
        "\n",
        "        self.do_evaluation_iterations = self.hyperparameters[\"do_evaluation_iterations\"]\n",
        "\n",
        "    def save_result(self):\n",
        "        \"\"\"Saves the result of an episode of the game. Overriding the method in Base Agent that does this because we only\n",
        "        want to keep track of the results during the evaluation episodes\"\"\"\n",
        "        if self.episode_number == 1 or not self.do_evaluation_iterations:\n",
        "            self.game_full_episode_scores.extend([self.total_episode_score_so_far])\n",
        "            ##self.rolling_results.append(np.mean(self.game_full_episode_scores[-1 * self.rolling_score_window:]))\n",
        "            self.rolling_results.append(np.mean(self.game_full_episode_scores[-1:]))\n",
        "            self.save_max_result_seen()\n",
        "\n",
        "        elif (self.episode_number - 1) % TRAINING_EPISODES_PER_EVAL_EPISODE == 0:\n",
        "            self.game_full_episode_scores.extend([self.total_episode_score_so_far for _ in range(TRAINING_EPISODES_PER_EVAL_EPISODE)])\n",
        "            ##self.rolling_results.extend([np.mean(self.game_full_episode_scores[-1 * self.rolling_score_window:]) for _ in range(TRAINING_EPISODES_PER_EVAL_EPISODE)])\n",
        "            self.rolling_results.extend([np.mean(self.game_full_episode_scores[-1:]) for _ in \n",
        "                                         range(TRAINING_EPISODES_PER_EVAL_EPISODE)])\n",
        "            self.save_max_result_seen()\n",
        "\n",
        "    def reset_game(self):\n",
        "        \"\"\"Resets the game information so we are ready to play a new episode\"\"\"\n",
        "        Base_Agent.reset_game(self)\n",
        "        if self.add_extra_noise: self.noise.reset()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Runs an episode on the game, saving the experience and running a learning step if appropriate\"\"\"\n",
        "        eval_ep = self.episode_number % TRAINING_EPISODES_PER_EVAL_EPISODE == 0 and self.do_evaluation_iterations\n",
        "        self.episode_step_number_val = 0\n",
        "        while not self.done:\n",
        "            self.episode_step_number_val += 1\n",
        "            self.action = self.pick_action(eval_ep)\n",
        "            self.conduct_action(self.action)\n",
        "            if self.time_for_critic_and_actor_to_learn():\n",
        "                for _ in range(self.hyperparameters[\"learning_updates_per_learning_session\"]):\n",
        "                    self.learn()\n",
        "            mask = False if self.episode_step_number_val >= self.environment._max_episode_steps else self.done\n",
        "            if not eval_ep: self.save_experience(experience=(self.state, self.action, self.reward, self.next_state, mask))\n",
        "            self.state = self.next_state\n",
        "            self.global_step_number += 1\n",
        "        print(self.total_episode_score_so_far)\n",
        "        if eval_ep: self.print_summary_of_latest_evaluation_episode()\n",
        "        self.episode_number += 1\n",
        "\n",
        "    def pick_action(self, eval_ep, state=None):\n",
        "        \"\"\"Picks an action using one of three methods: 1) Randomly if we haven't passed a certain number of steps,\n",
        "         2) Using the actor in evaluation mode if eval_ep is True  3) Using the actor in training mode if eval_ep is False.\n",
        "         The difference between evaluation and training mode is that training mode does more exploration\"\"\"\n",
        "        if state is None: state = self.state\n",
        "        if eval_ep: action = self.actor_pick_action(state=state, eval=True)\n",
        "        elif self.global_step_number < self.hyperparameters[\"min_steps_before_learning\"]:\n",
        "            action = self.environment.action_space.sample()\n",
        "            print(\"Picking random action \", action)\n",
        "        else: action = self.actor_pick_action(state=state)\n",
        "        if self.add_extra_noise:\n",
        "            action += self.noise.sample()\n",
        "        return action\n",
        "\n",
        "    def actor_pick_action(self, state=None, eval=False):\n",
        "        \"\"\"Uses actor to pick an action in one of two ways: 1) If eval = False and we aren't in eval mode then it picks\n",
        "        an action that has partly been randomly sampled 2) If eval = True then we pick the action that comes directly\n",
        "        from the network and so did not involve any random sampling\"\"\"\n",
        "        if state is None: state = self.state\n",
        "        state = torch.FloatTensor([state]).to(self.device)\n",
        "        if len(state.shape) == 1: state = state.unsqueeze(0)\n",
        "        if eval == False: action, _, _ = self.produce_action_and_action_info(state)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                _, z, action = self.produce_action_and_action_info(state)\n",
        "        action = action.detach().cpu().numpy()\n",
        "        return action[0]\n",
        "\n",
        "    def produce_action_and_action_info(self, state):\n",
        "        \"\"\"Given the state, produces an action, the log probability of the action, and the tanh of the mean action\"\"\"\n",
        "        actor_output = self.actor_local(state)\n",
        "        mean, log_std = actor_output[:, :self.action_size], actor_output[:, self.action_size:]\n",
        "        std = log_std.exp()\n",
        "        normal = Normal(mean, std)\n",
        "        x_t = normal.rsample()  #rsample means it is sampled using reparameterisation trick\n",
        "        action = torch.tanh(x_t)\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        log_prob -= torch.log(1 - action.pow(2) + EPSILON)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        return action, log_prob, torch.tanh(mean)\n",
        "\n",
        "    def time_for_critic_and_actor_to_learn(self):\n",
        "        \"\"\"Returns boolean indicating whether there are enough experiences to learn from and it is time to learn for the\n",
        "        actor and critic\"\"\"\n",
        "        return self.global_step_number > self.hyperparameters[\"min_steps_before_learning\"] and \\\n",
        "               self.enough_experiences_to_learn_from() and self.global_step_number % self.hyperparameters[\"update_every_n_steps\"] == 0\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Runs a learning iteration for the actor, both critics and (if specified) the temperature parameter\"\"\"\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, mask_batch = self.sample_experiences()\n",
        "        qf1_loss, qf2_loss = self.calculate_critic_losses(state_batch, action_batch, reward_batch, next_state_batch, mask_batch)\n",
        "        self.update_critic_parameters(qf1_loss, qf2_loss)\n",
        "\n",
        "        policy_loss, log_pi = self.calculate_actor_loss(state_batch)\n",
        "        if self.automatic_entropy_tuning: alpha_loss = self.calculate_entropy_tuning_loss(log_pi)\n",
        "        else: alpha_loss = None\n",
        "        self.update_actor_parameters(policy_loss, alpha_loss)\n",
        "\n",
        "    def sample_experiences(self):\n",
        "        return  self.memory.sample()\n",
        "\n",
        "    def calculate_critic_losses(self, state_batch, action_batch, reward_batch, next_state_batch, mask_batch):\n",
        "        \"\"\"Calculates the losses for the two critics. This is the ordinary Q-learning loss except the additional entropy\n",
        "         term is taken into account\"\"\"\n",
        "        with torch.no_grad():\n",
        "            next_state_action, next_state_log_pi, _ = self.produce_action_and_action_info(next_state_batch)\n",
        "            qf1_next_target = self.critic_target(torch.cat((next_state_batch, next_state_action), 1))\n",
        "            qf2_next_target = self.critic_target_2(torch.cat((next_state_batch, next_state_action), 1))\n",
        "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_pi\n",
        "            next_q_value = reward_batch + (1.0 - mask_batch) * self.hyperparameters[\"discount_rate\"] * (min_qf_next_target)\n",
        "        qf1 = self.critic_local(torch.cat((state_batch, action_batch), 1))\n",
        "        qf2 = self.critic_local_2(torch.cat((state_batch, action_batch), 1))\n",
        "        qf1_loss = F.mse_loss(qf1, next_q_value)\n",
        "        qf2_loss = F.mse_loss(qf2, next_q_value)\n",
        "        return qf1_loss, qf2_loss\n",
        "\n",
        "    def calculate_actor_loss(self, state_batch):\n",
        "        \"\"\"Calculates the loss for the actor. This loss includes the additional entropy term\"\"\"\n",
        "        action, log_pi, _ = self.produce_action_and_action_info(state_batch)\n",
        "        qf1_pi = self.critic_local(torch.cat((state_batch, action), 1))\n",
        "        qf2_pi = self.critic_local_2(torch.cat((state_batch, action), 1))\n",
        "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "        policy_loss = ((self.alpha * log_pi) - min_qf_pi).mean()\n",
        "        return policy_loss, log_pi\n",
        "\n",
        "    def calculate_entropy_tuning_loss(self, log_pi):\n",
        "        \"\"\"Calculates the loss for the entropy temperature parameter. This is only relevant if self.automatic_entropy_tuning\n",
        "        is True.\"\"\"\n",
        "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "        return alpha_loss\n",
        "\n",
        "    def update_critic_parameters(self, critic_loss_1, critic_loss_2):\n",
        "        \"\"\"Updates the parameters for both critics\"\"\"\n",
        "        self.take_optimisation_step(self.critic_optimizer, self.critic_local, critic_loss_1,\n",
        "                                    self.hyperparameters[\"Critic\"][\"gradient_clipping_norm\"])\n",
        "        self.take_optimisation_step(self.critic_optimizer_2, self.critic_local_2, critic_loss_2,\n",
        "                                    self.hyperparameters[\"Critic\"][\"gradient_clipping_norm\"])\n",
        "        self.soft_update_of_target_network(self.critic_local, self.critic_target,\n",
        "                                           self.hyperparameters[\"Critic\"][\"tau\"])\n",
        "        self.soft_update_of_target_network(self.critic_local_2, self.critic_target_2,\n",
        "                                           self.hyperparameters[\"Critic\"][\"tau\"])\n",
        "\n",
        "    def update_actor_parameters(self, actor_loss, alpha_loss):\n",
        "        \"\"\"Updates the parameters for the actor and (if specified) the temperature parameter\"\"\"\n",
        "        self.take_optimisation_step(self.actor_optimizer, self.actor_local, actor_loss,\n",
        "                                    self.hyperparameters[\"Actor\"][\"gradient_clipping_norm\"])\n",
        "        if alpha_loss is not None:\n",
        "            self.take_optimisation_step(self.alpha_optim, None, alpha_loss, None)\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "\n",
        "    def print_summary_of_latest_evaluation_episode(self):\n",
        "        \"\"\"Prints a summary of the latest episode\"\"\"\n",
        "        print(\" \")\n",
        "        print(\"----------------------------\")\n",
        "        print(\"Episode score {} \".format(self.total_episode_score_so_far))\n",
        "        print(\"----------------------------\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SEughYNGQWKb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title DDPG agent\n",
        "\n",
        "\n",
        "### uses reply buffer and Ornstein-Uhlenbeck Noise Exploration\n",
        "\n",
        "class DDPG(Base_Agent):\n",
        "    \"\"\"A DDPG Agent\"\"\"\n",
        "    agent_name = \"DDPG\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        Base_Agent.__init__(self, config)\n",
        "        self.hyperparameters = config.hyperparameters\n",
        "        self.critic_local = self.create_NN(input_dim=self.state_size + self.action_size, output_dim=1, key_to_use=\"Critic\")\n",
        "        self.critic_target = self.create_NN(input_dim=self.state_size + self.action_size, output_dim=1, key_to_use=\"Critic\")\n",
        "        Base_Agent.copy_model_over(self.critic_local, self.critic_target)\n",
        "\n",
        "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(),\n",
        "                                           lr=self.hyperparameters[\"Critic\"][\"learning_rate\"], eps=1e-4)\n",
        "        self.memory = Replay_Buffer(self.hyperparameters[\"Critic\"][\"buffer_size\"], self.hyperparameters[\"batch_size\"],\n",
        "                                    self.config.seed)\n",
        "        self.actor_local = self.create_NN(input_dim=self.state_size, output_dim=self.action_size, key_to_use=\"Actor\")\n",
        "        self.actor_target = self.create_NN(input_dim=self.state_size, output_dim=self.action_size, key_to_use=\"Actor\")\n",
        "        Base_Agent.copy_model_over(self.actor_local, self.actor_target)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),\n",
        "                                          lr=self.hyperparameters[\"Actor\"][\"learning_rate\"], eps=1e-4)\n",
        "        self.exploration_strategy = OU_Noise_Exploration(self.config)\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"Runs a step in the game\"\"\"\n",
        "        while not self.done:\n",
        "            # print(\"State \", self.state.shape)\n",
        "            self.action = self.pick_action()\n",
        "            self.conduct_action(self.action)\n",
        "            if self.time_for_critic_and_actor_to_learn():\n",
        "                for _ in range(self.hyperparameters[\"learning_updates_per_learning_session\"]):\n",
        "                    states, actions, rewards, next_states, dones = self.sample_experiences()\n",
        "                    self.critic_learn(states, actions, rewards, next_states, dones)\n",
        "                    self.actor_learn(states)\n",
        "            self.save_experience()\n",
        "            self.state = self.next_state #this is to set the state for the next iteration\n",
        "            self.global_step_number += 1\n",
        "        self.episode_number += 1\n",
        "\n",
        "    def sample_experiences(self):\n",
        "        return self.memory.sample()\n",
        "\n",
        "    def pick_action(self, state=None):\n",
        "        \"\"\"Picks an action using the actor network and then adds some noise to it to ensure exploration\"\"\"\n",
        "        if state is None: state = torch.from_numpy(self.state).float().unsqueeze(0).to(self.device)\n",
        "        self.actor_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local(state).cpu().data.numpy()\n",
        "        self.actor_local.train()\n",
        "        action = self.exploration_strategy.perturb_action_for_exploration_purposes({\"action\": action})\n",
        "        return action.squeeze(0)\n",
        "\n",
        "    def critic_learn(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"Runs a learning iteration for the critic\"\"\"\n",
        "        loss = self.compute_loss(states, next_states, rewards, actions, dones)\n",
        "        self.take_optimisation_step(self.critic_optimizer, self.critic_local, loss, self.hyperparameters[\"Critic\"][\"gradient_clipping_norm\"])\n",
        "        self.soft_update_of_target_network(self.critic_local, self.critic_target, self.hyperparameters[\"Critic\"][\"tau\"])\n",
        "\n",
        "    def compute_loss(self, states, next_states, rewards, actions, dones):\n",
        "        \"\"\"Computes the loss for the critic\"\"\"\n",
        "        with torch.no_grad():\n",
        "            critic_targets = self.compute_critic_targets(next_states, rewards, dones)\n",
        "        critic_expected = self.compute_expected_critic_values(states, actions)\n",
        "        loss = F.mse_loss(critic_expected, critic_targets)\n",
        "        return loss\n",
        "\n",
        "    def compute_critic_targets(self, next_states, rewards, dones):\n",
        "        \"\"\"Computes the critic target values to be used in the loss for the critic\"\"\"\n",
        "        critic_targets_next = self.compute_critic_values_for_next_states(next_states)\n",
        "        critic_targets = self.compute_critic_values_for_current_states(rewards, critic_targets_next, dones)\n",
        "        return critic_targets\n",
        "\n",
        "    def compute_critic_values_for_next_states(self, next_states):\n",
        "        \"\"\"Computes the critic values for next states to be used in the loss for the critic\"\"\"\n",
        "        with torch.no_grad():\n",
        "            actions_next = self.actor_target(next_states)\n",
        "            critic_targets_next = self.critic_target(torch.cat((next_states, actions_next), 1))\n",
        "        return critic_targets_next\n",
        "\n",
        "    def compute_critic_values_for_current_states(self, rewards, critic_targets_next, dones):\n",
        "        \"\"\"Computes the critic values for current states to be used in the loss for the critic\"\"\"\n",
        "        critic_targets_current = rewards + (self.hyperparameters[\"discount_rate\"] * critic_targets_next * (1.0 - dones))\n",
        "        return critic_targets_current\n",
        "\n",
        "    def compute_expected_critic_values(self, states, actions):\n",
        "        \"\"\"Computes the expected critic values to be used in the loss for the critic\"\"\"\n",
        "        critic_expected = self.critic_local(torch.cat((states, actions), 1))\n",
        "        return critic_expected\n",
        "\n",
        "    def time_for_critic_and_actor_to_learn(self):\n",
        "        \"\"\"Returns boolean indicating whether there are enough experiences to learn from and it is time to learn for the\n",
        "        actor and critic\"\"\"\n",
        "        return self.enough_experiences_to_learn_from() and self.global_step_number % self.hyperparameters[\"update_every_n_steps\"] == 0\n",
        "\n",
        "    def actor_learn(self, states):\n",
        "        \"\"\"Runs a learning iteration for the actor\"\"\"\n",
        "        if self.done: #we only update the learning rate at end of each episode\n",
        "            self.update_learning_rate(self.hyperparameters[\"Actor\"][\"learning_rate\"], self.actor_optimizer)\n",
        "        actor_loss = self.calculate_actor_loss(states)\n",
        "        self.take_optimisation_step(self.actor_optimizer, self.actor_local, actor_loss,\n",
        "                                    self.hyperparameters[\"Actor\"][\"gradient_clipping_norm\"])\n",
        "        self.soft_update_of_target_network(self.actor_local, self.actor_target, self.hyperparameters[\"Actor\"][\"tau\"])\n",
        "\n",
        "    def calculate_actor_loss(self, states):\n",
        "        \"\"\"Calculates the loss for the actor\"\"\"\n",
        "        actions_pred = self.actor_local(states)\n",
        "        actor_loss = -self.critic_local(torch.cat((states, actions_pred), 1)).mean()\n",
        "        return actor_loss"
      ],
      "metadata": {
        "cellView": "form",
        "id": "WgZLjS-nQdfD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Trainer Class\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    \"\"\"Runs games for given agents. Optionally will visualise and save the results\"\"\"\n",
        "    def __init__(self, config, agents):\n",
        "        self.config = config\n",
        "        self.agents = agents\n",
        "        self.agent_to_agent_group = self.create_agent_to_agent_group_dictionary()\n",
        "        self.agent_to_color_group = self.create_agent_to_color_dictionary()\n",
        "        self.results = None\n",
        "        self.colors = [\"red\", \"cyan\", \"blue\"]\n",
        "        self.colour_ix = 0\n",
        "\n",
        "    def create_agent_to_agent_group_dictionary(self):\n",
        "        \"\"\"Creates a dictionary that maps an agent to their wider agent group\"\"\"\n",
        "        agent_to_agent_group_dictionary = {\n",
        "            \"PPO\": \"Policy_Gradient_Agent\",   \n",
        "            \"SAC\": \"Actor_Critic_Agent\",\n",
        "            \"DDPG\": \"Actor_Critic_Agent\", \n",
        "        }\n",
        "        return agent_to_agent_group_dictionary\n",
        "\n",
        "    def create_agent_to_color_dictionary(self):\n",
        "        \"\"\"Creates a dictionary that maps an agent to a hex color (for plotting purposes)\n",
        "        See https://en.wikipedia.org/wiki/Web_colors and https://htmlcolorcodes.com/ for hex colors\"\"\"\n",
        "        agent_to_color_dictionary = {\n",
        "            \"PPO\": \"#FF0000\",\n",
        "            \"SAC\": \"#00F7FF\",\n",
        "            \"DDPG\": \"#0097FF\",\n",
        "        }\n",
        "        return agent_to_color_dictionary\n",
        "\n",
        "    def run_games_for_agents(self):\n",
        "        \"\"\"Run a set of games for each agent. Optionally visualising and/or saving the results\"\"\"\n",
        "        self.results = self.create_object_to_store_results()\n",
        "        for agent_number, agent_class in enumerate(self.agents):\n",
        "            agent_name = agent_class.agent_name\n",
        "            self.run_games_for_agent(agent_number + 1, agent_class)\n",
        "            if self.config.visualise_overall_agent_results:\n",
        "                agent_rolling_score_results = [results[1] for results in  self.results[agent_name]]\n",
        "                self.visualise_overall_agent_results(agent_rolling_score_results, agent_name, show_mean_and_std_range=True)\n",
        "        if self.config.file_to_save_data_results: self.save_obj(self.results, self.config.file_to_save_data_results)\n",
        "        if self.config.file_to_save_results_graph: plt.savefig(self.config.file_to_save_results_graph, bbox_inches=\"tight\")\n",
        "        plt.show()\n",
        "        return self.results\n",
        "\n",
        "\n",
        "    def create_object_to_store_results(self):\n",
        "        \"\"\"Creates a dictionary that we will store the results in if it doesn't exist, otherwise it loads it up\"\"\"\n",
        "        if self.config.overwrite_existing_results_file or not self.config.file_to_save_data_results or not os.path.isfile(self.config.file_to_save_data_results):\n",
        "            results = {}\n",
        "        else: results = self.load_obj(self.config.file_to_save_data_results)\n",
        "        return results\n",
        "\n",
        "    def run_games_for_agent(self, agent_number, agent_class):\n",
        "        \"\"\"Runs a set of games for a given agent, saving the results in self.results\"\"\"\n",
        "        agent_results = []\n",
        "        agent_name = agent_class.agent_name\n",
        "        agent_group = self.agent_to_agent_group[agent_name]\n",
        "        agent_round = 1\n",
        "        for run in range(self.config.runs_per_agent):\n",
        "            agent_config = copy.deepcopy(self.config)\n",
        "\n",
        "            if self.environment_has_changeable_goals(agent_config.environment) and self.agent_cant_handle_changeable_goals_without_flattening(agent_name):\n",
        "                print(\"Flattening changeable-goal environment for agent {}\".format(agent_name))\n",
        "                agent_config.environment = gym.wrappers.FlattenDictWrapper(agent_config.environment,\n",
        "                                                                           dict_keys=[\"observation\", \"desired_goal\"])\n",
        "\n",
        "            if self.config.randomise_random_seed: agent_config.seed = random.randint(0, 2**32 - 2)\n",
        "            agent_config.hyperparameters = agent_config.hyperparameters[agent_group]\n",
        "            print(\"AGENT NAME: {}\".format(agent_name))\n",
        "            print(\"\\033[1m\" + \"{}.{}: {}\".format(agent_number, agent_round, agent_name) + \"\\033[0m\", flush=True)\n",
        "            agent = agent_class(agent_config)\n",
        "            self.environment_name = agent.environment_title\n",
        "            print(agent.hyperparameters)\n",
        "            print(\"RANDOM SEED \" , agent_config.seed)\n",
        "            game_scores, rolling_scores, time_taken = agent.run_n_episodes()\n",
        "            print(\"Time taken: {}\".format(time_taken), flush=True)\n",
        "            self.print_two_empty_lines()\n",
        "            agent_results.append([game_scores, rolling_scores, len(rolling_scores), -1 * max(rolling_scores), time_taken])\n",
        "            if self.config.visualise_individual_results:\n",
        "                self.visualise_overall_agent_results([rolling_scores], agent_name, show_each_run=True)\n",
        "                plt.show()\n",
        "            agent_round += 1\n",
        "\n",
        "        self.results[agent_name] = agent_results\n",
        "\n",
        "    def environment_has_changeable_goals(self, env):\n",
        "        \"\"\"Determines whether environment is such that for each episode there is a different goal or not\"\"\"\n",
        "        return isinstance(env.reset(), dict)\n",
        "\n",
        "    def agent_cant_handle_changeable_goals_without_flattening(self, agent_name):\n",
        "        \"\"\"Boolean indicating whether the agent is set up to handle changeable goals\"\"\"\n",
        "        return \"HER\" not in agent_name\n",
        "\n",
        "    def visualise_overall_agent_results(self, agent_results, agent_name, show_mean_and_std_range=False, show_each_run=False,\n",
        "                                        color=None, ax=None, title=None, y_limits=None):\n",
        "        \"\"\"Visualises the results for one agent\"\"\"\n",
        "        assert isinstance(agent_results, list), \"agent_results must be a list of lists, 1 set of results per list\"\n",
        "        assert isinstance(agent_results[0], list), \"agent_results must be a list of lists, 1 set of results per list\"\n",
        "        assert bool(show_mean_and_std_range) ^ bool(show_each_run), \"either show_mean_and_std_range or show_each_run must be true\"\n",
        "        if not ax: ax = plt.gca()\n",
        "        if not color: color =  self.agent_to_color_group[agent_name]\n",
        "        if show_mean_and_std_range:\n",
        "            mean_minus_x_std, mean_results, mean_plus_x_std = self.get_mean_and_standard_deviation_difference_results(agent_results)\n",
        "            x_vals = list(range(len(mean_results)))\n",
        "            ax.plot(x_vals, mean_results, label=agent_name, color=color)\n",
        "            ax.plot(x_vals, mean_plus_x_std, color=color, alpha=0.1)\n",
        "            ax.plot(x_vals, mean_minus_x_std, color=color, alpha=0.1)\n",
        "            ax.fill_between(x_vals, y1=mean_minus_x_std, y2=mean_plus_x_std, alpha=0.1, color=color)\n",
        "        else:\n",
        "            for ix, result in enumerate(agent_results):\n",
        "                x_vals = list(range(len(agent_results[0])))\n",
        "                plt.plot(x_vals, result, label=agent_name + \"_{}\".format(ix+1), color=color)\n",
        "                color = self.get_next_color()\n",
        "\n",
        "        ax.set_facecolor('xkcd:white')\n",
        "\n",
        "        # Shrink current axis's height by 10% on the bottom\n",
        "        box = ax.get_position()\n",
        "        ax.set_position([box.x0, box.y0 + box.height * 0.05,\n",
        "                         box.width, box.height * 0.95])\n",
        "\n",
        "        # Put a legend below current axis\n",
        "        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15),\n",
        "                  fancybox=True, shadow=True, ncol=3)\n",
        "\n",
        "        if not title: title = self.environment_name\n",
        "\n",
        "        ax.set_title(title, fontsize=15, fontweight='bold')\n",
        "        ax.set_ylabel('Rolling Episode Scores')\n",
        "        ax.set_xlabel('Episode Number')\n",
        "        self.hide_spines(ax, ['right', 'top'])\n",
        "        ax.set_xlim([0, x_vals[-1]])\n",
        "\n",
        "        if y_limits is None: y_min, y_max = self.get_y_limits(agent_results)\n",
        "        else: y_min, y_max = y_limits\n",
        "\n",
        "        ax.set_ylim([y_min, y_max])\n",
        "\n",
        "        if self.config.show_solution_score:\n",
        "            self.draw_horizontal_line_with_label(ax, y_value=self.config.environment.get_score_to_win(), x_min=0,\n",
        "                                        x_max=self.config.num_episodes_to_run * 1.02, label=\"Target \\n score\")\n",
        "\n",
        "    def get_y_limits(self, results):\n",
        "        \"\"\"Extracts the minimum and maximum seen y_values from a set of results\"\"\"\n",
        "        min_result = float(\"inf\")\n",
        "        max_result = float(\"-inf\")\n",
        "        for result in results:\n",
        "            temp_max = np.max(result)\n",
        "            temp_min = np.min(result)\n",
        "            if temp_max > max_result:\n",
        "                max_result = temp_max\n",
        "            if temp_min < min_result:\n",
        "                min_result = temp_min\n",
        "        return min_result, max_result\n",
        "\n",
        "    def get_next_color(self):\n",
        "        \"\"\"Gets the next color in list self.colors. If it gets to the end then it starts from beginning\"\"\"\n",
        "        self.colour_ix += 1\n",
        "        if self.colour_ix >= len(self.colors): self.colour_ix = 0\n",
        "        color = self.colors[self.colour_ix]\n",
        "        return color\n",
        "\n",
        "    def get_mean_and_standard_deviation_difference_results(self, results):\n",
        "        \"\"\"From a list of lists of agent results it extracts the mean results and the mean results plus or minus\n",
        "         some multiple of the standard deviation\"\"\"\n",
        "        def get_results_at_a_time_step(results, timestep):\n",
        "            results_at_a_time_step = [result[timestep] for result in results]\n",
        "            return results_at_a_time_step\n",
        "        def get_standard_deviation_at_time_step(results, timestep):\n",
        "            results_at_a_time_step = [result[timestep] for result in results]\n",
        "            return np.std(results_at_a_time_step)\n",
        "        mean_results = [np.mean(get_results_at_a_time_step(results, timestep)) for timestep in range(len(results[0]))]\n",
        "        mean_minus_x_std = [mean_val - self.config.standard_deviation_results * get_standard_deviation_at_time_step(results, timestep) for\n",
        "                            timestep, mean_val in enumerate(mean_results)]\n",
        "        mean_plus_x_std = [mean_val + self.config.standard_deviation_results * get_standard_deviation_at_time_step(results, timestep) for\n",
        "                           timestep, mean_val in enumerate(mean_results)]\n",
        "        return mean_minus_x_std, mean_results, mean_plus_x_std\n",
        "\n",
        "    def hide_spines(self, ax, spines_to_hide):\n",
        "        \"\"\"Hides splines on a matplotlib image\"\"\"\n",
        "        for spine in spines_to_hide:\n",
        "            ax.spines[spine].set_visible(False)\n",
        "\n",
        "    def ignore_points_after_game_solved(self, mean_minus_x_std, mean_results, mean_plus_x_std):\n",
        "        \"\"\"Removes the datapoints after the mean result achieves the score required to solve the game\"\"\"\n",
        "        for ix in range(len(mean_results)):\n",
        "            if mean_results[ix] >= self.config.environment.get_score_to_win():\n",
        "                break\n",
        "        return mean_minus_x_std[:ix], mean_results[:ix], mean_plus_x_std[:ix]\n",
        "\n",
        "    def draw_horizontal_line_with_label(self, ax, y_value, x_min, x_max, label):\n",
        "        \"\"\"Draws a dotted horizontal line on the given image at the given point and with the given label\"\"\"\n",
        "        ax.hlines(y=y_value, xmin=x_min, xmax=x_max,\n",
        "                  linewidth=2, color='k', linestyles='dotted', alpha=0.5)\n",
        "        ax.text(x_max, y_value * 0.965, label)\n",
        "\n",
        "    def print_two_empty_lines(self):\n",
        "        print(\"-----------------------------------------------------------------------------------\")\n",
        "        print(\"-----------------------------------------------------------------------------------\")\n",
        "        print(\" \")\n",
        "\n",
        "    def save_obj(self, obj, name):\n",
        "        \"\"\"Saves given object as a pickle file\"\"\"\n",
        "        if name[-4:] != \".pkl\":\n",
        "            name += \".pkl\"\n",
        "        with open(name, 'wb') as f:\n",
        "            pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load_obj(self, name):\n",
        "        \"\"\"Loads a pickle file object\"\"\"\n",
        "        with open(name, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def visualise_preexisting_results(self, save_image_path=None, data_path=None, colors=None, show_image=True, ax=None,\n",
        "                                      title=None, y_limits=None):\n",
        "        \"\"\"Visualises saved data results and then optionally saves the image\"\"\"\n",
        "        if not data_path: preexisting_results = self.create_object_to_store_results()\n",
        "        else: preexisting_results = self.load_obj(data_path)\n",
        "        for ix, agent in enumerate(list(preexisting_results.keys())):\n",
        "            agent_rolling_score_results = [results[1] for results in preexisting_results[agent]]\n",
        "            if colors: color = colors[ix]\n",
        "            else: color = None\n",
        "            self.visualise_overall_agent_results(agent_rolling_score_results, agent, show_mean_and_std_range=True,\n",
        "                                                 color=color, ax=ax, title=title, y_limits=y_limits)\n",
        "        if save_image_path: plt.savefig(save_image_path, bbox_inches=\"tight\")\n",
        "        if show_image: plt.show()\n",
        "\n",
        "    def visualise_set_of_preexisting_results(self, results_data_paths, save_image_path=None, show_image=True, plot_titles=None,\n",
        "                                             y_limits=[None,None]):\n",
        "        \"\"\"Visualises a set of preexisting results on 1 plot by making subplots\"\"\"\n",
        "        assert isinstance(results_data_paths, list), \"all_results must be a list of data paths\"\n",
        "\n",
        "        num_figures = len(results_data_paths)\n",
        "        col_width = 15\n",
        "        row_height = 6\n",
        "\n",
        "        if num_figures <= 2:\n",
        "            fig, axes = plt.subplots(1, num_figures, figsize=(col_width, row_height ))\n",
        "        elif num_figures <= 4:\n",
        "            fig, axes = plt.subplots(2, num_figures, figsize=(row_height, col_width))\n",
        "        else:\n",
        "            raise ValueError(\"Need to tell this method how to deal with more than 4 plots\")\n",
        "        for ax_ix in range(len(results_data_paths)):\n",
        "            self.visualise_preexisting_results(show_image=False, data_path=results_data_paths[ax_ix], ax=axes[ax_ix],\n",
        "                                               title=plot_titles[ax_ix], y_limits=y_limits[ax_ix])\n",
        "        fig.tight_layout()\n",
        "        fig.subplots_adjust(bottom=0.25)\n",
        "\n",
        "        if save_image_path: plt.savefig(save_image_path) #, bbox_inches=\"tight\")\n",
        "        if show_image: plt.show()\n",
        "\n",
        "        # ax.imshow(z, aspect=\"auto\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p4PlN6RMjmo7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Start competition of agents\n",
        "\n",
        "config = Config()\n",
        "config.seed = 1\n",
        "config.environment = gym.make(\"MountainCarContinuous-v0\")   ## TODO replace with self-made environment from scratch\n",
        "config.num_episodes_to_run = 1 #450\n",
        "config.file_to_save_data_results = None\n",
        "config.file_to_save_results_graph = None\n",
        "config.show_solution_score = False\n",
        "config.visualise_individual_results = False\n",
        "config.visualise_overall_agent_results = True\n",
        "config.standard_deviation_results = 1.0\n",
        "config.runs_per_agent = 3\n",
        "config.use_GPU = torch.cuda.is_available()           ## returns True if yes False if not                         \n",
        "config.overwrite_existing_results_file = False\n",
        "config.randomise_random_seed = True\n",
        "config.save_model = False\n",
        "\n",
        "\n",
        "config.hyperparameters = {\n",
        "    \"Policy_Gradient_Agent\": {\n",
        "            \"learning_rate\": 0.05,\n",
        "            \"linear_hidden_units\": [30, 15],\n",
        "            \"final_layer_activation\": nn.Tanh(),\n",
        "            \"learning_iterations_per_round\": 10,\n",
        "            \"discount_rate\": 0.9,\n",
        "            \"batch_norm\": False,\n",
        "            \"clip_epsilon\": 0.2,\n",
        "            \"episodes_per_learning_round\": 10,      \n",
        "            \"normalise_rewards\": True,\n",
        "            \"gradient_clipping_norm\": 5,\n",
        "            \"mu\": 0.0,\n",
        "            \"theta\": 0.15,\n",
        "            \"sigma\": 0.2,\n",
        "            \"epsilon_decay_rate_denominator\": 1,\n",
        "            \"clip_rewards\": False\n",
        "        },\n",
        "\n",
        "    \"Actor_Critic_Agent\": {\n",
        "            \"Actor\": {\n",
        "                \"learning_rate\": 0.003,\n",
        "                \"linear_hidden_units\": [20, 20],\n",
        "                \"final_layer_activation\": None,\n",
        "                \"batch_norm\": False,\n",
        "                \"tau\": 0.005,\n",
        "                \"gradient_clipping_norm\": 5,\n",
        "                \"initialiser\":  nn.init.xavier_uniform_\n",
        "            },\n",
        "\n",
        "            \"Critic\": {\n",
        "                \"learning_rate\": 0.02,\n",
        "                \"linear_hidden_units\": [20, 20],\n",
        "                \"final_layer_activation\": None,\n",
        "                \"batch_norm\": False,\n",
        "                \"buffer_size\": 1000000,\n",
        "                \"tau\": 0.005,\n",
        "                \"gradient_clipping_norm\": 5,\n",
        "                \"initialiser\":  nn.init.xavier_uniform_\n",
        "            },\n",
        "\n",
        "        \"min_steps_before_learning\": 1000, #for SAC only\n",
        "        \"batch_size\": 256,\n",
        "        \"discount_rate\": 0.99,\n",
        "        \"mu\": 0.0,  # for O-H noise\n",
        "        \"theta\": 0.15,  # for O-H noise\n",
        "        \"sigma\": 0.25,  # for O-H noise\n",
        "        \"update_every_n_steps\": 20,\n",
        "        \"learning_updates_per_learning_session\": 10,            \n",
        "        \"automatically_tune_entropy_hyperparameter\": True,\n",
        "        \"entropy_term_weight\": None,\n",
        "        \"add_extra_noise\": True,\n",
        "        \"do_evaluation_iterations\": True,\n",
        "        \"clip_rewards\": False\n",
        "\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "AGENTS = [PPO, SAC, DDPG]\n",
        "trainer = Trainer(config, AGENTS)\n",
        "trainer.run_games_for_agents()"
      ],
      "metadata": {
        "id": "Vsk3fX0OKx0X",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
