{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czVdIlqnImH"
      },
      "source": [
        "## MUNIT\n",
        "\n",
        "Huang et al. 2018 : Multimodal Unsupervised Image-to-Image Translation https://arxiv.org/abs/1804.04732 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faEGyHlHDmP9"
      },
      "source": [
        "\n",
        "![Same- and cross-domain interaction of encoders and decoders](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/MUNIT-Generator.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfkorNJrnmNO"
      },
      "source": [
        "import glob\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1m6fuMHo_Y9"
      },
      "source": [
        "if len(os.listdir(\".\")) < 3:\n",
        "    !wget https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/horse2zebra.zip\n",
        "    !unzip horse2zebra.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None, mode='train'):\n",
        "        super().__init__()\n",
        "        self.transform = transform\n",
        "        self.files_A = sorted(glob.glob(os.path.join(root, '%sA' % mode) + '/*.*'))\n",
        "        self.files_B = sorted(glob.glob(os.path.join(root, '%sB' % mode) + '/*.*'))\n",
        "        if len(self.files_A) > len(self.files_B):\n",
        "            self.files_A, self.files_B = self.files_B, self.files_A\n",
        "        self.new_perm()\n",
        "        assert len(self.files_A) > 0, \"Make sure you downloaded the horse2zebra images!\"\n",
        "\n",
        "    def new_perm(self):\n",
        "        self.randperm = torch.randperm(len(self.files_B))[:len(self.files_A)]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n",
        "        item_B = self.transform(Image.open(self.files_B[self.randperm[index]]))\n",
        "        if item_A.shape[0] != 3: \n",
        "            item_A = item_A.repeat(3, 1, 1)\n",
        "        if item_B.shape[0] != 3: \n",
        "            item_B = item_B.repeat(3, 1, 1)\n",
        "        if index == len(self) - 1:\n",
        "            self.new_perm()\n",
        "        return item_A, item_B\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.files_A), len(self.files_B)) "
      ],
      "metadata": {
        "id": "WtlM8SuEPwl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N9KNJfoQ_-6"
      },
      "source": [
        "class AdaptiveInstanceNorm2d(nn.Module):\n",
        "    '''\n",
        "    AdaptiveInstanceNorm2d Class\n",
        "    Values:\n",
        "        channels: the number of channels the image has, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, s_dim=8, h_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
        "        self.style_scale_transform = self.mlp(s_dim, h_dim, channels)\n",
        "        self.style_shift_transform = self.mlp(s_dim, h_dim, channels)\n",
        "\n",
        "    @staticmethod\n",
        "    def mlp(self, in_dim, h_dim, out_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(in_dim, h_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h_dim, h_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(h_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, image, w):\n",
        "        '''\n",
        "        Function for completing a forward pass of AdaIN: Given an image and a style, \n",
        "        returns the normalized image that has been scaled and shifted by the style.\n",
        "        Parameters:\n",
        "          image: the feature map of shape (n_samples, channels, width, height)\n",
        "          w: the intermediate noise vector w to be made into the style (y)\n",
        "        '''\n",
        "        normalized_image = self.instance_norm(image)\n",
        "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
        "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
        "        transformed_image = style_scale * normalized_image + style_shift\n",
        "        return transformed_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPHZpt-nmIEL"
      },
      "source": [
        "class LayerNorm2d(nn.Module):\n",
        "    '''\n",
        "    LayerNorm2d Class\n",
        "    Values:\n",
        "        channels: number of channels in input, a scalar\n",
        "        affine: whether to apply affine denormalization, a bool\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, eps=1e-5, affine=True):\n",
        "        super().__init__()\n",
        "        self.affine = affine\n",
        "        self.eps = eps\n",
        "\n",
        "        if self.affine:\n",
        "            self.gamma = nn.Parameter(torch.rand(channels))\n",
        "            self.beta = nn.Parameter(torch.zeros(channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.flatten(1).mean(1).reshape(-1, 1, 1, 1)\n",
        "        std = x.flatten(1).std(1).reshape(-1, 1, 1, 1)\n",
        "\n",
        "        x = (x - mean) / (std + self.eps)\n",
        "\n",
        "        if self.affine:\n",
        "            x = x * self.gamma.reshape(1, -1, 1, 1) + self.beta.reshape(1, -1, 1, 1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtwbXUTRK4a9"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    '''\n",
        "    ResidualBlock Class\n",
        "    Values:\n",
        "        channels: number of channels throughout residual block, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, channels, s_dim=None, h_dim=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, channels, kernel_size=3)\n",
        "            ),\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, channels, kernel_size=3)\n",
        "            ),\n",
        "        )\n",
        "        self.use_style = s_dim is not None and h_dim is not None\n",
        "        if self.use_style:\n",
        "            self.norm1 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
        "            self.norm2 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
        "        else:\n",
        "            self.norm1 = nn.InstanceNorm2d(channels)\n",
        "            self.norm2 = nn.InstanceNorm2d(channels)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, s=None):\n",
        "        x_id = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x, s) if self.use_style else self.norm1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x, s) if self.use_style else self.norm2(x)\n",
        "        return x + x_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCd393-QH_i0"
      },
      "source": [
        "class ContentEncoder(nn.Module):\n",
        "    '''\n",
        "    ContentEncoder Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_downsample: number of downsampling layers, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, base_channels=64, n_downsample=2, n_res_blocks=4):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = base_channels\n",
        "\n",
        "        # Input convolutional layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(3, channels, kernel_size=7)\n",
        "            ),\n",
        "            nn.InstanceNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Downsampling layers\n",
        "        for i in range(n_downsample):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.InstanceNorm2d(2 * channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            channels *= 2\n",
        "\n",
        "        # Residual blocks\n",
        "        layers += [\n",
        "            ResidualBlock(channels) for _ in range(n_res_blocks)\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        self.out_channels = channels\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    @property\n",
        "    def channels(self):\n",
        "        return self.out_channels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cff4eEHOcLk"
      },
      "source": [
        "class StyleEncoder(nn.Module):\n",
        "    '''\n",
        "    StyleEncoder Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_downsample: number of downsampling layers, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "    '''\n",
        "\n",
        "    n_deepen_layers = 2\n",
        "\n",
        "    def __init__(self, base_channels=64, n_downsample=4, s_dim=8):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = base_channels\n",
        "\n",
        "        # Input convolutional layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(3, channels, kernel_size=7, padding=0)\n",
        "            ),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Downsampling layers\n",
        "        for i in range(self.n_deepen_layers):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "            channels *= 2\n",
        "        for i in range(n_downsample - self.n_deepen_layers):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.ReLU(inplace=True),\n",
        "            ]\n",
        "\n",
        "        # Apply global pooling and pointwise convolution to style_channels\n",
        "        layers += [\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(channels, s_dim, kernel_size=1),\n",
        "        ]\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf8KP7DRRrkn"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    Decoder Class\n",
        "    Values:\n",
        "        in_channels: number of channels from encoder output, a scalar\n",
        "        n_upsample: number of upsampling layers, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_channels, n_upsample=2, n_res_blocks=4, s_dim=8, h_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        channels = in_channels\n",
        "\n",
        "        # Residual blocks with AdaIN\n",
        "        self.res_blocks = nn.ModuleList([\n",
        "            ResidualBlock(channels, s_dim) for _ in range(n_res_blocks)\n",
        "        ])\n",
        "\n",
        "        # Upsampling blocks\n",
        "        layers = []\n",
        "        for i in range(n_upsample):\n",
        "            layers += [\n",
        "                nn.Upsample(scale_factor=2),\n",
        "                nn.ReflectionPad2d(2),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, channels // 2, kernel_size=5)\n",
        "                ),\n",
        "                LayerNorm2d(channels // 2),\n",
        "            ]\n",
        "            channels //= 2\n",
        "        \n",
        "        layers += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, 3, kernel_size=7)\n",
        "            ),\n",
        "            nn.Tanh(),\n",
        "        ]\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        for res_block in self.res_blocks:\n",
        "            x = res_block(x, s=s)\n",
        "        x = self.layers(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl_ZpENltpL_"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_downsample: number of downsampling layers, a scalar\n",
        "        n_res_blocks: number of residual blocks, a scalar\n",
        "        s_dim: the dimension of the style tensor (s), a scalar\n",
        "        h_dim: the hidden dimension of the MLP, a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_channels: int = 64,\n",
        "        n_c_downsample: int = 2,\n",
        "        n_s_downsample: int = 4,\n",
        "        n_res_blocks: int = 4,\n",
        "        s_dim: int = 8,\n",
        "        h_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.c_enc = ContentEncoder(\n",
        "            base_channels=base_channels, n_downsample=n_c_downsample, n_res_blocks=n_res_blocks,\n",
        "        )\n",
        "        self.s_enc = StyleEncoder(\n",
        "            base_channels=base_channels, n_downsample=n_s_downsample, s_dim=s_dim,\n",
        "        )\n",
        "        self.dec = Decoder(\n",
        "            self.c_enc.channels, n_upsample=n_c_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        content = self.c_enc(x)\n",
        "        style = self.s_enc(x)\n",
        "        return (content, style)\n",
        "    \n",
        "    def decode(self, content, style):\n",
        "        return self.dec(content, style)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI-JT2VsyJil"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    '''\n",
        "    Generator Class\n",
        "    Values:\n",
        "        base_channels: number of channels in first convolutional layer, a scalar\n",
        "        n_layers: number of downsampling layers, a scalar\n",
        "        n_discriminators: number of discriminators (all at different scales), a scalar\n",
        "    '''\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_channels: int = 64,\n",
        "        n_layers: int = 3,\n",
        "        n_discriminators: int = 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            self.patchgan_discriminator(base_channels, n_layers) for _ in range(n_discriminators)\n",
        "        ])\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def patchgan_discriminator(base_channels, n_layers):\n",
        "        '''\n",
        "        Function that constructs and returns one PatchGAN discriminator module.\n",
        "        '''\n",
        "        channels = base_channels\n",
        "        # Input convolutional layer\n",
        "        layers = [\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(3, channels, kernel_size=4, stride=2),\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Hidden convolutional layers\n",
        "        for _ in range(n_layers):\n",
        "            layers += [\n",
        "                nn.ReflectionPad2d(1),\n",
        "                nn.utils.spectral_norm(\n",
        "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
        "                ),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            ]\n",
        "            channels *= 2\n",
        "\n",
        "        # Output projection layer\n",
        "        layers += [\n",
        "            nn.utils.spectral_norm(\n",
        "                nn.Conv2d(channels, 1, kernel_size=1)\n",
        "            ),\n",
        "        ]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for discriminator in self.discriminators:\n",
        "            outputs.append(discriminator(x))\n",
        "            x = self.downsample(x)\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSZDUYjNWXgi"
      },
      "source": [
        "class CompositeLoss(nn.Module):\n",
        "    '''\n",
        "    CompositeLoss Class: implements all losses for MUNIT\n",
        "    '''\n",
        "\n",
        "    @staticmethod\n",
        "    def image_recon_loss(x, gen):\n",
        "        c, s = gen.encode(x)\n",
        "        recon = gen.decode(c, s)\n",
        "        return F.l1_loss(recon, x), c, s\n",
        "\n",
        "    @staticmethod\n",
        "    def latent_recon_loss(c, s, gen):\n",
        "        x_fake = gen.decode(c, s)\n",
        "        recon = gen.encode(x_fake)\n",
        "        return F.l1_loss(recon[0], c), F.l1_loss(recon[1], s), x_fake\n",
        "\n",
        "    @staticmethod\n",
        "    def adversarial_loss(x, dis, is_real):\n",
        "        preds = dis(x)\n",
        "        target = torch.ones_like if is_real else torch.zeros_like\n",
        "        loss = 0.0\n",
        "        for pred in preds:\n",
        "            loss += F.mse_loss(pred, target(pred))\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idFCK4ugfoUo"
      },
      "source": [
        "class MUNIT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        gen_channels: int = 64,\n",
        "        n_c_downsample: int = 2,\n",
        "        n_s_downsample: int = 4,\n",
        "        n_res_blocks: int = 4,\n",
        "        s_dim: int = 8,\n",
        "        h_dim: int = 256,\n",
        "        dis_channels: int = 64,\n",
        "        n_layers: int = 3,\n",
        "        n_discriminators: int = 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.gen_a = Generator(\n",
        "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
        "        )\n",
        "        self.gen_b = Generator(\n",
        "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
        "        )\n",
        "        self.dis_a = Discriminator(\n",
        "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
        "        )\n",
        "        self.dis_b = Discriminator(\n",
        "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
        "        )\n",
        "        self.s_dim = s_dim\n",
        "        self.loss = CompositeLoss\n",
        "\n",
        "    def forward(self, x_a, x_b):\n",
        "        s_a = torch.randn(x_a.size(0), self.s_dim, 1, 1, device=x_a.device).to(x_a.dtype)\n",
        "        s_b = torch.randn(x_b.size(0), self.s_dim, 1, 1, device=x_b.device).to(x_b.dtype)\n",
        "\n",
        "        # Encode real x and compute image reconstruction loss\n",
        "        x_a_loss, c_a, s_a_fake = self.loss.image_recon_loss(x_a, self.gen_a)\n",
        "        x_b_loss, c_b, s_b_fake = self.loss.image_recon_loss(x_b, self.gen_b)\n",
        "\n",
        "        # Decode real (c, s) and compute latent reconstruction loss\n",
        "        c_b_loss, s_a_loss, x_ba = self.loss.latent_recon_loss(c_b, s_a, self.gen_a)\n",
        "        c_a_loss, s_b_loss, x_ab = self.loss.latent_recon_loss(c_a, s_b, self.gen_b)\n",
        "\n",
        "        # Compute adversarial losses\n",
        "        gen_a_adv_loss = self.loss.adversarial_loss(x_ba, self.dis_a, True)\n",
        "        gen_b_adv_loss = self.loss.adversarial_loss(x_ab, self.dis_b, True)\n",
        "\n",
        "        # Sum up losses for gen\n",
        "        gen_loss = (\n",
        "            10 * x_a_loss + c_b_loss + s_a_loss + gen_a_adv_loss + \\\n",
        "            10 * x_b_loss + c_a_loss + s_b_loss + gen_b_adv_loss\n",
        "        )\n",
        "\n",
        "        # Sum up losses for dis\n",
        "        dis_loss = (\n",
        "            self.loss.adversarial_loss(x_ba.detach(), self.dis_a, False) + \\\n",
        "            self.loss.adversarial_loss(x_a.detach(), self.dis_a, True) + \\\n",
        "            self.loss.adversarial_loss(x_ab.detach(), self.dis_b, False) + \\\n",
        "            self.loss.adversarial_loss(x_b.detach(), self.dis_b, True)\n",
        "        )\n",
        "\n",
        "        return gen_loss, dis_loss, x_ab, x_ba"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYQrHbgPw5ie"
      },
      "source": [
        "# Initialize model\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
        "\n",
        "munit_config = {\n",
        "    'gen_channels': 64,\n",
        "    'n_c_downsample': 2,\n",
        "    'n_s_downsample': 4,\n",
        "    'n_res_blocks': 4,\n",
        "    's_dim': 8,\n",
        "    'h_dim': 256,\n",
        "    'dis_channels': 64,\n",
        "    'n_layers': 3,\n",
        "    'n_discriminators': 3,\n",
        "}\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "munit = MUNIT(**munit_config).to(device).apply(weights_init)\n",
        "\n",
        "# Initialize dataloader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(286),\n",
        "    transforms.RandomCrop(256),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])\n",
        "dataloader = DataLoader(\n",
        "    ImageDataset('horse2zebra', transform),\n",
        "    batch_size=1, pin_memory=True, shuffle=True,\n",
        ")\n",
        "\n",
        "# Initialize optimizers\n",
        "gen_params = list(munit.gen_a.parameters()) + list(munit.gen_b.parameters())\n",
        "dis_params = list(munit.dis_a.parameters()) + list(munit.dis_b.parameters())\n",
        "gen_optimizer = torch.optim.Adam(gen_params, lr=1e-4, betas=(0.5, 0.999))\n",
        "dis_optimizer = torch.optim.Adam(dis_params, lr=1e-4, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9d8AneYxuuT"
      },
      "source": [
        "# Parse torch version for autocast\n",
        "version = torch.__version__\n",
        "version = tuple(int(n) for n in version.split('.')[:-1])\n",
        "has_autocast = version >= (1, 6)\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def show_tensor_images(x_real, x_fake):\n",
        "    ''' For visualizing images '''\n",
        "    image_tensor = torch.cat((x_fake[:1, ...], x_real[:1, ...]), dim=0)\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat, nrow=1)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "def train(munit, dataloader, optimizers, device):\n",
        "\n",
        "    max_iters = 1000000\n",
        "    decay_every = 100000\n",
        "    cur_iter = 0\n",
        "\n",
        "    display_every = 500\n",
        "    mean_losses = [0., 0.]\n",
        "\n",
        "    while cur_iter < max_iters:\n",
        "        for (x_a, x_b) in tqdm(dataloader):\n",
        "            x_a = x_a.to(device)\n",
        "            x_b = x_b.to(device)\n",
        "\n",
        "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
        "            # If you're running older versions of torch, comment this out\n",
        "            # and use NVIDIA apex for mixed/half precision training\n",
        "            if has_autocast:\n",
        "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
        "                    outputs = munit(x_a, x_b)\n",
        "            else:\n",
        "                outputs = munit(x_a, x_b)\n",
        "            \n",
        "            losses, x_ab, x_ba = outputs[:-2], outputs[-2], outputs[-1]\n",
        "            munit.zero_grad()\n",
        "\n",
        "            for i, (optimizer, loss) in enumerate(zip(optimizers, losses)):\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                mean_losses[i] += loss.item() / display_every\n",
        "\n",
        "            cur_iter += 1\n",
        "\n",
        "            if cur_iter % display_every == 0:\n",
        "                print('Step {}: [G loss: {:.5f}][D loss: {:.5f}]'\n",
        "                      .format(cur_iter, *mean_losses))\n",
        "                show_tensor_images(x_ab, x_a)\n",
        "                show_tensor_images(x_ba, x_b)\n",
        "                mean_losses = [0., 0.]\n",
        "\n",
        "            if cur_iter == max_iters:\n",
        "                break\n",
        "\n",
        "            # Schedule learning rate by 0.5\n",
        "            if cur_iter % decay_every == 0:\n",
        "                for optimizer in optimizers:\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] *= 0.5\n",
        "\n",
        "train(\n",
        "    munit, dataloader,\n",
        "    [gen_optimizer, dis_optimizer],\n",
        "    device,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}