{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBanjANG8klc"
      },
      "outputs": [],
      "source": [
        "# REFERENCES and further reading:\n",
        "\n",
        "# Di Mauro, Gala et al. Random Probabilistic Circuits. UAI (2021).\n",
        "# Peharz et al. Einsum Networks: Fast and Scalable Learning of Tractable Probabilistic Circuits. ICML (2020). \n",
        "# Peharz et al. Probabilistic Deep Learning using Random Sum-Product Networks. UAI (2020). \n",
        "# Van de Wolfshaar and Pronobis. Deep Generalized Convolutional Sum-Product Networks for Probabilistic Image Representations. PGM (2020).\n",
        "# Molina, Vergari et al. SPFLOW : An easy and extensible library for deep probabilistic learning using Sum-Product Networks. CoRR (2019). \n",
        "# Molina, Vergari et al. Mixed Sum-Product Networks: A Deep Architecture for Hybrid Domains. AAAI (2018).\n",
        "# Di Mauro et al. Sum-Product Network structure learning by efficient product nodes discovery. AIxIA (2018).\n",
        "# Papamakarios et al. Masked Autoregressive Flow for Density Estimation. NeurIPS (2017). \n",
        "# Dinh et al. Density Estimation using RealNVP. ICLR (2017). \n",
        "# Desana and Schnörr. Learning Arbitrary Sum-Product Network Leaves with Expectation-Maximization. CoRR (2016). \n",
        "# Peharz et al. On Theoretical Properties of Sum-Product Networks. AISTATS (2015). \n",
        "# Dinh et al. NICE: Non-linear Independent Components Estimation. ICLR (2015). \n",
        "# Rahman et al. Cutset Networks: A Simple, Tractable, and Scalable Approach for Improving the Accuracy of Chow-Liu Trees. ECML-PKDD (2014).\n",
        "# Poon and Domingos. Sum-Product Networks: A New Deep Architecture. UAI (2011).\n",
        "# https://github.com/deeprob-org"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For Basic Knowledge\n",
        "JAX and Tensorflow: \n",
        "- Jax whole [github](https://github.com/probml) platform dedicated to it and [3 books](https://probml.github.io/pml-book/) (the book N0 is available [here](http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf) ), [other books](https://www.ics.uci.edu/~smyth/courses/cs274/books.html) and [lectures with excercises](https://www.ics.uci.edu/~smyth/courses/cs274/), [reading groups using those books](https://www.youtube.com/watch?v=FDTXBaMazNg&list=PLOk2cpmAEiU3YgtHRUm58zGkw66nF2NLZ&index=1), [also this](https://www.youtube.com/watch?v=1vtkeR5yieo&list=PLmp4AHm0u1g3xuIHtrT37yOZCj51lWqic) and [chapter implementations of the same content: see book2 and tutorials](https://github.com/probml/pyprobml/tree/master/notebooks), also [direct colab versions available ](https://github.com/probml/pyprobml/blob/auto_notebooks_md/notebooks.md#gauss_plot_2d.ipynb)\n",
        "- Jax examples [VE and more](https://github.com/google/jax/blob/main/examples/mnist_vae.py)\n",
        "- https://www.tensorflow.org/probability\n",
        "  also look at [overview](https://www.youtube.com/watch?v=BrwKURU-wpk) and a [great blog](https://blog.tensorflow.org/2018/12/an-introduction-to-probabilistic.html?_gl=1*stcllb*_ga*MzA2NjA0NzY2LjE2ODU2ODA1Mjg.*_ga_W0YLR4190T*MTY4NzE2MjcxMi4yLjEuMTY4NzE2NjY3OC4wLjAuMA..),\n",
        "- book with code [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers),\n",
        "  also github and [examples](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/jupyter_notebooks), also probabilistic programming library [PyMC](https://www.pymc.io/welcome.html)\n",
        "- Other tutorials:\n",
        "  libspn [examples](https://github.com/pronobis/libspn-keras),\n",
        "  tutorial by Luis [video](https://www.youtube.com/watch?v=ZK3NjrGLIQY&list=LL&index=1)  &  [code](https://github.com/luisroque/probabilistic-deep-learning/tree/main/workshops)\n",
        "- Several playlists from University of Tübingen for [theoretic background](https://www.youtube.com/@TubingenML)\n",
        "- General probabilistic learning by Phillip Henning [course website](https://uni-tuebingen.de/en/180804) with [code for lectures in JAX](https://github.com/philipphennig/ProbML_Apps/tree/a893802b8fd34672ac5d532f08de540a19e465b7) and [lectures](https://www.youtube.com/playlist?list=PL05umP7R6ij2YE8rRJSb-olDNbntAQ_Bx) ([older 2021](https://www.youtube.com/watch?v=UbaVGD4Lfis&list=PL05umP7R6ij1tHaOFY96m5uX3J21a6yNd)). Check out his [book with code](https://www.probabilistic-numerics.org/textbooks/) and Rasmusen's [book](http://gaussianprocess.org) and simple explanation on [gaussian process](https://distill.pub/2019/visual-exploration-gaussian-processes/) (some other explanations [1](https://domino.ai/blog/fitting-gaussian-process-models-python),[2](http://katbailey.github.io/post/gaussian-processes-for-dummies/),[3](https://nbviewer.org/github/adamian/adamian.github.io/blob/master/talks/Brown2016.ipynb)), [bayesian optimization etc.](https://distill.pub/2020/bayesian-optimization/), [constructing kernels](https://www.cs.toronto.edu/~duvenaud/cookbook/). libraries just for Gaussian process specifically: [GPFlow](https://gpflow.org/), [scikit-learn](https://scikit-learn.org/stable/modules/gaussian_process.html), [GPy](http://sheffieldml.github.io/GPy/). \n",
        "\n",
        "PyTorch:\n",
        "- [Torch distrubutions](https://pytorch.org/docs/stable/distributions.html)\n",
        "- [Code with tutorials in both: Jax and Pytorch](https://uvadlc-notebooks.readthedocs.io/en/latest/)\n",
        "- [GPyTorch](https://gpytorch.ai/) for gaussian inference\n",
        "- [Pyro](https://pyro.ai/), [deeprob](https://github.com/deeprob-org/deeprob-kit), [StarAI](https://github.com/UCLA-StarAI), [Several](https://arranger1044.github.io/probabilistic-circuits/code)\n",
        "\n",
        "Researchers:\n",
        "\n",
        "- [Kevin Murphy](https://www.cs.ubc.ca/~murphyk/)\n",
        "- Antonio: [has sorted library of all important resources](http://nolovedeeplearning.com/buysellexchange.html) \n",
        "- [YooJung](https://yoojungchoi.github.io/)\n",
        "- [Guy Van den Broeck](https://web.cs.ucla.edu/~guyvdb/)\n",
        "- Phillip Henning: [github](https://github.com/philipphennig) (check another lecture course with the book and code as well: [numerics](https://www.probabilistic-numerics.org/teaching/2022_Numerics_of_Machine_Learning/), [video lectures](https://www.youtube.com/watch?v=0Q1ZTLHULcw&list=PL05umP7R6ij1qQyGmhrto2iMJXcmfSz5J), [more lectures](https://www.youtube.com/watch?v=RqFwO3GwYf4&list=PL05umP7R6ij2lwDdj7IkuHoP9vHlEcH0s)\n",
        "\n",
        "Some other books:\n",
        "\n",
        "- [High-Dimensional Probability An Introduction with Applications in Data Science](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#)\n",
        "\n",
        "State-of-the-art Research:\n",
        "- [Bayesian Flow Networks paper](https://arxiv.org/abs/2308.07037) and [simple available implementation](https://github.com/Algomancer/Bayesian-Flow-Networks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GAbG17xAt7ee"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "from __future__ import annotations\n",
        "import abc\n",
        "from typing import Type, Optional, Union, List, Tuple, NamedTuple, Iterator, Any, Callable\n",
        "from collections import deque, defaultdict\n",
        "import itertools\n",
        "from itertools import combinations\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "from scipy import linalg\n",
        "from scipy.special import logsumexp, gammaln, log_softmax\n",
        "from scipy import sparse as sp\n",
        "from scipy import stats\n",
        "\n",
        "from enum import Enum\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "\n",
        "import warnings\n",
        "from sklearn import mixture, cluster, cross_decomposition\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.base import BaseEstimator, DensityMixin, ClassifierMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0Yvv5amHTg"
      },
      "source": [
        "# Data handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jrBqAe9rqwOY"
      },
      "outputs": [],
      "source": [
        "#@title context for each thread\n",
        "\n",
        "import contextlib\n",
        "import contextvars\n",
        "\n",
        "# Thread-safe context variables, i.e. each thread will have its own flags assignments\n",
        "_context_variables = contextvars.ContextVar(\n",
        "    'context_variables',\n",
        "    default={\n",
        "        'check_dtype': True,\n",
        "        'check_spn': True\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "def is_check_dtype_enabled() -> bool:\n",
        "    \"\"\"Returns whether the context flag 'check_dtype' is enabled.\"\"\"\n",
        "    return _context_variables.get()['check_dtype']\n",
        "\n",
        "\n",
        "def is_check_spn_enabled() -> bool:\n",
        "    \"\"\"Returns whether the context flag 'check_spn' is enabled.\"\"\"\n",
        "    return _context_variables.get()['check_spn']\n",
        "\n",
        "\n",
        "class ContextState(contextlib.ContextDecorator):\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Thread-safe Context State that disables some flags during execution.\n",
        "\n",
        "        Current supported flags are the following:\n",
        "        - check_dtype: bool = True, Whether to check (and cast when needed) Numpy arrays data types.\n",
        "        - check_spn: bool = True, Whether to check the SPNs structure properties.\n",
        "        \"\"\"\n",
        "        self.__token = None\n",
        "        self.__state = _context_variables.get().copy()\n",
        "        for flag, value in kwargs.items():\n",
        "            if flag not in self.__state:\n",
        "                raise ValueError(\"Cannot set an unknown flag called '{}', suitable flags are: {}\".format(\n",
        "                    flag, ', '.join(self.__state.keys())\n",
        "                ))\n",
        "            self.__state[flag] = value\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.__token = _context_variables.set(self.__state)\n",
        "\n",
        "    def __exit__(self, *exc):\n",
        "        _context_variables.reset(self.__token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6BNdNrhlmHTi"
      },
      "outputs": [],
      "source": [
        "#@title data transforms\n",
        "\n",
        "class DataTransform(abc.ABC):\n",
        "    \"\"\"Abstract data transformation.\"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def fit(self, data: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the data transform with some data.\n",
        "\n",
        "        :param data: The data for fitting.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply the data transform to some data.\n",
        "\n",
        "        :param data: The data to transform.\n",
        "        :return: The transformed data.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def backward(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Apply the backward data transform to some data.\n",
        "\n",
        "        :param data: The data to transform.\n",
        "        :return: The transformed data.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class DataFlatten(DataTransform):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Build the data flatten transformation.\n",
        "        \"\"\"\n",
        "        self.shape = None\n",
        "\n",
        "    def fit(self, data: np.ndarray):\n",
        "        self.shape = data.shape[1:]\n",
        "\n",
        "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
        "        return np.reshape(data, [len(data), -1])\n",
        "\n",
        "    def backward(self, data: np.ndarray) -> np.ndarray:\n",
        "        return np.reshape(data, [len(data), *self.shape])\n",
        "\n",
        "\n",
        "class DataNormalizer(DataTransform):\n",
        "    def __init__(\n",
        "        self,\n",
        "        interval: Optional[Tuple[float, float]] = None,\n",
        "        clip: bool = False,\n",
        "        dtype=np.float32\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Build the data normalizer transformation.\n",
        "\n",
        "        :param interval: The normalizing interval. If None data will be normalized in [0, 1].\n",
        "        :param clip: Whether to clip data if out of interval.\n",
        "        :param dtype: The type for type conversion.\n",
        "        :raises ValueError: If the normalizing interval is out of domain.\n",
        "        \"\"\"\n",
        "        if interval is None:\n",
        "            interval = (0.0, 1.0)\n",
        "        elif interval[0] >= interval[1]:\n",
        "            raise ValueError(\"The normalizing interval must be (a, b) with a < b\")\n",
        "\n",
        "        self.interval = interval\n",
        "        self.clip = clip\n",
        "        self.dtype = dtype\n",
        "        self.prev_dtype = None\n",
        "        self.min = None\n",
        "        self.max = None\n",
        "\n",
        "    def fit(self, data: np.ndarray):\n",
        "        self.prev_dtype = data.dtype\n",
        "        self.min = np.min(data, axis=0)\n",
        "        self.max = np.max(data, axis=0)\n",
        "\n",
        "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
        "        a, b = self.interval\n",
        "        data = (data - self.min) / (self.max - self.min)\n",
        "        data = data * (b - a) + a\n",
        "        if self.clip:\n",
        "            data = np.clip(data, a, b)\n",
        "        return data.astype(self.dtype)\n",
        "\n",
        "    def backward(self, data: np.ndarray) -> np.ndarray:\n",
        "        a, b = self.interval\n",
        "        data = (data - a) / (b - a)\n",
        "        data = (self.max - self.min) * data + self.min\n",
        "        return data.astype(self.prev_dtype)\n",
        "\n",
        "\n",
        "class DataStandardizer(DataTransform):\n",
        "    def __init__(self, sample_wise: bool = True, eps: float = 1e-7, dtype=np.float32):\n",
        "        \"\"\"\n",
        "        Build the data standardizer transformation.\n",
        "\n",
        "        :param sample_wise: Whether to apply sample wise standardization.\n",
        "        :param eps: The epsilon value for standardization.\n",
        "        :param dtype: The type for type conversion.\n",
        "        :raises ValueError: If the epsilon value is out of domain.\n",
        "        \"\"\"\n",
        "        if eps <= 0.0:\n",
        "            raise ValueError(\"The epsilon value must be positive\")\n",
        "        self.sample_wise = sample_wise\n",
        "        self.eps = eps\n",
        "        self.dtype = dtype\n",
        "        self.prev_dtype = None\n",
        "        self.mean = None\n",
        "        self.stddev = None\n",
        "\n",
        "    def fit(self, data: np.ndarray):\n",
        "        self.prev_dtype = data.dtype\n",
        "        axis = 0 if self.sample_wise else None\n",
        "        self.mean = np.mean(data, axis=axis)\n",
        "        self.stddev = np.std(data, axis=axis)\n",
        "\n",
        "    def forward(self, data: np.ndarray) -> np.ndarray:\n",
        "        data = (data - self.mean) / (self.stddev + self.eps)\n",
        "        return data.astype(self.dtype)\n",
        "\n",
        "    def backward(self, data: np.ndarray) -> np.ndarray:\n",
        "        data = (self.stddev + self.eps) * data + self.mean\n",
        "        return data.astype(self.prev_dtype)\n",
        "\n",
        "\n",
        "def ohe_data(data: np.ndarray, domain: Union[List[int], np.ndarray]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    One-Hot-Encoding function.\n",
        "\n",
        "    :param data: The 1D data to encode.\n",
        "    :param domain: The domain to use.\n",
        "    :return: The One Hot encoded data.\n",
        "    \"\"\"\n",
        "    ohe = np.zeros((len(data), len(domain)), dtype=np.float32)\n",
        "    ohe[np.equal.outer(data, domain)] = 1.0\n",
        "    return ohe\n",
        "\n",
        "\n",
        "def mixed_ohe_data(data: np.ndarray, domains: List[Union[list, tuple]]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    One-Hot-Encoding function, applied on mixed data (both continuous and non-binary discrete).\n",
        "    Note that One-Hot-Encoding is applied only on categorical random variables having more than two values.\n",
        "\n",
        "    :param data: The data matrix to encode.\n",
        "    :param domains: The domains to use.\n",
        "    :return: The One Hot encoded data.\n",
        "    :raises ValueError: If there are inconsistencies between the data and domains.\n",
        "    \"\"\"\n",
        "    _, n_features = data.shape\n",
        "    if len(domains) != n_features:\n",
        "        raise ValueError(\"Each data column should correspond to a random variable having a domain\")\n",
        "\n",
        "    ohe = []\n",
        "    for i in range(n_features):\n",
        "        if len(domains[i]) > 2:\n",
        "            ohe.append(ohe_data(data[:, i], domains[i]))\n",
        "        else:\n",
        "            ohe.append(data[:, i])\n",
        "    return np.column_stack(ohe)\n",
        "\n",
        "\n",
        "def ecdf_data(data: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Empirical Cumulative Distribution Function (ECDF).\n",
        "\n",
        "    :param data: The data.\n",
        "    :return: The result of the ECDF on data.\n",
        "    \"\"\"\n",
        "    return stats.rankdata(data, method='max') / len(data)\n",
        "\n",
        "\n",
        "def check_data_dtype(data: np.ndarray, dtype: Type[np.dtype] = np.float32):\n",
        "    \"\"\"\n",
        "    Check whether the data is compatible with a given dtype (defaults to np.float32).\n",
        "    If the data dtype is not compatible, then cast it.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param dtype: The desidered dtype compatibility (defaults to np.float32).\n",
        "    :return: The casted data if necessary, otherwise returns data itself.\n",
        "    \"\"\"\n",
        "    if not is_check_dtype_enabled():\n",
        "        # Skip data dtype check and casting\n",
        "        return data\n",
        "\n",
        "    # Get flags for floating point data and type\n",
        "    is_data_fp = data.dtype in [np.float32, np.float64]\n",
        "    is_dtype_fp = dtype in [np.float32, np.float64]\n",
        "\n",
        "    if is_dtype_fp:\n",
        "        if not is_data_fp or data.dtype.itemsize < np.dtype(dtype).itemsize:\n",
        "            # If dtype is FP and data is not FP or it is a \"smaller\" FP, then cast it\n",
        "            return data.astype(dtype)\n",
        "    elif is_data_fp or data.dtype.itemsize < np.dtype(dtype).itemsize:\n",
        "        # If dtype is integral and data is FP or it is a \"smaller\" integral, then cast it\n",
        "        return data.astype(dtype)\n",
        "\n",
        "    # Data is compatible w.r.t. dtype\n",
        "    # i.e. it is FP if dtype is FP and integral if dtype is integral, and it is at least as \"big\" as dtype\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aygrJr8V8S4r"
      },
      "source": [
        "# Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ae7_i5i14hoo"
      },
      "outputs": [],
      "source": [
        "#@title statistics helpers\n",
        "\n",
        "\n",
        "def compute_mean_quantiles(data: np.ndarray, n_quantiles: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the mean quantiles of a dataset (Poon-Domingos).\n",
        "\n",
        "    :param data: The data.\n",
        "    :param n_quantiles: The number of quantiles.\n",
        "    :return: The mean quantiles.\n",
        "    :raises ValueError: If the number of quantiles is not valid.\n",
        "    \"\"\"\n",
        "    n_samples = len(data)\n",
        "    if n_quantiles <= 0 or n_quantiles > n_samples:\n",
        "        raise ValueError(\"The number of quantiles must be positive and less or equal than the number of samples\")\n",
        "\n",
        "    # Split the dataset in quantiles regions\n",
        "    data = np.sort(data, axis=0)\n",
        "    values_per_quantile = np.array_split(data, n_quantiles, axis=0)\n",
        "\n",
        "    # Compute the mean quantiles\n",
        "    mean_per_quantiles = [np.mean(x, axis=0) for x in values_per_quantile]\n",
        "    return np.stack(mean_per_quantiles, axis=0)\n",
        "\n",
        "\n",
        "def compute_mutual_information(priors: np.ndarray, joints: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the mutual information between each features, given priors and joints distributions.\n",
        "\n",
        "    :param priors: The priors probability distributions, as a (N, D) Numpy array\n",
        "                   having priors[i, k] = P(X_i=k).\n",
        "    :param joints: The joints probability distributions, as a (N, N, D, D) Numpy array\n",
        "                   having joints[i, j, k, l] = P(X_i=k, X_j=l).\n",
        "    :return: The mutual information between each pair of features, as a (N, N) Numpy symmetric matrix.\n",
        "    :raises ValueError: If there are inconsistencies between priors and joints arrays.\n",
        "    :raises ValueError: If joints array is not symmetric.\n",
        "    :raises ValueError: If priors or joints arrays don't encode valid probability distributions.\n",
        "    \"\"\"\n",
        "    n_variables, n_values = priors.shape\n",
        "    if joints.shape != (n_variables, n_variables, n_values, n_values):\n",
        "        raise ValueError(\"There are inconsistencies between priors and joints distributions\")\n",
        "    if not np.all(joints == joints.transpose([1, 0, 3, 2])):\n",
        "        raise ValueError(\"The joints probability distributions are expected to be symmetric\")\n",
        "    if not np.allclose(np.sum(priors, axis=1), 1.0):\n",
        "        raise ValueError(\"The priors probability distributions are not valid\")\n",
        "    if not np.allclose(np.sum(joints, axis=(2, 3)), 1.0):\n",
        "        raise ValueError(\"The joints probability distributions are not valid \")\n",
        "\n",
        "    outers = np.multiply.outer(priors, priors).transpose([0, 2, 1, 3])\n",
        "    # Ignore warnings of logarithm at zero (because NaNs on the diagonal will be zeroed later anyway)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        mutual_info = np.sum(joints * (np.log(joints) - np.log(outers)), axis=(2, 3))\n",
        "    np.fill_diagonal(mutual_info, 0.0)\n",
        "    return mutual_info\n",
        "\n",
        "\n",
        "def estimate_priors_joints(data: np.ndarray, alpha: float = 0.1) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Estimate both priors and joints probability distributions from binary data.\n",
        "\n",
        "    This function returns both the prior distributions and the joint distributions.\n",
        "    Note that priors[i, k] = P(X_i=k) and joints[i, j, k, l] = P(X_i=k, X_j=l).\n",
        "\n",
        "    :param data: The binary data matrix.\n",
        "    :param alpha: The Laplace smoothing factor.\n",
        "    :return: A pair of priors and joints distributions.\n",
        "    :raises ValueError: If the Laplace smoothing factor is out of domain.\n",
        "    \"\"\"\n",
        "    if alpha < 0.0:\n",
        "        raise ValueError(\"The Laplace smoothing factor must be non-negative\")\n",
        "\n",
        "    # Check the data dtype\n",
        "    data = check_data_dtype(data, dtype=np.float32)\n",
        "\n",
        "    # Compute the counts\n",
        "    n_samples, n_features = data.shape\n",
        "    counts_ones = np.dot(data.T, data)\n",
        "    counts_features = np.diag(counts_ones)\n",
        "    counts_cols = counts_features * np.ones_like(counts_ones)\n",
        "    counts_rows = np.transpose(counts_cols)\n",
        "\n",
        "    # Compute the prior probabilities\n",
        "    priors = np.empty(shape=(n_features, 2), dtype=data.dtype)\n",
        "    priors[:, 1] = (counts_features + 2 * alpha) / (n_samples + 4 * alpha)\n",
        "    priors[:, 0] = 1.0 - priors[:, 1]\n",
        "\n",
        "    # Compute the joints probabilities\n",
        "    joints = np.empty(shape=(n_features, n_features, 2, 2), dtype=data.dtype)\n",
        "    joints[:, :, 0, 0] = n_samples - counts_cols - counts_rows + counts_ones\n",
        "    joints[:, :, 0, 1] = counts_cols - counts_ones\n",
        "    joints[:, :, 1, 0] = counts_rows - counts_ones\n",
        "    joints[:, :, 1, 1] = counts_ones\n",
        "    joints = (joints + alpha) / (n_samples + 4 * alpha)\n",
        "\n",
        "    # Correct smoothing on the diagonal of joints array\n",
        "    idx_features = np.arange(n_features)\n",
        "    joints[idx_features, idx_features, 0, 0] = priors[:, 0]\n",
        "    joints[idx_features, idx_features, 0, 1] = 0.0\n",
        "    joints[idx_features, idx_features, 1, 0] = 0.0\n",
        "    joints[idx_features, idx_features, 1, 1] = priors[:, 1]\n",
        "\n",
        "    return priors, joints\n",
        "\n",
        "\n",
        "def compute_gini(probs: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Gini index given some probabilities.\n",
        "\n",
        "    :param probs: The probabilities.\n",
        "    :return: The Gini index.\n",
        "    :raises ValueError: If the probabilities doesn't sum up to one.\n",
        "    \"\"\"\n",
        "    if not np.isclose(np.sum(probs), 1.0):\n",
        "        raise ValueError(\"Probabilities must sum up to one\")\n",
        "    return 1.0 - np.sum(probs ** 2.0)\n",
        "\n",
        "\n",
        "def compute_bpp(avg_ll: float, shape: Union[int, tuple, list]):\n",
        "    \"\"\"\n",
        "    Compute the average number of bits per pixel (BPP).\n",
        "\n",
        "    :param avg_ll: The average log-likelihood, expressed in nats.\n",
        "    :param shape: The number of dimensions or, alternatively, a sequence of dimensions.\n",
        "    :return: The average number of bits per pixel.\n",
        "    \"\"\"\n",
        "    return -avg_ll / (np.log(2.0) * np.prod(shape))\n",
        "\n",
        "\n",
        "def compute_fid(\n",
        "    mean1: np.ndarray,\n",
        "    cov1: np.ndarray,\n",
        "    mean2: np.ndarray,\n",
        "    cov2: np.ndarray,\n",
        "    blocksize: int = 64,\n",
        "    eps: float = 1e-6\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the Frechet Inception Distance (FID) between two multivariate Gaussian distributions.\n",
        "    This implementation has been readapted from https://github.com/mseitzer/pytorch-fid.\n",
        "\n",
        "    :param mean1: The mean of the first multivariate Gaussian.\n",
        "    :param cov1: The covariance of the first multivariate Gaussian.\n",
        "    :param mean2: The mean of the second multivariate Gaussian.\n",
        "    :param cov2: The covariance of the second multivariate Gaussian.\n",
        "    :param blocksize: The block size used by the matrix square root algorithm.\n",
        "    :param eps: Epsilon value used to avoid singular matrices.\n",
        "    :return: The FID score.\n",
        "    :raises ValueError: If there is a shape mismatch between input arrays.\n",
        "    \"\"\"\n",
        "    if mean1.ndim != 1 or mean2.ndim != 1:\n",
        "        raise ValueError(\"Mean arrays must be one-dimensional\")\n",
        "    if cov1.ndim != 2 or cov2.ndim != 2:\n",
        "        raise ValueError(\"Covariance arrays must be two-dimensional\")\n",
        "    if mean1.shape != mean2.shape:\n",
        "        raise ValueError(\"Shape mismatch between mean arrays\")\n",
        "    if cov1.shape != cov2.shape:\n",
        "        raise ValueError(\"Shape mismatch between covariance arrays\")\n",
        "\n",
        "    # Compute the matrix square root of the dot product between covariance matrices\n",
        "    sqrtcov, _ = linalg.sqrtm(np.dot(cov1, cov2), disp=False, blocksize=blocksize)\n",
        "    if np.any(np.isinf(sqrtcov)):  # Matrix square root can give Infinity values in case of singular matrices\n",
        "        epsdiag = np.zeros_like(cov1)\n",
        "        np.fill_diagonal(epsdiag, eps)\n",
        "        sqrtcov, _ = linalg.sqrtm(np.dot(cov1 + epsdiag, cov2 + epsdiag), disp=False, blocksize=blocksize)\n",
        "\n",
        "    # Numerical errors might give a complex output, even if the input arrays are real\n",
        "    if np.iscomplexobj(sqrtcov) and np.isrealobj(cov1) and np.isrealobj(cov2):\n",
        "        sqrtcov = sqrtcov.real\n",
        "\n",
        "    # Compute the dot product of the difference between mean arrays\n",
        "    diffm = mean1 - mean2\n",
        "    diffmdot = np.dot(diffm, diffm)\n",
        "\n",
        "    # Return the final FID score\n",
        "    return diffmdot + np.trace(cov1) + np.trace(cov2) - 2.0 * np.trace(sqrtcov)\n",
        "\n",
        "\n",
        "def compute_prior_counts(\n",
        "    data: np.ndarray\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the counts of the values of an RV given the data.\n",
        "\n",
        "    :param data: The binary data matrix.\n",
        "    :return: The counts.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    counts_features = data.sum(axis=0)\n",
        "\n",
        "    # Compute the prior counts\n",
        "    prior_counts = np.empty(shape=(n_features, 2), dtype=np.float32)\n",
        "    prior_counts[:, 1] = counts_features\n",
        "    prior_counts[:, 0] = n_samples - prior_counts[:, 1]\n",
        "    return prior_counts\n",
        "\n",
        "\n",
        "def compute_joint_counts(\n",
        "    data: np.ndarray\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the counts of the configurations of an RV and its parent given the data.\n",
        "\n",
        "    :param data: The binary data matrix.\n",
        "    :return: The counts.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    counts_ones = np.dot(data.T, data)\n",
        "    counts_features = np.diag(counts_ones)\n",
        "    counts_cols = counts_features * np.ones_like(counts_ones)\n",
        "    counts_rows = np.transpose(counts_cols)\n",
        "\n",
        "    # Compute the joint counts\n",
        "    joint_counts = np.empty(shape=(n_features, n_features, 2, 2), dtype=np.float32)\n",
        "    joint_counts[:, :, 0, 0] = n_samples - counts_cols - counts_rows + counts_ones\n",
        "    joint_counts[:, :, 0, 1] = counts_cols - counts_ones\n",
        "    joint_counts[:, :, 1, 0] = counts_rows - counts_ones\n",
        "    joint_counts[:, :, 1, 1] = counts_ones\n",
        "    return joint_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QLOzzZrcuB0L"
      },
      "outputs": [],
      "source": [
        "#@title node and leaf\n",
        "\n",
        "\n",
        "#------------------------------------------------------------- Correct Data Type\n",
        "\n",
        "\n",
        "def check_data_dtype(data: np.ndarray, dtype: Type[np.dtype] = np.float32):\n",
        "    \"\"\"\n",
        "    Check whether the data is compatible with a given dtype (defaults to np.float32).\n",
        "    If the data dtype is not compatible, then cast it.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param dtype: The desidered dtype compatibility (defaults to np.float32).\n",
        "    :return: The casted data if necessary, otherwise returns data itself.\n",
        "    \"\"\"\n",
        "    if not is_check_dtype_enabled():\n",
        "        # Skip data dtype check and casting\n",
        "        return data\n",
        "\n",
        "    # Get flags for floating point data and type\n",
        "    is_data_fp = data.dtype in [np.float32, np.float64]\n",
        "    is_dtype_fp = dtype in [np.float32, np.float64]\n",
        "\n",
        "    if is_dtype_fp:\n",
        "        if not is_data_fp or data.dtype.itemsize < np.dtype(dtype).itemsize:\n",
        "            # If dtype is FP and data is not FP or it is a \"smaller\" FP, then cast it\n",
        "            return data.astype(dtype)\n",
        "    elif is_data_fp or data.dtype.itemsize < np.dtype(dtype).itemsize:\n",
        "        # If dtype is integral and data is FP or it is a \"smaller\" integral, then cast it\n",
        "        return data.astype(dtype)\n",
        "\n",
        "    # Data is compatible w.r.t. dtype\n",
        "    # i.e. it is FP if dtype is FP and integral if dtype is integral, and it is at least as \"big\" as dtype\n",
        "    return data\n",
        "\n",
        "#-------------------------------------------------------------------------- Node\n",
        "\n",
        "class Node(abc.ABC):\n",
        "    def __init__(self, scope: List[int], children: Optional[List[Node]] = None):\n",
        "        \"\"\"\n",
        "        Initialize a SPN node given the children list and its scope.\n",
        "\n",
        "        :param scope: The scope.\n",
        "        :param children: A list of nodes. If None, children are initialized as an empty list.\n",
        "        :raises ValueError: If the scope is empty.\n",
        "        :raises ValueError: If the scope contains duplicates.\n",
        "        \"\"\"\n",
        "        if not scope:\n",
        "            raise ValueError(\"The scope must not be empty\")\n",
        "        if len(scope) != len(set(scope)):\n",
        "            raise ValueError(\"The scope must not contain duplicates\")\n",
        "        if children is None:\n",
        "            children = list()\n",
        "\n",
        "        self.id = 0\n",
        "        self.scope = scope\n",
        "        self.children = children\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the likelihood of the node given some input.\n",
        "\n",
        "        :param x: The inputs.\n",
        "        :return: The resulting likelihoods.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the logarithmic likelihood of the node given some input.\n",
        "\n",
        "        :param x: The inputs.\n",
        "        :return: The resulting log-likelihoods.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class Sum(Node):\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: Optional[List[int]] = None,\n",
        "        children: Optional[List[Node]] = None,\n",
        "        weights: Optional[Union[List[float], np.ndarray]] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a SPN sum node given a list of children and their weights and a scope.\n",
        "\n",
        "        :param scope: The scope. If None, the scope is initialized based on children scopes.\n",
        "        :param children: A list of nodes. If None, children are initialized as an empty list.\n",
        "        :param weights: The weights associated to each children node. It can be None.\n",
        "        :raises ValueError: If both scope and children are None.\n",
        "        :raises ValueError: If children nodes have different scopes.\n",
        "        :raises ValueError: If the length of weights and children are different.\n",
        "        :raises ValueError: If weights don't sum up to 1.\n",
        "        \"\"\"\n",
        "        if children is None:\n",
        "            if scope is None:\n",
        "                raise ValueError(\"Cannot infer Sum node's scope without children\")\n",
        "        else:\n",
        "            if scope is None:\n",
        "                scope = children[0].scope\n",
        "            s_scope = set(scope)\n",
        "            if any(map(lambda c: set(c.scope) != s_scope, children[1:])):\n",
        "                raise ValueError(\"Children of Sum node have different scopes\")\n",
        "            if weights is not None and len(weights) != len(children):\n",
        "                raise ValueError(\"Weights and children length mismatch\")\n",
        "\n",
        "        if weights is not None:\n",
        "            if isinstance(weights, list):\n",
        "                weights = np.array(weights, dtype=np.float32)\n",
        "            if not np.isclose(np.sum(weights), 1.0):\n",
        "                raise ValueError(\"Weights don't sum up to 1\")\n",
        "        self.weights = weights\n",
        "\n",
        "        super().__init__(scope, children)\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        \"\"\"\n",
        "        Random initialize the node's parameters for Expectation-Maximization (EM).\n",
        "\n",
        "        :param random_state: The random state.\n",
        "        \"\"\"\n",
        "        weights = random_state.dirichlet(np.ones(len(self.children)))\n",
        "        self.weights = weights.astype(np.float32)\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, step_size: float):\n",
        "        \"\"\"\n",
        "        Compute a batch Expectation-Maximization (EM) step.\n",
        "\n",
        "        :param stats: The sufficient statistics of each sample.\n",
        "        :param step_size: The step size of update.\n",
        "        \"\"\"\n",
        "        unnorm_weights = self.weights * np.sum(stats, axis=1) + np.finfo(np.float32).eps\n",
        "        weights = unnorm_weights / np.sum(unnorm_weights)\n",
        "\n",
        "        # Update the parameters\n",
        "        self.weights = (1.0 - step_size) * self.weights + step_size * weights\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.expand_dims(np.dot(x, self.weights), axis=1)\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        return logsumexp(x, b=self.weights, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "class Product(Node):\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: Optional[List[int]] = None,\n",
        "        children: Optional[List[Node]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a product node given a list of children and its scope.\n",
        "\n",
        "        :param scope: The scope. If None, the scope is initialized based on children scopes.\n",
        "        :param children: A list of nodes. If None, children are initialized as an empty list.\n",
        "        :raises ValueError: If both scope and children are None.\n",
        "        :raises ValueError: If children nodes don't have disjointed scopes.\n",
        "        \"\"\"\n",
        "        if children is None:\n",
        "            if scope is None:\n",
        "                raise ValueError(\"Cannot infer Product node's scope without children\")\n",
        "        else:\n",
        "            c_scope = list(sum([c.scope for c in children], []))\n",
        "            s_scope = set(c_scope)\n",
        "            if scope is None:\n",
        "                if len(c_scope) != len(s_scope):\n",
        "                    raise ValueError(\"Children of Product node don't have disjointed scopes\")\n",
        "                scope = c_scope\n",
        "            elif set(scope) != s_scope:\n",
        "                raise ValueError(\"Children of Product node don't have disjointed scopes\")\n",
        "\n",
        "        super().__init__(scope, children)\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.prod(x, axis=1, keepdims=True)\n",
        "\n",
        "    def log_likelihood(self, x: np.append) -> np.ndarray:\n",
        "        return np.sum(x, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def assign_ids(root: Node) -> Node:\n",
        "    \"\"\"\n",
        "    Assign the ids to the nodes of a SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The same SPN with each node having modified ids.\n",
        "    :raises ValueError: If the SPN structure is not a DAG.\n",
        "    \"\"\"\n",
        "    nodes = topological_order(root)\n",
        "    if nodes is None:\n",
        "        raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "\n",
        "    next_id = 0\n",
        "    for node in nodes:\n",
        "        node.id = next_id\n",
        "        next_id += 1\n",
        "    return root\n",
        "\n",
        "\n",
        "def bfs(root: Node) -> Iterator[Node]:\n",
        "    \"\"\"\n",
        "    Compute the Breadth First Search (BFS) ordering for a SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The BFS nodes iterator.\n",
        "    \"\"\"\n",
        "    seen, queue = {root}, deque([root])\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        yield node\n",
        "        for c in node.children:\n",
        "            if c not in seen:\n",
        "                seen.add(c)\n",
        "                queue.append(c)\n",
        "\n",
        "\n",
        "def dfs_post_order(root: Node) -> Iterator[Node]:\n",
        "    \"\"\"\n",
        "    Compute Depth First Search (DFS) Post-Order ordering for a SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The DFS Post-Order nodes iterator.\n",
        "    \"\"\"\n",
        "    seen, stack = {root}, [root]\n",
        "    while stack:\n",
        "        node = stack[-1]\n",
        "        if set(node.children).issubset(seen):\n",
        "            stack.pop()\n",
        "            yield node\n",
        "            continue\n",
        "        for c in node.children:\n",
        "            if c not in seen:\n",
        "                seen.add(c)\n",
        "                stack.append(c)\n",
        "\n",
        "\n",
        "def topological_order(root: Node) -> Optional[List[Node]]:\n",
        "    \"\"\"\n",
        "    Compute the Topological Ordering for a SPN, using the Kahn's Algorithm.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: A list of nodes that form a topological ordering.\n",
        "             If the SPN graph is not acyclic, it returns None.\n",
        "    \"\"\"\n",
        "    ordering = list()\n",
        "    num_outgoings = defaultdict(int)\n",
        "    num_outgoings[root] = 0\n",
        "\n",
        "    # Initialize the number of outgoings edges for each node\n",
        "    for node in bfs(root):\n",
        "        for c in node.children:\n",
        "            num_outgoings[c] += 1\n",
        "\n",
        "    # Check the unusual case where the root node have outgoings edges, i.e. a trivial cycle has been found\n",
        "    if num_outgoings[root] != 0:\n",
        "        return None\n",
        "\n",
        "    # Non-layered topological ordering implementation\n",
        "    queue = deque([root])\n",
        "    while queue:\n",
        "        node = queue.popleft()\n",
        "        ordering.append(node)\n",
        "        for c in node.children:\n",
        "            num_outgoings[c] -= 1\n",
        "            if num_outgoings[c] == 0:\n",
        "                queue.append(c)\n",
        "\n",
        "    # Check if a cycle has been found\n",
        "    if sum(num_outgoings.values()) != 0:\n",
        "        return None\n",
        "    return ordering\n",
        "\n",
        "\n",
        "def topological_order_layered(root: Node) -> Optional[List[List[Node]]]:\n",
        "    \"\"\"\n",
        "    Compute the Topological Ordering Layered for a SPN, using the Kahn's Algorithm.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: A list of layers that form a topological ordering.\n",
        "             If the SPN graph is not acyclic, it returns None.\n",
        "    \"\"\"\n",
        "    ordering = list()\n",
        "    num_outgoings = defaultdict(int)\n",
        "    num_outgoings[root] = 0\n",
        "\n",
        "    # Initialize the number of outgoings edges for each node\n",
        "    for node in bfs(root):\n",
        "        for c in node.children:\n",
        "            num_outgoings[c] += 1\n",
        "\n",
        "    # Check the unusual case where the root node have outgoings edges, i.e. a trivial cycle has been found\n",
        "    if num_outgoings[root] != 0:\n",
        "        return None\n",
        "\n",
        "    # Layered topological ordering implementation\n",
        "    ordering.append([root])\n",
        "    while True:\n",
        "        layer = list()\n",
        "        for node in ordering[-1]:\n",
        "            for c in node.children:\n",
        "                num_outgoings[c] -= 1\n",
        "                if num_outgoings[c] == 0:\n",
        "                    layer.append(c)\n",
        "        if not layer:\n",
        "            break\n",
        "        ordering.append(layer)\n",
        "\n",
        "    # Check if a cycle has been found\n",
        "    if sum(num_outgoings.values()) != 0:\n",
        "        return None\n",
        "    return ordering\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------- Leaf\n",
        "\n",
        "class LeafType(Enum):\n",
        "    \"\"\"\n",
        "    The type of the distribution leaf.\n",
        "    It can be either discrete or continuous.\n",
        "    \"\"\"\n",
        "    DISCRETE = 1\n",
        "    CONTINUOUS = 2\n",
        "\n",
        "\n",
        "class Leaf(Node):\n",
        "    LEAF_TYPE = None\n",
        "\n",
        "    def __init__(self, scope: Union[int, List[int]]):\n",
        "        \"\"\"\n",
        "        Initialize a leaf node given its scope.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param kwargs: Additional arguments.\n",
        "        \"\"\"\n",
        "        super().__init__([scope] if isinstance(scope, int) else scope)\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        \"\"\"\n",
        "        Random initialize the leaf's parameters for Expectation-Maximization (EM).\n",
        "\n",
        "        :param random_state: The random state.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        \"\"\"\n",
        "        Compute a batch Expectation-Maximization (EM) step.\n",
        "\n",
        "        :param stats: The sufficient statistics of each sample.\n",
        "        :param data: The data regarding random variables of the leaf.\n",
        "        :param step_size: The step size of update.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def fit(self, data: np.ndarray, domain: Union[list, tuple], **kwargs):\n",
        "        \"\"\"\n",
        "        Fit the distribution parameters given the domain and some training data.\n",
        "\n",
        "        :param data: The training data.\n",
        "        :param domain: The domain of the distribution leaf.\n",
        "        :param kwargs: Optional parameters.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the likelihood of the distribution leaf given some input.\n",
        "\n",
        "        :param x: The inputs.\n",
        "        :return: The resulting likelihoods.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the logarithmic likelihood of the distribution leaf given some input.\n",
        "\n",
        "        :param x: The inputs.\n",
        "        :return: The resulting log-likelihoods.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def mpe(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the maximum at posteriori values.\n",
        "\n",
        "        :param x: The inputs.\n",
        "        :return: The distribution's maximum at posteriori values.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample from the leaf distribution.\n",
        "\n",
        "        :param x: The samples with possible NaN values.\n",
        "        :return: The completed samples.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def moment(self, k: int = 1) -> float:\n",
        "        \"\"\"\n",
        "        Compute the moment of a given order.\n",
        "\n",
        "        :param k: The order of the moment.\n",
        "        :return: The moment of order k.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def params_count(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of parameters of the distribution leaf.\n",
        "\n",
        "        :return: The number of parameters.\n",
        "        \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def params_dict(self) -> dict:\n",
        "        \"\"\"\n",
        "        Get a dictionary representation of the distribution parameters.\n",
        "\n",
        "        :return: A dictionary containing the distribution parameters.\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "class Bernoulli(Leaf):\n",
        "    LEAF_TYPE = LeafType.DISCRETE\n",
        "\n",
        "    def __init__(self, scope: int, p: float = 0.5):\n",
        "        \"\"\"\n",
        "        Initialize a Bernoulli leaf node given its scope.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param p: The Bernoulli probability.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        super().__init__(scope)\n",
        "        if p < 0.0 or p > 1.0:\n",
        "            raise ValueError(\"The Bernoulli probability must be in [0, 1]\")\n",
        "\n",
        "        self.p = p\n",
        "\n",
        "    def fit(self, data: np.ndarray, domain: list, alpha: float = 0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit the distribution parameters given the domain and some training data.\n",
        "\n",
        "        :param data: The training data.\n",
        "        :param domain: The domain of the distribution leaf.\n",
        "        :param alpha: The Laplace smoothing factor.\n",
        "        :param kwargs: Optional parameters.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        if domain != [0, 1]:\n",
        "            raise ValueError(\"The domain must be binary for a Bernoulli distribution\")\n",
        "        if alpha < 0.0:\n",
        "            raise ValueError(\"The Laplace smoothing factor must be non-negative\")\n",
        "\n",
        "        # Check the data dtype\n",
        "        data = check_data_dtype(data, dtype=np.float32)\n",
        "\n",
        "        # Estimate using Laplace smoothing\n",
        "        self.p = (np.sum(data) + alpha) / (len(data) + 2 * alpha)\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        self.p = random_state.rand()\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        alpha = np.finfo(np.float16).eps  # Use a very small Laplace smoothing factor\n",
        "        data = np.squeeze(data, axis=1)\n",
        "        total_stats = np.sum(stats)\n",
        "        p = (np.dot(stats, data) + alpha) / (total_stats + 2 * alpha)\n",
        "\n",
        "        # Update the parameters\n",
        "        self.p = (1.0 - step_size) * self.p + step_size * p\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        ls = np.ones([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        ls[~mask] = ss.bernoulli.pmf(x[~mask], self.p)\n",
        "        return ls\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        lls = np.zeros([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        lls[~mask] = ss.bernoulli.logpmf(x[~mask], self.p)\n",
        "        return lls\n",
        "\n",
        "    def mpe(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = 0 if self.p < 0.5 else 1\n",
        "        return x\n",
        "\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = ss.bernoulli.rvs(self.p, size=np.count_nonzero(mask))\n",
        "        return x\n",
        "\n",
        "    def moment(self, k: int = 1) -> float:\n",
        "        return ss.bernoulli.moment(k, self.p)\n",
        "\n",
        "    def params_count(self):\n",
        "        return 1\n",
        "\n",
        "    def params_dict(self):\n",
        "        return {'p': self.p}\n",
        "\n",
        "\n",
        "class Categorical(Leaf):\n",
        "    LEAF_TYPE = LeafType.DISCRETE\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: int,\n",
        "        categories: Optional[Union[List, np.ndarray]] = None,\n",
        "        probabilities: Optional[Union[List, np.ndarray]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a Categorical leaf node given its scope.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param categories: The possible categories.\n",
        "        :param probabilities: The probabilities associated to each category.\n",
        "        \"\"\"\n",
        "        super().__init__(scope)\n",
        "        self.categories = None\n",
        "        self.probabilities = None\n",
        "        self.distribution = None\n",
        "\n",
        "        if categories is not None and probabilities is not None:\n",
        "            if len(categories) != len(probabilities):\n",
        "                raise ValueError(\"Each category must be associated a probability\")\n",
        "            if not np.isclose(np.sum(probabilities), 1.0):\n",
        "                raise ValueError(\"Probabilities parameter must sum up to 1\")\n",
        "            if isinstance(categories, list):\n",
        "                categories = np.array(categories, np.int64)\n",
        "            if isinstance(probabilities, list):\n",
        "                probabilities = np.array(probabilities, np.float32)\n",
        "            self.categories = np.array(categories, np.int64)\n",
        "            self.probabilities = np.array(probabilities, np.float32)\n",
        "            self.distribution = ss.rv_discrete(values=(self.categories, self.probabilities))\n",
        "        elif categories is not None or probabilities is not None:\n",
        "            raise ValueError(\"Partial defined parameters (categories, probabilities) are not handled\")\n",
        "\n",
        "    def fit(self, data: np.ndarray, domain: list, alpha: float = 0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit the distribution parameters given the domain and some training data.\n",
        "\n",
        "        :param data: The training data.\n",
        "        :param domain: The domain of the distribution leaf.\n",
        "        :param alpha: The Laplace smoothing factor.\n",
        "        :param kwargs: Optional parameters.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        if not isinstance(domain, list):\n",
        "            raise ValueError(\"The domain must be categorical for a Categorical distribution\")\n",
        "        if alpha < 0.0:\n",
        "            raise ValueError(\"The Laplace smoothing factor must be non-negative\")\n",
        "\n",
        "        self.probabilities = np.empty(len(domain), np.float32)\n",
        "        for i, d in enumerate(domain):\n",
        "            self.probabilities[i] = (len(data[data == d]) + alpha) / (len(data) + len(domain) * alpha)\n",
        "        self.categories = np.array(domain, np.int64)\n",
        "        self.distribution = ss.rv_discrete(values=(self.categories, self.probabilities))\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        \"\"\"\n",
        "        Random initialize the leaf's parameters for Expectation-Maximization (EM).\n",
        "\n",
        "        :param random_state: The random state.\n",
        "        :raises ValueError: If the categories are not initialized.\n",
        "        \"\"\"\n",
        "        if self.categories is None:\n",
        "            raise ValueError(\"Categorical leaf distribution is not initialized\")\n",
        "\n",
        "        # Initialize the categories probabilities using a dirichlet distribution\n",
        "        self.probabilities = random_state.dirichlet(np.ones(len(self.categories)))\n",
        "        self.distribution = ss.rv_discrete(values=(self.categories, self.probabilities))\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        alpha = np.finfo(np.float16).eps  # Use a very small Laplace smoothing factor\n",
        "        data = np.squeeze(data, axis=1)\n",
        "        total_stats = np.sum(stats)\n",
        "\n",
        "        # Compute the probabilities for each category\n",
        "        probabilities = np.empty(len(self.categories), np.float32)\n",
        "        for i, d in enumerate(self.categories):\n",
        "            probabilities[i] = (np.sum(stats[data == d]) + alpha) / (total_stats + len(self.categories) * alpha)\n",
        "\n",
        "        # Update the parameters\n",
        "        self.probabilities = (1.0 - step_size) * self.probabilities + step_size * probabilities\n",
        "        self.distribution = ss.rv_discrete(values=(self.categories, self.probabilities))\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        ls = np.ones([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        ls[~mask] = self.distribution.pmf(x[~mask].astype(np.int64, copy=False))\n",
        "        return ls\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        lls = np.zeros([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        lls[~mask] = self.distribution.logpmf(x[~mask].astype(np.int64, copy=False))\n",
        "        return lls\n",
        "\n",
        "    def mpe(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = self.categories[self.probabilities.argmax()]\n",
        "        return x\n",
        "\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = self.distribution.rvs(size=np.count_nonzero(mask))\n",
        "        return x\n",
        "\n",
        "    def moment(self, k: int = 1) -> float:\n",
        "        return self.distribution.moment(k)\n",
        "\n",
        "    def params_count(self) -> int:\n",
        "        return 2 * len(self.categories)\n",
        "\n",
        "    def params_dict(self) -> dict:\n",
        "        if self.distribution is None:\n",
        "            return {'categories': None, 'probabilities': None}\n",
        "        return {'categories': self.categories, 'probabilities': self.probabilities}\n",
        "\n",
        "\n",
        "class Isotonic(Leaf):\n",
        "    LEAF_TYPE = LeafType.CONTINUOUS\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: int,\n",
        "        densities: Optional[Union[List[float], np.ndarray]] = None,\n",
        "        breaks: Optional[Union[List[float], np.ndarray]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a histogram-Isotonic leaf node given its scope.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param densities: The densities. They must sum up to one.\n",
        "        :param breaks: The breaks values, such that len(breaks) == len(densities) + 1.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        super().__init__(scope)\n",
        "        self.densities = None\n",
        "        self.breaks = None\n",
        "        self.distribution = None\n",
        "\n",
        "        if densities is not None and breaks is not None:\n",
        "            if len(breaks) != len(densities) + 1:\n",
        "                raise ValueError(\"Invalid histogram parameters shapes\")\n",
        "            if not np.isclose(np.sum(densities), 1.0):\n",
        "                raise ValueError(\"Densities parameter must sum up to 1\")\n",
        "            if isinstance(densities, list):\n",
        "                densities = np.array(densities, np.float32)\n",
        "            if isinstance(breaks, list):\n",
        "                breaks = np.array(breaks, np.float32)\n",
        "            self.densities = densities\n",
        "            self.breaks = breaks\n",
        "            self.distribution = ss.rv_histogram(histogram=(densities, breaks))\n",
        "        elif densities is not None or breaks is not None:\n",
        "            raise ValueError(\"Partial defined parameters (densities, breaks) are not handled\")\n",
        "\n",
        "    def fit(self, data: np.ndarray, domain: tuple, alpha: float = 0.1, **kwargs):\n",
        "        \"\"\"\n",
        "        Fit the distribution parameters given the domain and some training data.\n",
        "\n",
        "        :param data: The training data.\n",
        "        :param domain: The domain of the distribution leaf.\n",
        "        :param alpha: The Laplace smoothing factor.\n",
        "        :param kwargs: Optional parameters.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        if not isinstance(domain, tuple):\n",
        "            raise ValueError(\"The domain must be continuous for an Isotonic distribution\")\n",
        "        if alpha < 0.0:\n",
        "            raise ValueError(\"The Laplace smoothing factor must be non-negative\")\n",
        "        histogram, breaks = np.histogram(data, bins='fd')\n",
        "\n",
        "        # Apply Laplace smoothing and obtain the densities\n",
        "        densities = (histogram + alpha) / (len(data) + len(histogram) * alpha)\n",
        "        densities = densities.astype(np.float32, copy=False)\n",
        "        breaks = breaks.astype(np.float32, copy=False)\n",
        "\n",
        "        # Build the distribution\n",
        "        self.densities = densities\n",
        "        self.breaks = breaks\n",
        "        self.distribution = ss.rv_histogram(histogram=(densities, breaks))\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        raise NotImplementedError(\"EM parameters initialization not yet implemented for Isotonic distributions\")\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        raise NotImplementedError(\"EM step not yet implemented for Isotonic distributions\")\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        ls = np.ones([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        ood_mask = ~mask & ((x <= self.distribution.a) | (x >= self.distribution.b))\n",
        "        ls[~mask] = self.distribution.pdf(x[~mask])\n",
        "        ls[ood_mask] = np.finfo(np.float32).eps\n",
        "        return ls\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        lls = np.zeros([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        ood_mask = ~mask & ((x <= self.distribution.a) | (x >= self.distribution.b))\n",
        "        lls[~mask] = self.distribution.logpdf(x[~mask])\n",
        "        lls[ood_mask] = np.log(np.finfo(np.float64).eps)\n",
        "        return lls\n",
        "\n",
        "    def mpe(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        idx = np.argmax(self.densities)\n",
        "        x[mask] = (self.breaks[idx] + self.breaks[idx + 1]) / 2.0\n",
        "        return x\n",
        "\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = self.distribution.ppf(q=np.random.rand(np.count_nonzero(mask)))\n",
        "        return x\n",
        "\n",
        "    def moment(self, k: int = 1) -> np.ndarray:\n",
        "        return self.distribution.moment(k)\n",
        "\n",
        "    def params_count(self) -> int:\n",
        "        return 2 * len(self.densities) + 1\n",
        "\n",
        "    def params_dict(self) -> dict:\n",
        "        if self.distribution is None:\n",
        "            return {'densities': None, 'breaks': None}\n",
        "        return {'densities': self.densities, 'breaks': self.breaks}\n",
        "\n",
        "\n",
        "class Uniform(Leaf):\n",
        "    LEAF_TYPE = LeafType.CONTINUOUS\n",
        "\n",
        "    def __init__(self, scope: int, start: float = 0.0, width: float = 1.0):\n",
        "        \"\"\"\n",
        "        Initialize an Uniform leaf node given its scope.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param start: The start of the uniform distribution.\n",
        "        :param width: The width of the uniform distribution.\n",
        "        \"\"\"\n",
        "        super().__init__(scope)\n",
        "        self.start = start\n",
        "        self.width = width\n",
        "\n",
        "    def fit(self, data: np.ndarray, domain: tuple, **kwargs):\n",
        "        if not isinstance(domain, tuple):\n",
        "            raise ValueError(\"The domain must be continuous for an Uniform distribution\")\n",
        "\n",
        "        # Estimate the parameters of a uniform distribution\n",
        "        self.start, self.width = ss.uniform.fit(data)\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        raise NotImplementedError(\"EM parameters initialization not yet implemented for Uniform distributions\")\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        raise NotImplementedError(\"EM step not yet implemented for Uniform distributions\")\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        ls = np.ones([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        ls[~mask] = ss.uniform.pdf(x[~mask], self.start, self.width)\n",
        "        return ls\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        lls = np.zeros([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        lls[~mask] = ss.uniform.logpdf(x[~mask], self.start, self.width)\n",
        "        return lls\n",
        "\n",
        "    def mpe(self, x: np.ndarray) -> np.array:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = self.start\n",
        "        return x\n",
        "\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = ss.uniform.rvs(self.start, self.width, size=np.count_nonzero(mask))\n",
        "        return x\n",
        "\n",
        "    def moment(self, k: int = 1) -> float:\n",
        "        return ss.uniform.moment(k, self.start, self.width)\n",
        "\n",
        "    def params_count(self) -> int:\n",
        "        return 2\n",
        "\n",
        "    def params_dict(self) -> dict:\n",
        "        return {\n",
        "            'start': self.start,\n",
        "            'width': self.width\n",
        "        }\n",
        "\n",
        "\n",
        "class Gaussian(Leaf):\n",
        "    LEAF_TYPE = LeafType.CONTINUOUS\n",
        "\n",
        "    def __init__(self, scope: int, mean: float = 0.0, stddev: float = 1.0):\n",
        "        \"\"\"\n",
        "        Initialize a Gaussian leaf node given its scope.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param mean: The mean parameter.\n",
        "        :param stddev: The standard deviation parameter.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        super().__init__(scope)\n",
        "        if stddev <= 1e-5:\n",
        "            raise ValueError(\"The standard deviation of a Gaussian must be greater than 1e-5\")\n",
        "\n",
        "        self.mean = mean\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def fit(self, data: np.ndarray, domain: tuple, **kwargs):\n",
        "        if not isinstance(domain, tuple):\n",
        "            raise ValueError(\"The domain must be continuous for a Gaussian distribution\")\n",
        "\n",
        "        self.mean, self.stddev = ss.norm.fit(data)\n",
        "        self.stddev = max(self.stddev, 1e-5)\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        self.mean = 1e-1 * random_state.randn()\n",
        "        self.stddev = 0.5 + 1e-1 * np.tanh(random_state.randn())\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        data = np.squeeze(data, axis=1)\n",
        "        total_stats = np.sum(stats) + np.finfo(np.float32).eps\n",
        "        mean = np.sum(stats * data) / total_stats\n",
        "        stddev = np.sqrt(np.sum(stats * (data - mean) ** 2.0) / total_stats)\n",
        "        stddev = max(stddev, 1e-5)\n",
        "\n",
        "        # Update the parameters\n",
        "        self.mean = (1.0 - step_size) * self.mean + step_size * mean\n",
        "        self.stddev = (1.0 - step_size) * self.stddev + step_size * stddev\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        ls = np.ones([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        ls[~mask] = ss.norm.pdf(x[~mask], self.mean, self.stddev)\n",
        "        return ls\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        lls = np.zeros([len(x), 1], dtype=np.float32)\n",
        "        mask = np.isnan(x)\n",
        "        lls[~mask] = ss.norm.logpdf(x[~mask], self.mean, self.stddev)\n",
        "        return lls\n",
        "\n",
        "    def mpe(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = self.mean\n",
        "        return x\n",
        "\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mask = np.isnan(x)\n",
        "        x[mask] = ss.norm.rvs(self.mean, self.stddev, size=np.count_nonzero(mask))\n",
        "        return x\n",
        "\n",
        "    def moment(self, k: int = 1) -> float:\n",
        "        return ss.norm.moment(k, self.mean, self.stddev)\n",
        "\n",
        "    def params_count(self) -> int:\n",
        "        return 2\n",
        "\n",
        "    def params_dict(self) -> dict:\n",
        "        return {\n",
        "            'mean': self.mean,\n",
        "            'stddev': self.stddev\n",
        "        }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pwro9cZE6xNO"
      },
      "outputs": [],
      "source": [
        "#@title tree\n",
        "\n",
        "\n",
        "class TreeNode:\n",
        "    \"\"\"A simple class to model a node of a tree.\"\"\"\n",
        "    def __init__(self, node_id: int, parent: TreeNode = None):\n",
        "        \"\"\"\n",
        "        Initialize a binary CLT.\n",
        "\n",
        "        :param node_id: The ID of the node.\n",
        "        :param parent: The parent node.\n",
        "        \"\"\"\n",
        "        self.id = node_id\n",
        "        self.__parent = None\n",
        "        self.__children = []\n",
        "        self.set_parent(parent)\n",
        "\n",
        "    def get_id(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the ID of the node.\n",
        "\n",
        "        :return: The ID of the node.\n",
        "        \"\"\"\n",
        "        return self.id\n",
        "\n",
        "    def get_parent(self) -> TreeNode:\n",
        "        \"\"\"\n",
        "        Get the parent node.\n",
        "\n",
        "        :return: The parent node, None if the node has no parent.\n",
        "        \"\"\"\n",
        "        return self.__parent\n",
        "\n",
        "    def get_children(self) -> List[TreeNode]:\n",
        "        \"\"\"\n",
        "        Get the children list of the node.\n",
        "\n",
        "        :return: The children list of the node.\n",
        "        \"\"\"\n",
        "        return self.__children\n",
        "\n",
        "    def set_parent(self, parent: TreeNode):\n",
        "        \"\"\"\n",
        "        Set the parent node and update its children list.\n",
        "\n",
        "        :param parent: The parent node.\n",
        "        \"\"\"\n",
        "        if self.__parent is None and parent is not None:\n",
        "            self.__parent = parent\n",
        "            self.__parent.get_children().append(self)\n",
        "\n",
        "    def is_leaf(self) -> bool:\n",
        "        \"\"\"\n",
        "        Check whether the node is leaf.\n",
        "\n",
        "        :return: True if the node is leaf, False otherwise.\n",
        "        \"\"\"\n",
        "        return len(self.__children) == 0\n",
        "\n",
        "    def get_n_nodes(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the number of the nodes of the tree rooted at self.\n",
        "\n",
        "        :return: The number of nodes of the tree rooted at self.\n",
        "        \"\"\"\n",
        "        n_nodes = 0\n",
        "        queue = [self]\n",
        "        while queue:\n",
        "            current_node = queue.pop(0)\n",
        "            queue.extend(current_node.get_children())\n",
        "            n_nodes += 1\n",
        "        return n_nodes\n",
        "\n",
        "    def get_tree_scope(self) -> Tuple[list, list]:\n",
        "        \"\"\"\n",
        "        Return the list of predecessors and the related scope of the tree rooted at self.\n",
        "        Note that tree[root] must be -1, as it doesn't have a predecessor.\n",
        "\n",
        "        :return tree: List of predecessors.\n",
        "        :return scope: The related scope list.\n",
        "        \"\"\"\n",
        "        tree = []\n",
        "        scope = []\n",
        "        queue = [self]\n",
        "        while queue:\n",
        "            current_node = queue.pop(0)\n",
        "            queue.extend(current_node.get_children())\n",
        "            scope.append(current_node.id)\n",
        "            tree.append(current_node.get_parent().id if current_node.get_parent() is not None else -1)\n",
        "        tree[scope.index(self.id)] = -1\n",
        "        tree = [scope.index(t) if t != -1 else -1 for t in tree]\n",
        "        return tree, scope\n",
        "\n",
        "\n",
        "def build_tree_structure(tree: Union[List[int], np.ndarray], scope: Optional[List[int]] = None) -> TreeNode:\n",
        "    \"\"\"\n",
        "    Build a Tree node recursive data structure given a tree structure encoded as a list of predecessors.\n",
        "    Note that tree[root] must be -1, as it doesn't have a predecessor.\n",
        "    Optionally, a scope can be used to specify the tree node ids.\n",
        "\n",
        "    :param tree: The tree structure, as a sequence of predecessors.\n",
        "    :param scope: An optional scope, as a list of ids.\n",
        "    :return: The Tree node structure's root.\n",
        "    :raises ValueError: If the tree structure is not compatible with the root node.\n",
        "    :raises ValueError: If the scope contains duplicates.\n",
        "    :raises ValueError: If the scope is incompatible with the tree structure.\n",
        "    \"\"\"\n",
        "    if isinstance(tree, np.ndarray):\n",
        "        tree = tree.tolist()\n",
        "    if tree.count(-1) != 1:\n",
        "        raise ValueError(\"Invalid tree structure\")\n",
        "    root_idx = tree.index(-1)\n",
        "\n",
        "    if scope is None:\n",
        "        root_id = root_idx\n",
        "        nodes = [TreeNode(node_id) for node_id in range(len(tree))]\n",
        "        for node_id, parent_id in enumerate(tree):\n",
        "            if parent_id != -1:\n",
        "                nodes[node_id].set_parent(nodes[parent_id])\n",
        "    else:\n",
        "        if len(set(scope)) != len(scope):\n",
        "            raise ValueError(\"The scope must not contain duplicates\")\n",
        "        if len(scope) != len(tree):\n",
        "            raise ValueError(\"Invalid scope's number of variables\")\n",
        "\n",
        "        root_id = scope[root_idx]\n",
        "        nodes = {node_id: TreeNode(node_id) for node_id in scope}\n",
        "        for node_idx, parent_idx in enumerate(tree):\n",
        "            if parent_idx != -1:\n",
        "                node_id = scope[node_idx]\n",
        "                parent_id = scope[parent_idx]\n",
        "                nodes[node_id].set_parent(nodes[parent_id])\n",
        "\n",
        "    return nodes[root_id]\n",
        "\n",
        "\n",
        "def compute_bfs_ordering(tree: Union[List[int], np.ndarray]) -> Union[List[int], np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute the breadth-first-search variable ordering given a tree structure.\n",
        "    Note that tree[root] must be -1, as it doesn't have a predecessor.\n",
        "\n",
        "    :param tree: The tree structure, as a sequence of predecessors.\n",
        "    :return: The BFS variable ordering as a Numpy array.\n",
        "    \"\"\"\n",
        "    # Build the tree structure first\n",
        "    root = build_tree_structure(tree)\n",
        "\n",
        "    # Pre-Order exploration\n",
        "    ordering = list()\n",
        "    nodes_queue = deque([root])\n",
        "    while nodes_queue:\n",
        "        node = nodes_queue.popleft()\n",
        "        ordering.append(node.get_id())\n",
        "        if not node.is_leaf():\n",
        "            nodes_queue.extend(node.get_children())\n",
        "\n",
        "    if isinstance(tree, list):\n",
        "        return ordering\n",
        "    return np.array(ordering, dtype=tree.dtype)\n",
        "\n",
        "\n",
        "def maximum_spanning_tree(root: int, adj_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Compute the maximum spanning tree of a graph starting from a given root node.\n",
        "\n",
        "    :param root: The root node index.\n",
        "    :param adj_matrix: The graph's adjacency matrix.\n",
        "    :return: The breadth first traversal ordering and the maximum spanning tree.\n",
        "             The maximum spanning tree is given as a list of predecessors.\n",
        "    \"\"\"\n",
        "    # Compute the maximum spanning tree of an adjacency matrix\n",
        "    # Note adding one to the adjacency matrix, because the graph must be fully connected\n",
        "    mst = sp.csgraph.minimum_spanning_tree(-(adj_matrix + 1.0), overwrite=True)\n",
        "    bfs, tree = sp.csgraph.breadth_first_order(\n",
        "        mst, directed=False, i_start=root, return_predecessors=True\n",
        "    )\n",
        "    tree[root] = -1\n",
        "    return bfs, tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "RMG4iya39xUZ"
      },
      "outputs": [],
      "source": [
        "#@title partition\n",
        "\n",
        "\n",
        "class Partition:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        row_ids: list,\n",
        "        col_ids: list,\n",
        "        uncond_vars: list,\n",
        "        parent_partition: Optional[Partition] = None,\n",
        "        is_naive: Optional[bool] = False,\n",
        "        is_conj: Optional[bool] = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Create a partition, i.e. an object modeling a data slice (and some of its properties)\n",
        "        by keeping track of its indices (i.e. row_ids and col_ids).\n",
        "\n",
        "        :param row_ids: The row indices of the modeled slice.\n",
        "        :param col_ids: The column indices of the modeled slice.\n",
        "        :param uncond_vars:  Ordered list of variables from which the conjunction variables will be extracted\n",
        "         to horizontally split the current partition.\n",
        "        :param parent_partition: The optional parent partition\n",
        "        :param is_naive: If True and determinism is not required, a naive factorization will be learnt over the data\n",
        "         slice modeled by the current partition; otherwise, if True and determinism is required, a disjunction\n",
        "         will be learnt over the data slice modeled by the current partition.\n",
        "        :param is_conj: True if the modeled slice is associated to a conjunction, i.e. every row in the slice is\n",
        "         equal to the others.\n",
        "        \"\"\"\n",
        "        self.row_ids = np.array(row_ids)\n",
        "        self.col_ids = np.array(col_ids)\n",
        "        self.uncond_vars = list(uncond_vars)\n",
        "\n",
        "        self.parent_partition = parent_partition\n",
        "        self.set_parent_partition(parent_partition)\n",
        "        self.sub_partitions = []\n",
        "\n",
        "        self.is_naive = is_naive\n",
        "        self.is_conj = is_conj\n",
        "        # discarded assignments, see build_leaf() in xpc.py\n",
        "        self.disc_assignments = None\n",
        "\n",
        "    def set_parent_partition(self, parent_partition: Partition):\n",
        "        \"\"\"\n",
        "        Set the parent partition and update its sub_partitions attribute.\n",
        "\n",
        "        :param parent_partition: The parent partition.\n",
        "        \"\"\"\n",
        "        if parent_partition is not None:\n",
        "            parent_partition.sub_partitions.append(self)\n",
        "\n",
        "    def is_partitioned(self):\n",
        "        \"\"\"\n",
        "        :return: True if the partition is partitioned, False otherwise.\n",
        "        \"\"\"\n",
        "        return len(self.sub_partitions) != 0\n",
        "\n",
        "    def is_horizontally_partitioned(self):\n",
        "        \"\"\"\n",
        "        :return: True if the partition is horizontally partitioned, False otherwise.\n",
        "        \"\"\"\n",
        "        ret = False\n",
        "        if self.is_partitioned():\n",
        "            ret = len(self.row_ids) > len(self.sub_partitions[0].row_ids)\n",
        "        return ret\n",
        "\n",
        "    def get_slice(self, data: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Slice the input data matrix according to self.\n",
        "\n",
        "        :param data: The data to be sliced.\n",
        "        :return: The data slice.\n",
        "        \"\"\"\n",
        "        return data[self.row_ids][:, self.col_ids]\n",
        "\n",
        "    def get_vertical_split(self) -> list[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        If possible, split vertically the current partition.\n",
        "        \"\"\"\n",
        "        vertical_split = []\n",
        "        cond_vars = [col_id for col_id in self.col_ids if col_id not in self.uncond_vars]\n",
        "        if len(cond_vars) != 0 and len(cond_vars) != len(self.col_ids):\n",
        "            vertical_split = [np.asarray(cond_vars), np.asarray(self.uncond_vars)]\n",
        "        return vertical_split\n",
        "\n",
        "    def get_conj_row_ids(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        conj: list,\n",
        "        min_part_inst: int,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Return the row ids of the instances satisfying the given conjunction.\n",
        "        The row ids must be found within the slice modeled by the self partition.\n",
        "\n",
        "        :param data: The input data.\n",
        "        :param conj: Conjunction modeled as a list of two lists: the first contains\n",
        "                     the IDs of the variables, the second  the related assignment.\n",
        "                     For example, [[8,3],[1,0]] models the conjunction X8=1 and X3=0.\n",
        "        :param min_part_inst: the minimum number of instances allowed to return.\n",
        "        :return: The row ids of the instances satisfying the given conjunction iff\n",
        "                 the number of such instances is greater or equal than the minimum number\n",
        "                 of instances allowed to return; otherwise, an empty array.\n",
        "        \"\"\"\n",
        "        if len(self.row_ids) < min_part_inst:\n",
        "            conj_row_ids = np.empty(0, dtype=np.int32)\n",
        "        else:\n",
        "            conj_row_ids = self.row_ids.copy()\n",
        "            for i in range(len(conj[0])):\n",
        "                conj_row_ids = conj_row_ids[np.where(data[np.array(conj_row_ids), conj[0][i]] == conj[1][i])[0]]\n",
        "                if len(conj_row_ids) < min_part_inst:\n",
        "                    conj_row_ids = np.empty(0, dtype=np.int32)\n",
        "                    break\n",
        "        return conj_row_ids\n",
        "\n",
        "    def get_horizontal_split(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        min_part_inst: int,\n",
        "        conj_len: int,\n",
        "        arity: int,\n",
        "        sd: bool,\n",
        "        random_state: np.random.RandomState\n",
        "    ) -> Tuple[list, np.ndarray, list]:\n",
        "        \"\"\"\n",
        "        If possible, split horizontally the current partition.\n",
        "\n",
        "        :param data: The input data matrix.\n",
        "        :param min_part_inst: The minimum number of instances allowed per partition.\n",
        "        :param conj_len: The conjunction length.\n",
        "        :param arity: The maximum number of subpartitions for an horizontal partitioned partition.\n",
        "        :param sd: True if the generated tree will be used to model a SD PC, False otherwise.\n",
        "        :param random_state: The random state.\n",
        "        \"\"\"\n",
        "        if len(self.uncond_vars) < conj_len or len(self.row_ids) < 2 * min_part_inst:\n",
        "            return [], np.array([]), []\n",
        "\n",
        "        uncond_vars = self.uncond_vars.copy()\n",
        "        if not sd:\n",
        "            random_state.shuffle(uncond_vars)\n",
        "        conj_vars = uncond_vars[:conj_len]\n",
        "\n",
        "        # list of all possible assignments for a conjunction with length conj_len\n",
        "        assignments = [list(assignment) for assignment in itertools.product([0, 1], repeat=len(conj_vars))]\n",
        "        random_state.shuffle(assignments)\n",
        "\n",
        "        discarded_row_ids = self.row_ids.copy()\n",
        "        conj_row_ids_l = []\n",
        "        for assignment in assignments:\n",
        "            conj = [conj_vars, assignment]\n",
        "            conj_row_ids = self.get_conj_row_ids(data, conj, min_part_inst)\n",
        "            if len(conj_row_ids) == len(self.row_ids):\n",
        "                return [], discarded_row_ids, conj_vars\n",
        "            if len(conj_row_ids) != 0 and len(discarded_row_ids) - len(conj_row_ids) >= min_part_inst:\n",
        "                discarded_row_ids = np.setdiff1d(discarded_row_ids, conj_row_ids)\n",
        "                conj_row_ids_l.append(conj_row_ids)\n",
        "                if len(conj_row_ids_l) == arity - 1 or len(discarded_row_ids) < 2 * min_part_inst:\n",
        "                    break\n",
        "\n",
        "        if conj_row_ids_l:\n",
        "            return conj_row_ids_l, discarded_row_ids, conj_vars\n",
        "        return [], np.array([]), []\n",
        "\n",
        "\n",
        "def generate_random_partitioning(\n",
        "    data: np.ndarray,\n",
        "    min_part_inst: int,\n",
        "    n_max_parts: int,\n",
        "    conj_len: int,\n",
        "    arity: int,\n",
        "    sd: bool,\n",
        "    uncond_vars: list,\n",
        "    random_state: np.random.RandomState\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a random partition tree.\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param min_part_inst: The minimum number of instances allowed per partition.\n",
        "    :param n_max_parts: The maximum number of partitions in the tree.\n",
        "    :param conj_len: The conjunction length.\n",
        "    :param arity: The maximum number of subpartitions for an horizontal partitioned partition.\n",
        "    :param sd: True if the generated tree will be used to model a SD PC, False otherwise.\n",
        "    :param uncond_vars: Ordered list of variables from which the first *conj_len* ones\n",
        "     are extracted as conjunction variables to partition the root partition.\n",
        "    :param random_state: The random state.\n",
        "\n",
        "    :return partition_root: The partition root of the tree.\n",
        "    :return cl_parts_l: List containing the leaf partitions over which a CLTree will be learnt.\n",
        "    :return conj_vars_l: List of lists. Every sublist contains the variables of a conjunction (e.g. [[3, 5]]).\n",
        "     If a sublist occurs before another, then the former has been used first. There are no duplicates.\n",
        "    :return n_partitions: The number of partitions in the generated tree.\n",
        "    \"\"\"\n",
        "    partition_root = Partition(row_ids=np.arange(data.shape[0]),\n",
        "                               col_ids=uncond_vars,\n",
        "                               uncond_vars=uncond_vars,\n",
        "                               parent_partition=None)\n",
        "    n_partitions = 0\n",
        "    conj_vars_l = []\n",
        "    cl_parts_l = []\n",
        "    leaves = [partition_root]\n",
        "    while leaves and n_partitions + len(leaves) < n_max_parts:\n",
        "        # randomly pop a leaf partition\n",
        "        part = leaves.pop(random_state.randint(len(leaves)))\n",
        "\n",
        "        conj_row_ids_l, discarded_row_ids, conj_vars = \\\n",
        "            part.get_horizontal_split(data, min_part_inst, conj_len, arity, sd, random_state)\n",
        "\n",
        "        if len(discarded_row_ids):\n",
        "            if conj_vars not in conj_vars_l:\n",
        "                conj_vars_l.append(conj_vars)\n",
        "\n",
        "            # this ensures a general definition of the list uncond_vars, preserving its order\n",
        "            uncond_vars = [uv for uv in part.uncond_vars if uv not in conj_vars]\n",
        "\n",
        "            part_buffer = [\n",
        "                Partition(row_ids=discarded_row_ids,\n",
        "                          col_ids=part.col_ids.copy(),\n",
        "                          uncond_vars=uncond_vars.copy(),\n",
        "                          parent_partition=part)]\n",
        "\n",
        "            for conj_row_ids in conj_row_ids_l:\n",
        "                part_buffer.append(\n",
        "                    Partition(row_ids=conj_row_ids,\n",
        "                              col_ids=part.col_ids.copy(),\n",
        "                              uncond_vars=uncond_vars.copy(),\n",
        "                              parent_partition=part))\n",
        "\n",
        "            discarded_assignments = \\\n",
        "                {tuple(assignment) for assignment in itertools.product([0, 1], repeat=len(conj_vars))}\n",
        "\n",
        "            for k in range(len(part_buffer)):\n",
        "                part = part_buffer[k]\n",
        "                vertical_split = part.get_vertical_split()\n",
        "                if vertical_split:\n",
        "                    n_partitions += 1\n",
        "                    is_conj = False if not k else True\n",
        "                    p = Partition(row_ids=part.row_ids.copy(),\n",
        "                                  col_ids=vertical_split[0].copy(),\n",
        "                                  uncond_vars=[],\n",
        "                                  parent_partition=part,\n",
        "                                  is_naive=True,\n",
        "                                  is_conj=is_conj)\n",
        "                    if is_conj:\n",
        "                        discarded_assignments.remove(tuple(p.get_slice(data)[0]))\n",
        "\n",
        "                    leaves.append(\n",
        "                        Partition(row_ids=part.row_ids.copy(),\n",
        "                                  col_ids=vertical_split[1].copy(),\n",
        "                                  uncond_vars=vertical_split[1].copy(),\n",
        "                                  parent_partition=part))\n",
        "                else:\n",
        "                    leaves.append(part)\n",
        "\n",
        "            if part_buffer[0].sub_partitions:\n",
        "                part_buffer[0].sub_partitions[0].disc_assignments = \\\n",
        "                    np.array(list(discarded_assignments))\n",
        "        else:\n",
        "            n_partitions += 1\n",
        "            cl_parts_l.append(part)\n",
        "\n",
        "    # in case the process ended because n_partitions + len(leaves) > n_max_parts\n",
        "    n_partitions += len(leaves)\n",
        "    cl_parts_l.extend(leaves)\n",
        "    return partition_root, cl_parts_l, conj_vars_l, n_partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "63OjnZpHVWEX"
      },
      "outputs": [],
      "source": [
        "#@title random graph generation\n",
        "\n",
        "class RegionGraph:\n",
        "    def __init__(self, n_features: int, depth: int, random_state: Optional[RandomState] = None):\n",
        "        \"\"\"\n",
        "        Initialize a region graph.\n",
        "\n",
        "        A region graph is defined w.r.t. a set of indices of random variable in a SPN.\n",
        "        A *region* R is defined as a non-empty subset of the indices,\n",
        "        and represented as sorted tuples with unique entries.\n",
        "        A *partition* P of a region R is defined as a collection of non-empty sets,\n",
        "        which are non-overlapping, and whose\n",
        "        union is R. R is also called *parent* region of P.\n",
        "        Any region C such that C is in partition P is called *child region* of P.\n",
        "        So, a region is represented as a sorted tuple of integers (unique elements)\n",
        "        and a partition is represented as a sorted tuple of regions (non-overlapping, not-empty, at least 2).\n",
        "        A *region graph* is an acyclic, directed, bi-partite graph over regions and partitions.\n",
        "        So, any child of a region R is a partition of R, and any child of a partition is\n",
        "        a child region of the partition. The root of the region graph\n",
        "        is a sorted tuple composed of all the elements. The leaves of the region graph must also be regions.\n",
        "        They are called input regions, or leaf regions.\n",
        "        Given a region graph, we can easily construct a corresponding SPN:\n",
        "        1) Associate I distributions to each input region.\n",
        "        2) Associate K sum nodes to each other (non-input) region.\n",
        "        3) For each partition P in the region graph,\n",
        "        take all cross-products (as product nodes) of distributions/sum nodes associated with the child regions.\n",
        "        Connect these products as children of all sum nodes in the parent region of P.\n",
        "        In the end, this procedure will always deliver a complete and decomposable SPN.\n",
        "\n",
        "        :param n_features: The number of features.\n",
        "        :param depth: The maximum depth.\n",
        "        :param random_state: The random state. It can be either None, a seed integer or a Numpy RandomState.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        if n_features <= 0:\n",
        "            raise ValueError(\"The number of features must be positive\")\n",
        "        if depth <= 0:\n",
        "            raise ValueError(\"The region graph depth must be positive\")\n",
        "        if depth > int(np.log2(n_features)):\n",
        "            raise ValueError(\"Invalid region graph depth based on the number of features\")\n",
        "\n",
        "        self.items = tuple(range(n_features))\n",
        "        self.depth = depth\n",
        "\n",
        "        # Check the random state\n",
        "        self.random_state = check_random_state(random_state)\n",
        "\n",
        "    def random_layers(self) -> List[List[tuple]]:\n",
        "        \"\"\"\n",
        "        Generate a list of layers randomly over a single repetition of features.\n",
        "\n",
        "        :return: A list of layers, alternating between regions and partitions.\n",
        "        \"\"\"\n",
        "        root = [self.items]\n",
        "        layers = [root]\n",
        "\n",
        "        for i in range(self.depth):\n",
        "            regions = []\n",
        "            partitions = []\n",
        "            for r in layers[i * 2]:\n",
        "                mid = len(r) // 2\n",
        "                permutation = self.random_state.permutation(r).tolist()\n",
        "                p0 = tuple(sorted(permutation[:mid]))\n",
        "                p1 = tuple(sorted(permutation[mid:]))\n",
        "                regions.append(p0)\n",
        "                regions.append(p1)\n",
        "                partitions.append((p0, p1))\n",
        "            layers.append(partitions)\n",
        "            layers.append(regions)\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def make_layers(self, n_repetitions: int = 1) -> List[List[tuple]]:\n",
        "        \"\"\"\n",
        "        Generate a random graph's layers over multiple repetitions of features.\n",
        "\n",
        "        :param n_repetitions: The number of repetitions.\n",
        "        :return: A list of layers, alternating between regions and partitions.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        if n_repetitions <= 0:\n",
        "            raise ValueError(\"The number of repetitions must be positve\")\n",
        "\n",
        "        root = [self.items]\n",
        "        graph_layers = [root] + [[]] * (self.depth * 2)\n",
        "\n",
        "        for _ in range(n_repetitions):\n",
        "            layers = self.random_layers()\n",
        "            for h in range(1, len(layers)):\n",
        "                graph_layers[h] = graph_layers[h] + layers[h]\n",
        "\n",
        "        return graph_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XKUPCP3i1sYS"
      },
      "outputs": [],
      "source": [
        "#@title binary Chow-Liu Tree : CLT\n",
        "\n",
        "\n",
        "# a random state type is either an integer seed value or a Numpy RandomState instance.\n",
        "RandomState = Union[int, np.random.RandomState]\n",
        "\n",
        "\n",
        "def check_random_state(random_state: Optional[RandomState] = None) -> np.random.RandomState:\n",
        "    \"\"\"\n",
        "    Check a possible input random state and return it as a Numpy's RandomState object.\n",
        "\n",
        "    :param random_state: The random state to check. If None a new Numpy RandomState will be returned.\n",
        "                         If not None, it can be either a seed integer or a np.random.RandomState instance.\n",
        "                         In the latter case, itself will be returned.\n",
        "    :return: A Numpy's RandomState object.\n",
        "    :raises ValueError: If the random state is not None or a seed integer or a Numpy RandomState object.\n",
        "    \"\"\"\n",
        "    if random_state is None:\n",
        "        return np.random.RandomState()\n",
        "    if isinstance(random_state, int):\n",
        "        return np.random.RandomState(random_state)\n",
        "    if isinstance(random_state, np.random.RandomState):\n",
        "        return random_state\n",
        "    raise ValueError(\"The random state must be either None, a seed integer or a Numpy RandomState object\")\n",
        "\n",
        "\n",
        "class BinaryCLT(Leaf):\n",
        "    LEAF_TYPE = LeafType.DISCRETE\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: List[int],\n",
        "        root: Optional[int] = None,\n",
        "        tree: Optional[Union[List[int], np.ndarray]] = None,\n",
        "        params: Optional[Union[List[List[List[float]]], np.ndarray]] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Binary Chow-Liu Tree (CLT) multi-variate leaf node.\n",
        "\n",
        "        :param scope: The scope of the leaf.\n",
        "        :param root: The root node of the CLT. If None it will be chosen randomly.\n",
        "        :param tree: A sequence of variable ids predecessors (encoding the tree structure).\n",
        "        :param params: The CLT conditional probability tables (CPTs), as a (N, 2, 2) Numpy array in logarithmic scale.\n",
        "                       Note that params[i, l, k] = log P(X_i=k | Pa(X_i)=l).\n",
        "        :raises ValueError: If the root variable is not in scope.\n",
        "        :raises ValueError: If the tree structure is not compatible with the number of variables and root node.\n",
        "        :raises ValueError: If the CPTs parameters are invalid.\n",
        "        \"\"\"\n",
        "        super().__init__(scope)\n",
        "\n",
        "        if tree is not None:\n",
        "            if isinstance(tree, list):\n",
        "                tree = np.array(tree, dtype=np.int32)\n",
        "\n",
        "            # Check tree structure with respect to the scope\n",
        "            if len(tree) != len(self.scope):\n",
        "                raise ValueError(\"Invalid tree structure's number of variables\")\n",
        "\n",
        "            # Check root node with respect to the tree structure\n",
        "            if root is None:\n",
        "                root, = np.argwhere(tree == -1)\n",
        "                if len(root) != 1:\n",
        "                    raise ValueError(\"Invalid tree structure's root node\")\n",
        "                root = root.item()\n",
        "            elif root not in self.scope:\n",
        "                raise ValueError(\"The root variable must be in scope\")\n",
        "            else:\n",
        "                root = self.scope.index(root)\n",
        "            if tree[root] != -1:\n",
        "                raise ValueError(\"Invalid tree structure's root node\")\n",
        "\n",
        "            # Compute BFS variable ordering\n",
        "            bfs = compute_bfs_ordering(tree)\n",
        "        else:\n",
        "            bfs = None\n",
        "            # Check root node with respect to the scope\n",
        "            if root is not None:\n",
        "                if root not in self.scope:\n",
        "                    raise ValueError(\"The root variable must be in scope\")\n",
        "                root = self.scope.index(root)\n",
        "        self.root = root\n",
        "        self.tree = tree\n",
        "        self.bfs = bfs\n",
        "\n",
        "        # Initialize the parameters\n",
        "        if isinstance(params, list):\n",
        "            params = np.array(params, dtype=np.float32)\n",
        "            if params.shape != (len(self.scope), 2, 2):\n",
        "                raise ValueError(\"Invalid conditional probability table (CPT) shape\")\n",
        "            if not np.allclose(np.exp(params).sum(axis=2), 1.0):\n",
        "                raise ValueError(\"Invalid conditional probability table (CPT) values\")\n",
        "        self.params = params\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_clt_parameters(\n",
        "        bfs: np.ndarray,\n",
        "        tree: np.ndarray,\n",
        "        priors: np.ndarray,\n",
        "        joints: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the parameters of the CLTree given the tree structure and the priors and joints distributions.\n",
        "\n",
        "        This function returns the conditional probability tables (CPTs) in a tensorized form.\n",
        "        Note that params[i, l, k] = P(X_i=k | Pa(X_i)=l).\n",
        "        A special case is made for the root distribution which is not conditioned.\n",
        "        Note that params[root, :, k] = P(X_root=k).\n",
        "\n",
        "        :param bfs: The bfs structure, i.e. a sequence of successors in a breadth-first traversal.\n",
        "        :param tree: The tree structure, i.e. a sequence of predecessors in a tree structure.\n",
        "        :param priors: The priors distributions.\n",
        "        :param joints: The joints distributions.\n",
        "        :return: The conditional probability tables (CPTs) in a tensorized form.\n",
        "        \"\"\"\n",
        "        root_id = bfs[0]\n",
        "        n_features = len(bfs)\n",
        "        vs = np.arange(n_features)\n",
        "\n",
        "        # Compute the conditional probabilities (by einsum operation)\n",
        "        params = np.einsum('ikl,il->ilk', joints[vs, tree], np.reciprocal(priors[tree]))\n",
        "        params[root_id] = priors[root_id]\n",
        "\n",
        "        # Re-normalize the factors, because there can be FP32 approximation errors\n",
        "        params /= np.sum(params, axis=2, keepdims=True)\n",
        "        return params\n",
        "\n",
        "    def em_init(self, random_state: np.random.RandomState):\n",
        "        if self.tree is None:\n",
        "            raise ValueError(\"The CLT's structure must be already initialized\")\n",
        "\n",
        "        probs = random_state.rand(len(self.scope), 2)\n",
        "        probs[self.root, 0] = probs[self.root, 1]\n",
        "        self.params[:, :, 1] = probs\n",
        "        self.params[:, :, 0] = 1.0 - probs\n",
        "        self.params = np.log(self.params)\n",
        "\n",
        "    def em_step(self, stats: np.ndarray, data: np.ndarray, step_size: float):\n",
        "        if self.tree is None:\n",
        "            raise ValueError(\"The CLT's structure must be already initialized\")\n",
        "\n",
        "        alpha = np.finfo(np.float16).eps  # Use a very small Laplace smoothing factor\n",
        "        total_stats = np.sum(stats)\n",
        "        weighted_features = np.expand_dims(stats, axis=1) * data\n",
        "\n",
        "        # Compute prior distributions\n",
        "        priors_stats = np.sum(weighted_features, axis=0)\n",
        "        priors = np.empty(shape=(len(self.scope), 2), dtype=np.float32)\n",
        "        priors[:, 1] = (priors_stats + 2.0 * alpha) / (total_stats + 4.0 * alpha)\n",
        "        priors[:, 0] = 1.0 - priors[:, 1]\n",
        "\n",
        "        # Compute conditional sufficient statistics\n",
        "        conditional_stats = np.empty(shape=(len(self.scope), 2), dtype=np.float32)\n",
        "        conditional_stats[:, 1] = np.sum(weighted_features * data[:, self.tree], axis=0)\n",
        "        conditional_stats[:, 0] = priors_stats - conditional_stats[:, 1]\n",
        "\n",
        "        # Update the parameters\n",
        "        params = np.empty_like(self.params)\n",
        "        params[:, :, 1] = (conditional_stats + alpha) / (total_stats * priors[self.tree] + 4.0 * alpha)\n",
        "        params[:, :, 0] = 1.0 - params[:, :, 1]\n",
        "        params[self.root, 0] = params[self.root, 1] = priors[self.root]\n",
        "        params = (1.0 - step_size) * np.exp(self.params) + step_size * params\n",
        "\n",
        "        # Re-normalize the factors, because there can be FP32 approximation errors\n",
        "        params /= np.sum(params, axis=2, keepdims=True)\n",
        "        self.params = np.log(params)\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        domain: List[list],\n",
        "        alpha: float = 0.1,\n",
        "        random_state: Optional[RandomState] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit the distribution parameters (and structure if necessary) given the domain and some training data.\n",
        "\n",
        "        :param data: The training data.\n",
        "        :param domain: The domain of the distribution leaf.\n",
        "        :param alpha: The Laplace smoothing factor.\n",
        "        :param random_state: The random state. It can be either None, a seed integer or a Numpy RandomState.\n",
        "        :param kwargs: Optional parameters.\n",
        "        :raises ValueError: If the random state is not valid.\n",
        "        :raises ValueError: If a parameter is out of domain.\n",
        "        \"\"\"\n",
        "        _, n_features = data.shape\n",
        "        if len(domain) != n_features:\n",
        "            raise ValueError(\"Each data column should correspond to a random variable having a domain\")\n",
        "        if not all(d == [0, 1] for d in domain):\n",
        "            raise ValueError(\"The domains must be binary for a Binary CLT distribution\")\n",
        "        if alpha < 0.0:\n",
        "            raise ValueError(\"The Laplace smoothing factor must be non-negative\")\n",
        "\n",
        "        # Check the random state\n",
        "        random_state = check_random_state(random_state)\n",
        "\n",
        "        # Choose a root variable randomly, if not specified\n",
        "        if self.root is None:\n",
        "            self.root = random_state.choice(len(self.scope))\n",
        "\n",
        "        # Estimate the priors and joints probabilities\n",
        "        priors, joints = estimate_priors_joints(data, alpha=alpha)\n",
        "\n",
        "        if self.tree is None:\n",
        "            # Compute the mutual information\n",
        "            mutual_info = compute_mutual_information(priors, joints)\n",
        "\n",
        "            # Compute the CLT structure\n",
        "            self.bfs, self.tree = maximum_spanning_tree(self.root, mutual_info)\n",
        "\n",
        "        # Compute the CLT parameters (in log-space), using the joints and priors probabilities\n",
        "        params = self.compute_clt_parameters(self.bfs, self.tree, priors, joints)\n",
        "        self.params = np.log(params)\n",
        "\n",
        "    def message_passing(\n",
        "        self, x: np.ndarray,\n",
        "        obs_mask: np.ndarray,\n",
        "        return_lls: bool = True,\n",
        "        reduce: str = 'mar'\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the messages passed from the leaves to the root node.\n",
        "\n",
        "        :param x: The input data.\n",
        "        :param obs_mask: The mask of observed values.\n",
        "        :param return_lls: Whether to compute and return the log-likelihoods.\n",
        "        :param reduce: The method used to reduce the messages of missing values.\n",
        "                       It can be either 'mar' (marginalize the message) or 'mpe' (maximum probable explanation).\n",
        "        :return: The messages array if return_lls is False.\n",
        "                 The log-likelihoods if return_lls is True.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = x.shape\n",
        "        messages = np.zeros(shape=(n_features, n_samples, 2), dtype=np.float32)\n",
        "\n",
        "        # Let's proceed bottom-up\n",
        "        for j in reversed(self.bfs[1:]):\n",
        "            mask = obs_mask[:, j]\n",
        "            mis_mask = ~mask\n",
        "            obs_values = x[mask, j].astype(np.int64)\n",
        "            msg = np.expand_dims(messages[j], axis=1)\n",
        "\n",
        "            # Compute the messages for observed data\n",
        "            messages[self.tree[j], mask] += self.params[j, :, obs_values] + msg[mask, :, obs_values]\n",
        "\n",
        "            # Compute the messages for unobserved data\n",
        "            if np.any(mis_mask):\n",
        "                parent_msg = self.params[j] + msg[mis_mask]\n",
        "                if reduce == 'mar':\n",
        "                    messages[self.tree[j], mis_mask] += logsumexp(parent_msg, axis=2)\n",
        "                elif reduce == 'mpe':\n",
        "                    messages[self.tree[j], mis_mask] += np.max(parent_msg, axis=2)\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown reduce method called {}\".format(reduce))\n",
        "\n",
        "        if not return_lls:\n",
        "            return messages\n",
        "\n",
        "        lls = np.empty(n_samples, dtype=np.float32)\n",
        "        mask = obs_mask[:, self.root]\n",
        "        mis_mask = ~mask\n",
        "        obs_values = x[mask, self.root].astype(np.int64)\n",
        "        msg = messages[self.root]\n",
        "\n",
        "        # Compute the messages for observed data at root node\n",
        "        lls[mask] = self.params[self.root, 0, obs_values] + msg[mask, obs_values]\n",
        "\n",
        "        # Compute the messages for unobserved data at root node\n",
        "        if np.any(mis_mask):\n",
        "            lls[mis_mask] = logsumexp(self.params[self.root, 0] + msg[mis_mask], axis=1)\n",
        "\n",
        "        return lls\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.exp(self.log_likelihood(x))\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        n_samples, n_features = x.shape\n",
        "\n",
        "        # Build the mask of samples with missing values (used for marginalization)\n",
        "        mis_mask = np.isnan(x)\n",
        "        mar_mask = np.any(mis_mask, axis=1)\n",
        "\n",
        "        if np.any(mar_mask):\n",
        "            evi_mask = ~mar_mask\n",
        "            obs_mask = ~mis_mask\n",
        "            lls = np.empty(n_samples, dtype=np.float32)\n",
        "\n",
        "            # Vectorized implementation of full-evidence inference\n",
        "            vs = np.arange(n_features)\n",
        "            z = x[evi_mask]\n",
        "            z_cond = z[:, self.tree].astype(np.int64, copy=False)\n",
        "            z_vals = z[:, vs].astype(np.int64, copy=False)\n",
        "            lls[evi_mask] = np.sum(self.params[vs, z_cond, z_vals], axis=1)\n",
        "\n",
        "            # Semi-vectorized implementation of marginal inference\n",
        "            z = x[mar_mask]\n",
        "            lls[mar_mask] = self.message_passing(z, obs_mask[mar_mask], return_lls=True, reduce='mar')\n",
        "            return np.expand_dims(lls, axis=1)\n",
        "\n",
        "        # Vectorized implementation (without masking) of full-evidence inference\n",
        "        vs = np.arange(n_features)\n",
        "        x_cond = x[:, self.tree].astype(np.int64, copy=False)\n",
        "        x_vals = x[:, vs].astype(np.int64, copy=False)\n",
        "        lls = np.sum(self.params[vs, x_cond, x_vals], axis=1, keepdims=True)\n",
        "        return lls\n",
        "\n",
        "    def mpe(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mis_mask = np.isnan(x)\n",
        "        obs_mask = ~mis_mask\n",
        "\n",
        "        # Semi-vectorized implementation of MPE inference\n",
        "        messages = self.message_passing(x, obs_mask, return_lls=False, reduce='mpe')\n",
        "\n",
        "        # Compute MPE at the root feature\n",
        "        mask = mis_mask[:, self.root]\n",
        "        msg = self.params[self.root, 0] + messages[self.root, mask]\n",
        "        x[mask, self.root] = np.argmax(msg, axis=1)\n",
        "\n",
        "        # Compute MPE at the other features, by using the accumulated messages\n",
        "        for j in self.bfs[1:]:\n",
        "            mask = mis_mask[:, j]\n",
        "            obs_parent_values = x[mask, self.tree[j]].astype(np.int64)\n",
        "            msg = self.params[j, obs_parent_values] + messages[j, mask]\n",
        "            x[mask, j] = np.argmax(msg, axis=1)\n",
        "        return x\n",
        "\n",
        "    def sample(self, x: np.ndarray) -> np.ndarray:\n",
        "        x = np.copy(x)\n",
        "        mis_mask = np.isnan(x)\n",
        "        obs_mask = ~mis_mask\n",
        "\n",
        "        # Semi-vectorized implementation of conditional sampling\n",
        "        messages = self.message_passing(x, obs_mask, return_lls=False, reduce='mar')\n",
        "\n",
        "        # Sample the root feature\n",
        "        mask = mis_mask[:, self.root]\n",
        "        log_probs = self.params[self.root, 0, 1] + messages[self.root, mask, 1]\n",
        "        x[mask, self.root] = ss.bernoulli.rvs(np.exp(log_probs))\n",
        "\n",
        "        # Sample the other features, by using the accumulated messages\n",
        "        for j in self.bfs[1:]:\n",
        "            mask = mis_mask[:, j]\n",
        "            obs_parent_values = x[mask, self.tree[j]].astype(np.int64)\n",
        "            log_probs = self.params[j, obs_parent_values, 1] + messages[j, mask, obs_parent_values]\n",
        "            x[mask, j] = ss.bernoulli.rvs(np.exp(log_probs))\n",
        "        return x\n",
        "\n",
        "    def moment(self, k: int = 1) -> float:\n",
        "        raise NotImplementedError(\"Computation of moments on Binary CLTs not yet implemented\")\n",
        "\n",
        "    def params_count(self) -> int:\n",
        "        return 1 + len(self.tree) + self.params.size\n",
        "\n",
        "    def params_dict(self) -> dict:\n",
        "        return {\n",
        "            'root': None if self.root is None else self.scope[self.root],\n",
        "            'tree': self.tree,\n",
        "            'params': self.params\n",
        "        }\n",
        "\n",
        "    def to_pc(self) -> Node:\n",
        "        \"\"\"\n",
        "        Convert a Chow-Liu Tree into a smooth, deterministic and structured-decomposable PC\n",
        "\n",
        "        :return: A smooth, deterministic and structured-decomposable PC.\n",
        "        \"\"\"\n",
        "        # Build the tree structure\n",
        "        root = build_tree_structure(self.tree, scope=self.scope)\n",
        "\n",
        "        # Build the factors dictionary\n",
        "        factors = {self.scope[i]: np.exp(self.params[i]) for i in range(len(self.tree))}\n",
        "\n",
        "        # Post-Order exploration\n",
        "        neg_buffer, pos_buffer = [], []\n",
        "        nodes_stack = [root]\n",
        "        last_node_visited = None\n",
        "        while nodes_stack:\n",
        "            node = nodes_stack[-1]\n",
        "            if node.is_leaf() or (last_node_visited in node.get_children()):\n",
        "                leaves: List[Union[Bernoulli, Sum]] = [\n",
        "                    Bernoulli(node.get_id(), p=0.0),\n",
        "                    Bernoulli(node.get_id(), p=1.0)\n",
        "                ]\n",
        "                if not node.is_leaf():\n",
        "                    neg_prod = Product(children=[leaves[0]] + neg_buffer[-len(node.get_children()):])\n",
        "                    pos_prod = Product(children=[leaves[1]] + pos_buffer[-len(node.get_children()):])\n",
        "                    del neg_buffer[-len(node.get_children()):]\n",
        "                    del pos_buffer[-len(node.get_children()):]\n",
        "                    sum_children = [neg_prod, pos_prod]\n",
        "                else:\n",
        "                    sum_children = leaves\n",
        "                weights = factors[node.get_id()]\n",
        "                neg_buffer.append(\n",
        "                    Sum(children=sum_children, weights=weights[0])\n",
        "                )\n",
        "                pos_buffer.append(\n",
        "                    Sum(children=sum_children, weights=weights[1])\n",
        "                )\n",
        "                last_node_visited = nodes_stack.pop()\n",
        "            else:\n",
        "                nodes_stack.extend(node.get_children())\n",
        "        # Equivalently, pos = neg_buffer[0]\n",
        "        pc = pos_buffer[0]\n",
        "        return assign_ids(pc)\n",
        "\n",
        "    def get_scopes(self):\n",
        "        \"\"\"\n",
        "        Return a list containing the scope of every node in the PC equivalent to the\n",
        "        current CLTree (see to_pc() method). Every scope occurs once in the list.\n",
        "\n",
        "        :return: The list of scopes.\n",
        "        \"\"\"\n",
        "        scopes = []\n",
        "        scopes_stack = []\n",
        "\n",
        "        # Post-Order exploration\n",
        "        root = build_tree_structure(self.tree, scope=self.scope)\n",
        "        nodes_stack = [root]\n",
        "        last_node_visited = None\n",
        "        while nodes_stack:\n",
        "            node = nodes_stack[-1]\n",
        "            if node.is_leaf() or (last_node_visited in node.get_children()):\n",
        "                if node.is_leaf():\n",
        "                    scopes_stack.append([node.get_id()])\n",
        "                else:\n",
        "                    scopes_temp = scopes_stack[-len(node.get_children()):]\n",
        "                    del scopes_stack[-len(node.get_children()):]\n",
        "                    scopes_temp.append([node.get_id()])\n",
        "                    merged_scope = [var for scope in scopes_temp for var in scope]\n",
        "                    scopes_stack.append(merged_scope)\n",
        "                    scopes.append(merged_scope)\n",
        "                last_node_visited = nodes_stack.pop()\n",
        "            else:\n",
        "                nodes_stack.extend(node.get_children())\n",
        "\n",
        "        return scopes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U22u4th91Hqj"
      },
      "outputs": [],
      "source": [
        "#@title binary cutset network : CNet\n",
        "\n",
        "\n",
        "\n",
        "class ORNode(Node):\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: Optional[List[int]],\n",
        "        children: Optional[List[Node]] = None,\n",
        "        weights: Optional[Union[List[float], np.ndarray]] = None,\n",
        "        or_id: Optional[int] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize an OR node given weights, child instances and child nodes.\n",
        "\n",
        "        :param scope: The scope of the OR node.\n",
        "        :param children: The child nodes of the OR node.\n",
        "        :param weights: The weights of the OR node.\n",
        "        :param or_id: The id of the OR node.\n",
        "        \"\"\"\n",
        "        if weights is not None:\n",
        "            if isinstance(weights, list):\n",
        "                weights = np.array(weights, dtype=np.float32)\n",
        "            if not np.isclose(np.sum(weights), 1.0):\n",
        "                raise ValueError(\"Weights don't sum up to 1\")\n",
        "        self.weights = weights\n",
        "        self.or_id = or_id\n",
        "        self.row_indices = None\n",
        "        self.col_indices = None\n",
        "        self.clt = None\n",
        "\n",
        "        super().__init__(scope, children)\n",
        "\n",
        "    def assign_indices(\n",
        "        self,\n",
        "        row_indices: Optional[List[int], np.ndarray],\n",
        "        col_indices: Optional[List[int], np.ndarray]\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Assign the corresponding indices of the OR node's partition in the original data set.\n",
        "\n",
        "        :param row_indices: Row indices of the partition.\n",
        "        :param col_indices: Column indices of the partition.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.row_indices = row_indices\n",
        "        self.col_indices = col_indices\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        pass\n",
        "\n",
        "\n",
        "class BinaryCNet(ORNode):\n",
        "    def __init__(\n",
        "        self,\n",
        "        scope: Optional[List[int]],\n",
        "        children: Optional[List[Node]] = None,\n",
        "        weights: Optional[Union[List[float], np.ndarray]] = None,\n",
        "        or_id: Optional[int] = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a binary cutset network (CNet).\n",
        "\n",
        "        :param scope: The scope of the binary CNet.\n",
        "        :param children: The child OR nodes of the binary CNet.\n",
        "        :param weights: The weights of the current OR node.\n",
        "        :param or_id: The id of the current OR node.\n",
        "        \"\"\"\n",
        "        super().__init__(scope, children, weights, or_id)\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        alpha: float = 0.01,\n",
        "        min_n_samples: int = 10,\n",
        "        min_n_features: int = 1,\n",
        "        min_mean_entropy: float = 0.01\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit the structure and the MLE parameters given some training data and hyper-parameters.\n",
        "\n",
        "        :param data: The training data.\n",
        "        :param alpha: The Laplace smoothing factor.\n",
        "        :param min_n_samples: The minimum number of samples to split.\n",
        "        :param min_n_features: The minimum number of features to split.\n",
        "        :param min_mean_entropy: The minimum mean entropy of RVs given the data to split.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        n_samples, n_features = data.shape\n",
        "        self.scope = list(range(n_features))\n",
        "        self.assign_indices(row_indices=np.arange(n_samples), col_indices=np.arange(n_features))\n",
        "        root = BinaryCNet(scope=list(range(n_features)))\n",
        "        root.assign_indices(row_indices=np.arange(n_samples), col_indices=np.arange(n_features))\n",
        "        node_stack = [root]\n",
        "        while node_stack:\n",
        "            node = node_stack.pop(0)\n",
        "            partition = data[node.row_indices][:, node.col_indices]\n",
        "            n_samples, n_features = partition.shape\n",
        "            if n_samples <= min_n_samples or n_features <= min_n_features:\n",
        "                # stopped due to few samples or features\n",
        "                node.fit_clt(data=partition, alpha=alpha)\n",
        "                continue\n",
        "            best_or_idx, mean_entropy, max_info_gain = self.__select_variable_entropy(partition, alpha=alpha)\n",
        "            if mean_entropy < min_mean_entropy or max_info_gain <= 0:\n",
        "                # stopped due to small entropy or negative information gain\n",
        "                node.fit_clt(data=partition, alpha=alpha)\n",
        "                continue\n",
        "            left_row_indices = node.row_indices[partition[:, best_or_idx] == 0]\n",
        "            right_row_indices = node.row_indices[partition[:, best_or_idx] == 1]\n",
        "            child_col_indices = np.delete(node.col_indices, obj=best_or_idx)\n",
        "            left_weight = (len(left_row_indices) + alpha) / (len(node.row_indices) + 2 * alpha)\n",
        "            right_weight = 1 - left_weight\n",
        "            new_scope = node.scope.copy()\n",
        "            del new_scope[best_or_idx]\n",
        "            left_child = BinaryCNet(scope=new_scope)\n",
        "            left_child.assign_indices(row_indices=left_row_indices, col_indices=child_col_indices)\n",
        "            right_child = BinaryCNet(scope=new_scope)\n",
        "            right_child.assign_indices(row_indices=right_row_indices, col_indices=child_col_indices)\n",
        "            node_stack.append(left_child)\n",
        "            node_stack.append(right_child)\n",
        "            node.children = [left_child, right_child]\n",
        "            node.weights = [left_weight, right_weight]\n",
        "            node.or_id = node.scope[best_or_idx]\n",
        "        self.or_id = root.or_id\n",
        "        self.children = root.children\n",
        "        self.weights = root.weights\n",
        "\n",
        "    def fit_clt(\n",
        "        self,\n",
        "        data: np.ndarray,\n",
        "        alpha: float = 0.01\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Fit a Binary CLT for the RVs in the scope of the current OR node.\n",
        "\n",
        "        :param data: The data partition.\n",
        "        :param alpha: The laplace smoothing factor.\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        clt = BinaryCLT(scope=self.scope)\n",
        "        clt.fit(data=data, domain=[[0, 1]] * len(self.scope), alpha=alpha)\n",
        "        self.clt = clt\n",
        "\n",
        "    @staticmethod\n",
        "    def __select_variable_entropy(\n",
        "        data: np.ndarray,\n",
        "        alpha: float = 0.01\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Select the best cut node based on the reduced entropy (information gain).\n",
        "\n",
        "        :param data: The training data partition.\n",
        "        :param alpha: The Laplace smoothing factor.\n",
        "        :return: The index of the selected RV,\n",
        "                 the mean entropy of the RVs in the partition,\n",
        "                 the information gain of the selected RV.\n",
        "        \"\"\"\n",
        "        n_samples, n_features = data.shape\n",
        "        counts_features = np.sum(data, axis=0)\n",
        "\n",
        "        prior_counts = compute_prior_counts(data)\n",
        "        joint_counts = compute_joint_counts(data)\n",
        "        priors = (prior_counts + 2 * alpha) / (n_samples + 4 * alpha)\n",
        "        priors[:, 0] = 1.0 - priors[:, 1]\n",
        "        mean_entropy = -(priors * np.log(priors)).sum() / n_features\n",
        "\n",
        "        conditionals = np.empty((n_features, n_features, 2, 2), dtype=np.float32)\n",
        "        # as we are computing the probabilities for all nodes after cutting on a node,\n",
        "        # the laplace smoothing factor is essentially the same as computing general prior probabilities\n",
        "        conditionals[:, :, 0, 0] = ((joint_counts[:, :, 0, 0] + 2 * alpha).T / (prior_counts[:, 0] + 4 * alpha)).T\n",
        "        conditionals[:, :, 0, 1] = ((joint_counts[:, :, 0, 1] + 2 * alpha).T / (prior_counts[:, 0] + 4 * alpha)).T\n",
        "        conditionals[:, :, 1, 0] = ((joint_counts[:, :, 1, 0] + 2 * alpha).T / (prior_counts[:, 1] + 4 * alpha)).T\n",
        "        conditionals[:, :, 1, 1] = ((joint_counts[:, :, 1, 1] + 2 * alpha).T / (prior_counts[:, 1] + 4 * alpha)).T\n",
        "\n",
        "        vs = np.repeat(np.arange(n_features)[None, :], n_features, axis=0)\n",
        "        vs = vs[~np.eye(vs.shape[0], dtype=bool)].reshape(vs.shape[0], -1)\n",
        "        parents = np.repeat(np.arange(n_features)[:, None], n_features - 1, axis=1)\n",
        "\n",
        "        ratio_features = counts_features / n_samples\n",
        "        entropies = ratio_features * \\\n",
        "                    np.mean(-np.sum(conditionals[parents, vs, 1, :] * np.log(conditionals[parents, vs, 1, :]), axis=-1),\n",
        "                            axis=1) + \\\n",
        "                    (1 - ratio_features) * \\\n",
        "                    np.mean(-np.sum(conditionals[parents, vs, 0, :] * np.log(conditionals[parents, vs, 0, :]), axis=-1),\n",
        "                            axis=1)\n",
        "        info_gains = mean_entropy - entropies\n",
        "        selected_idx = np.argmax(info_gains)\n",
        "        return selected_idx, mean_entropy, info_gains[selected_idx]\n",
        "\n",
        "    def __is_leaf(self):\n",
        "        \"\"\"\n",
        "        Check if the current OR node is a leaf.\n",
        "\n",
        "        :return: True if the OR node has fitted a binary CLT, otherwise False\n",
        "        \"\"\"\n",
        "        return True if self.clt else False\n",
        "\n",
        "    def likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.exp(self.log_likelihood(x))\n",
        "\n",
        "    def log_likelihood(self, x: np.ndarray) -> np.ndarray:\n",
        "        n_samples, n_features = x.shape\n",
        "        root = copy.copy(self)\n",
        "        root.row_indices, root.col_indices = np.arange(n_samples), np.arange(n_features)\n",
        "        node_stack = [root]\n",
        "        log_likes = np.zeros(n_samples)\n",
        "        while node_stack:\n",
        "            node = node_stack.pop(0)\n",
        "            partition = x[node.row_indices][:, node.col_indices]\n",
        "            if node.__is_leaf():\n",
        "                log_likes[node.row_indices] += node.clt.log_likelihood(partition).squeeze()\n",
        "                continue\n",
        "            node_idx = node.scope.index(node.or_id)\n",
        "            left_child = copy.copy(node.children[0])\n",
        "            right_child = copy.copy(node.children[1])\n",
        "            left_child.row_indices = node.row_indices[partition[:, node_idx] == 0]\n",
        "            right_child.row_indices = node.row_indices[partition[:, node_idx] == 1]\n",
        "            log_likes[left_child.row_indices] += np.log(node.weights[0])\n",
        "            log_likes[right_child.row_indices] += np.log(node.weights[1])\n",
        "            left_child.col_indices = np.delete(node.col_indices, obj=node_idx)\n",
        "            right_child.col_indices = np.delete(node.col_indices, obj=node_idx)\n",
        "            node_stack.append(left_child)\n",
        "            node_stack.append(right_child)\n",
        "        return log_likes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niUT_YVq1AtA"
      },
      "outputs": [],
      "source": [
        "## for saving, loading and plotting graphs: https://github.com/deeprob-org/deeprob-kit/blob/main/deeprob/spn/structure/io.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQQ1EKZj8sZ6"
      },
      "source": [
        "# Sampling, evaluation and inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sVZfRYYF8w-S"
      },
      "outputs": [],
      "source": [
        "#@title evaluation\n",
        "\n",
        "##---------------------------------------- evaluation helpers for validity check\n",
        "\n",
        "\n",
        "def collect_nodes(root: Node) -> List[Node]:\n",
        "    \"\"\"\n",
        "    Get all the nodes in a SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: A list of nodes.\n",
        "    \"\"\"\n",
        "    return filter_nodes_by_type(root)\n",
        "\n",
        "\n",
        "def filter_nodes_by_type(\n",
        "    root: Node,\n",
        "    ntype: Union[Type[Node], Tuple[Type[Node], ...]] = Node\n",
        ") -> List[Union[Node, Leaf, Sum, Product]]:\n",
        "    \"\"\"\n",
        "    Get the nodes of some specified types in a SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param ntype: The node type. Multiple node types can be specified as a tuple.\n",
        "    :return: A list of nodes of some specific types.\n",
        "    \"\"\"\n",
        "    return list(filter(lambda n: isinstance(n, ntype), bfs(root)))\n",
        "\n",
        "def check_spn(\n",
        "    root: Node,\n",
        "    labeled: bool = True,\n",
        "    smooth: bool = False,\n",
        "    decomposable: bool = False,\n",
        "    structured_decomposable: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Check a SPN have certain properties. Defaults to checking only 'labeled'.\n",
        "    This function combines several checks over a SPN, hence reducing the computational effort\n",
        "    used to retrieve the nodes from the SPN.\n",
        "\n",
        "    :param root: The root node of the SPN.\n",
        "    :param labeled: Whether to check if the SPN is correctly labeled.\n",
        "    :param smooth: Whether to check if the SPN is smooth.\n",
        "    :param decomposable: Whether to check if the SPN is decomposable.\n",
        "    :param structured_decomposable: Whether to check if the SPN is structured decomposable.\n",
        "    :raises ValueError: If the SPN doesn't have a certain property.\n",
        "    \"\"\"\n",
        "    if not is_check_spn_enabled():  # Skip the checks entirely, if specified\n",
        "        return\n",
        "\n",
        "    # Collect the nodes starting from the root node (cache)\n",
        "    nodes = collect_nodes(root)\n",
        "\n",
        "    # Check the SPN nodes are correctly labeled\n",
        "    if labeled:\n",
        "        result = is_labeled(root, nodes=nodes)\n",
        "        if result is not None:\n",
        "            raise ValueError(f\"SPN is not correctly labeled: {result}\")\n",
        "\n",
        "    # Check the SPN is smooth\n",
        "    if smooth:\n",
        "        result = is_smooth(root, nodes=nodes)\n",
        "        if result is not None:\n",
        "            raise ValueError(f\"SPN is not smooth: {result}\")\n",
        "\n",
        "    # Check the SPN is decomposable\n",
        "    if decomposable:\n",
        "        result = is_decomposable(root, nodes=nodes)\n",
        "        if result is not None:\n",
        "            raise ValueError(f\"SPN is not decomposable: {result}\")\n",
        "\n",
        "    # Check the SPN is structured decomposable\n",
        "    if structured_decomposable:\n",
        "        result = is_structured_decomposable(root, nodes=nodes)\n",
        "        if result is not None:\n",
        "            raise ValueError(f\"SPN is not structured decomposable: {result}\")\n",
        "\n",
        "\n",
        "def is_labeled(root: Node, nodes: Optional[List[Node]] = None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Check if the SPN is labeled correctly.\n",
        "    It checks that the initial id is zero and each id is consecutive.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param nodes: The list of nodes. If None, it will be retrieved starting from the root node.\n",
        "    :return: None if the SPN is labeled correctly, a reason otherwise.\n",
        "    \"\"\"\n",
        "    if nodes is None:\n",
        "        nodes = collect_nodes(root)\n",
        "\n",
        "    ids = set(map(lambda n: n.id, nodes))\n",
        "    if None in ids:\n",
        "        return \"Some nodes have missing ids\"\n",
        "    if len(ids) != len(nodes):\n",
        "        return \"Some nodes have repeated ids\"\n",
        "    if min(ids) != 0:\n",
        "        return \"Node ids are not starting at 0\"\n",
        "    if max(ids) != len(ids) - 1:\n",
        "        return \"Node ids are not consecutive\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def is_smooth(root: Node, nodes: Optional[List[Node]] = None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Check if the SPN is smooth (or complete).\n",
        "    It checks that each child of a sum node has the same scope.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param nodes: The list of nodes. If None, it will be retrieved starting from the root node.\n",
        "    :return: None if the SPN is smooth, a reason otherwise.\n",
        "    \"\"\"\n",
        "    if nodes is None:\n",
        "        nodes = collect_nodes(root)\n",
        "    sum_nodes: List[Sum] = list(filter(lambda n: isinstance(n, Sum), nodes))\n",
        "\n",
        "    for node in sum_nodes:\n",
        "        if len(node.children) == 0:\n",
        "            return f\"Sum node #{node.id} has no children\"\n",
        "        if len(node.children) != len(node.weights):\n",
        "            return f\"Weights and children length mismatch in node #{node.id}\"\n",
        "        if any(map(lambda c: set(c.scope) != set(node.scope), node.children)):\n",
        "            return f\"Children of Sum node #{node.id} have different scopes\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def is_decomposable(root: Node, nodes: Optional[List[Node]] = None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Check if the SPN is decomposable (or consistent).\n",
        "    It checks that each child of a product node has disjointed scopes.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param nodes: The list of nodes. If None, it will be retrieved starting from the root node.\n",
        "    :return: None if the SPN is decomposable, a reason otherwise.\n",
        "    \"\"\"\n",
        "    if nodes is None:\n",
        "        nodes = collect_nodes(root)\n",
        "    product_nodes: List[Product] = list(filter(lambda n: isinstance(n, Product), nodes))\n",
        "\n",
        "    for node in product_nodes:\n",
        "        if len(node.children) == 0:\n",
        "            return f\"Product node #{node.id} has no children\"\n",
        "        s_scope = set(sum([c.scope for c in node.children], []))\n",
        "        if set(node.scope) != s_scope:\n",
        "            return f\"Children of Product node #{node.id} don't have disjointed scopes\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def is_structured_decomposable(root: Node, nodes: Optional[List[Node]] = None) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Check if the PC is structured decomposable.\n",
        "    It checks that product nodes follow a vtree.\n",
        "    Note that if a PC is structured decomposable then it's also decomposable.\n",
        "\n",
        "    :param root: The root of the PC.\n",
        "    :param nodes: The list of nodes. If None, it will be retrieved starting from the root node.\n",
        "    :return: None if the PC is structured decomposable, a reason otherwise.\n",
        "    \"\"\"\n",
        "    # Shortcut: a PC is structured decomposable if it is compatible with itself\n",
        "    if nodes is None:\n",
        "        nodes = collect_nodes(root)\n",
        "    return are_compatible(root, root, nodes_a=nodes, nodes_b=nodes)\n",
        "\n",
        "\n",
        "def are_compatible(\n",
        "    root_a: Node,\n",
        "    root_b: Node,\n",
        "    nodes_a: Optional[List[Node]] = None,\n",
        "    nodes_b: Optional[List[Node]] = None\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Check if two PCs are compatible.\n",
        "\n",
        "    :param root_a: The root of the first PC.\n",
        "    :param root_b: The root of the second PC.\n",
        "    :param nodes_a: The list of nodes of the first PC. If None, it will be retrieved starting from the root node.\n",
        "    :param nodes_b: The list of nodes of the second PC. If None, it will be retrieved starting from the root node.\n",
        "    :return: None if the two PCs are compatible, a reason otherwise.\n",
        "    \"\"\"\n",
        "    if nodes_a is None:\n",
        "        nodes_a = collect_nodes(root_a)\n",
        "    if nodes_b is None:\n",
        "        nodes_b = collect_nodes(root_b)\n",
        "\n",
        "    # Check smoothness and decomposability first\n",
        "    res = is_smooth(root_a, nodes_a)\n",
        "    if res is not None:\n",
        "        return f'First PC: {res}'\n",
        "    res = is_decomposable(root_a, nodes_a)\n",
        "    if res is not None:\n",
        "        return f'First PC: {res}'\n",
        "    res = is_smooth(root_b, nodes_b)\n",
        "    if res is not None:\n",
        "        return f'Second PC: {res}'\n",
        "    res = is_decomposable(root_b, nodes_b)\n",
        "    if res is not None:\n",
        "        return f'Second PC: {res}'\n",
        "\n",
        "    # Get scopes as sets\n",
        "    scopes_a = collect_scopes(nodes_a)\n",
        "    scopes_b = collect_scopes(nodes_b)\n",
        "    scopes_a = list(map(lambda s: set(s), scopes_a))\n",
        "    scopes_b = list(map(lambda s: set(s), scopes_b))\n",
        "\n",
        "    # Quadratic in the number of product nodes\n",
        "    for s1 in scopes_a:\n",
        "        for s2 in scopes_b:\n",
        "            int_len = len(s1.intersection(s2))\n",
        "            if int_len != 0 and int_len != min(len(s1), len(s2)):\n",
        "                return f\"Incompatibility found between scope {s1} and scope {s2}\"\n",
        "    return None\n",
        "\n",
        "\n",
        "def collect_scopes(nodes: List[Node]) -> List[Tuple[int]]:\n",
        "    \"\"\"\n",
        "    Collect the scopes of each node.\n",
        "\n",
        "    :param nodes: The list of nodes.\n",
        "    :return: A list of scopes.\n",
        "    \"\"\"\n",
        "    scopes = list()\n",
        "    for n in nodes:\n",
        "        if isinstance(n, Product):\n",
        "            scopes.append(tuple(sorted(n.scope)))\n",
        "        elif isinstance(n, BinaryCLT):\n",
        "            scopes.extend([tuple(sorted(scope)) for scope in n.get_scopes()])\n",
        "        elif not isinstance(n, Sum) and not isinstance(n, Leaf):\n",
        "            raise NotImplementedError(f\"Case not considered for {type(n)} nodes\")\n",
        "    return scopes\n",
        "\n",
        "\n",
        "##------------------------------------------------------------------- evaluetion\n",
        "\n",
        "def parallel_layerwise_eval(\n",
        "    layers: List[List[Node]],\n",
        "    eval_func: Callable[[Node], None],\n",
        "    reverse: bool = False,\n",
        "    n_jobs: int = -1\n",
        "):\n",
        "    \"\"\"\n",
        "    Execute a function per node layerwise in parallel.\n",
        "\n",
        "    :param layers: The layers, i.e., the layered topological ordering.\n",
        "    :param eval_func: The evaluation function for a given node.\n",
        "    :param reverse: Whether to reverse the layered topological ordering.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention.\n",
        "    \"\"\"\n",
        "    if reverse:\n",
        "        layers = reversed(layers)\n",
        "\n",
        "    # Run parallel threads using joblib\n",
        "    with joblib.parallel_backend('threading', n_jobs=n_jobs):\n",
        "        with joblib.Parallel() as parallel:\n",
        "            for layer in layers:\n",
        "                parallel(joblib.delayed(eval_func)(node) for node in layer)\n",
        "\n",
        "\n",
        "def eval_bottom_up(\n",
        "    root: Node,\n",
        "    x: np.ndarray,\n",
        "    leaf_func: Callable[[Leaf, np.ndarray, Any], np.ndarray],\n",
        "    node_func: Callable[[Node, np.ndarray, Any], np.ndarray],\n",
        "    leaf_func_kwargs: Optional[dict] = None,\n",
        "    node_func_kwargs: Optional[dict] = None,\n",
        "    return_results: bool = False,\n",
        "    n_jobs: int = 0\n",
        ") -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Evaluate the SPN bottom up given some inputs and leaves and nodes evaluation functions.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param x: The inputs.\n",
        "    :param leaf_func: The function to compute at each leaf node.\n",
        "    :param node_func: The function to compute at each inner node.\n",
        "    :param leaf_func_kwargs: The optional parameters of the leaf evaluation function.\n",
        "    :param node_func_kwargs: The optional parameters of the inner nodes evaluation function.\n",
        "    :param return_results: A flag indicating if this function must return the log likelihoods of each node of the SPN.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention. Set to 0 to disable.\n",
        "    :return: The outputs. Additionally, it returns the output of each node.\n",
        "    :raises ValueError: If a parameter is out of domain.\n",
        "    \"\"\"\n",
        "    if leaf_func_kwargs is None:\n",
        "        leaf_func_kwargs = dict()\n",
        "    if node_func_kwargs is None:\n",
        "        node_func_kwargs = dict()\n",
        "\n",
        "    # Check the SPN\n",
        "    check_spn(root, labeled=True, smooth=True, decomposable=True)\n",
        "\n",
        "    def eval_forward(n):\n",
        "        if isinstance(n, Leaf):\n",
        "            ls[n.id] = leaf_func(n, x[:, n.scope], **leaf_func_kwargs)\n",
        "        else:\n",
        "            children_ls = np.stack([ls[c.id] for c in n.children], axis=1)\n",
        "            ls[n.id] = node_func(n, children_ls, **node_func_kwargs)\n",
        "\n",
        "    if n_jobs == 0:\n",
        "        # Compute the topological ordering\n",
        "        ordering = topological_order(root)\n",
        "        if ordering is None:\n",
        "            raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "        n_nodes, n_samples = len(ordering), len(x)\n",
        "        ls = np.empty(shape=(n_nodes, n_samples), dtype=np.float32)\n",
        "        for node in reversed(ordering):\n",
        "            eval_forward(node)\n",
        "    else:\n",
        "        # Compute the layered topological ordering\n",
        "        layers = topological_order_layered(root)\n",
        "        if layers is None:\n",
        "            raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "        n_nodes, n_samples = sum(map(len, layers)), len(x)\n",
        "        ls = np.empty(shape=(n_nodes, n_samples), dtype=np.float32)\n",
        "        parallel_layerwise_eval(layers, eval_forward, reverse=True, n_jobs=n_jobs)\n",
        "\n",
        "    if return_results:\n",
        "        return ls[root.id], ls\n",
        "    return ls[root.id]\n",
        "\n",
        "\n",
        "def eval_top_down(\n",
        "    root: Node,\n",
        "    x: np.ndarray,\n",
        "    lls: np.ndarray,\n",
        "    leaf_func: Callable[[Leaf, np.ndarray, Any], np.ndarray],\n",
        "    sum_func: Callable[[Sum, np.ndarray, Any], np.ndarray],\n",
        "    leaf_func_kwargs: Optional[dict] = None,\n",
        "    sum_func_kwargs: Optional[dict] = None,\n",
        "    inplace: bool = False,\n",
        "    n_jobs: int = 0\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Evaluate the SPN top down given some inputs, the likelihoods of each node and a leaves evaluation function.\n",
        "    The leaves to evaluate are chosen by following the nodes given by the sum nodes evaluation function.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param x: The inputs with some NaN values.\n",
        "    :param lls: The log-likelihoods of each node.\n",
        "    :param leaf_func: The leaves evaluation function.\n",
        "    :param sum_func: The sum nodes evaluation function.\n",
        "    :param leaf_func_kwargs: The optional parameters of the leaf evaluation function.\n",
        "    :param sum_func_kwargs: The optional parameters of the sum nodes evaluation function.\n",
        "    :param inplace: Whether to make inplace assignments.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention. Set to 0 to disable.\n",
        "    :return: The NaN-filled inputs.\n",
        "    :raises ValueError: If a parameter is out of domain.\n",
        "    \"\"\"\n",
        "    if leaf_func_kwargs is None:\n",
        "        leaf_func_kwargs = dict()\n",
        "    if sum_func_kwargs is None:\n",
        "        sum_func_kwargs = dict()\n",
        "\n",
        "    # Check the SPN\n",
        "    check_spn(root, labeled=True, smooth=True, decomposable=True)\n",
        "\n",
        "    # Copy the input array, if not inplace mode\n",
        "    if not inplace:\n",
        "        x = np.copy(x)\n",
        "\n",
        "    def eval_backward(n):\n",
        "        if isinstance(n, Leaf):\n",
        "            mask = np.ix_(masks[n.id], n.scope)\n",
        "            x[mask] = leaf_func(n, x[mask], **leaf_func_kwargs)\n",
        "        elif isinstance(n, Product):\n",
        "            for c in n.children:\n",
        "                masks[c.id] |= masks[n.id]\n",
        "        elif isinstance(n, Sum):\n",
        "            children_lls = np.stack([lls[c.id] for c in n.children], axis=1)\n",
        "            branch = sum_func(n, children_lls, **sum_func_kwargs)\n",
        "            for i, c in enumerate(n.children):\n",
        "                masks[c.id] |= masks[n.id] & (branch == i)\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Top down evaluation not implemented for node of type {n.__class__.__name__}\")\n",
        "\n",
        "    if n_jobs == 0:\n",
        "        # Compute the topological ordering\n",
        "        ordering = topological_order(root)\n",
        "        if ordering is None:\n",
        "            raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "        n_nodes, n_samples = len(ordering), len(x)\n",
        "\n",
        "        # Build the array consisting of top-down path masks\n",
        "        masks = np.zeros(shape=(n_nodes, n_samples), dtype=np.bool_)\n",
        "        masks[root.id] = True\n",
        "        for node in ordering:\n",
        "            eval_backward(node)\n",
        "    else:\n",
        "        # Compute the layered topological ordering\n",
        "        layers = topological_order_layered(root)\n",
        "        if layers is None:\n",
        "            raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "        n_nodes, n_samples = sum(map(len, layers)), len(x)\n",
        "\n",
        "        # Build the array consisting of top-down path masks\n",
        "        masks = np.zeros(shape=(n_nodes, n_samples), dtype=np.bool_)\n",
        "        masks[root.id] = True\n",
        "        parallel_layerwise_eval(layers, eval_backward, reverse=False, n_jobs=n_jobs)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "##--------------------------------------------------------------Eval Gradient\n",
        "\n",
        "def eval_backward(root: Node, lls: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the log-gradients at each SPN node.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param lls: The log-likelihoods at each node.\n",
        "    :return: The log-gradients w.r.t. the nodes.\n",
        "    :raises ValueError: If a parameter is out of domain.\n",
        "    \"\"\"\n",
        "    # Check the SPN\n",
        "    check_spn(root, labeled=True, smooth=True, decomposable=True)\n",
        "\n",
        "    nodes = topological_order(root)\n",
        "    if nodes is None:\n",
        "        raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "\n",
        "    n_nodes, n_samples = lls.shape\n",
        "    if n_nodes != len(nodes):\n",
        "        raise ValueError(\"Incompatible log-likelihoods broadcasting at each node\")\n",
        "\n",
        "    # Initialize the log-gradients array and the cached log-gradients dictionary of lists\n",
        "    grads = np.empty(shape=(n_nodes, n_samples), dtype=np.float32)\n",
        "    cached_grads = defaultdict(list)\n",
        "\n",
        "    # Initialize the identity log-gradient at root node\n",
        "    grads[root.id] = 0.0\n",
        "\n",
        "    for node in nodes:\n",
        "        # Compute log-gradient at the underlying node by logsumexp\n",
        "        # Note that at this point of topological ordering, the node have no incoming arcs\n",
        "        # Hence, we can finally compute the log-gradients w.r.t. this node\n",
        "        if node.id != root.id:\n",
        "            grads[node.id] = logsumexp(cached_grads[node.id], axis=0)\n",
        "            del cached_grads[node.id]  # Cached log-gradients no longer necessary\n",
        "\n",
        "        if isinstance(node, Sum):\n",
        "            for c, w in zip(node.children, node.weights):\n",
        "                g = grads[node.id] + np.log(w)\n",
        "                cached_grads[c.id].append(g)\n",
        "        elif isinstance(node, Product):\n",
        "            for c in node.children:\n",
        "                g = grads[node.id] + lls[node.id] - lls[c.id]\n",
        "                cached_grads[c.id].append(g)\n",
        "        elif isinstance(node, Leaf):\n",
        "            pass  # Leaves have no children\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Gradient evaluation not implemented for node of type {}\".format(node.__class__.__name__)\n",
        "            )\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XqiNz6XSptaN"
      },
      "outputs": [],
      "source": [
        "#@title inference\n",
        "\n",
        "def likelihood(\n",
        "    root: Node,\n",
        "    x: np.ndarray,\n",
        "    return_results: bool = False,\n",
        "    n_jobs: int = 0\n",
        ") -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Compute the likelihoods of the SPN given some inputs.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param x: The inputs. They can be marginalized using NaNs.\n",
        "    :param return_results: A flag indicating if this function must return the likelihoods of each node of the SPN.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention. Set to 0 to disable.\n",
        "    :return: The likelihood values. Additionally, it returns the likelihood values of each node.\n",
        "    \"\"\"\n",
        "    return eval_bottom_up(\n",
        "        root, x,\n",
        "        leaf_func=node_likelihood,\n",
        "        node_func=node_likelihood,\n",
        "        return_results=return_results,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "\n",
        "\n",
        "def log_likelihood(\n",
        "    root: Node,\n",
        "    x: np.ndarray,\n",
        "    return_results: bool = False,\n",
        "    n_jobs: int = 0\n",
        ") -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Compute the logarithmic likelihoods of the SPN given some inputs.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param x: The inputs. They can be marginalized using NaNs.\n",
        "    :param return_results: A flag indicating if this function must return the log likelihoods of each node of the SPN.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention. Set to 0 to disable.\n",
        "    :return: The log likelihood values. Additionally, it returns the log likelihood values of each node.\n",
        "    \"\"\"\n",
        "    return eval_bottom_up(\n",
        "        root, x,\n",
        "        leaf_func=node_log_likelihood,\n",
        "        node_func=node_log_likelihood,\n",
        "        return_results=return_results,\n",
        "        n_jobs=n_jobs\n",
        "    )\n",
        "\n",
        "\n",
        "def mpe(root: Node, x: np.ndarray, inplace: bool = False, n_jobs: int = 0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the Most Probable Explanation of a SPN given some inputs.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param x: The inputs. They can be marginalized using NaNs.\n",
        "    :param inplace: Whether to make inplace assignments.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention. Set to 0 to disable.\n",
        "    :return: The NaN-filled inputs.\n",
        "    \"\"\"\n",
        "    _, lls = log_likelihood(root, x, return_results=True)\n",
        "    with ContextState(check_spn=False):  # We've already checked the SPN in forward mode\n",
        "        return eval_top_down(\n",
        "            root, x, lls,\n",
        "            leaf_func=leaf_mpe,\n",
        "            sum_func=sum_mpe,\n",
        "            inplace=inplace,\n",
        "            n_jobs=n_jobs\n",
        "        )\n",
        "\n",
        "\n",
        "def node_likelihood(node: Node, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the likelihood of a node given the list of likelihoods of its children.\n",
        "\n",
        "    :param node: The internal node.\n",
        "    :param x: The array of likelihoods of the children.\n",
        "    :return: The likelihoods of the node given the inputs.\n",
        "    \"\"\"\n",
        "    ls = node.likelihood(x)\n",
        "    return np.squeeze(ls, axis=1)\n",
        "\n",
        "\n",
        "def node_log_likelihood(node: Node, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the log-likelihood of a node given the list of log-likelihoods of its children.\n",
        "\n",
        "    :param node: The internal node.\n",
        "    :param x: The array of log-likelihoods of the children.\n",
        "    :return: The log-likelihoods of the node given the inputs.\n",
        "    \"\"\"\n",
        "    lls = node.log_likelihood(x)\n",
        "    return np.squeeze(np.maximum(lls, -1e31), axis=1)\n",
        "\n",
        "\n",
        "def leaf_mpe(node: Leaf, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the maximum likelihood estimate of a leaf node.\n",
        "\n",
        "    :param node: The leaf node.\n",
        "    :param x: The inputs with some NaN values.\n",
        "    :return: The most proable explanation.\n",
        "    \"\"\"\n",
        "    return node.mpe(x)\n",
        "\n",
        "\n",
        "def sum_mpe(node: Sum, lls: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Choose the branch that maximize the posterior estimate likelihood.\n",
        "\n",
        "    :param node: The sum node.\n",
        "    :param lls: The log-likelihoods of the children nodes.\n",
        "    :return: The branch that maximize the posterior estimate likelihood.\n",
        "    \"\"\"\n",
        "    weighted_lls = lls + np.log(node.weights)\n",
        "    return np.argmax(weighted_lls, axis=1)\n",
        "\n",
        "\n",
        "##----------------------------------------- capture moment on node scale\n",
        "\n",
        "def moment(root: Node, order: int = 1) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute non-central moments of a given order of a smooth and decomposable SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param order: The order of the moment. If scalar, it will be used for all the random variables.\n",
        "    :return: The non-central moments with respect to each variable in the scope.\n",
        "    :raises ValueError: If the order of the moment is negative.\n",
        "    \"\"\"\n",
        "    scope = root.scope\n",
        "    if order < 0:\n",
        "        raise ValueError(\"The order of the moment must be non-negative\")\n",
        "    if order == 0:  # Completely skip computation for 0-order moments\n",
        "        return np.ones(len(scope), dtype=np.float32)\n",
        "\n",
        "    # Compute the moments w.r.t. each random variable by proceeding bottom-up\n",
        "    moments = np.ones(shape=[len(scope), len(scope)], dtype=np.float32)\n",
        "    return eval_bottom_up(\n",
        "        root, moments,\n",
        "        leaf_func=leaf_moment,\n",
        "        node_func=node_likelihood,\n",
        "        leaf_func_kwargs={'order': order}\n",
        "    )\n",
        "\n",
        "\n",
        "def leaf_moment(node: Leaf, x: np.ndarray, order: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the moment of a leaf node.\n",
        "\n",
        "    :param node: The leaf node.\n",
        "    :param x: The inputs of the leaf. Actually, it's used only to infer the output shape.\n",
        "    :param order: The order of the moment.\n",
        "    :return: The moment of the leaf node.\n",
        "    \"\"\"\n",
        "    m = np.ones(len(x), dtype=np.float32)\n",
        "    m[node.scope] = node.moment(k=order)\n",
        "    return m\n",
        "\n",
        "\n",
        "def expectation(root: Node) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the expectation values of a SPN w.r.t. each of the random variables.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The expectation w.r.t. each of the random variables.\n",
        "    \"\"\"\n",
        "    return moment(root, order=1)\n",
        "\n",
        "\n",
        "def variance(root: Node) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the variance values of a SPN w.r.t. each of the random variables.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The variance w.r.t. each of the random variables.\n",
        "    \"\"\"\n",
        "    fst_moment = moment(root, order=1)\n",
        "    snd_moment = moment(root, order=2)\n",
        "    return snd_moment - fst_moment ** 2.0\n",
        "\n",
        "\n",
        "def skewness(root: Node) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the skewness values of a SPN w.r.t. each of the random variables.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The skewness w.r.t. each of the random variables.\n",
        "    \"\"\"\n",
        "    # This implementation is derived by expanding the third central moment\n",
        "    # and obtaining a definition based on non-central moments\n",
        "    fst_moment = moment(root, order=1)\n",
        "    snd_moment = moment(root, order=2)\n",
        "    thd_moment = moment(root, order=3)\n",
        "    g1 = fst_moment ** 2.0\n",
        "    g2 = snd_moment - g1\n",
        "    g3 = 3.0 * snd_moment + 2.0 * g1\n",
        "    return (thd_moment - fst_moment * g3) / (g2 ** 1.5)\n",
        "\n",
        "\n",
        "def kurtosis(root: Node) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the kurtosis values of a SPN w.r.t. each of the random variables.\n",
        "    This function returns the kurtosis based on Fisher's definition, i.e.\n",
        "    3.0 is subtracted from the result to give 0.0 for a normal distribution.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The kurtosis w.r.t. each of the random variables.\n",
        "    \"\"\"\n",
        "    # This implementation is derived from Moors' interpretation\n",
        "    # (More @ https://en.wikipedia.org/wiki/Kurtosis#Moors'_interpretation)\n",
        "    # by expanding Var[Z^2] + 1 and obtaining a definition based on non-central moments\n",
        "    fst_moment = moment(root, order=1)\n",
        "    snd_moment = moment(root, order=2)\n",
        "    thd_moment = moment(root, order=3)\n",
        "    fhd_moment = moment(root, order=4)\n",
        "    g1 = fst_moment ** 2.0\n",
        "    g2 = snd_moment - g1\n",
        "    g3 = 4.0 * (g1 ** 2.0 + fst_moment * thd_moment)\n",
        "    g4 = snd_moment * (8.0 * g1 - snd_moment)\n",
        "    return -2.0 + (fhd_moment - g3 + g4) / (g2 ** 2.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0XBFnAJFq_EK"
      },
      "outputs": [],
      "source": [
        "#@title sampling\n",
        "\n",
        "\n",
        "def sample(root: Node, x: np.ndarray, inplace: bool = False, n_jobs: int = 0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Sample some features from the distribution represented by the SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param x: The inputs with possible NaN values to fill with sampled values.\n",
        "    :param inplace: Whether to make inplace assignments.\n",
        "    :param n_jobs: The number of parallel jobs. It follows the joblib's convention. Set to 0 to disable.\n",
        "     Warning: disrupts seed determinism.\n",
        "    :return: The inputs that are NaN-filled with samples from appropriate distributions.\n",
        "    \"\"\"\n",
        "    # First evaluate the SPN bottom-up, then top-down\n",
        "    _, lls = log_likelihood(root, x, return_results=True, n_jobs=n_jobs)\n",
        "    with ContextState(check_spn=False):  # We've already checked the SPN in forward mode\n",
        "        return eval_top_down(\n",
        "            root, x, lls,\n",
        "            leaf_func=leaf_sample,\n",
        "            sum_func=sum_sample,\n",
        "            inplace=inplace,\n",
        "            n_jobs=n_jobs\n",
        "        )\n",
        "\n",
        "\n",
        "def leaf_sample(node: Leaf, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Sample some values from the distribution leaf.\n",
        "\n",
        "    :param node: The distribution leaf node.\n",
        "    :param x: The inputs with possible NaN values to fill with sampled values.\n",
        "    :return: The completed samples.\n",
        "    \"\"\"\n",
        "    return node.sample(x)\n",
        "\n",
        "\n",
        "def sum_sample(node: Sum, lls: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Choose the sub-distribution from which sample.\n",
        "\n",
        "    :param node: The sum node.\n",
        "    :param lls: The log-likelihoods of the children nodes.\n",
        "    :return: The index of the sub-distribution to follow.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = lls.shape\n",
        "    gumbel = stats.gumbel_l.rvs(0.0, 1.0, size=(n_samples, n_features))\n",
        "    weighted_lls = lls + np.log(node.weights) + gumbel\n",
        "    return np.argmax(weighted_lls, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tMPJ-uMRsIxn"
      },
      "outputs": [],
      "source": [
        "#@title prune the spn\n",
        "\n",
        "\n",
        "def prune(root: Node, copy: bool = True) -> Node:\n",
        "    \"\"\"\n",
        "    Prune (or simplify) the given SPN to a minimal and equivalent SPN.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :param copy: Whether to copy the SPN before pruning it.\n",
        "    :return: A minimal and equivalent SPN.\n",
        "    :raises ValueError: If the SPN structure is not a directed acyclic graph (DAG).\n",
        "    :raises ValueError: If an unknown node type is found.\n",
        "    \"\"\"\n",
        "    # Copy the SPN before proceeding, if specified\n",
        "    if copy:\n",
        "        root = deepcopy(root)\n",
        "\n",
        "    # Check the SPN\n",
        "    check_spn(root, labeled=True, smooth=True, decomposable=True)\n",
        "\n",
        "    nodes = topological_order(root)\n",
        "    if nodes is None:\n",
        "        raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "\n",
        "    # Build a dictionary that maps each id of a node to the corresponding node object\n",
        "    nodes_map = dict(map(lambda n: (n.id, n), nodes))\n",
        "\n",
        "    # Proceed by reversed topological order\n",
        "    for node in reversed(nodes):\n",
        "        # Skip leaves\n",
        "        if isinstance(node, Leaf):\n",
        "            continue\n",
        "\n",
        "        # Retrieve the children nodes from the mapping\n",
        "        children_nodes = list(map(lambda n: nodes_map[n.id], node.children))\n",
        "        if len(children_nodes) == 1:\n",
        "            nodes_map[node.id] = children_nodes[0]\n",
        "        elif isinstance(node, Product):\n",
        "            # Subsequent product nodes, concatenate the children of them\n",
        "            children = list()\n",
        "            for child in children_nodes:\n",
        "                if not isinstance(child, Product):\n",
        "                    children.append(child)\n",
        "                    continue\n",
        "                product_children = map(lambda n: nodes_map[n.id], child.children)\n",
        "                children.extend(product_children)\n",
        "            nodes_map[node.id].children = children\n",
        "        elif isinstance(node, Sum):\n",
        "            # Subsequent sum nodes, concatenate the children of them and adjust the weights accordingly\n",
        "            # Important! This implementation take care also of directed acyclic graphs (DAGs)\n",
        "            children_weights = defaultdict(float)\n",
        "            for i, child in enumerate(children_nodes):\n",
        "                if not isinstance(child, Sum):\n",
        "                    children_weights[child] += node.weights[i]\n",
        "                    continue\n",
        "                sum_children = map(lambda n: nodes_map[n.id], child.children)\n",
        "                for j, sum_child in enumerate(sum_children):\n",
        "                    children_weights[sum_child] += node.weights[i] * child.weights[j]\n",
        "            children, weights = zip(*children_weights.items())\n",
        "            nodes_map[node.id].weights = np.array(weights, dtype=node.weights.dtype)\n",
        "            nodes_map[node.id].children = children\n",
        "        else:\n",
        "            raise ValueError(\"Unknown node type called {}\".format(node.__class__.__name__))\n",
        "\n",
        "    return assign_ids(nodes_map[root.id])\n",
        "\n",
        "\n",
        "def marginalize(root: Node, keep_scope: List[int], copy: bool = True) -> Node:\n",
        "    \"\"\"\n",
        "    Marginalize some random variables of a SPN, obtaining the compilation of a marginal query.\n",
        "\n",
        "    :param root: The root of the SPN to marginalize.\n",
        "    :param keep_scope: The scope of the random variables to keep.\n",
        "                       All the other random variables will be marginalized.\n",
        "    :param copy: Whether to copy the SPN before marginalizing it.\n",
        "    :return: A SPN in which an EVI query is equivalent to a MAR query under the given scope.\n",
        "    :raises ValueError: If the scope of the random variables to keep is not valid.\n",
        "    :raises ValueError: If the SPN structure is not a directed acyclic graph (DAG).\n",
        "    :raises ValueError: If an unknown node type is found.\n",
        "    :raises NotImplementedError: If non-BinaryCLT multivariate leaves are found.\n",
        "    \"\"\"\n",
        "    if not keep_scope:\n",
        "        raise ValueError(\"The scope of the random variables to keep must not be empty\")\n",
        "    keep_scope_s = set(keep_scope)\n",
        "    if len(keep_scope) != len(keep_scope_s):\n",
        "        raise ValueError(\"The scope of the random variables to keep must not contain duplicates\")\n",
        "    if not keep_scope_s.issubset(set(root.scope)):\n",
        "        raise ValueError(\"The scope of the random variables to keep must be a subset of the scope of the SPN\")\n",
        "\n",
        "    # Copy the SPN before proceeding, if specified\n",
        "    if copy:\n",
        "        root = deepcopy(root)\n",
        "\n",
        "    # Check the SPN\n",
        "    check_spn(root, labeled=True, smooth=True, decomposable=True)\n",
        "\n",
        "    nodes = topological_order(root)\n",
        "    if nodes is None:\n",
        "        raise ValueError(\"SPN structure is not a directed acyclic graph (DAG)\")\n",
        "\n",
        "    # Build a dictionary that maps each id of a node to the corresponding node object\n",
        "    nodes_map = dict(map(lambda n: (n.id, n), nodes))\n",
        "\n",
        "    # Proceed by reversed topological order\n",
        "    for node in reversed(nodes):\n",
        "        if isinstance(node, Leaf):\n",
        "            # Marginalize leaves, set to None if the leaf is fully marginalized\n",
        "            if isinstance(node, BinaryCLT):\n",
        "                # Convert the binary Chow-Liu Tree to a SPN and marginalize that instead\n",
        "                clt_scope = list(keep_scope_s.intersection(node.scope))\n",
        "                if clt_scope:\n",
        "                    with ContextState(check_spn=False):  # Disable checking the SPN obtained by CLT to PC conversion\n",
        "                        nodes_map[node.id] = marginalize(node.to_pc(), clt_scope, copy=False)\n",
        "                else:\n",
        "                    nodes_map[node.id] = None\n",
        "            elif len(node.scope) == 1:\n",
        "                nodes_map[node.id] = node if node.scope[0] in keep_scope else None\n",
        "            else:\n",
        "                raise NotImplementedError(\n",
        "                    \"Structural marginalization for arbitrarily multivariate leaves not yet implemented\"\n",
        "                )\n",
        "            continue\n",
        "\n",
        "        # Retrieve the children nodes from the mapping\n",
        "        children_nodes = list(filter(\n",
        "            lambda n: n is not None, map(lambda n: nodes_map[n.id], node.children)\n",
        "        ))\n",
        "\n",
        "        if not children_nodes:\n",
        "            nodes_map[node.id] = None\n",
        "        elif len(children_nodes) == 1:\n",
        "            nodes_map[node.id] = children_nodes[0]\n",
        "        else:\n",
        "            if isinstance(node, Product):\n",
        "                nodes_map[node.id].scope = list(sum(map(lambda n: n.scope, children_nodes), []))\n",
        "                nodes_map[node.id].children = children_nodes\n",
        "            elif isinstance(node, Sum):\n",
        "                nodes_map[node.id].scope = children_nodes[0].scope\n",
        "                nodes_map[node.id].children = children_nodes\n",
        "            else:\n",
        "                raise ValueError(\"Unknown node type called {}\".format(node.__class__.__name__))\n",
        "\n",
        "    root = assign_ids(nodes_map[root.id])\n",
        "    return prune(root, copy=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H7JJJNCQWG9w"
      },
      "outputs": [],
      "source": [
        "#@title spn stats calculator\n",
        "\n",
        "def compute_statistics(root: Node) -> dict:\n",
        "    \"\"\"\n",
        "    Compute some statistics of a SPN given its root.\n",
        "    The computed statistics are the following:\n",
        "\n",
        "    - n_nodes, the number of nodes\n",
        "    - n_sum, the number of sum nodes\n",
        "    - n_prod, the number of product nodes\n",
        "    - n_leaves, the number of leaves\n",
        "    - n_edges, the number of edges\n",
        "    - n_params, the number of parameters\n",
        "    - depth, the depth of the network\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: A dictionary containing the statistics.\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'n_nodes': len(collect_nodes(root)),\n",
        "        'n_sum': len(filter_nodes_by_type(root, Sum)),\n",
        "        'n_prod': len(filter_nodes_by_type(root, Product)),\n",
        "        'n_leaves': len(filter_nodes_by_type(root, Leaf)),\n",
        "        'n_edges': compute_edges_count(root),\n",
        "        'n_params': compute_parameters_count(root),\n",
        "        'depth': compute_depth(root)\n",
        "    }\n",
        "    return stats\n",
        "\n",
        "\n",
        "def compute_edges_count(root: Node) -> int:\n",
        "    \"\"\"\n",
        "    Get the number of edges of a SPN given its root.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The number of edges.\n",
        "    \"\"\"\n",
        "    return sum(len(n.children) for n in filter_nodes_by_type(root, (Sum, Product)))\n",
        "\n",
        "\n",
        "def compute_parameters_count(root: Node) -> int:\n",
        "    \"\"\"\n",
        "    Get the number of parameters of a SPN given its root.\n",
        "\n",
        "    :param root:  The root of the SPN.\n",
        "    :return: The number of parameters.\n",
        "    \"\"\"\n",
        "    n_weights = sum(len(n.weights) for n in filter_nodes_by_type(root, Sum))\n",
        "    n_leaf_params = sum(n.params_count() for n in filter_nodes_by_type(root, Leaf))\n",
        "    return n_weights + n_leaf_params\n",
        "\n",
        "\n",
        "def compute_depth(root: Node) -> int:\n",
        "    \"\"\"\n",
        "    Get the depth of the SPN given its root.\n",
        "\n",
        "    :param root: The root of the SPN.\n",
        "    :return: The depth of the network.\n",
        "    \"\"\"\n",
        "    depths = dict()\n",
        "    for node in bfs(root):\n",
        "        d = depths.setdefault(node, 0)\n",
        "        for c in node.children:\n",
        "            depths[c] = d + 1\n",
        "    return max(depths.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTqskaaWmGjE"
      },
      "source": [
        "# Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GYgaTA_KDDDF"
      },
      "outputs": [],
      "source": [
        "#@title cluster and nonlinear independence test by randomized dependence coefficient (rdc)\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------- cluster methods\n",
        "\n",
        "def gmm(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    n: int = 2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Execute GMM clustering on some data.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param n: The number of clusters.\n",
        "    :return: An array where each element is the cluster where the corresponding data belong.\n",
        "    \"\"\"\n",
        "    # Convert the data using One Hot Encoding, in case of non-binary discrete features\n",
        "    if any(len(d) > 2 for d in domains):\n",
        "        data = mixed_ohe_data(data, domains)\n",
        "\n",
        "    # Apply GMM\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings\n",
        "        return mixture.GaussianMixture(n, n_init=3, random_state=random_state).fit_predict(data)\n",
        "\n",
        "\n",
        "def kmeans(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    n: int = 2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Execute K-Means clustering on some data.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param n: The number of clusters.\n",
        "    :return: An array where each element is the cluster where the corresponding data belong.\n",
        "    \"\"\"\n",
        "    # Convert the data using One Hot Encoding, in case of non-binary discrete features\n",
        "    if any(len(d) > 2 for d in domains):\n",
        "        data = mixed_ohe_data(data, domains)\n",
        "\n",
        "    # Apply K-Means\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings\n",
        "        return cluster.KMeans(n, n_init=5, random_state=random_state).fit_predict(data)\n",
        "\n",
        "\n",
        "def kmeans_mb(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    n: int = 2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Execute MiniBatch K-Means clustering on some data.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param n: The number of clusters.\n",
        "    :return: An array where each element is the cluster where the corresponding data belong.\n",
        "    \"\"\"\n",
        "    # Convert the data using One Hot Encoding, in case of non-binary discrete features\n",
        "    if any(len(d) > 2 for d in domains):\n",
        "        data = mixed_ohe_data(data, domains)\n",
        "\n",
        "    # Apply K-Means MiniBatch\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings\n",
        "        warnings.simplefilter(action='ignore', category=UserWarning)  # Ignore user warnings\n",
        "        return cluster.MiniBatchKMeans(n, n_init=5, random_state=random_state).fit_predict(data)\n",
        "\n",
        "\n",
        "def dbscan(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    n: int = 2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Execute DBSCAN clustering on some data (only on discrete data).\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param n: The number of clusters.\n",
        "    :return: An array where each element is the cluster where the corresponding data belong.\n",
        "    :raises ValueError: If the leaf distributions are NOT discrete.\n",
        "    \"\"\"\n",
        "    # Control if distribution are binary\n",
        "    if not all(x.LEAF_TYPE == LeafType.DISCRETE for x in distributions):\n",
        "        raise ValueError('DBScan clustering can be applied only on discrete attributes')\n",
        "\n",
        "    # Convert the data using One Hot Encoding, in case of non-binary discrete features\n",
        "    if any(len(d) > 2 for d in domains):\n",
        "        data = mixed_ohe_data(data, domains)\n",
        "\n",
        "    # Apply DBSCAN\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings\n",
        "        return cluster.DBSCAN(eps = 0.25, n_jobs=-1).fit_predict(data)\n",
        "\n",
        "\n",
        "def wald(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    n: int = 2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Execute Ward (Hierarchical) clustering on some data (only discrete data).\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param n: The number of clusters.\n",
        "    :return: An array where each element is the cluster where the corresponding data belong.\n",
        "    :raises ValueError: If the leaf distributions are NOT discrete.\n",
        "    \"\"\"\n",
        "    # Control if distribution are binary\n",
        "    if not all(x.LEAF_TYPE == LeafType.DISCRETE for x in distributions):\n",
        "        raise ValueError('DBScan clustering can be applied only on discrete attributes')\n",
        "\n",
        "    # Convert the data using One Hot Encoding, in case of non-binary discrete features\n",
        "    if any(len(d) > 2 for d in domains):\n",
        "        data = mixed_ohe_data(data, domains)\n",
        "\n",
        "    # Apply Wald\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings\n",
        "        return cluster.AgglomerativeClustering(n, linkage='ward').fit_predict(data)\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------------------- rdc\n",
        "\n",
        "def rdc_cols(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    d: float = 0.3,\n",
        "    k: int = 20,\n",
        "    s: float = 1.0 / 6.0,\n",
        "    nl: Callable[[np.ndarray], np.ndarray] = np.sin\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Split the features using the RDC (Randomized Dependency Coefficient) method.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param d: The threshold value that regulates the independence tests among the features.\n",
        "    :param k: The size of the latent space.\n",
        "    :param s: The standard deviation of the gaussian distribution.\n",
        "    :param nl: The non linear function to use.\n",
        "    :return: A features partitioning.\n",
        "    \"\"\"\n",
        "    # Compute the RDC scores matrix\n",
        "    rdc_matrix = rdc_scores(data, distributions, domains, random_state, k=k, s=s, nl=nl)\n",
        "\n",
        "    # Compute the adjacency matrix\n",
        "    adj_matrix = (rdc_matrix > d).astype(np.int32)\n",
        "\n",
        "    # Compute the connected components of the adjacency matrix\n",
        "    adj_matrix = sp.csr_matrix(adj_matrix)\n",
        "    _, clusters = sp.csgraph.connected_components(adj_matrix, directed=False, return_labels=True)\n",
        "    return clusters\n",
        "\n",
        "\n",
        "def rdc_rows(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    n: int = 2,\n",
        "    k: int = 20,\n",
        "    s: float = 1.0 / 6.0,\n",
        "    nl: Callable[[np.ndarray], np.ndarray] = np.sin\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Split the samples using the RDC (Randomized Dependency Coefficient) method.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param n: The number of clusters for KMeans.\n",
        "    :param k: The size of the latent space.\n",
        "    :param s: The standard deviation of the gaussian distribution.\n",
        "    :param nl: The non linear function to use.\n",
        "    :return: A samples partitioning.\n",
        "    \"\"\"\n",
        "    # Transform the samples by RDC\n",
        "    rdc_samples = np.concatenate(\n",
        "        rdc_transform(data, distributions, domains, random_state, k, s, nl), axis=1\n",
        "    )\n",
        "\n",
        "    # Apply K-Means to the transformed samples\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings for K-Means\n",
        "        return cluster.KMeans(n, n_init=5, random_state=random_state).fit_predict(rdc_samples)\n",
        "\n",
        "\n",
        "def rdc_scores(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    k: int = 20,\n",
        "    s: float = 1.0 / 6.0,\n",
        "    nl: Callable[[np.ndarray], np.ndarray] = np.sin\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the RDC (Randomized Dependency Coefficient) score for each pair of features.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param k: The size of the latent space.\n",
        "    :param s: The standard deviation of the gaussian distribution.\n",
        "    :param nl: The non linear function to use.\n",
        "    :return: The RDC score matrix.\n",
        "    \"\"\"\n",
        "    # Apply RDC transformation to the features\n",
        "    _, n_features = data.shape\n",
        "    rdc_features = rdc_transform(data, distributions, domains, random_state, k, s, nl)\n",
        "    pairwise_comparisons = list(combinations(range(n_features), 2))\n",
        "\n",
        "    # Run Canonical Component Analysis (CCA) on RDC-transformed features\n",
        "    rdc_matrix = np.empty(shape=(n_features, n_features), dtype=np.float32)\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(action='ignore', category=ConvergenceWarning)  # Ignore convergence warnings for CCA\n",
        "        for i, j in pairwise_comparisons:\n",
        "            score = rdc_cca(i, j, rdc_features)\n",
        "            rdc_matrix[i, j] = rdc_matrix[j, i] = score\n",
        "    np.fill_diagonal(rdc_matrix, 1.0)\n",
        "    return rdc_matrix\n",
        "\n",
        "\n",
        "def rdc_cca(i: int, j: int, features: List[np.ndarray]) -> float:\n",
        "    \"\"\"\n",
        "    Compute the RDC (Randomized Dependency Coefficient) using CCA (Canonical Correlation Analysis).\n",
        "\n",
        "    :param i: The index of the first feature.\n",
        "    :param j: The index of the second feature.\n",
        "    :param features: The list of the features.\n",
        "    :return: The RDC coefficient (the largest canonical correlation coefficient).\n",
        "    \"\"\"\n",
        "    cca = cross_decomposition.CCA(n_components=1)\n",
        "    x_cca, y_cca = cca.fit_transform(features[i], features[j])\n",
        "    x_cca, y_cca = x_cca.squeeze(), y_cca.squeeze()\n",
        "    return np.corrcoef(x_cca, y_cca)[0, 1]\n",
        "\n",
        "\n",
        "def rdc_transform(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    k: int = 20,\n",
        "    s: float = 1.0 / 6.0,\n",
        "    nl: Callable[[np.ndarray], np.ndarray] = np.sin\n",
        ") -> List[np.ndarray]:\n",
        "    \"\"\"\n",
        "    Execute the RDC (Randomized Dependency Coefficient) pipeline on some data.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions.\n",
        "    :param domains: The data domains.\n",
        "    :param random_state: The random state.\n",
        "    :param k: The size of the latent space.\n",
        "    :param s: The standard deviation of the gaussian distribution.\n",
        "    :param nl: The non-linear function to use.\n",
        "    :return: The transformed data.\n",
        "    :raises ValueError: If an unknown distribution type is found.\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    for i, dist in enumerate(distributions):\n",
        "        if dist.LEAF_TYPE == LeafType.DISCRETE:\n",
        "            feature_matrix = ohe_data(data[:, i], domains[i])\n",
        "        elif dist.LEAF_TYPE == LeafType.CONTINUOUS:\n",
        "            feature_matrix = np.expand_dims(data[:, i], axis=-1)\n",
        "        else:\n",
        "            raise ValueError(\"Unknown distribution type {}\".format(dist.LEAF_TYPE))\n",
        "        x = np.apply_along_axis(ecdf_data, 0, feature_matrix)\n",
        "        features.append(x.astype(np.float32))\n",
        "\n",
        "    samples = []\n",
        "    for x in features:\n",
        "        stddev = np.sqrt(s / x.shape[1])\n",
        "        w = stddev * random_state.randn(x.shape[1], k).astype(np.float32)\n",
        "        b = stddev * random_state.randn(k).astype(np.float32)\n",
        "        y = nl(np.dot(x, w) + b)\n",
        "        samples.append(y)\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uo4NRqzPmGjj"
      },
      "outputs": [],
      "source": [
        "#@title random selection and splitting of rows and columns\n",
        "\n",
        "def random_rows(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    a: float = 2.0,\n",
        "    b: float = 2.0\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Choose a binary partition horizontally randomly.\n",
        "    The proportion of the split is sampled from a beta distribution.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions (not used).\n",
        "    :param domains: The data domains (not used).\n",
        "    :param random_state: The random state.\n",
        "    :param a: The alpha parameter of the beta distribution.\n",
        "    :param b: The beta parameter of the beta distribution.\n",
        "    :return: A binary partition.\n",
        "    \"\"\"\n",
        "    n_samples, _ = data.shape\n",
        "    p = random_state.beta(a, b)\n",
        "    return random_state.binomial(1, p, size=n_samples)\n",
        "\n",
        "\n",
        "def random_cols(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    random_state: np.random.RandomState,\n",
        "    a: float = 2.0,\n",
        "    b: float = 2.0\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Choose a binary partition vertically randomly.\n",
        "    The proportion of the split is sampled from a beta distribution.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The data distributions (not used).\n",
        "    :param domains: The data domains (not used).\n",
        "    :param random_state: The random state.\n",
        "    :param a: The alpha parameter of the beta distribution.\n",
        "    :param b: The beta parameter of the beta distribution.\n",
        "    :return: A binary partition.\n",
        "    \"\"\"\n",
        "    _, n_features = data.shape\n",
        "    p = random_state.beta(a, b)\n",
        "    return random_state.binomial(1, p, size=n_features)\n",
        "\n",
        "\n",
        "\n",
        "SplitRowsFunc = Callable[\n",
        "    [np.ndarray,                # The data\n",
        "     List[Type[Leaf]],          # The distributions\n",
        "     List[Union[list, tuple]],  # The domains\n",
        "     np.random.RandomState,     # The random state\n",
        "     Any],                      # Other arguments\n",
        "    np.ndarray                  # The rows ids\n",
        "]\n",
        "\n",
        "\n",
        "def split_rows_clusters(\n",
        "    data: np.ndarray,\n",
        "    clusters: np.ndarray\n",
        ") -> Tuple[List[np.ndarray], List[float]]:\n",
        "    \"\"\"\n",
        "    Split the data horizontally given the clusters.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param clusters: The clusters.\n",
        "    :return: (slices, weights) where slices is a list of partial data and\n",
        "             weights is a list of proportions of the local data in respect to the original data.\n",
        "    \"\"\"\n",
        "    slices = list()\n",
        "    weights = list()\n",
        "    n_samples = len(data)\n",
        "    unique_clusters = np.unique(clusters)\n",
        "    for c in unique_clusters:\n",
        "        local_data = data[clusters == c, :]\n",
        "        slices.append(local_data)\n",
        "        weights.append(len(local_data) / n_samples)\n",
        "    return slices, weights\n",
        "\n",
        "\n",
        "def get_split_rows_method(split_rows: str) -> SplitRowsFunc:\n",
        "    \"\"\"\n",
        "    Get the rows splitting method given a string.\n",
        "\n",
        "    :param split_rows: The string of the method do get.\n",
        "    :return: The corresponding rows splitting function.\n",
        "    :raises ValueError: If the rows splitting method is unknown.\n",
        "    \"\"\"\n",
        "    if split_rows == 'kmeans':\n",
        "        return kmeans\n",
        "    if split_rows == 'kmeans_mb':\n",
        "        return kmeans_mb\n",
        "    if split_rows == 'dbscan':\n",
        "        return dbscan\n",
        "    if split_rows == 'wald':\n",
        "        return wald\n",
        "    if split_rows == 'gmm':\n",
        "        return gmm\n",
        "    if split_rows == 'rdc':\n",
        "        return rdc_rows\n",
        "    if split_rows == 'random':\n",
        "        return random_rows\n",
        "    raise ValueError(\"Unknown split rows method called {}\".format(split_rows))\n",
        "\n",
        "\n",
        "#---------------------------------------------------\n",
        "\n",
        "SplitColsFunc = Callable[\n",
        "    [np.ndarray,                # The data\n",
        "     List[Type[Leaf]],          # The distributions\n",
        "     List[Union[list, tuple]],  # The domains\n",
        "     np.random.RandomState,     # The random state\n",
        "     Any],                      # Other arguments\n",
        "    np.ndarray                  # The columns ids\n",
        "]\n",
        "\n",
        "\n",
        "def split_cols_clusters(\n",
        "    data: np.ndarray,\n",
        "    clusters: np.ndarray,\n",
        "    scope: List[int]\n",
        ") -> Tuple[List[np.ndarray], List[List[int]]]:\n",
        "    \"\"\"\n",
        "    Split the data vertically given the clusters.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param clusters: The clusters.\n",
        "    :param scope: The original scope.\n",
        "    :return: (slices, scopes) where slices is a list of partial data and\n",
        "             scopes is a list of partial scopes.\n",
        "    \"\"\"\n",
        "    slices = list()\n",
        "    scopes = list()\n",
        "    scope = np.asarray(scope)\n",
        "    unique_clusters = np.unique(clusters)\n",
        "    for c in unique_clusters:\n",
        "        cols = (clusters == c)\n",
        "        slices.append(data[:, cols])\n",
        "        scopes.append(scope[cols].tolist())\n",
        "    return slices, scopes\n",
        "\n",
        "\n",
        "def get_split_cols_method(split_cols: str) -> SplitColsFunc:\n",
        "    \"\"\"\n",
        "    Get the columns splitting method given a string.\n",
        "\n",
        "    :param split_cols: The string of the method do get.\n",
        "    :return: The corresponding columns splitting function.\n",
        "    :raises ValueError: If the columns splitting method is unknown.\n",
        "    \"\"\"\n",
        "    # if split_cols == 'gvs':\n",
        "    #     return gvs_cols\n",
        "    # if split_cols == 'rgvs':\n",
        "    #     return rgvs_cols\n",
        "    # if split_cols == 'wrgvs':\n",
        "    #     return wrgvs_cols\n",
        "    # if split_cols == 'ebvs':\n",
        "    #     return entropy_cols\n",
        "    # if split_cols == 'ebvs_ae':\n",
        "    #     return entropy_adaptive_cols\n",
        "    # if split_cols == 'gbvs':\n",
        "    #     return gini_cols\n",
        "    # if split_cols == 'gbvs_ag':\n",
        "    #     return gini_adaptive_cols\n",
        "    if split_cols == 'rdc':\n",
        "        return rdc_cols\n",
        "    if split_cols == 'random':\n",
        "        return random_cols\n",
        "    raise ValueError(\"Unknown split rows method called {}\".format(split_cols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta3_pJJRPGbd"
      },
      "outputs": [],
      "source": [
        "## other methods of splitting: https://github.com/deeprob-org/deeprob-kit/tree/main/deeprob/spn/learning/splitting\n",
        "## remember if using those --> in previous cell also un-uncomment corresponding if clauses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBuM5qIX86XK"
      },
      "source": [
        "# Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UjyUhLrg1s7Q"
      },
      "outputs": [],
      "source": [
        "#@title Expectation Maximization EM\n",
        "\n",
        "def expectation_maximization(\n",
        "    root: Node,\n",
        "    data: np.ndarray,\n",
        "    num_iter: int = 100,\n",
        "    batch_perc: float = 0.1,\n",
        "    step_size: float = 0.5,\n",
        "    random_init: bool = True,\n",
        "    random_state: Optional[RandomState] = None,\n",
        "    verbose: bool = True\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn the parameters of a SPN by batch Expectation-Maximization (EM).\n",
        "    See https://arxiv.org/abs/1604.07243 and https://arxiv.org/abs/2004.06231 for details.\n",
        "\n",
        "    :param root: The spn structure.\n",
        "    :param data: The data to use to learn the parameters.\n",
        "    :param num_iter: The number of iterations.\n",
        "    :param batch_perc: The percentage of data to use for each step.\n",
        "    :param step_size: The step size for batch EM.\n",
        "    :param random_init: Whether to random initialize the weights of the SPN.\n",
        "    :param random_state: The random state. It can be either None, a seed integer or a Numpy RandomState.\n",
        "    :param verbose: Whether to enable verbose learning.\n",
        "    :return: The spn with learned parameters.\n",
        "    :raises ValueError: If a parameter is out of domain.\n",
        "    \"\"\"\n",
        "    if num_iter <= 0:\n",
        "        raise ValueError(\"The number of iterations must be positive\")\n",
        "    if batch_perc <= 0.0 or batch_perc >= 1.0:\n",
        "        raise ValueError(\"The batch percentage must be in (0, 1)\")\n",
        "    if step_size <= 0.0 or step_size >= 1.0:\n",
        "        raise ValueError(\"The step size must be in (0, 1)\")\n",
        "\n",
        "    # Check the SPN\n",
        "    check_spn(root, labeled=True, smooth=True, decomposable=True)\n",
        "\n",
        "    # Compute the batch size\n",
        "    n_samples = len(data)\n",
        "    batch_size = int(batch_perc * n_samples)\n",
        "\n",
        "    # Compute a list-based cache for accessing nodes\n",
        "    cached_nodes = {\n",
        "        'sum': filter_nodes_by_type(root, Sum),\n",
        "        'leaf': filter_nodes_by_type(root, Leaf)\n",
        "    }\n",
        "\n",
        "    # Check the random state\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    # Random initialize the parameters of the SPN, if specified\n",
        "    if random_init:\n",
        "        # Initialize the sum parameters\n",
        "        for node in cached_nodes['sum']:\n",
        "            node.em_init(random_state)\n",
        "\n",
        "        # Initialize the leaf parameters\n",
        "        for node in cached_nodes['leaf']:\n",
        "            node.em_init(random_state)\n",
        "\n",
        "    # Initialize the tqdm bar, if verbose is specified\n",
        "    iterator = range(num_iter)\n",
        "    if verbose:\n",
        "        iterator = tqdm(\n",
        "            iterator, leave=None, unit='batch',\n",
        "            bar_format='{desc}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
        "        )\n",
        "\n",
        "    for _ in iterator:\n",
        "        # Sample a batch of data randomly with uniform distribution\n",
        "        batch_indices = random_state.choice(n_samples, size=batch_size, replace=False)\n",
        "        batch_data = data[batch_indices]\n",
        "\n",
        "        # Prevent checking the SPN at every forward inference step, we already did that!\n",
        "        with ContextState(check_spn=False):\n",
        "            # Forward step, obtaining the LLs at each node\n",
        "            root_ll, lls = log_likelihood(root, batch_data, return_results=True)\n",
        "        mean_ll = np.mean(root_ll)\n",
        "\n",
        "        # Backward step, compute the log-gradients required to compute the sufficient statistics\n",
        "        grads = eval_backward(root, lls)\n",
        "\n",
        "        # Update the weights of each sum node\n",
        "        for node in cached_nodes['sum']:\n",
        "            children_ll = lls[list(map(lambda c: c.id, node.children))]\n",
        "            stats = np.exp(children_ll - root_ll + grads[node.id])\n",
        "            node.em_step(stats, step_size)\n",
        "\n",
        "        # Update the parameters of each leaf node\n",
        "        for node in cached_nodes['leaf']:\n",
        "            stats = np.exp(lls[node.id] - root_ll + grads[node.id])\n",
        "            node.em_step(stats, batch_data[:, node.scope], step_size)\n",
        "\n",
        "        # Update the progress bar\n",
        "        if verbose:\n",
        "            iterator.set_description('Batch Avg. LL: {:.4f}'.format(mean_ll))\n",
        "\n",
        "    return root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OqT0IDL3y6YA"
      },
      "outputs": [],
      "source": [
        "#@title SPN leaf methods\n",
        "\n",
        "#: A signature for a learn SPN leaf function.\n",
        "LearnLeafFunc = Callable[\n",
        "    [np.ndarray,                # The data\n",
        "     List[Type[Leaf]],          # The distributions\n",
        "     List[Union[list, tuple]],  # The domains\n",
        "     List[int],                 # The scope\n",
        "     Any],                      # Other arguments\n",
        "    Node                        # A SPN node\n",
        "]\n",
        "\n",
        "\n",
        "def get_learn_leaf_method(learn_leaf: str) -> LearnLeafFunc:\n",
        "    \"\"\"\n",
        "    Get the learn leaf method.\n",
        "\n",
        "    :param learn_leaf: The learn leaf method string to use.\n",
        "    :return: A learn leaf function.\n",
        "    :raises ValueError: If the leaf learning method is unknown.\n",
        "    \"\"\"\n",
        "    if learn_leaf == 'mle':\n",
        "        return learn_mle\n",
        "    if learn_leaf == 'isotonic':\n",
        "        return learn_isotonic\n",
        "    if learn_leaf == 'binary-clt':\n",
        "        return learn_binary_clt\n",
        "    raise ValueError(\"Unknown learn leaf method called {}\".format(learn_leaf))\n",
        "\n",
        "\n",
        "def learn_mle(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    scope: List[int],\n",
        "    alpha: float = 0.1,\n",
        "    random_state: Optional[RandomState] = None\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn a leaf using Maximum Likelihood Estimate (MLE).\n",
        "    If the data is multivariate, a naive factorized model is learned.\n",
        "\n",
        "    :param data: The data, where each column correspond to a random variable.\n",
        "    :param distributions: The distributions of the random variables.\n",
        "    :param domains: The domains of the random variables.\n",
        "    :param scope: The scope of the leaf.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    :param random_state: The random state. It can be None.\n",
        "    :return: A leaf distribution.\n",
        "    :raises ValueError: If there are inconsistencies between the data, distributions and domains.\n",
        "    \"\"\"\n",
        "    if len(scope) != len(distributions) or len(domains) != len(distributions):\n",
        "        raise ValueError(\"Each data column should correspond to a random variable having a distribution and a domain\")\n",
        "\n",
        "    if len(scope) == 1:\n",
        "        sc, dist, dom = scope[0], distributions[0], domains[0]\n",
        "        leaf = dist(sc)\n",
        "        leaf.fit(data, dom, alpha=alpha)\n",
        "        return leaf\n",
        "\n",
        "    return learn_naive_factorization(\n",
        "        data, distributions, domains, scope, learn_mle,\n",
        "        alpha=alpha, random_state=random_state\n",
        "    )\n",
        "\n",
        "\n",
        "def learn_isotonic(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    scope: List[int],\n",
        "    alpha: float = 0.1,\n",
        "    random_state: Optional[RandomState] = None\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn a leaf using Isotonic method.\n",
        "    If the data is multivariate, a naive factorized model is learned.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The distribution of the random variables.\n",
        "    :param domains: The domain of the random variables.\n",
        "    :param scope: The scope of the leaf.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    :param random_state: The random sate. It can be None.\n",
        "    :return: A leaf distribution.\n",
        "    :raises ValueError: If there are inconsistencies between the data, distributions and domains.\n",
        "    \"\"\"\n",
        "    if len(scope) != len(distributions) or len(domains) != len(distributions):\n",
        "        raise ValueError(\"Each data column should correspond to a random variable having a distribution and a domain\")\n",
        "\n",
        "    if len(scope) == 1:\n",
        "        sc, dist, dom = scope[0], distributions[0], domains[0]\n",
        "        leaf = Isotonic(sc) if dist.LEAF_TYPE == LeafType.CONTINUOUS else dist(sc)\n",
        "        leaf.fit(data, dom, alpha=alpha)\n",
        "        return leaf\n",
        "\n",
        "    return learn_naive_factorization(\n",
        "        data, distributions, domains, scope, learn_isotonic,\n",
        "        alpha=alpha, random_state=random_state\n",
        "    )\n",
        "\n",
        "\n",
        "def learn_binary_clt(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    scope: List[int],\n",
        "    to_pc: bool = False,\n",
        "    alpha: float = 0.1,\n",
        "    random_state: Optional[RandomState] = None\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn a leaf using a Binary Chow-Liu Tree (CLT).\n",
        "    If the data is univariate, a Maximum Likelihood Estimate (MLE) leaf is returned.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The distributions of the random variables.\n",
        "    :param domains: The domains of the random variables.\n",
        "    :param scope: The scope of the leaf.\n",
        "    :param to_pc: Whether to convert the CLT into an equivalent PC.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    :param random_state: The random state. It can be None.\n",
        "    :return: A leaf distribution.\n",
        "    :raises ValueError: If there are inconsistencies between the data, distributions and domains.\n",
        "    :raises ValueError: If the data doesn't follow a Bernoulli distribution.\n",
        "    \"\"\"\n",
        "    if len(scope) != len(distributions) or len(domains) != len(distributions):\n",
        "        raise ValueError(\"Each data column should correspond to a random variable having a distribution and a domain\")\n",
        "    if any(d != Bernoulli for d in distributions):\n",
        "        raise ValueError(\"Binary Chow-Liu trees are only available for Bernoulli data\")\n",
        "\n",
        "    # If univariate, learn using MLE instead\n",
        "    if len(scope) == 1:\n",
        "        return learn_mle(\n",
        "            data, distributions, domains, scope,\n",
        "            alpha=alpha, random_state=random_state\n",
        "        )\n",
        "\n",
        "    # If multivariate, learn a binary CLTree\n",
        "    leaf = BinaryCLT(scope)\n",
        "    leaf.fit(data, domains, alpha=alpha, random_state=random_state)\n",
        "\n",
        "    # Make the conversion to a probabilistic circuit, if specified\n",
        "    if to_pc:\n",
        "        return leaf.to_pc()\n",
        "    return leaf\n",
        "\n",
        "\n",
        "def learn_naive_factorization(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    scope: List[int],\n",
        "    learn_leaf_func: LearnLeafFunc,\n",
        "    **learn_leaf_kwargs\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn a leaf as a naive factorized model.\n",
        "\n",
        "    :param data: The data.\n",
        "    :param distributions: The distribution of the random variables.\n",
        "    :param domains: The domain of the random variables.\n",
        "    :param scope: The scope of the leaf.\n",
        "    :param learn_leaf_func: The function to use to learn the sub-distributions parameters.\n",
        "    :param learn_leaf_kwargs: Additional parameters for learn_leaf_func.\n",
        "    :return: A naive factorized model.\n",
        "    :raises ValueError: If there are inconsistencies between the data, distributions and domains.\n",
        "    \"\"\"\n",
        "    if len(scope) != len(distributions) or len(domains) != len(distributions):\n",
        "        raise ValueError(\"Each data column should correspond to a random variable having a distribution and a domain\")\n",
        "\n",
        "    node = Product(scope)\n",
        "    for i, s in enumerate(scope):\n",
        "        leaf = learn_leaf_func(data[:, [i]], [distributions[i]], [domains[i]], [s], **learn_leaf_kwargs)\n",
        "        leaf.id = i + 1  # Set the leaves ids sequentially\n",
        "        node.children.append(leaf)\n",
        "    return node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-FRO9Sj_wtqL"
      },
      "outputs": [],
      "source": [
        "#@title learnSPN method\n",
        "\n",
        "\n",
        "class OperationKind(Enum):\n",
        "    \"\"\"\n",
        "    Operation kind used by LearnSPN algorithm.\n",
        "    \"\"\"\n",
        "    REM_FEATURES = 1\n",
        "    CREATE_LEAF = 2\n",
        "    SPLIT_NAIVE = 3\n",
        "    SPLIT_ROWS = 4\n",
        "    SPLIT_COLS = 5\n",
        "\n",
        "\n",
        "class Task(NamedTuple):\n",
        "    \"\"\"\n",
        "    Recursive task information used by LearnSPN algorithm.\n",
        "    \"\"\"\n",
        "    parent: Node\n",
        "    data: np.ndarray\n",
        "    scope: List[int]\n",
        "    no_cols_split: bool = False\n",
        "    no_rows_split: bool = False\n",
        "    is_first: bool = False\n",
        "\n",
        "\n",
        "def learn_spn(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: List[Union[list, tuple]],\n",
        "    learn_leaf: Union[str, LearnLeafFunc] = 'mle',\n",
        "    split_rows: Union[str, SplitRowsFunc] = 'kmeans',\n",
        "    split_cols: Union[str, SplitColsFunc] = 'rdc',\n",
        "    learn_leaf_kwargs: dict = None,\n",
        "    split_rows_kwargs: dict = None,\n",
        "    split_cols_kwargs: dict = None,\n",
        "    min_rows_slice: int = 256,\n",
        "    min_cols_slice: int = 2,\n",
        "    random_state: Optional[RandomState] = None,\n",
        "    verbose: bool = True\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn the structure and parameters of a SPN given some training data and several hyperparameters.\n",
        "\n",
        "    :param data: The training data.\n",
        "    :param distributions: A list of distribution classes (one for each feature).\n",
        "    :param domains: A list of domains (one for each feature). Each domain is either a list of values, for discrete\n",
        "                    distributions, or a tuple (consisting of min value and max value), for continuous distributions.\n",
        "    :param learn_leaf: The method to use to learn a distribution leaf node,\n",
        "                       It can be either 'mle', 'isotonic', 'binary-clt' or a custom LearnLeafFunc.\n",
        "    :param split_rows: The rows splitting method.\n",
        "                       It can be either 'kmeans', 'gmm', 'rdc', 'random' or a custom SplitRowsFunc function.\n",
        "    :param split_cols: The columns splitting method.\n",
        "                       It can be either 'gvs', 'rgvs', 'wrgvs', 'ebvs', 'ebvs_ae', 'gbvs', 'gbvs_ag', 'rdc', 'random'\n",
        "                       or a custom SplitColsFunc function.\n",
        "    :param learn_leaf_kwargs: The parameters of the learn leaf method.\n",
        "    :param split_rows_kwargs: The parameters of the rows splitting method.\n",
        "    :param split_cols_kwargs: The parameters of the cols splitting method.\n",
        "    :param min_rows_slice: The minimum number of samples required to split horizontally.\n",
        "    :param min_cols_slice: The minimum number of features required to split vertically.\n",
        "    :param random_state: The random state. It can be either None, a seed integer or a Numpy RandomState.\n",
        "    :param verbose: Whether to enable verbose mode.\n",
        "    :return: A learned valid SPN.\n",
        "    :raises ValueError: If a parameter is out of scope.\n",
        "    \"\"\"\n",
        "    if len(distributions) == 0:\n",
        "        raise ValueError(\"The list of distribution classes must be non-empty\")\n",
        "    if len(domains) == 0:\n",
        "        raise ValueError(\"The list of domains must be non-empty\")\n",
        "    if min_rows_slice <= 0:\n",
        "        raise ValueError(\"The minimum number of samples required to split horizontally must be positive\")\n",
        "    if min_cols_slice <= 0:\n",
        "        raise ValueError(\"The minimum number of samples required to split vertically must be positive\")\n",
        "\n",
        "    n_samples, n_features = data.shape\n",
        "    if len(distributions) != n_features or len(domains) != n_features:\n",
        "        raise ValueError(\"Each data column should correspond to a random variable having a distribution and a domain\")\n",
        "\n",
        "    # Setup the learn leaf, split rows and split cols functions\n",
        "    learn_leaf_func = get_learn_leaf_method(learn_leaf) if isinstance(learn_leaf, str) else learn_leaf\n",
        "    split_rows_func = get_split_rows_method(split_rows) if isinstance(split_rows, str) else split_rows\n",
        "    split_cols_func = get_split_cols_method(split_cols) if isinstance(split_cols, str) else split_cols\n",
        "\n",
        "    if learn_leaf_kwargs is None:\n",
        "        learn_leaf_kwargs = dict()\n",
        "    if split_rows_kwargs is None:\n",
        "        split_rows_kwargs = dict()\n",
        "    if split_cols_kwargs is None:\n",
        "        split_cols_kwargs = dict()\n",
        "\n",
        "    # Setup the initial scope as [0, # of features - 1]\n",
        "    initial_scope = list(range(n_features))\n",
        "\n",
        "    # Check the random state\n",
        "    random_state = check_random_state(random_state)\n",
        "\n",
        "    # Add the random state to learning leaf parameters\n",
        "    learn_leaf_kwargs['random_state'] = random_state\n",
        "\n",
        "    # Initialize the progress bar (with unspecified total), if verbose is enabled\n",
        "    if verbose:\n",
        "        tk = tqdm(\n",
        "            total=np.inf, leave=None, unit='node',\n",
        "            bar_format='{n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}]'\n",
        "        )\n",
        "\n",
        "    tasks = deque()\n",
        "    tmp_node = Product(initial_scope)\n",
        "    tasks.append(Task(tmp_node, data, initial_scope, is_first=True))\n",
        "\n",
        "    while tasks:\n",
        "        # Get the next task\n",
        "        task = tasks.popleft()\n",
        "\n",
        "        # Select the operation to apply\n",
        "        n_samples, n_features = task.data.shape\n",
        "        # Get the indices of uninformative features\n",
        "        zero_var_idx = np.isclose(np.var(task.data, axis=0), 0.0)\n",
        "        # If all the features are uninformative, then split using Naive Bayes model\n",
        "        if np.all(zero_var_idx):\n",
        "            op = OperationKind.SPLIT_NAIVE\n",
        "        # If only some of the features are uninformative, then remove them\n",
        "        elif np.any(zero_var_idx):\n",
        "            op = OperationKind.REM_FEATURES\n",
        "        # Create a leaf node if the data split dimension is small or last rows splitting failed\n",
        "        elif task.no_rows_split or n_features < min_cols_slice or n_samples < min_rows_slice:\n",
        "            op = OperationKind.CREATE_LEAF\n",
        "        # Use rows splitting if previous columns splitting failed or it is the first task\n",
        "        elif task.no_cols_split or task.is_first:\n",
        "            op = OperationKind.SPLIT_ROWS\n",
        "        # Defaults to columns splitting\n",
        "        else:\n",
        "            op = OperationKind.SPLIT_COLS\n",
        "\n",
        "        if op == OperationKind.REM_FEATURES:\n",
        "            node = Product(task.scope)\n",
        "\n",
        "            # Model the removed features using Naive Bayes\n",
        "            rem_scope = [task.scope[i] for i, in np.argwhere(zero_var_idx)]\n",
        "            dists, doms = [distributions[s] for s in rem_scope], [domains[s] for s in rem_scope]\n",
        "            naive = learn_naive_factorization(\n",
        "                task.data[:, zero_var_idx], dists, doms, rem_scope,\n",
        "                learn_leaf_func=learn_leaf_func, **learn_leaf_kwargs\n",
        "            )\n",
        "            node.children.append(naive)\n",
        "\n",
        "            # Add the tasks regarding non-removed features\n",
        "            is_first = task.is_first and len(tasks) == 0\n",
        "            oth_scope = [task.scope[i] for i, in np.argwhere(~zero_var_idx)]\n",
        "            tasks.append(Task(node, task.data[:, ~zero_var_idx], oth_scope, is_first=is_first))\n",
        "            task.parent.children.append(node)\n",
        "        elif op == OperationKind.CREATE_LEAF:\n",
        "            # Create a leaf node\n",
        "            dists, doms = [distributions[s] for s in task.scope], [domains[s] for s in task.scope]\n",
        "            leaf = learn_leaf_func(task.data, dists, doms, task.scope, **learn_leaf_kwargs)\n",
        "            task.parent.children.append(leaf)\n",
        "        elif op == OperationKind.SPLIT_NAIVE:\n",
        "            # Split the data using a naive factorization\n",
        "            dists, doms = [distributions[s] for s in task.scope], [domains[s] for s in task.scope]\n",
        "            node = learn_naive_factorization(\n",
        "                task.data, dists, doms, task.scope,\n",
        "                learn_leaf_func=learn_leaf_func, **learn_leaf_kwargs\n",
        "            )\n",
        "            task.parent.children.append(node)\n",
        "        elif op == OperationKind.SPLIT_ROWS:\n",
        "            # Split the data by rows (sum node)\n",
        "            dists, doms = [distributions[s] for s in task.scope], [domains[s] for s in task.scope]\n",
        "            clusters = split_rows_func(task.data, dists, doms, random_state, **split_rows_kwargs)\n",
        "            slices, weights = split_rows_clusters(task.data, clusters)\n",
        "\n",
        "            # Check whether only one partitioning is returned\n",
        "            if len(slices) == 1:\n",
        "                tasks.append(Task(task.parent, task.data, task.scope, no_cols_split=False, no_rows_split=True))\n",
        "                continue\n",
        "\n",
        "            # Add sub-tasks and append Sum node\n",
        "            node = Sum(task.scope, weights=weights)\n",
        "            for local_data in slices:\n",
        "                tasks.append(Task(node, local_data, task.scope))\n",
        "            task.parent.children.append(node)\n",
        "        elif op == OperationKind.SPLIT_COLS:\n",
        "            # Split the data by columns (product node)\n",
        "            dists, doms = [distributions[s] for s in task.scope], [domains[s] for s in task.scope]\n",
        "            clusters = split_cols_func(task.data, dists, doms, random_state, **split_cols_kwargs)\n",
        "            slices, scopes = split_cols_clusters(task.data, clusters, task.scope)\n",
        "\n",
        "            # Check whether only one partitioning is returned\n",
        "            if len(slices) == 1:\n",
        "                tasks.append(Task(task.parent, task.data, task.scope, no_cols_split=True, no_rows_split=False))\n",
        "                continue\n",
        "\n",
        "            # Add sub-tasks and append Product node\n",
        "            node = Product(task.scope)\n",
        "            for i, local_data in enumerate(slices):\n",
        "                tasks.append(Task(node, local_data, scopes[i]))\n",
        "            task.parent.children.append(node)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Operation of kind {} not implemented\".format(op))\n",
        "\n",
        "        if verbose:\n",
        "            tk.update()\n",
        "            tk.refresh()\n",
        "\n",
        "    if verbose:\n",
        "        tk.close()\n",
        "\n",
        "    root = tmp_node.children[0]\n",
        "    return assign_ids(root)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yzrVwFi43ROs"
      },
      "outputs": [],
      "source": [
        "#@title learn binary cutset network CNet\n",
        "\n",
        "def compute_or_bd_scores(\n",
        "    data: np.ndarray,\n",
        "    ess: float = 0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the BDeu scores for the candidate OR nodes given the data.\n",
        "\n",
        "    :param data: The binary data matrix.\n",
        "    :param ess: The equivalent sample size (ESS).\n",
        "    :return: The score array.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    prior_counts = compute_prior_counts(data=data)\n",
        "    alpha_i = ess\n",
        "    alpha_ik = ess / 2\n",
        "    log_gamma_nodes = gammaln(alpha_i) - gammaln(n_samples + alpha_i) \\\n",
        "                      + np.sum(gammaln(prior_counts + alpha_ik) - gammaln(alpha_ik), axis=-1)\n",
        "    return log_gamma_nodes\n",
        "\n",
        "\n",
        "def compute_clt_bd_scores(\n",
        "    data: np.ndarray,\n",
        "    ess: float = 0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the pairwise BDeu scores for constructing a CLT given the data.\n",
        "\n",
        "    :param data: The binary data matrix.\n",
        "    :param ess: The equivalent sample size (ESS).\n",
        "    :return: The pairwise BDeu score matrix.\n",
        "    \"\"\"\n",
        "    joint_counts = compute_joint_counts(data=data)\n",
        "    alpha_ij = ess / 2\n",
        "    alpha_ijk = ess / (2 * 2)\n",
        "    parent_counts = np.sum(joint_counts, axis=-2)\n",
        "    log_gamma_pairs = gammaln(alpha_ij) - gammaln(parent_counts + alpha_ij) \\\n",
        "                      + np.sum(gammaln(joint_counts + alpha_ijk) - gammaln(alpha_ijk), axis=-2)\n",
        "    return np.sum(log_gamma_pairs, axis=-1)\n",
        "\n",
        "\n",
        "def estimate_clt_params_bayesian(\n",
        "    clt: BinaryCLT,\n",
        "    data: np.ndarray,\n",
        "    ess: float = 0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute the Bayesian posterior parameters for a CLT.\n",
        "\n",
        "    :param clt: The CLT.\n",
        "    :param data: The binary data matrix.\n",
        "    :param ess: The equivalent sample size (ESS).\n",
        "    :return: The CLT parameters in the log space.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    priors, joints = estimate_priors_joints(data, alpha=ess / 4)\n",
        "\n",
        "    vs = np.arange(n_features)\n",
        "    params = np.einsum('ikl,il->ilk', joints[vs, clt.tree], np.reciprocal(priors[clt.tree]))\n",
        "    params[clt.root] = priors[clt.root]\n",
        "\n",
        "    # Re-normalize the factors, because there can be FP32 approximation errors\n",
        "    params /= np.sum(params, axis=2, keepdims=True)\n",
        "    return np.log(params)\n",
        "\n",
        "\n",
        "def eval_tree_score(\n",
        "    tree: Optional[List[int], np.ndarray],\n",
        "    clt_scores: np.ndarray,\n",
        "    or_scores: np.ndarray\n",
        "):\n",
        "    \"\"\"\n",
        "    Evaluate the BDeu score for a tree structure.\n",
        "\n",
        "    :param tree: The tree structure.\n",
        "    :param clt_scores: The pairwise score matrix.\n",
        "    :param or_scores: The OR score array.\n",
        "    :return: The BDeu score of the tree structure.\n",
        "    \"\"\"\n",
        "    root_idx = tree.argmin()\n",
        "    parent_indices_no_root = np.delete(tree, obj=root_idx)\n",
        "    child_indices_no_root = np.delete(np.arange(len(tree)), obj=root_idx)\n",
        "    return np.sum(clt_scores[child_indices_no_root, parent_indices_no_root]) + or_scores[root_idx]\n",
        "\n",
        "\n",
        "def select_cand_cuts(\n",
        "    data: np.ndarray,\n",
        "    ess: float = 0.1,\n",
        "    n_cand_cuts: int = 10\n",
        "):\n",
        "    \"\"\"\n",
        "    Select the candidate cutting nodes.\n",
        "\n",
        "    :param data: The binary data.\n",
        "    :param ess: The equivalent sample size (ESS).\n",
        "    :param n_cand_cuts: The number of candidate cutting nodes.\n",
        "    :return: The indices of the selected nodes.\n",
        "    \"\"\"\n",
        "    # Compute the counts\n",
        "    n_samples, n_features = data.shape\n",
        "    counts_features = data.sum(axis=0)\n",
        "\n",
        "    prior_counts = compute_prior_counts(data)\n",
        "    joint_counts = compute_joint_counts(data)\n",
        "    smoothing_joint, smoothing_prior = ess / 2, ess\n",
        "    if ess < 0.01:\n",
        "        prior_counts = prior_counts.astype(np.float64)\n",
        "        joint_counts = joint_counts.astype(np.float64)\n",
        "    log_priors = np.log(prior_counts + smoothing_joint) - np.log(n_samples + smoothing_prior)\n",
        "    mean_entropy = -(log_priors * np.exp(log_priors)).sum() / n_features\n",
        "\n",
        "    conditionals = np.empty((n_features, n_features, 2, 2), dtype=prior_counts.dtype)\n",
        "    conditionals[:, :, 0, 0] = ((joint_counts[:, :, 0, 0] + smoothing_joint).T /\n",
        "                                (prior_counts[:, 0] + smoothing_prior)).T\n",
        "    conditionals[:, :, 0, 1] = ((joint_counts[:, :, 0, 1] + smoothing_joint).T /\n",
        "                                (prior_counts[:, 0] + smoothing_prior)).T\n",
        "    conditionals[:, :, 1, 0] = ((joint_counts[:, :, 1, 0] + smoothing_joint).T /\n",
        "                                (prior_counts[:, 1] + smoothing_prior)).T\n",
        "    conditionals[:, :, 1, 1] = ((joint_counts[:, :, 1, 1] + smoothing_joint).T /\n",
        "                                (prior_counts[:, 1] + smoothing_prior)).T\n",
        "\n",
        "    vs = np.repeat(np.arange(n_features)[None, :], n_features, axis=0)\n",
        "    vs = vs[~np.eye(vs.shape[0], dtype=bool)].reshape(vs.shape[0], -1)\n",
        "    parents = np.repeat(np.arange(n_features)[:, None], n_features - 1, axis=1)\n",
        "\n",
        "    ratio_features = counts_features / n_samples\n",
        "    entropies = ratio_features * \\\n",
        "                np.mean(-np.sum(conditionals[parents, vs, 1, :] * np.log(conditionals[parents, vs, 1, :]), axis=-1),\n",
        "                        axis=1) + \\\n",
        "                (1 - ratio_features) * \\\n",
        "                np.mean(-np.sum(conditionals[parents, vs, 0, :] * np.log(conditionals[parents, vs, 0, :]), axis=-1),\n",
        "                        axis=1)\n",
        "\n",
        "    info_gains = mean_entropy - entropies\n",
        "    selected_idx = np.argmax(info_gains) if n_cand_cuts == 1 else np.argpartition(info_gains,\n",
        "                                                                                  -n_cand_cuts)[-n_cand_cuts:]\n",
        "    return selected_idx\n",
        "\n",
        "\n",
        "def learn_cnet_bd(\n",
        "    data: np.ndarray,\n",
        "    ess: float = 0.1,\n",
        "    n_cand_cuts: int = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Learn a binary CNet using the Bayesian-Dirichlet equivalent uniform (BDeu) score.\n",
        "\n",
        "    :param cnet: The binary CNet.\n",
        "    :param data: The training data.\n",
        "    :param ess: The equivalent sample size (ESS).\n",
        "    :param n_cand_cuts: The number of candidate cutting nodes.\n",
        "    :return: A binary CNet.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    root = BinaryCNet(scope=list(range(n_features)))\n",
        "    root.assign_indices(row_indices=np.arange(n_samples), col_indices=np.arange(n_features))\n",
        "    root.fit_clt(data=data)\n",
        "    # use Bayesian posterior parameters.\n",
        "    root.clt.params = estimate_clt_params_bayesian(clt=root.clt, data=data, ess=ess)\n",
        "    or_score_matrix = compute_or_bd_scores(data=data, ess=ess)\n",
        "    clt_score_matrix = compute_clt_bd_scores(data=data, ess=ess)\n",
        "    clt_score = eval_tree_score(tree=root.clt.tree, clt_scores=clt_score_matrix, or_scores=or_score_matrix)\n",
        "\n",
        "    node_stack = [[root, ess, clt_score]]\n",
        "    while node_stack:\n",
        "        node, node_ess, node_clt_score = node_stack.pop(0)\n",
        "        if len(node.scope) == 1:\n",
        "            continue\n",
        "\n",
        "        partition = data[node.row_indices][:, node.col_indices]\n",
        "        or_score_matrix = compute_or_bd_scores(data=partition, ess=node_ess)\n",
        "\n",
        "        k = min(n_cand_cuts, len(node.scope))\n",
        "        search_indices = select_cand_cuts(data=partition, ess=node_ess, n_cand_cuts=k)\n",
        "\n",
        "        best_or_idx = -1\n",
        "        best_cnet_score = -np.inf\n",
        "        best_left_clt = None\n",
        "        best_right_clt = None\n",
        "        best_left_clt_score = -np.inf\n",
        "        best_right_clt_score = -np.inf\n",
        "\n",
        "        for i in search_indices:\n",
        "            left_row_indices = node.row_indices[partition[:, i] == 0]\n",
        "            right_row_indices = node.row_indices[partition[:, i] == 1]\n",
        "\n",
        "            if len(left_row_indices) == 0 or len(right_row_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            child_col_indices = np.delete(node.col_indices, obj=i)\n",
        "            left_partition = data[left_row_indices][:, child_col_indices]\n",
        "            right_partition = data[right_row_indices][:, child_col_indices]\n",
        "            new_scope = node.scope.copy()\n",
        "            del new_scope[i]\n",
        "\n",
        "            left_clt = BinaryCLT(scope=new_scope)\n",
        "            left_or_score_matrix = compute_or_bd_scores(data=left_partition, ess=node_ess / 2)\n",
        "            left_clt_score_matrix = compute_clt_bd_scores(data=left_partition, ess=node_ess / 2)\n",
        "\n",
        "            right_clt = BinaryCLT(scope=new_scope)\n",
        "            right_or_score_matrix = compute_or_bd_scores(data=right_partition, ess=node_ess / 2)\n",
        "            right_clt_score_matrix = compute_clt_bd_scores(data=right_partition, ess=node_ess / 2)\n",
        "\n",
        "            left_clt.fit(data=left_partition, domain=[[0, 1]] * len(new_scope), alpha=0.01)\n",
        "            right_clt.fit(data=right_partition, domain=[[0, 1]] * len(new_scope), alpha=0.01)\n",
        "\n",
        "            left_clt.params = estimate_clt_params_bayesian(left_clt, data=left_partition, ess=node_ess / 2)\n",
        "            right_clt.params = estimate_clt_params_bayesian(right_clt, data=right_partition, ess=node_ess / 2)\n",
        "\n",
        "            left_clt_score = eval_tree_score(tree=left_clt.tree,\n",
        "                                             clt_scores=left_clt_score_matrix,\n",
        "                                             or_scores=left_or_score_matrix)\n",
        "            right_clt_score = eval_tree_score(tree=right_clt.tree,\n",
        "                                              clt_scores=right_clt_score_matrix,\n",
        "                                              or_scores=right_or_score_matrix)\n",
        "            cnet_score = left_clt_score + right_clt_score + or_score_matrix[i]\n",
        "\n",
        "            if cnet_score > best_cnet_score:\n",
        "                best_cnet_score = cnet_score\n",
        "                best_or_idx = i\n",
        "                best_left_clt = left_clt\n",
        "                best_right_clt = right_clt\n",
        "                best_left_clt_score = left_clt_score\n",
        "                best_right_clt_score = right_clt_score\n",
        "\n",
        "        if best_cnet_score > node_clt_score:\n",
        "            node.or_id = node.scope[best_or_idx]\n",
        "            node.clt = None\n",
        "            left_row_indices = node.row_indices[partition[:, best_or_idx] == 0]\n",
        "            right_row_indices = node.row_indices[partition[:, best_or_idx] == 1]\n",
        "            child_col_indices = np.delete(node.col_indices, obj=best_or_idx)\n",
        "            left_weight = (len(left_row_indices) + node_ess / 2) / (len(node.row_indices) + node_ess)\n",
        "            right_weight = 1 - left_weight\n",
        "            new_scope = node.scope.copy()\n",
        "            del new_scope[best_or_idx]\n",
        "\n",
        "            left_child = BinaryCNet(scope=new_scope)\n",
        "            left_child.clt = best_left_clt\n",
        "            left_child.assign_indices(row_indices=left_row_indices, col_indices=child_col_indices)\n",
        "            right_child = BinaryCNet(scope=new_scope)\n",
        "            right_child.clt = best_right_clt\n",
        "            right_child.assign_indices(row_indices=right_row_indices, col_indices=child_col_indices)\n",
        "\n",
        "            node_stack.append([left_child, node_ess / 2, best_left_clt_score])\n",
        "            node_stack.append([right_child, node_ess / 2, best_right_clt_score])\n",
        "            node.weights = [left_weight, right_weight]\n",
        "            node.children = [left_child, right_child]\n",
        "    return root\n",
        "\n",
        "\n",
        "def learn_cnet_bic(\n",
        "    data: np.ndarray,\n",
        "    alpha: float = 0.01,\n",
        "    n_cand_cuts: int = 10,\n",
        "):\n",
        "    \"\"\"\n",
        "    Learn a binary CNet using the Bayesian Information Criterion (BIC) score.\n",
        "\n",
        "    :param data: The binary data.\n",
        "    :param alpha: The Laplace smoothing factor.\n",
        "    :param n_cand_cuts: The number of candidate cutting nodes.\n",
        "    :return: A binary CNet.\n",
        "    \"\"\"\n",
        "    n_samples, n_features = data.shape\n",
        "    root = BinaryCNet(scope=list(range(n_features)))\n",
        "    root.assign_indices(row_indices=np.arange(n_samples), col_indices=np.arange(n_features))\n",
        "    root.fit_clt(data=data)\n",
        "    clt_score = np.sum(root.clt.log_likelihood(data)) - 0.5 * np.log(n_samples) * (2 * n_features - 1)\n",
        "\n",
        "    node_stack = [[root, clt_score]]\n",
        "    while node_stack:\n",
        "        node, node_clt_score = node_stack.pop(0)\n",
        "        if len(node.scope) == 1:\n",
        "            continue\n",
        "\n",
        "        partition = data[node.row_indices][:, node.col_indices]\n",
        "\n",
        "        k = min(n_cand_cuts, len(node.scope))\n",
        "        search_indices = select_cand_cuts(partition, ess=4 * alpha, n_cand_cuts=k)\n",
        "\n",
        "        best_or_idx = -1\n",
        "        best_cnet_score = -np.inf\n",
        "        best_left_clt = None\n",
        "        best_right_clt = None\n",
        "        best_left_clt_score = 0.0\n",
        "        best_right_clt_score = 0.0\n",
        "        for i in search_indices:\n",
        "            left_row_indices = node.row_indices[partition[:, i] == 0]\n",
        "            right_row_indices = node.row_indices[partition[:, i] == 1]\n",
        "\n",
        "            if len(left_row_indices) == 0 or len(right_row_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            child_col_indices = np.delete(node.col_indices, obj=i)\n",
        "            left_partition = data[left_row_indices][:, child_col_indices]\n",
        "            right_partition = data[right_row_indices][:, child_col_indices]\n",
        "            new_scope = node.scope.copy()\n",
        "            del new_scope[i]\n",
        "\n",
        "            left_weight = (len(left_row_indices) + alpha) / (len(node.row_indices) + 2 * alpha)\n",
        "            right_weight = 1 - left_weight\n",
        "\n",
        "            left_clt = BinaryCLT(scope=new_scope)\n",
        "            right_clt = BinaryCLT(scope=new_scope)\n",
        "\n",
        "            left_clt.fit(data=left_partition, domain=[[0, 1]] * len(new_scope), alpha=alpha)\n",
        "            right_clt.fit(data=right_partition, domain=[[0, 1]] * len(new_scope), alpha=alpha)\n",
        "\n",
        "            left_clt_score = np.sum(left_clt.log_likelihood(left_partition)) \\\n",
        "                             - 0.5 * np.log(len(data)) * (2 * len(new_scope) - 1)\n",
        "            right_clt_score = np.sum(right_clt.log_likelihood(right_partition)) \\\n",
        "                              - 0.5 * np.log(len(data)) * (2 * len(new_scope) - 1)\n",
        "            or_score = len(left_partition) * np.log(left_weight) + len(right_partition) * np.log(right_weight) \\\n",
        "                       - 0.5 * np.log(len(data))\n",
        "            cnet_score = left_clt_score + right_clt_score + or_score\n",
        "\n",
        "            if cnet_score > best_cnet_score:\n",
        "                best_cnet_score = cnet_score\n",
        "                best_or_idx = i\n",
        "                best_left_clt = left_clt\n",
        "                best_right_clt = right_clt\n",
        "                best_left_clt_score = left_clt_score\n",
        "                best_right_clt_score = right_clt_score\n",
        "\n",
        "        if best_cnet_score > node_clt_score:\n",
        "            node.or_id = node.scope[best_or_idx]\n",
        "            node.clt = None\n",
        "            left_row_indices = node.row_indices[partition[:, best_or_idx] == 0]\n",
        "            right_row_indices = node.row_indices[partition[:, best_or_idx] == 1]\n",
        "            child_col_indices = np.delete(node.col_indices, obj=best_or_idx)\n",
        "\n",
        "            left_weight = (len(left_row_indices) + alpha) / (len(node.row_indices) + 2 * alpha)\n",
        "            right_weight = 1 - left_weight\n",
        "            new_scope = node.scope.copy()\n",
        "            del new_scope[best_or_idx]\n",
        "\n",
        "            left_child = BinaryCNet(scope=new_scope)\n",
        "            left_child.clt = best_left_clt\n",
        "            left_child.assign_indices(row_indices=left_row_indices, col_indices=child_col_indices)\n",
        "            right_child = BinaryCNet(scope=new_scope)\n",
        "            right_child.clt = best_right_clt\n",
        "            right_child.assign_indices(row_indices=right_row_indices, col_indices=child_col_indices)\n",
        "\n",
        "            node_stack.append([left_child, best_left_clt_score])\n",
        "            node_stack.append([right_child, best_right_clt_score])\n",
        "            node.weights = [left_weight, right_weight]\n",
        "            node.children = [left_child, right_child]\n",
        "    return root\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kdGRQib179pe"
      },
      "outputs": [],
      "source": [
        "#@title learning eXtremely randomized Probabilistic Circuit (XPC) or its ensemble\n",
        "\n",
        "\n",
        "# SD stands for Structured Decomposable\n",
        "SD_LEVEL_0 = 0  # non-SD ensemble of non-SD PCs\n",
        "SD_LEVEL_1 = 1  # non-SD ensemble OF SD PCs\n",
        "SD_LEVEL_2 = 2  # SD ensemble\n",
        "SD_LEVELS = [SD_LEVEL_0, SD_LEVEL_1, SD_LEVEL_2]\n",
        "\n",
        "ROOT = -1\n",
        "\n",
        "\n",
        "def build_disjunction(\n",
        "    data: np.ndarray,\n",
        "    scope: list,\n",
        "    assignments: Optional[np.ndarray] = None,\n",
        "    alpha: float = 0.01\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Build a disjunction (sum node) of conjunctions (product nodes).\n",
        "    If assignments are given, every conjunction is associated to a specific assignment (the number of conjunctions\n",
        "    is the same as the given assignments); otherwise, every conjunction will be associated to a specific\n",
        "    assignment occurring in the input data (the number of conjunctions is the same as the unique assignments\n",
        "    occurring in the data).\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param scope: The scope.\n",
        "    :param assignments: The optional assignments.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    \"\"\"\n",
        "    unq_data, counts = np.unique(data, axis=0, return_counts=True)\n",
        "    assignments = unq_data if assignments is None else assignments\n",
        "    assert unq_data.shape[0] <= assignments.shape[0]\n",
        "\n",
        "    weights = np.zeros(assignments.shape[0])\n",
        "    for i in range(assignments.shape[0]):\n",
        "        index = np.where(np.all(assignments[i] == unq_data, axis=1))[0]\n",
        "        if len(index):\n",
        "            weights[i] = counts[index[0]]\n",
        "    weights = (weights + alpha) / (weights + alpha).sum()\n",
        "\n",
        "    prod_nodes = []\n",
        "    for i in range(assignments.shape[0]):\n",
        "        children = []\n",
        "        for j in range(assignments.shape[1]):\n",
        "            children.append(Bernoulli(scope=[scope[j]], p=assignments[i, j]))\n",
        "        prod_nodes.append(Product(children=children))\n",
        "\n",
        "    disjunction = Sum(children=prod_nodes, weights=weights) if len(prod_nodes) > 1 else prod_nodes[0]\n",
        "    return assign_ids(disjunction)\n",
        "\n",
        "\n",
        "def build_leaf(\n",
        "    data: np.ndarray,\n",
        "    part: Partition,\n",
        "    use_clt: bool,\n",
        "    trees_dict: dict,\n",
        "    det: bool,\n",
        "    alpha: float\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Build a multivariate leaf distribution for an XPC.\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param part: The partition associated to the leaf to build.\n",
        "    :param use_clt: True if it is possible to use CLTrees as leaf nodes, False otherwise.\n",
        "    :param trees_dict: A dictionary of trees (see the function build_trees_dict).\n",
        "    :param det: True to force determinism, False otherwise.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    \"\"\"\n",
        "    data_slice = part.get_slice(data)\n",
        "    scope = part.col_ids.tolist()\n",
        "\n",
        "    if part.is_conj:\n",
        "        leaf = Product(children=[Bernoulli(scope=[scope[k]], p=float(data_slice[0][k])) for k in range(len(scope))])\n",
        "\n",
        "    elif part.is_naive or not use_clt:\n",
        "        if not det or part.disc_assignments.shape[0] == 2 ** part.disc_assignments.shape[1]:\n",
        "            leaf = learn_mle(data_slice, [Bernoulli] * len(scope), [[0, 1]] * len(scope), scope, alpha)\n",
        "        else:\n",
        "            leaf = build_disjunction(data=data_slice, scope=scope, assignments=part.disc_assignments, alpha=alpha)\n",
        "\n",
        "    else:\n",
        "        if trees_dict is not None:\n",
        "            tree, scope = trees_dict[len(scope)]\n",
        "            data_slice = data[part.row_ids][:, scope]\n",
        "        else:\n",
        "            tree, scope = None, part.col_ids.tolist()\n",
        "        leaf = BinaryCLT(tree=tree, scope=scope)\n",
        "        leaf.fit(data_slice, domain=[[0, 1]] * len(scope), alpha=0.01)\n",
        "\n",
        "    return leaf\n",
        "\n",
        "\n",
        "def greedy_vars_ordering(\n",
        "    data: np.ndarray,\n",
        "    conj_len: int,\n",
        "    alpha: float = 0.01\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Return the ordering of the random variables according to the implemented heuristic.\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param conj_len: The conjunction length.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "\n",
        "    :return ordering: The ordering.\n",
        "    \"\"\"\n",
        "    priors, joints = estimate_priors_joints(data, alpha)\n",
        "    mut_info = compute_mutual_information(priors, joints)\n",
        "    sums = np.sum(mut_info, axis=0)\n",
        "\n",
        "    ordering = []\n",
        "    free_vars = np.arange(data.shape[1]).tolist()\n",
        "    while free_vars:\n",
        "        peek_var = free_vars[np.argmax(sums[free_vars])]\n",
        "        free_vars.remove(peek_var)\n",
        "        ordering.append(peek_var)\n",
        "        if len(free_vars) > conj_len - 1:\n",
        "            idx = np.argpartition(-mut_info[peek_var][free_vars], conj_len - 1)[:conj_len - 1]\n",
        "            vars_ = np.array(free_vars)[idx].tolist()\n",
        "        else:\n",
        "            vars_ = free_vars.copy()\n",
        "        free_vars = list(set(free_vars) - set(vars_))\n",
        "        ordering.extend(vars_)\n",
        "    return ordering\n",
        "\n",
        "\n",
        "def build_trees_dict(\n",
        "    data: np.ndarray,\n",
        "    cl_parts_l: list,\n",
        "    conj_vars_l: list,\n",
        "    alpha: float,\n",
        "    random_state: np.random.RandomState\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Return a dictionary where:\n",
        "     - a key refers to a scope length\n",
        "     - a value is a list of two lists: the first is a list of predecessors, the second its scope.\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param cl_parts_l: List of lists. Every sublist is associated to a specific XPC and contains\n",
        "     the leaf partitions over which a CLTree will be learnt.\n",
        "    :param conj_vars_l: List of lists. Every sublist contains the variables of a conjunction (e.g. [[3, 5]]).\n",
        "     If a sublist occurs before another, then the former has been used first. There are no duplicates.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    :param random_state: The random state.\n",
        "\n",
        "    :return tree_dict: The dictionary.\n",
        "    \"\"\"\n",
        "    # Compute the mutual information for each slice associated to every partition in cl_parts_l\n",
        "    # and add it to a cumulative matrix (cumulative_info).\n",
        "    n_vars = data.shape[1]\n",
        "    cumulative_info = np.zeros((n_vars, n_vars))\n",
        "    for cl_parts in cl_parts_l:\n",
        "        for part in cl_parts:\n",
        "            priors, joints = estimate_priors_joints(part.get_slice(data), alpha)\n",
        "            mi = compute_mutual_information(priors, joints)\n",
        "            cumulative_info[part.col_ids[:, None], part.col_ids] += mi\n",
        "\n",
        "    # Free_vars are the variables not involved in any conjunction and will appear at the bottom of the circuit\n",
        "    free_vars = list(set(np.arange(n_vars)) - set([var for conj_vars in conj_vars_l for var in conj_vars]))\n",
        "\n",
        "    # Create a tree for each scope in scopes\n",
        "    scopes = conj_vars_l + [free_vars] if free_vars else conj_vars_l\n",
        "    trees = []\n",
        "    for scope in scopes:\n",
        "        _, tree = maximum_spanning_tree(\n",
        "            adj_matrix=cumulative_info[scope][:, scope],\n",
        "            root=scope.index(random_state.choice(scope))\n",
        "        )\n",
        "        trees.append(list(tree))\n",
        "\n",
        "    # Concatenate trees and create the dictionary\n",
        "    # The root of every tree is added as child to the root node of the tree with the minimum higher length scope.\n",
        "    tree_dict = dict()\n",
        "    tree = trees[-1].copy()\n",
        "    scope = scopes[-1].copy()\n",
        "    for k in reversed(range(0, len(trees) - 1)):\n",
        "        tree_dict[len(scope)] = [tree.copy(), scope.copy()]\n",
        "        tree += [t + len(scope) if t != ROOT else t for t in trees[k]]\n",
        "        tree[tree.index(ROOT)] = tree.index(ROOT, len(scope))\n",
        "        scope += scopes[k]\n",
        "\n",
        "    return tree_dict\n",
        "\n",
        "\n",
        "def build_xpc(\n",
        "    data: np.ndarray,\n",
        "    part_root: Partition,\n",
        "    trees_dict: dict,\n",
        "    det: bool,\n",
        "    use_clt: bool,\n",
        "    alpha: float\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Build the XPC induced by the partitions tree in a bottom up way.\n",
        "    The building process is based on the post-order traversal exploration of the partitions tree.\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param part_root: The root partition of the tree.\n",
        "    :param trees_dict: None if no dependency tree has to be respected, a dictionary of trees otherwise.\n",
        "    :param det: True to force determinism, False otherwise.\n",
        "    :param use_clt: True to use CLTrees as leaf nodes, False otherwise.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "\n",
        "    :return: the XPC induced by the partition tree\n",
        "    \"\"\"\n",
        "    partitions_stack = [part_root]\n",
        "    pc_nodes_stack = []\n",
        "    last_part_visited = None\n",
        "\n",
        "    while partitions_stack:\n",
        "        part = partitions_stack[-1]\n",
        "        if not part.is_partitioned() or (last_part_visited in part.sub_partitions):\n",
        "            if part.is_partitioned():\n",
        "                pc_child_nodes = pc_nodes_stack[-len(part.sub_partitions):]\n",
        "                pc_nodes_stack = pc_nodes_stack[:len(pc_nodes_stack) - len(part.sub_partitions)]\n",
        "                if part.is_horizontally_partitioned():\n",
        "                    # Create sum node\n",
        "                    weights = [len(sub_part.row_ids) / len(part.row_ids) for sub_part in part.sub_partitions]\n",
        "                    pc_nodes_stack.append(Sum(weights=weights, children=pc_child_nodes))\n",
        "                else:\n",
        "                    # Create product node\n",
        "                    pc_child_nodes_ = []\n",
        "                    for c in pc_child_nodes:\n",
        "                        if isinstance(c, Product) or (isinstance(c, Sum) and len(c.children) == 1):\n",
        "                            pc_child_nodes_.extend(c.children)\n",
        "                        else:\n",
        "                            pc_child_nodes_.append(c)\n",
        "                    pc_prod_node = Product(children=pc_child_nodes_)\n",
        "                    pc_nodes_stack.append(pc_prod_node)\n",
        "            else:\n",
        "                # Create leaf (it could be either a PC or a multivariate leaf)\n",
        "                leaf = build_leaf(data, part, use_clt, trees_dict, det, alpha)\n",
        "                pc_nodes_stack.append(leaf)\n",
        "            last_part_visited = partitions_stack.pop()\n",
        "        else:\n",
        "            partitions_stack.extend(part.sub_partitions[::-1])\n",
        "\n",
        "    xpc = pc_nodes_stack[0]\n",
        "    assign_ids(xpc)\n",
        "    return xpc\n",
        "\n",
        "\n",
        "def learn_xpc(\n",
        "    data: np.ndarray,\n",
        "    det: bool,\n",
        "    sd: bool,\n",
        "    min_part_inst: int,\n",
        "    conj_len: int,\n",
        "    arity: int,\n",
        "    n_max_parts: int = 200,\n",
        "    use_clt: bool = True,\n",
        "    use_greedy_ordering: Optional[bool] = False,\n",
        "    alpha: int = 0.01,\n",
        "    random_seed: int = 42\n",
        ") -> Tuple[Node, dict]:\n",
        "    \"\"\"\n",
        "    Learn an eXtremely randomized Probabilistic Circuit (XPC).\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param det: True to force determinism, False otherwise.\n",
        "    :param sd: True to force structured decomposability, False otherwise.\n",
        "    :param min_part_inst: The minimum number of instances allowed per partition.\n",
        "    :param conj_len: The conjunction length.\n",
        "    :param arity: The maximum number of children for a sum node.\n",
        "    :param n_max_parts: The maximum number of partitions for the partitions tree.\n",
        "    :param use_clt: True to use CLTrees as multivariate leaves, False otherwise.\n",
        "    :param use_greedy_ordering: True to use a greedy ordering, False otherwise.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    :param random_seed: Random State.\n",
        "    \"\"\"\n",
        "    assert arity > 1 or arity <= 2 ** conj_len, 'Arity must be in the interval [2, 2 ** conj_len]'\n",
        "    assert sd or not use_greedy_ordering, 'Using the greedy ordering makes sense only if sd = True.'\n",
        "\n",
        "    random_state = np.random.RandomState(random_seed)\n",
        "    if use_greedy_ordering:\n",
        "        ordering = greedy_vars_ordering(data, conj_len)\n",
        "    else:\n",
        "        ordering = np.arange(data.shape[1]).tolist()\n",
        "        random_state.shuffle(ordering)\n",
        "\n",
        "    part_root, cl_parts_l, conj_vars_l, n_parts = \\\n",
        "        generate_random_partitioning(\n",
        "            data=data,\n",
        "            sd=sd,\n",
        "            min_part_inst=min_part_inst,\n",
        "            conj_len=conj_len,\n",
        "            arity=arity,\n",
        "            n_max_parts=n_max_parts,\n",
        "            uncond_vars=ordering,\n",
        "            random_state=random_state)\n",
        "    assert n_parts > 1, 'No partitioning found.'\n",
        "\n",
        "    trees_dict = None\n",
        "    if sd and use_clt:\n",
        "        trees_dict = build_trees_dict(data, [cl_parts_l], conj_vars_l, alpha, random_state)\n",
        "\n",
        "    # creating useful dictionary\n",
        "    utils = {'part_root': part_root, 'cl_parts_l': cl_parts_l, 'conj_vars_l': conj_vars_l,\n",
        "             'n_parts': n_parts, 'trees_dict': trees_dict}\n",
        "    xpc = build_xpc(data, part_root, trees_dict, det, use_clt, alpha)\n",
        "    return xpc, utils\n",
        "\n",
        "\n",
        "def learn_expc(\n",
        "    data: np.ndarray,\n",
        "    ensemble_dim: int,\n",
        "    det: bool,\n",
        "    sd_level: int,\n",
        "    min_part_inst: int,\n",
        "    conj_len: int,\n",
        "    arity: int,\n",
        "    n_max_parts: int = 200,\n",
        "    use_clt: bool = True,\n",
        "    alpha: int = 0.01,\n",
        "    random_seed: int = 42\n",
        ") -> Tuple[Node, list]:\n",
        "    \"\"\"\n",
        "    Learn an Ensemble (i.e. a mixture) of eXtremely randomized Probabilistic Circuit (EXPC).\n",
        "\n",
        "    :param data: The input data matrix.\n",
        "    :param ensemble_dim: The number of circuits in the ensemble/mixture.\n",
        "    :param det: True to force determinism, False otherwise.\n",
        "    :param sd_level: 0 a non-SD ensemble of non-SD PCs, 1 for a non-SD ensemble of SD PCs and 2 for a SD ensemble.\n",
        "    :param min_part_inst: The minimum number of instances allowed per partition.\n",
        "    :param conj_len: The conjunction length.\n",
        "    :param arity: The maximum number of children for a Sum node.\n",
        "    :param n_max_parts: The maximum number of partitions for the partitions tree.\n",
        "    :param use_clt: True to use CLTrees as multivariate leaves, False otherwise.\n",
        "    :param alpha: Laplace smoothing factor.\n",
        "    :param random_seed: A random seed.\n",
        "    \"\"\"\n",
        "    assert sd_level in SD_LEVELS, 'Choose a value in {0, 1, 2}.'\n",
        "    assert arity > 1 or arity <= 2 ** conj_len, 'Arity must be in the interval [2, 2 ** conj_len].'\n",
        "    assert not (sd_level == SD_LEVEL_2 and conj_len == 1), 'No randomness in this setting. Change hyper parameters.'\n",
        "\n",
        "    random_state = np.random.RandomState(random_seed)\n",
        "    conj_vars_l_l = [None] * ensemble_dim\n",
        "    cl_parts_l_l = [None] * ensemble_dim\n",
        "    trees_dict_l = [None] * ensemble_dim\n",
        "    part_root_l = [None] * ensemble_dim\n",
        "    n_parts_l = [None] * ensemble_dim\n",
        "    xpc_l = [None] * ensemble_dim\n",
        "\n",
        "    sd = (sd_level in [SD_LEVEL_1, SD_LEVEL_2])\n",
        "    if sd_level == SD_LEVEL_2:\n",
        "        ordering = greedy_vars_ordering(data, conj_len)\n",
        "    else:\n",
        "        ordering = np.arange(data.shape[1]).tolist()\n",
        "\n",
        "    for i in range(ensemble_dim):\n",
        "        if sd_level != SD_LEVEL_2:\n",
        "            np.random.shuffle(ordering)\n",
        "        part_root_l[i], cl_parts_l_l[i], conj_vars_l_l[i], n_parts_l[i] = \\\n",
        "            generate_random_partitioning(\n",
        "                data=data,\n",
        "                sd=sd,\n",
        "                min_part_inst=min_part_inst,\n",
        "                conj_len=conj_len,\n",
        "                arity=arity,\n",
        "                n_max_parts=n_max_parts,\n",
        "                uncond_vars=ordering,\n",
        "                random_state=random_state)\n",
        "    assert not all(n_parts == 1 for n_parts in n_parts_l), 'No Partitioning Found'\n",
        "\n",
        "    if sd_level == SD_LEVEL_0 or not use_clt:\n",
        "        # no tree structure to respect\n",
        "        trees_dict = None\n",
        "        for i in range(ensemble_dim):\n",
        "            print('Learning XPC %s/%s' % (i + 1, ensemble_dim))\n",
        "            xpc_l[i] = build_xpc(data, part_root_l[i], trees_dict, det, use_clt, alpha)\n",
        "    elif sd_level == SD_LEVEL_1:\n",
        "        for i in range(ensemble_dim):\n",
        "            print('Learning XPC %s/%s' % (i + 1, ensemble_dim))\n",
        "            # learn a tree for each XPC\n",
        "            trees_dict_l[i] = build_trees_dict(data, [cl_parts_l_l[i]], conj_vars_l_l[i], alpha, random_state)\n",
        "            xpc_l[i] = build_xpc(data, part_root_l[i], trees_dict_l[i], det, use_clt, alpha)\n",
        "    elif sd_level == SD_LEVEL_2:\n",
        "        # learn a tree structure for the whole ensemble\n",
        "        print('Learning a dependency tree for the ensemble..')\n",
        "        trees_dict = build_trees_dict(data, cl_parts_l_l, max(conj_vars_l_l, key=len), alpha, random_state)\n",
        "        trees_dict_l = [trees_dict] * ensemble_dim\n",
        "        for i in range(ensemble_dim):\n",
        "            print('Building XPC %s/%s' % (i + 1, ensemble_dim))\n",
        "            xpc_l[i] = build_xpc(data, part_root_l[i], trees_dict, det, use_clt, alpha)\n",
        "\n",
        "    # creating useful list of dictionaries\n",
        "    utils = [{'part_root': part_root_l[i], 'cl_parts_l': cl_parts_l_l[i], 'conj_vars_l': conj_vars_l_l[i],\n",
        "              'n_parts': n_parts_l[i], 'trees_dict': trees_dict_l[i]} for i in range(ensemble_dim)]\n",
        "    expc = Sum(weights=np.full(ensemble_dim, 1 / ensemble_dim), children=xpc_l)\n",
        "    assign_ids(expc)\n",
        "    return expc, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hd8tJQi-86XN"
      },
      "outputs": [],
      "source": [
        "#@title wrapped as estimator and classifier\n",
        "\n",
        "\n",
        "def learn_estimator(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: Optional[List[Union[list, tuple]]] = None,\n",
        "    method: str = 'learnspn',\n",
        "    **kwargs\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn a SPN density estimator given some training data, the features distributions and domains.\n",
        "\n",
        "    :param data: The training data.\n",
        "    :param distributions: A list of distribution classes (one for each feature).\n",
        "    :param domains: A list of domains (one for each feature). Each domain is either a list of values, for discrete\n",
        "                    distributions, or a tuple (consisting of min value and max value), for continuous distributions.\n",
        "                    If None, domains are determined automatically.\n",
        "    :param method: The method used for structure learning. It can be either 'learnspn', 'xpc' or 'ensemble-xpc'.\n",
        "    :param kwargs: Additional parameters for structure learning.\n",
        "    :return: A learned valid and optimized SPN.\n",
        "    :raises ValueError: If the method used for structure learning is not known.\n",
        "    :raises ValueError: If the method is 'xpc' or 'ensemble-xpc' but the variable domains are not binary.\n",
        "    \"\"\"\n",
        "    if domains is None:\n",
        "        domains = compute_data_domains(data, distributions)\n",
        "\n",
        "    if method == 'learnspn':\n",
        "        root = learn_spn(data, distributions, domains, **kwargs)\n",
        "        return prune(root, copy=False)\n",
        "    if method == 'xpc':\n",
        "        if not all(d == [0, 1] for d in domains):\n",
        "            raise ValueError(\"The domains must be binary for learning a XPC\")\n",
        "        root, _ = learn_xpc(data, **kwargs)\n",
        "        return root\n",
        "    if method == 'ensemble-xpc':\n",
        "        if not all(d == [0, 1] for d in domains):\n",
        "            raise ValueError(\"The domains must be binary for learning an Ensemble-XPC\")\n",
        "        root, _ = learn_expc(data, **kwargs)\n",
        "        return root\n",
        "    raise ValueError(\"Unknown SPN learning method called {}\".format(method))\n",
        "\n",
        "\n",
        "def learn_classifier(\n",
        "    data: np.ndarray,\n",
        "    distributions: List[Type[Leaf]],\n",
        "    domains: Optional[List[Union[list, tuple]]] = None,\n",
        "    class_idx: int = -1,\n",
        "    verbose: bool = True,\n",
        "    **kwargs\n",
        ") -> Node:\n",
        "    \"\"\"\n",
        "    Learn a SPN classifier given some training data, the features distributions and domains and\n",
        "    the class index in the training data.\n",
        "\n",
        "    :param data: The training data.\n",
        "    :param distributions: A list of distribution classes (one for each feature).\n",
        "    :param domains: A list of domains (one for each feature). Each domain is either a list of values, for discrete\n",
        "                    distributions, or a tuple (consisting of min value and max value), for continuous distributions.\n",
        "                    If None, domains are determined automatically.\n",
        "    :param class_idx: The index of the class feature in the training data.\n",
        "    :param verbose: Whether to enable verbose mode.\n",
        "    :param kwargs: Other parameters for structure learning.\n",
        "    :return: A learned valid and optimized SPN.\n",
        "    \"\"\"\n",
        "    if domains is None:\n",
        "        domains = compute_data_domains(data, distributions)\n",
        "\n",
        "    n_samples, _ = data.shape\n",
        "    classes = data[:, class_idx]\n",
        "\n",
        "    # Initialize the tqdm wrapped unique classes array, if verbose is enabled\n",
        "    unique_classes = np.unique(classes)\n",
        "    if verbose:\n",
        "        unique_classes = tqdm(unique_classes, bar_format='{l_bar}{bar:24}{r_bar}', unit='class')\n",
        "\n",
        "    # Learn each sub-spn's structure individually\n",
        "    weights = []\n",
        "    children = []\n",
        "    for c in unique_classes:\n",
        "        local_data = data[classes == c]\n",
        "        weight = len(local_data) / n_samples\n",
        "        branch = learn_spn(local_data, distributions, domains, verbose=verbose, **kwargs)\n",
        "        weights.append(weight)\n",
        "        children.append(prune(branch, copy=False))\n",
        "\n",
        "    root = Sum(children=children, weights=weights)\n",
        "    return assign_ids(root)\n",
        "\n",
        "\n",
        "def compute_data_domains(data: np.ndarray, distributions: List[Type[Leaf]]) -> List[Union[list, tuple]]:\n",
        "    \"\"\"\n",
        "    Compute the domains based on the training data and the features distributions.\n",
        "\n",
        "    :param data: The training data.\n",
        "    :param distributions: A list of distribution classes.\n",
        "    :return: A list of domains. Each domain is either a list of values, for discrete distributions, or\n",
        "             a tuple (consisting of min value and max value), for continuous distributions.\n",
        "    :raises ValueError: If an unknown distribution type is found.\n",
        "    \"\"\"\n",
        "    domains = []\n",
        "    for i, d in enumerate(distributions):\n",
        "        col = data[:, i]\n",
        "        if d.LEAF_TYPE == LeafType.DISCRETE:\n",
        "            vals = np.unique(col).tolist()\n",
        "            domains.append(vals)\n",
        "        elif d.LEAF_TYPE == LeafType.CONTINUOUS:\n",
        "            vmin = np.min(col).item()\n",
        "            vmax = np.max(col).item()\n",
        "            domains.append((vmin, vmax))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown distribution type {}\".format(d.LEAF_TYPE))\n",
        "    return domains"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGIFg9HcPWWA"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FDE_r6FCPc6g"
      },
      "outputs": [],
      "source": [
        "#@title vanilla spn\n",
        "\n",
        "class SPNEstimator(BaseEstimator, DensityMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        distributions: List[Type[Leaf]],\n",
        "        domains: Optional[List[Union[list, tuple]]] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Scikit-learn density estimator model for Sum Product Networks (SPNs).\n",
        "\n",
        "        :param distributions: A list of distribution classes (one for each feature).\n",
        "        :param domains: A list of domains (one for each feature).\n",
        "        :param kwargs: Additional arguments to pass to the SPN learner.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.distributions = distributions\n",
        "        self.domains = domains\n",
        "        self.kwargs = kwargs\n",
        "        self.spn_ = None\n",
        "        self.n_features_ = 0\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: Optional[np.ndarray] = None):\n",
        "        \"\"\"\n",
        "        Fit the SPN density estimator.\n",
        "\n",
        "        :param X: The training data.\n",
        "        :param y: Ignored, only for scikit-learn API convention.\n",
        "        :return: Itself.\n",
        "        \"\"\"\n",
        "        self.spn_ = learn_estimator(X, self.distributions, self.domains, **self.kwargs)\n",
        "        self.n_features_ = X.shape[1]\n",
        "        return self\n",
        "\n",
        "    def predict_log_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict using the SPN density estimator, i.e. compute the log-likelihood.\n",
        "\n",
        "        :param X: The inputs. They can be marginalized using NaNs.\n",
        "        :return: The log-likelihood of the inputs.\n",
        "        \"\"\"\n",
        "        return log_likelihood(self.spn_, X)\n",
        "\n",
        "    def mpe(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the un-observed variable by maximum at posterior estimation (MPE).\n",
        "\n",
        "        :param X: The inputs having some NaN values.\n",
        "        :return: The MPE assignment to un-observed variables.\n",
        "        \"\"\"\n",
        "        return mpe(self.spn_, X, inplace=False)\n",
        "\n",
        "    def sample(self, n: Optional[int] = None, X: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample from the modeled distribution.\n",
        "\n",
        "        :param n: The number of samples. It must be None if X is not None. If None, n=1 is assumed.\n",
        "        :param X: Data used for conditional sampling. It can be None for full sampling.\n",
        "        :return: The samples.\n",
        "        :raise ValueError: If both parameters 'n' and 'X' are passed by.\n",
        "        \"\"\"\n",
        "        if n is not None and X is not None:\n",
        "            raise ValueError(\"Only one between 'n' and 'X' can be specified\")\n",
        "\n",
        "        if X is not None:\n",
        "            # Conditional sampling\n",
        "            return sample(self.spn_, X, inplace=False)\n",
        "        else:\n",
        "            # Full sampling\n",
        "            n = 1 if n is None else n\n",
        "            x = np.tile(np.nan, [n, self.n_features_])\n",
        "            return sample(self.spn_, x, inplace=True)\n",
        "\n",
        "    def score(self, X: np.ndarray, y: Optional[np.ndarray] = None) -> dict:\n",
        "        \"\"\"\n",
        "        Return the mean log-likelihood and two standard deviations on the given test data.\n",
        "\n",
        "        :param X: The inputs. They can be marginalized using NaNs.\n",
        "        :param y: Ignored. Specified only for scikit-learn API compatibility.\n",
        "        :return: A dictionary consisting of two keys \"mean_ll\" and \"stddev_ll\",\n",
        "                 representing respectively the mean log-likelihood and two standard deviations.\n",
        "        \"\"\"\n",
        "        ll = self.predict_log_proba(X)\n",
        "        mean_ll = np.mean(ll)\n",
        "        stddev_ll = np.std(ll)\n",
        "        return {\n",
        "            'mean_ll': mean_ll,\n",
        "            'stddev_ll': 2.0 * stddev_ll / np.sqrt(len(X))\n",
        "        }\n",
        "\n",
        "\n",
        "class SPNClassifier(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(\n",
        "        self,\n",
        "        distributions: List[Type[Leaf]],\n",
        "        domains: Optional[List[Union[list, tuple]]] = None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Scikit-learn classifier model for Sum Product Networks (SPNs).\n",
        "\n",
        "        :param distributions: A list of distribution classes (one for each feature).\n",
        "        :param domains: A list of domains (one for each feature).\n",
        "        :param kwargs: Additional arguments to pass to the SPN learner.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.distributions = distributions\n",
        "        self.domains = domains\n",
        "        self.kwargs = kwargs\n",
        "        self.spn_ = None\n",
        "        self.n_features_ = 0\n",
        "        self.n_classes_ = 0\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the SPN density estimator.\n",
        "\n",
        "        :param X: The training data.\n",
        "        :param y: The data labels.\n",
        "        :return: Itself.\n",
        "        \"\"\"\n",
        "        # Build the training data, consisting of labels\n",
        "        y = np.expand_dims(y, axis=1)\n",
        "        data = np.hstack([X, y])\n",
        "\n",
        "        # Constructs the list of distributions\n",
        "        n_classes = len(np.unique(y))\n",
        "        if n_classes == 2:\n",
        "            # Use bernoulli for binary classification\n",
        "            distributions = self.distributions + [Bernoulli]\n",
        "        else:\n",
        "            # otherwise, use a categorical distribution\n",
        "            distributions = self.distributions + [Categorical]\n",
        "\n",
        "        self.spn_ = learn_classifier(data, distributions, self.domains, **self.kwargs)\n",
        "        self.n_features_ = X.shape[1]\n",
        "        self.n_classes_ = n_classes\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict using the SPN classifier.\n",
        "\n",
        "        :param X: The inputs. They can be marginalized using NaNs.\n",
        "        :return: The predicted classes.\n",
        "        \"\"\"\n",
        "        # Build the testing data, having X as features assignments and NaNs for labels\n",
        "        data = np.hstack([X, np.full([len(X), 1], np.nan)])\n",
        "\n",
        "        # Make classification using maximum probable explanation (MPE)\n",
        "        mpe(self.spn_, data, inplace=True)\n",
        "\n",
        "        # Return the classifications for each sample\n",
        "        return data[:, -1]\n",
        "\n",
        "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict using the SPN classifier, using probabilities.\n",
        "\n",
        "        :param X: The inputs. They can be marginalized using NaNs.\n",
        "        :return: The prediction probabilities for each class.\n",
        "        \"\"\"\n",
        "        return np.exp(self.predict_log_proba(X))\n",
        "\n",
        "    def predict_log_proba(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict using the SPN classifier, using log-probabilities.\n",
        "\n",
        "        :param X: The inputs. They can be marginalized using NaNs.\n",
        "        :return: The prediction log-probabilities for each class.\n",
        "        \"\"\"\n",
        "        # Build the testing data, having X as features assignments and NaNs for labels\n",
        "        data = np.hstack([X, np.tile(np.nan, [len(X), 1])])\n",
        "\n",
        "        # Make probabilistic classification by computing the log-likelihoods at sub-class SPN\n",
        "        _, lls = log_likelihood(self.spn_, data, return_results=True)\n",
        "\n",
        "        # Collect the predicted class probabilities\n",
        "        class_ids = [c.id for c in self.spn_.children]\n",
        "        class_ll = np.log(self.spn_.weights) + lls[class_ids]\n",
        "        return log_softmax(class_ll, axis=1)\n",
        "\n",
        "    def sample(self, n: Optional[int] = None, y: Optional[np.ndarray] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample from the modeled conditional distribution.\n",
        "\n",
        "        :param n: The number of samples. It must be None if y is not None. If None, n=1 is assumed.\n",
        "        :param y: Labels used for conditional sampling. It can be None for un-conditional sampling.\n",
        "        :return: The samples.\n",
        "        \"\"\"\n",
        "        if n is not None and y is not None:\n",
        "            raise ValueError(\"Only one between 'n' and 'y' can be specified\")\n",
        "\n",
        "        # Conditional sampling\n",
        "        if y is not None:\n",
        "            y = np.expand_dims(y, axis=1)\n",
        "            x = np.hstack([np.tile(np.nan, [len(y), self.n_features_]), y])\n",
        "            return sample(self.spn_, x, inplace=False)\n",
        "\n",
        "        # Full sampling\n",
        "        n = 1 if n is None else n\n",
        "        x = np.tile(np.nan, [n, self.n_features_ + 1])\n",
        "        return sample(self.spn_, x, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUWPaxLYPfDi"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "YvrvAb7XPfDk"
      },
      "outputs": [],
      "source": [
        "#@title naive spn\n",
        "\n",
        "# Sample some binary data randomly\n",
        "np.random.seed(42)\n",
        "n_samples, n_features = 1000, 10\n",
        "data = np.random.binomial(1, p=0.4, size=[n_samples, n_features])\n",
        "\n",
        "# Set the features distributions and domains\n",
        "distributions = [Bernoulli] * n_features\n",
        "domains = [[0, 1]] * n_features  # Use lists to specify discrete domains\n",
        "\n",
        "# Learn a naive factorized model from a subset of the data\n",
        "scope = [5, 1, 7]\n",
        "dists = [distributions[s] for s in scope]\n",
        "doms = [domains[s] for s in scope]\n",
        "naive = learn_naive_factorization(\n",
        "    data[:, scope], dists, doms, scope,\n",
        "    learn_leaf_func=learn_mle,  # Use MLE to learn the leaf distributions\n",
        "    alpha=0.01  # Additional learn_mle parameters, for example the Laplace smoothing factor\n",
        ")\n",
        "\n",
        "# Compute the average likelihood\n",
        "    ## ls is same as: scipy.stats.bernoulli.pmf(data[~mask], naive.children[child_id].p) than multiply scope columns (variables) on each other\n",
        "    ## reason of using thi is that is \"uses ones like zeros\"\n",
        "    ## look for it in Bernoulli class (P.S. pmf is same for discrete as pdf for continuous)\n",
        "\n",
        "ls = likelihood(naive, data)\n",
        "print(\"Average Likelihood: {:.4f}\".format(np.mean(ls)))\n",
        "\n",
        "# Print some statistics about the model's structure and parameters\n",
        "print(\"SPN structure and parameters statistics:\")\n",
        "print(compute_statistics(naive))\n",
        "\n",
        "len(data), data.shape, naive.children[0].p, (naive.children[0].p * naive.children[1].p * naive.children[2].p), set(ls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZAwlt_NJelE"
      },
      "outputs": [],
      "source": [
        "#@title snp classifier (from latent) part 1\n",
        "\n",
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Setup the MNIST datasets\n",
        "n_classes = 10\n",
        "n_features = (image_c, image_h, image_w) = (1, 28, 28)\n",
        "n_dimensions = np.prod(n_features).item()\n",
        "transform = transforms.ToTensor()\n",
        "data_train = datasets.MNIST('datasets', train=True, transform=transform, download=True)\n",
        "data_test = datasets.MNIST('datasets', train=False, transform=transform, download=True)\n",
        "\n",
        "# Build the autoencoder for features extraction\n",
        "latent_dim = 24  # Use 24 features as latent space\n",
        "encoder = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(n_dimensions, 512), nn.ReLU(inplace=True),\n",
        "    nn.Linear(512, 256), nn.ReLU(inplace=True),\n",
        "    nn.Linear(256, latent_dim), nn.Tanhshrink(),\n",
        ")\n",
        "decoder = nn.Sequential(\n",
        "    nn.Linear(latent_dim, 256), nn.ReLU(inplace=True),\n",
        "    nn.Linear(256, 512), nn.ReLU(inplace=True),\n",
        "    nn.Linear(512, 784), nn.Sigmoid(),\n",
        "    nn.Unflatten(1, n_features)\n",
        ")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "autoencoder = nn.Sequential(encoder, decoder).to(device)\n",
        "\n",
        "# Train the autoencoder, by minimizing the reconstruction binary cross-entropy\n",
        "epochs = 25\n",
        "batch_size = 100\n",
        "lr = 1e-3\n",
        "train_loader = data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
        "criterion = nn.BCELoss()\n",
        "tk_epochs = tqdm(range(epochs), bar_format='{l_bar}{bar:24}{r_bar}', unit='epoch')\n",
        "for epoch in tk_epochs:\n",
        "    train_loss = 0.0\n",
        "    for (inputs, _) in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = autoencoder(inputs)\n",
        "        loss = criterion(outputs, inputs)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "    train_loss /= len(train_loader)\n",
        "    tk_epochs.set_description('Train Loss: {}'.format(round(train_loss, 4)))\n",
        "\n",
        "# Compute the (train data) latent space features using the encoder\n",
        "train_loader = data.DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
        "x_train = np.empty([len(data_train), latent_dim], dtype=np.float32)\n",
        "y_train = np.empty(len(data_train), dtype=np.int64)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = encoder(inputs).cpu()\n",
        "        x_train[i * batch_size:i * batch_size + batch_size] = outputs.numpy()\n",
        "        y_train[i * batch_size:i * batch_size + batch_size] = targets.numpy()\n",
        "\n",
        "# Compute the (test data) latent space features using the encoder\n",
        "test_loader = data.DataLoader(data_test, batch_size=batch_size, shuffle=False)\n",
        "x_test = np.empty([len(data_test), latent_dim], dtype=np.float32)\n",
        "y_test = np.empty(len(data_test), dtype=np.int64)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = encoder(inputs).cpu()\n",
        "        x_test[i * batch_size:i * batch_size + batch_size] = outputs.numpy()\n",
        "        y_test[i * batch_size:i * batch_size + batch_size] = targets.numpy()\n",
        "\n",
        "# Preprocess the datasets using standardization\n",
        "transform = DataStandardizer()\n",
        "transform.fit(x_train)\n",
        "x_train = transform.forward(x_train)\n",
        "x_test = transform.forward(x_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xguRvmBSbSc"
      },
      "outputs": [],
      "source": [
        "#@title snp classifier (from latent) part 2\n",
        "\n",
        "# Learn the SPN structure and parameters, as a classifier\n",
        "# Note that we consider the train data as features + targets\n",
        "distributions = [Gaussian] * latent_dim + [Categorical]\n",
        "data_train = np.column_stack([x_train, y_train])\n",
        "root = learn_classifier(\n",
        "    data_train,\n",
        "    distributions,\n",
        "    learn_leaf='mle',     # Learn leaf distributions by MLE\n",
        "    split_rows='kmeans',  # Use K-Means for splitting rows\n",
        "    split_cols='rdc',     # Use RDC for splitting columns\n",
        "    min_rows_slice=200,   # The minimum number of rows required to split furthermore\n",
        "    split_rows_kwargs={'n': 2},   # Use n=2 number of clusters for K-Means\n",
        "    split_cols_kwargs={'d': 0.3}  # Use d=0.3 as threshold for RDC independence test\n",
        ")\n",
        "\n",
        "# Print some statistics about the model's structure and parameters\n",
        "print(\"SPN structure and parameters statistics:\")\n",
        "print(compute_statistics(root))\n",
        "\n",
        "\n",
        "# Make some predictions on the test set classes\n",
        "# This is done by running a Maximum Probable Explanation (MPE) query\n",
        "nan_classes = np.full([len(x_test), 1], np.nan)\n",
        "data_test = np.column_stack([x_test, nan_classes])\n",
        "mpe(root, data_test, inplace=True)   ## for top down (from leaves to classes) leaf mask selection https://stackoverflow.com/questions/62505046/what-does-numpy-ix-function-do-and-what-is-the-output-used-for\n",
        "y_pred = data_test[:, -1]\n",
        "\n",
        "# Plot a classification report\n",
        "print(\"Classification Report:\")\n",
        "print(sk.metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "# Sample some examples for each class\n",
        "# This is done by conditional sampling w.r.t. the example classes\n",
        "n_samples = 10\n",
        "nan_features = np.full([n_samples * n_classes, latent_dim], np.nan) ## 100 X 24 nans\n",
        "classes = np.tile(np.arange(n_classes), [1, n_samples]).T           ## 100 X 1 (np.repeat repeates individual elements np.tile repeates the array)\n",
        "samples = np.column_stack([nan_features, classes])                  ## 100 X 25 (24 nans and 1 for clas 0-9. In 100 rows we have each class 10 times)\n",
        "sample(root, samples, inplace=True)                                 ## It's first bottomup and than topdown evaluation (look into sampling part)\n",
        "                                ## for single top leaf we just directly sample from scipy.stats.cauchy.rvs()\n",
        "                                ## In Product node we just combine node with each its children by union (overlap is considered just once)\n",
        "                                  #Note: node is above child below and we are going down)\n",
        "                                ## for sampling for Sum of nodes we add randoms (scipy.stats.gumbel_l.rvs()) and add log(weights) to children nodes probabilities and tale argmax\n",
        "                                  #than we just combine Sum node with just one of its children by union (overlap is considered just once)\n",
        "                                  #that just one child is the one to which corresponds the argmax of Sum.\n",
        "features = samples[:, :-1]\n",
        "\n",
        "# Apply the inverse preprocessing transformation\n",
        "# Then apply the features extractor's decoder and plot the examples on a grid\n",
        "with torch.no_grad():\n",
        "    images = transform.backward(features)\n",
        "    inputs = torch.tensor(images, dtype=torch.float32, device=device)\n",
        "    data_images = decoder(inputs).cpu()\n",
        "    samples_filename = 'spn-latent-mnist-samples.png'\n",
        "    print(\"Plotting generated samples to {} ...\".format(samples_filename))\n",
        "    torchvision.utils.save_image(data_images, samples_filename, nrow=n_samples, padding=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4i0Yvv5amHTg",
        "aygrJr8V8S4r",
        "GQQ1EKZj8sZ6",
        "NBuM5qIX86XK",
        "KGIFg9HcPWWA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
