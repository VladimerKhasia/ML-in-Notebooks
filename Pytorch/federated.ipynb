{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "15jyFDn3fORy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime \n",
        "import time\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Normal\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "mp.set_sharing_strategy('file_system')\n",
        "# ## mp.set_start_method('spawn')        \n",
        "# ## For Colab : https://colab.research.google.com/github/pnavaro/python-notebooks/blob/master/notebooks/10-Multiprocessing.ipynb#scrollTo=a8yyJ5xMi6lU\n",
        "# ##             https://stackoverflow.com/questions/61939952/mp-set-start-methodspawn-triggered-an-error-saying-the-context-is-already-be\n",
        "# try:\n",
        "#    mp.set_start_method('spawn', force=True)\n",
        "#    print(\"spawned\")\n",
        "# except RuntimeError:\n",
        "#    pass\n",
        "# torch.set_num_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pGN46b1SNNlw"
      },
      "outputs": [],
      "source": [
        "#@title FedLearners\n",
        "\n",
        "\n",
        "def train_handler(args, net, net_id, dataidxs, reduction = \"mean\"):\n",
        "    train_dataloader, _ = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs)\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=args.lr, momentum=args.rho, weight_decay=args.reg)    \n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=args.lr, weight_decay=args.reg)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction=reduction).to(args.device)\n",
        "    return train_dataloader, optimizer, criterion \n",
        "\n",
        "\n",
        "def update_global(args, networks, selected, freqs):\n",
        "    if args.arch == \"cnn\":\n",
        "        global_para = networks[\"global_model\"].state_dict()\n",
        "        for idx, net_id  in enumerate(selected):\n",
        "            net_para = networks[\"nets\"][net_id].cpu().state_dict()\n",
        "            if idx == 0:\n",
        "                for key in net_para:\n",
        "                    global_para[key] = net_para[key] * freqs[idx]\n",
        "            else:\n",
        "                for key in net_para:\n",
        "                    global_para[key] += net_para[key] * freqs[idx]\n",
        "        networks[\"global_model\"].load_state_dict(global_para)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Wrong arch!\")\n",
        "\n",
        "\n",
        "class BaseAlgorithm():\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        raise NotImplementedError() \n",
        "    \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        raise NotImplementedError()  \n",
        "\n",
        "\n",
        "class FED(BaseAlgorithm):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        net.to(\"cpu\")\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "        \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        fed_freqs = [1 / len(selected) for r in selected]\n",
        "        update_global(args, networks, selected, fed_freqs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Bayesian Federated Learning algorithm\"\n",
        "\n",
        "\n",
        "class FEDAvg(FED):     \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        total_data_points = sum([len(net_dataidx_map[r]) for r in selected])\n",
        "        fed_avg_freqs = [len(net_dataidx_map[r]) / total_data_points for r in selected]\n",
        "        update_global(args, networks, selected, fed_avg_freqs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with FedAVG global update rule\"\n",
        "\n",
        "\n",
        "class FEDProx(FEDAvg):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        net.to(args.device)\n",
        "        global_net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        global_weight_collector = list(global_net.to(args.device).parameters())\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                fed_prox_reg = 0.0\n",
        "                for param_index, param in enumerate(net.parameters()):\n",
        "                    fed_prox_reg += ((args.mu / 2) * torch.norm((param - global_weight_collector[param_index]))**2)\n",
        "                loss += fed_prox_reg\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        net.to(\"cpu\")\n",
        "        global_net.to(\"cpu\")\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with FedProx global update rule\"\n",
        "\n",
        "\n",
        "class FEDNova(BaseAlgorithm):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        net.to(\"cpu\")\n",
        "        tau = len(train_dataloader) * args.epochs\n",
        "        a_i = (tau - args.rho * (1 - pow(args.rho, tau)) / (1 - args.rho)) / (1 - args.rho)\n",
        "        global_net_para = global_net.state_dict()\n",
        "        net_para = net.state_dict()\n",
        "        norm_grad = deepcopy(global_net.state_dict())\n",
        "        for key in norm_grad:\n",
        "            norm_grad[key] = torch.true_divide(global_net_para[key]-net_para[key], a_i)\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "        torch.save(norm_grad, f\"{args.logdir}/clients/norm_grad_{net_id}.pt\")\n",
        "\n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        total_data_points = sum([len(net_dataidx_map[r]) for r in selected])\n",
        "        freqs = [len(net_dataidx_map[r]) / total_data_points for r in selected]\n",
        "        norm_grad_total = deepcopy(networks[\"global_model\"].state_dict())\n",
        "        for key in norm_grad_total:\n",
        "            norm_grad_total[key] = 0.0\n",
        "        for i in enumerate(selected):\n",
        "            norm_grad = torch.load(f\"{args.logdir}/clients/norm_grad_{r}.pt\")\n",
        "            for key in norm_grad_total:\n",
        "                norm_grad_total[key] += norm_grad[key] * freqs[i]\n",
        "        coeff = 0.0\n",
        "        for i, r in enumerate(selected):\n",
        "            tau = math.ceil(len(net_dataidx_map[r])/args.batch_size) * args.epochs\n",
        "            a_i = (tau - args.rho * (1 - pow(args.rho, tau)) / (1 - args.rho)) / (1 - args.rho)\n",
        "            coeff = coeff + a_i * freqs[i]\n",
        "        global_para = networks[\"global_model\"].state_dict()\n",
        "        for key in global_para:\n",
        "            if global_para[key].type() == 'torch.LongTensor':\n",
        "                global_para[key] -= (coeff * norm_grad_total[key]).type(torch.LongTensor)\n",
        "            elif global_para[key].type() == 'torch.cuda.LongTensor':\n",
        "                global_para[key] -= (coeff * norm_grad_total[key]).type(torch.cuda.LongTensor)\n",
        "            else:\n",
        "                global_para[key] -= coeff * norm_grad_total[key]\n",
        "        networks[\"global_model\"].load_state_dict(global_para)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with FedNOVA global update rule\"\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
