{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "15jyFDn3fORy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime \n",
        "import time\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Normal\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "mp.set_sharing_strategy('file_system')\n",
        "# ## mp.set_start_method('spawn')        \n",
        "# ## For Colab : https://colab.research.google.com/github/pnavaro/python-notebooks/blob/master/notebooks/10-Multiprocessing.ipynb#scrollTo=a8yyJ5xMi6lU\n",
        "# ##             https://stackoverflow.com/questions/61939952/mp-set-start-methodspawn-triggered-an-error-saying-the-context-is-already-be\n",
        "# try:\n",
        "#    mp.set_start_method('spawn', force=True)\n",
        "#    print(\"spawned\")\n",
        "# except RuntimeError:\n",
        "#    pass\n",
        "# torch.set_num_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pGN46b1SNNlw"
      },
      "outputs": [],
      "source": [
        "#@title FedLearners\n",
        "\n",
        "\n",
        "def train_handler(args, net, net_id, dataidxs, reduction = \"mean\"):\n",
        "    train_dataloader, _ = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs)\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=args.lr, momentum=args.rho, weight_decay=args.reg)    \n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=args.lr, weight_decay=args.reg)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction=reduction).to(args.device)\n",
        "    return train_dataloader, optimizer, criterion \n",
        "\n",
        "\n",
        "def update_global(args, networks, selected, freqs):\n",
        "    if args.arch == \"cnn\":\n",
        "        global_para = networks[\"global_model\"].state_dict()\n",
        "        for idx, net_id  in enumerate(selected):\n",
        "            net_para = networks[\"nets\"][net_id].cpu().state_dict()\n",
        "            if idx == 0:\n",
        "                for key in net_para:\n",
        "                    global_para[key] = net_para[key] * freqs[idx]\n",
        "            else:\n",
        "                for key in net_para:\n",
        "                    global_para[key] += net_para[key] * freqs[idx]\n",
        "        networks[\"global_model\"].load_state_dict(global_para)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Wrong arch!\")\n",
        "\n",
        "\n",
        "class BaseAlgorithm():\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        raise NotImplementedError() \n",
        "    \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        raise NotImplementedError()  \n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
