{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "15jyFDn3fORy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime \n",
        "import time\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Normal\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "mp.set_sharing_strategy('file_system')\n",
        "# ## mp.set_start_method('spawn')        \n",
        "# ## For Colab : https://colab.research.google.com/github/pnavaro/python-notebooks/blob/master/notebooks/10-Multiprocessing.ipynb#scrollTo=a8yyJ5xMi6lU\n",
        "# ##             https://stackoverflow.com/questions/61939952/mp-set-start-methodspawn-triggered-an-error-saying-the-context-is-already-be\n",
        "# try:\n",
        "#    mp.set_start_method('spawn', force=True)\n",
        "#    print(\"spawned\")\n",
        "# except RuntimeError:\n",
        "#    pass\n",
        "# torch.set_num_threads(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Handle The Data and Produce Non-IID Partitions\n",
        "\n",
        "\n",
        "class CIFAR10_Class(data.Dataset):\n",
        "    def __init__(self, root, dataidxs=None, train=True, transform=None, target_transform=None, download=False):\n",
        "        self.root = root\n",
        "        self.dataidxs = dataidxs\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.download = download\n",
        "        self.data, self.target = self.__build_truncated_dataset__()\n",
        "\n",
        "    def __build_truncated_dataset__(self):\n",
        "        if os.path.isfile(os.path.join(self.root, 'cifar-10/padded_train_data.pt')) == False:\n",
        "            save_data_cifar10(self.root)\n",
        "        if self.train:\n",
        "            data = torch.load(os.path.join(self.root, 'cifar-10/padded_train_data.pt'))\n",
        "            target = np.load(os.path.join(self.root, 'cifar-10/train_target.npy'))\n",
        "        else:\n",
        "            data = torch.load(os.path.join(self.root, 'cifar-10/test_data.pt'))\n",
        "            target = np.load(os.path.join(self.root, 'cifar-10/test_target.npy'))\n",
        "        if self.dataidxs is not None:\n",
        "            data = data[self.dataidxs]\n",
        "            target = target[self.dataidxs]\n",
        "        return data, target\n",
        "\n",
        "    def truncate_channel(self, index):\n",
        "        for i in range(index.shape[0]):\n",
        "            gs_index = index[i]\n",
        "            self.data[gs_index, :, :, 1] = 0.0\n",
        "            self.data[gs_index, :, :, 2] = 0.0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = self.data[index], self.target[index]\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "\n",
        "def get_dataloader(dataset, datadir, train_bs, test_bs, dataidxs=None):\n",
        "    if dataset == 'cifar10':\n",
        "        dl_obj = CIFAR10_Class\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomCrop(32),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        transform_test = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        " \n",
        "    ### elif ........... add here\n",
        "\n",
        "    train_ds = dl_obj(datadir, dataidxs=dataidxs, train=True, transform=transform_train, download=True)\n",
        "    test_ds = dl_obj(datadir, train=False, transform=transform_test, download=True)\n",
        "\n",
        "    train_dl = data.DataLoader(dataset=train_ds, batch_size=train_bs, shuffle=True, drop_last=False, num_workers=0)\n",
        "    test_dl = data.DataLoader(dataset=test_ds, batch_size=test_bs, shuffle=False, drop_last=False, num_workers=0)\n",
        "    return train_dl, test_dl\n",
        "\n",
        "def partition_data(args):\n",
        "    if args.dataset == 'cifar10':\n",
        "        y_train = CIFAR10_Class(args.datadir, train=True, download=True, transform=transforms.Compose([transforms.ToTensor()])).target\n",
        "        K = 10\n",
        "    ### elif ............... add here\n",
        "    else:\n",
        "        assert NotImplementedError(f\"{args.dataset} is not available!\")\n",
        "\n",
        "    N = y_train.shape[0]\n",
        "\n",
        "    if args.partition == \"iid_10_clients\":\n",
        "        idxs = np.random.permutation(N)\n",
        "        batch_idxs = np.array_split(idxs, args.n_parties)\n",
        "        net_dataidx_map = {i: batch_idxs[i] for i in range(args.n_parties)}\n",
        "\n",
        "    elif args.partition == \"noniid_10_clients\":\n",
        "        min_size = 0\n",
        "        min_require_size = 10\n",
        "        net_dataidx_map = {}\n",
        "        \n",
        "        while min_size < min_require_size:\n",
        "            idx_batch = [[] for _ in range(args.n_parties)]\n",
        "            for k in range(K):\n",
        "                idx_k = np.where(y_train == k)[0]\n",
        "                np.random.shuffle(idx_k)\n",
        "                proportions = np.random.dirichlet(np.repeat(args.beta, args.n_parties))\n",
        "                proportions = np.array([p * (len(idx_j) < N / args.n_parties) for p, idx_j in zip(proportions, idx_batch)])\n",
        "                proportions = proportions / proportions.sum()\n",
        "                proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
        "                idx_batch = [idx_j + idx.tolist() for idx_j, idx in zip(idx_batch, np.split(idx_k, proportions))]\n",
        "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
        "\n",
        "        for j in range(args.n_parties):\n",
        "            np.random.shuffle(idx_batch[j])\n",
        "            net_dataidx_map[j] = idx_batch[j]\n",
        "    \n",
        "    elif args.partition > \"noniid_cause_labels_to:0\" and args.partition <= \"noniid_cause_labels_to:9\":\n",
        "        num = int(args.partition[23:])\n",
        "\n",
        "        times=[0 for i in range(K)]\n",
        "        contain=[]\n",
        "        for i in range(args.n_parties):\n",
        "            current=[i%K]\n",
        "            times[i%K]+=1\n",
        "            j=1\n",
        "            while (j<num):\n",
        "                ind=random.randint(0,K-1)\n",
        "                if (ind not in current):\n",
        "                    j=j+1\n",
        "                    current.append(ind)\n",
        "                    times[ind]+=1\n",
        "            contain.append(current)\n",
        "        net_dataidx_map ={i:np.ndarray(0,dtype=np.int64) for i in range(args.n_parties)}\n",
        "        \n",
        "        for i in range(K):\n",
        "            idx_k = np.where(y_train==i)[0]\n",
        "            np.random.shuffle(idx_k)\n",
        "            split = np.array_split(idx_k,times[i])\n",
        "            ids=0\n",
        "            for j in range(args.n_parties):\n",
        "                if i in contain[j]:\n",
        "                    net_dataidx_map[j]=np.append(net_dataidx_map[j],split[ids])\n",
        "                    ids+=1\n",
        "\n",
        "    elif args.partition in [\"noniid_cause_quantity\"]:\n",
        "        idxs = np.random.permutation(N)\n",
        "        min_size = 0\n",
        "        while min_size < 10:\n",
        "            proportions = np.random.dirichlet(np.repeat(args.beta, args.n_parties))\n",
        "            proportions = proportions/proportions.sum()\n",
        "            min_size = np.min(proportions*len(idxs))\n",
        "        proportions = (np.cumsum(proportions)*len(idxs)).astype(int)[:-1]\n",
        "        batch_idxs = np.split(idxs,proportions)\n",
        "        net_dataidx_map = {i: batch_idxs[i] for i in range(args.n_parties)}\n",
        "        \n",
        "    net_cls_counts = {}\n",
        "\n",
        "    for net_i, dataidx in net_dataidx_map.items():\n",
        "        unq, unq_cnt = np.unique(y_train[dataidx], return_counts=True)\n",
        "        tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
        "        net_cls_counts[net_i] = tmp\n",
        "\n",
        "    print(f'Data statistics: {net_cls_counts}')\n",
        "    return net_dataidx_map\n",
        "\n",
        "def data_handler(args):\n",
        "    print(\"Partitioning data\")\n",
        "    net_dataidx_map = partition_data(args)\n",
        "    _, test_dl_global = get_dataloader(args.dataset, args.datadir, args.batch_size, 32)                                                   \n",
        "    return test_dl_global, net_dataidx_map\n",
        "\n",
        "###-------------------------- Save ---------------------------------\n",
        "\n",
        "def save_data_cifar10(datadir):\n",
        "    os.makedirs(os.path.join(datadir, 'cifar-10'), exist_ok=True)\n",
        "    transform_train = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Lambda(lambda x: F.pad(\n",
        "                        Variable(x.unsqueeze(0), requires_grad=False),\n",
        "                        (4, 4, 4, 4), mode='reflect').data.squeeze())\n",
        "                ])\n",
        "    cifar_train_dataobj = CIFAR10(datadir, True, None, None, True)\n",
        "    train_data = cifar_train_dataobj.data\n",
        "    train_target = np.array(cifar_train_dataobj.targets)\n",
        "    padded_train_data = torch.zeros((train_data.shape[0], 3, 40, 40))\n",
        "    for i, x in enumerate(train_data):\n",
        "        padded_train_data[i] = transform_train(x)\n",
        "    torch.save(padded_train_data, os.path.join(datadir, 'cifar-10/padded_train_data.pt'))\n",
        "    np.save(os.path.join(datadir, 'cifar-10/train_target.npy'), train_target)\n",
        "\n",
        "    cifar_test_dataobj = CIFAR10(datadir, False, None, None)\n",
        "    test_data = cifar_test_dataobj.data\n",
        "    test_target = np.array(cifar_test_dataobj.targets)\n",
        "    torch.save(test_data, os.path.join(datadir, 'cifar-10/test_data.pt'))\n",
        "    np.save(os.path.join(datadir, 'cifar-10/test_target.npy'), test_target)\n",
        "\n",
        "\n",
        "save_data_cifar10(\"./data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Helper Snippets\n",
        "\n",
        "\n",
        "###----------------------------- model snippets ------------------------------\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dims, output_dim=10, input_channel=3):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channel, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], output_dim)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, self.input_dim)\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def init_nets(args):\n",
        "    nets = {net_i: None for net_i in range(args.n_parties)}\n",
        "    if args.dataset == 'cifar10':\n",
        "        input_channel = 3\n",
        "        input_dim = (16 * 5 * 5)\n",
        "        hidden_dims=[120, 84]\n",
        "        output_dim = 10\n",
        "    ### elif ...... add here if needed\n",
        "\n",
        "    for net_i in range(args.n_parties):        \n",
        "        if args.arch.lower() == \"cnn\":\n",
        "            net = CNN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, input_channel=input_channel)\n",
        "        ### elif .... add here if needed\n",
        "        else:\n",
        "            raise ValueError(\"Unknown architecture: {}\".format(args.arch))\n",
        "        nets[net_i] = net\n",
        "\n",
        "\n",
        "    if args.arch.lower() == \"cnn\":\n",
        "        global_net = CNN(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim, input_channel=input_channel)\n",
        "    ### elif .... add here if needed\n",
        "    else:\n",
        "        raise ValueError(\"Unknown architecture: {}\".format(args.arch))\n",
        "\n",
        "    if args.is_same_initial:\n",
        "        global_para = global_net.state_dict() \n",
        "        for net_id, net in nets.items():\n",
        "            net.load_state_dict(global_para)\n",
        "\n",
        "    return global_net, nets\n",
        "\n",
        "\n",
        "### --------------------------- Scorrer Snippets -------------------------------\n",
        "\n",
        "## Expected Calibration Error (ECE) Naeini et al., 2015 \n",
        "   ##--> approximate difference in expectation between confidence and accuracy of machine learning models\n",
        "def ece(preds, target, device, minibatch=True):\n",
        "    confidences, predictions = torch.max(preds, 1)\n",
        "    _, target_cls = torch.max(target, 1)\n",
        "    accuracies = predictions.eq(target_cls)\n",
        "    n_bins = 100 \n",
        "    bin_boundaries = torch.linspace(0, 1, n_bins + 1)\n",
        "    bin_lowers = bin_boundaries[:-1]\n",
        "    bin_uppers = bin_boundaries[1:]\n",
        "\n",
        "    ece = torch.zeros(1, device=device)\n",
        "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
        "        in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())\n",
        "        prop_in_bin = in_bin.float().mean()\n",
        "        if prop_in_bin.item() > 0:\n",
        "            accuracy_in_bin = accuracies[in_bin].float().mean()\n",
        "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
        "            ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin * 100\n",
        "\n",
        "    return ece.item()\n",
        "\n",
        "## Negative log-likelihood (NLL) Quinonero-Candela et al., 2005 --> metric for evaluating predictive uncertainty\n",
        "def nll(preds, target, minibatch=True):\n",
        "    logpred = torch.log(preds + 1e-8)\n",
        "    if minibatch:\n",
        "        return -(logpred * target).sum(1).item()\n",
        "    else:\n",
        "        return -(logpred * target).sum(1).mean().item()\n",
        "\n",
        "## Accuracy \n",
        "def acc(preds, target, minibatch=True):\n",
        "    preds = preds.argmax(1)\n",
        "    target = target.argmax(1)\n",
        "    if minibatch:\n",
        "        return (((preds == target) * 1.0).sum() * 100).item()\n",
        "    else:\n",
        "        return (((preds == target) * 1.0).mean() * 100).item()\n",
        "\n",
        "def compute_scores(model, dataloader, args, device=\"cpu\", n_sample=1):\n",
        "    was_training = False\n",
        "    if model.training:\n",
        "        model.eval()\n",
        "        was_training = True\n",
        "\n",
        "    if type(dataloader) == type([1]):\n",
        "        pass\n",
        "    else:\n",
        "        dataloader = [dataloader]\n",
        "        \n",
        "    preds = []\n",
        "    targets = []\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        for tmp in dataloader:\n",
        "            for batch_idx, (x, target) in enumerate(tmp):\n",
        "                x, target = x.to(device), target.to(device,dtype=torch.int64)\n",
        "                \n",
        "                outs = []\n",
        "                for _ in range(n_sample):\n",
        "                    out = model(x)\n",
        "                    out = F.softmax(out, 1)\n",
        "                    outs.append(out)\n",
        "\n",
        "                preds.append(torch.stack(outs).mean(0))\n",
        "                targets.append(F.one_hot(target, model.output_dim))\n",
        "\n",
        "    targets = torch.cat(targets)\n",
        "    preds = torch.cat(preds)\n",
        "\n",
        "    _acc = acc(preds, targets, minibatch=False)\n",
        "    _ece = ece(preds, targets, device, minibatch=False)\n",
        "    _nll = nll(preds, targets, minibatch=False)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "    return _acc, _ece, _nll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pGN46b1SNNlw"
      },
      "outputs": [],
      "source": [
        "#@title FedLearners\n",
        "\n",
        "\n",
        "def train_handler(args, net, net_id, dataidxs, reduction = \"mean\"):\n",
        "    train_dataloader, _ = get_dataloader(args.dataset, args.datadir, args.batch_size, 32, dataidxs)\n",
        "    if args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=args.lr, momentum=args.rho, weight_decay=args.reg)    \n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=args.lr, weight_decay=args.reg)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss(reduction=reduction).to(args.device)\n",
        "    return train_dataloader, optimizer, criterion \n",
        "\n",
        "\n",
        "def update_global(args, networks, selected, freqs):\n",
        "    if args.arch == \"cnn\":\n",
        "        global_para = networks[\"global_model\"].state_dict()\n",
        "        for idx, net_id  in enumerate(selected):\n",
        "            net_para = networks[\"nets\"][net_id].cpu().state_dict()\n",
        "            if idx == 0:\n",
        "                for key in net_para:\n",
        "                    global_para[key] = net_para[key] * freqs[idx]\n",
        "            else:\n",
        "                for key in net_para:\n",
        "                    global_para[key] += net_para[key] * freqs[idx]\n",
        "        networks[\"global_model\"].load_state_dict(global_para)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Wrong arch!\")\n",
        "\n",
        "\n",
        "class BaseAlgorithm():\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        raise NotImplementedError() \n",
        "    \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        raise NotImplementedError()  \n",
        "\n",
        "\n",
        "class FED(BaseAlgorithm):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        net.to(\"cpu\")\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "        \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        fed_freqs = [1 / len(selected) for r in selected]\n",
        "        update_global(args, networks, selected, fed_freqs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Bayesian Federated Learning algorithm\"\n",
        "\n",
        "\n",
        "class FEDAvg(FED):     \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        total_data_points = sum([len(net_dataidx_map[r]) for r in selected])\n",
        "        fed_avg_freqs = [len(net_dataidx_map[r]) / total_data_points for r in selected]\n",
        "        update_global(args, networks, selected, fed_avg_freqs)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with FedAVG global update rule\"\n",
        "\n",
        "\n",
        "class FEDProx(FEDAvg):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        net.to(args.device)\n",
        "        global_net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        global_weight_collector = list(global_net.to(args.device).parameters())\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                fed_prox_reg = 0.0\n",
        "                for param_index, param in enumerate(net.parameters()):\n",
        "                    fed_prox_reg += ((args.mu / 2) * torch.norm((param - global_weight_collector[param_index]))**2)\n",
        "                loss += fed_prox_reg\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        net.to(\"cpu\")\n",
        "        global_net.to(\"cpu\")\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with FedProx global update rule\"\n",
        "\n",
        "\n",
        "class FEDNova(BaseAlgorithm):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        net.to(\"cpu\")\n",
        "        tau = len(train_dataloader) * args.epochs\n",
        "        a_i = (tau - args.rho * (1 - pow(args.rho, tau)) / (1 - args.rho)) / (1 - args.rho)\n",
        "        global_net_para = global_net.state_dict()\n",
        "        net_para = net.state_dict()\n",
        "        norm_grad = deepcopy(global_net.state_dict())\n",
        "        for key in norm_grad:\n",
        "            norm_grad[key] = torch.true_divide(global_net_para[key]-net_para[key], a_i)\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "        torch.save(norm_grad, f\"{args.logdir}/clients/norm_grad_{net_id}.pt\")\n",
        "\n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        total_data_points = sum([len(net_dataidx_map[r]) for r in selected])\n",
        "        freqs = [len(net_dataidx_map[r]) / total_data_points for r in selected]\n",
        "        norm_grad_total = deepcopy(networks[\"global_model\"].state_dict())\n",
        "        for key in norm_grad_total:\n",
        "            norm_grad_total[key] = 0.0\n",
        "        for i in enumerate(selected):\n",
        "            norm_grad = torch.load(f\"{args.logdir}/clients/norm_grad_{r}.pt\")\n",
        "            for key in norm_grad_total:\n",
        "                norm_grad_total[key] += norm_grad[key] * freqs[i]\n",
        "        coeff = 0.0\n",
        "        for i, r in enumerate(selected):\n",
        "            tau = math.ceil(len(net_dataidx_map[r])/args.batch_size) * args.epochs\n",
        "            a_i = (tau - args.rho * (1 - pow(args.rho, tau)) / (1 - args.rho)) / (1 - args.rho)\n",
        "            coeff = coeff + a_i * freqs[i]\n",
        "        global_para = networks[\"global_model\"].state_dict()\n",
        "        for key in global_para:\n",
        "            if global_para[key].type() == 'torch.LongTensor':\n",
        "                global_para[key] -= (coeff * norm_grad_total[key]).type(torch.LongTensor)\n",
        "            elif global_para[key].type() == 'torch.cuda.LongTensor':\n",
        "                global_para[key] -= (coeff * norm_grad_total[key]).type(torch.cuda.LongTensor)\n",
        "            else:\n",
        "                global_para[key] -= coeff * norm_grad_total[key]\n",
        "        networks[\"global_model\"].load_state_dict(global_para)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with FedNOVA global update rule\"\n",
        "\n",
        "\n",
        "class Scaffold(BaseAlgorithm):\n",
        "    def local_update(self, args, net, global_net, net_id, dataidxs):\n",
        "        c_global_para = torch.load(f\"{args.logdir}/clients/c_global.pt\", map_location=args.device)\n",
        "        c_local_para = torch.load(f\"{args.logdir}/clients/c_{net_id}.pt\", map_location=args.device)\n",
        "        net.to(args.device)\n",
        "        global_net.to(args.device)\n",
        "        train_dataloader, optimizer, criterion = train_handler(args, net, net_id, dataidxs)\n",
        "        cnt = 0\n",
        "        for epoch in range(args.epochs):\n",
        "            for x, target in train_dataloader:\n",
        "                x, target = x.to(args.device), target.to(args.device)\n",
        "                optimizer.zero_grad()\n",
        "                x.requires_grad = True\n",
        "                target.requires_grad = False\n",
        "                target = target.long()\n",
        "                out = net(x)\n",
        "                loss = criterion(out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                net_para = net.state_dict()\n",
        "                for key in net_para:\n",
        "                    net_para[key] = net_para[key] - args.lr * (c_global_para[key] - c_local_para[key])\n",
        "                net.load_state_dict(net_para)\n",
        "                cnt += 1\n",
        "        net.to(\"cpu\")\n",
        "        c_new_para = torch.load(f\"{args.logdir}/clients/c_{net_id}.pt\")\n",
        "        c_delta_para = torch.load(f\"{args.logdir}/clients/c_{net_id}.pt\")\n",
        "        c_global_para = torch.load(f\"{args.logdir}/clients/c_global.pt\")\n",
        "        c_local_para = torch.load(f\"{args.logdir}/clients/c_{net_id}.pt\")\n",
        "        global_model_para = global_net.state_dict()\n",
        "        net_para = net.state_dict()\n",
        "        for key in net_para:\n",
        "            c_new_para[key] = c_new_para[key] - c_global_para[key] + (global_model_para[key] - net_para[key]) / (cnt * args.lr)\n",
        "            c_delta_para[key] = c_new_para[key] - c_local_para[key]\n",
        "        torch.save(net.state_dict(), f\"{args.logdir}/clients/client_{net_id}.pt\")\n",
        "        torch.save(c_new_para, f\"{args.logdir}/clients/c_{net_id}.pt\")\n",
        "        torch.save(c_delta_para, f\"{args.logdir}/clients/c_delta_{net_id}.pt\")\n",
        "        \n",
        "    def global_update(self, args, networks, selected, net_dataidx_map):\n",
        "        total_delta = deepcopy(networks[\"global_model\"].state_dict())\n",
        "        for key in total_delta:\n",
        "            total_delta[key] = 0.0\n",
        "        for r in selected:\n",
        "            c_delta_para = torch.load(f\"{args.logdir}/clients/c_delta_{r}.pt\")\n",
        "            for key in total_delta:\n",
        "                total_delta[key] += c_delta_para[key] / len(selected)\n",
        "        c_global_para = torch.load(f\"{args.logdir}/clients/c_global.pt\")\n",
        "        for key in c_global_para:\n",
        "            if c_global_para[key].type() == 'torch.LongTensor':\n",
        "                c_global_para[key] += total_delta[key].type(torch.LongTensor)\n",
        "            elif c_global_para[key].type() == 'torch.cuda.LongTensor':\n",
        "                c_global_para[key] += total_delta[key].type(torch.cuda.LongTensor)\n",
        "            else:\n",
        "                c_global_para[key] += total_delta[key]\n",
        "        torch.save(c_global_para, f\"{args.logdir}/clients/c_global.pt\")\n",
        "        total_data_points = sum([len(net_dataidx_map[r]) for r in selected])\n",
        "        fed_avg_freqs = [len(net_dataidx_map[r]) / total_data_points for r in selected]\n",
        "        global_para = networks[\"global_model\"].state_dict()\n",
        "        for i, r in enumerate(selected):\n",
        "            net_para = networks[\"nets\"][r].cpu().state_dict()\n",
        "            if i == 0:\n",
        "                for key in net_para:\n",
        "                    global_para[key] = net_para[key] * fed_avg_freqs[i]\n",
        "            else:\n",
        "                for key in net_para:\n",
        "                    global_para[key] += net_para[key] * fed_avg_freqs[i]\n",
        "        networks[\"global_model\"].load_state_dict(global_para)\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"Federated Learning algorithm with Scaffold global update rule\"\n",
        "\n",
        "\n",
        "def get_algorithm(args):\n",
        "    if args.alg.lower() == \"fed\":\n",
        "        return FED\n",
        "    elif args.alg.lower() == \"fedavg\":\n",
        "        return FEDAvg\n",
        "    elif args.alg.lower() == \"fedprox\":\n",
        "        return FEDProx\n",
        "    elif args.alg.lower() == \"fednova\":\n",
        "        return FEDNova\n",
        "    elif args.alg.lower() == \"scaffold\":\n",
        "        return Scaffold\n",
        "    else:\n",
        "        raise NotImplementedError(f\"{args.alg} is not implemented!\") "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
